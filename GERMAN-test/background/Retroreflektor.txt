Retroreflektor

Ein Retroreflektor (von rückwärts, "reflexio" Zurückbeugung) ist eine Vorrichtung, die einfallende elektromagnetische Wellen weitgehend unabhängig von der Einfallsrichtung sowie der Ausrichtung des Reflektors großteils in die Richtung reflektiert, aus der sie gekommen sind. Dies wird als Retroreflexion bezeichnet.

Diffus streuende Oberflächen strahlen nur wenig Licht zur Lichtquelle zurück. Sie erscheinen dennoch meist heller als ein Spiegel, denn bei einem Planspiegel hängt die Rückstrahlung von seiner Orientierung ab, die nur in Ausnahmefällen senkrecht zum Betrachter ausgerichtet ist. Aus diesem Grund erscheint auch eine regennasse Fahrbahn, die nur von den eigenen Fahrzeugscheinwerfern beleuchtet wird, in der Nacht dunkler als die diffus rückstreuende Oberfläche eines trockenen Straßenbelags.

In der Funktechnik unterstützen Radarreflektoren die Ortung angestrahlter Objekte, etwa einer Ballonsonde oder eines Brückenpfeilers an einer Wasserstraße.

Ein Tripelprisma aus geschliffenem Glas dient bei der Vermessung einem Laser als Umkehrpunkt. An einer Reflexlichtschranke wird eine retroreflektierende Folie oder ein Rückstrahler verwendet, um den Strahl wieder zum Sensor unmittelbar neben der Lichtquelle zurückzulenken.

Im Straßenverkehr erhöhen Retroreflektoren an Menschen, Hindernissen, Verkehrszeichen, Leiteinrichtungen und Fahrzeugen deren Erkennbarkeit nachts im Scheinwerferlicht. Die Augen des Fahrers sind in der Regel etwa 1 m vom Scheinwerfer ihres Fahrzeuges entfernt. Da ein idealer Retroreflektor das Licht theoretisch exakt in Einfallsrichtung zurückwerfen würde, haben Retroreflektoren für den Straßenverkehr eine kleine Winkelstreuung von etwa 1°. Retroreflektoren haben einen Grenzwinkel der Retroreflexion gegenüber ihrer Flächennormalen, ab dem sie nicht mehr reflektieren. Die Parameter "Winkelstreuung" und "Grenzwinkel" von Retroreflektoren dienen zu ihrer Charakterisierung.

In weißer oder gefärbter Ausführung gibt es Rückstrahler aus transparenten Kunststoffspritzguss- oder Glasplatten, die vorn glatt sind, hinten eine Struktur aus vielen Würfelecken haben. Sie sind manchmal auch hinten verspiegelt, unverspiegelt wird jedoch auch eine Retroreflexion erreicht. Auch diese Elemente werden oft als Katzenaugen bezeichnet.

Retroreflektorfolien sind aus geprägter Alufolie, rückseitig geprägter Plastikfolie oder enthalten transparente, retroreflektierende Kügelchen.

Katzenaugen bestehen aus gefassten Glaskörpern (bikonvex, silberverspiegelt und schutzlackiert).

Sind Retroreflektoren durch Tautropfen oder Raureif beschlagen, kann dies die Retroreflexion durch Streuung mindern oder ganz verhindern; Autobahnwegweiser zeigen in solchen Fällen große dunkle Flecken.

Manche Fahrradscheinwerfer haben Retroreflexzone integriert.
Retroreflektierende Materialien sollten bei Sicherheitsanwendungen mit diffus reflektierenden Oberflächen kombiniert werden, damit sie auch dann erkennbar sind, wenn sie mit Fremdlicht aus anderen Richtungen bestrahlt werden. Aus diesem Grund tragen Leitpfosten nicht nur Rückstrahler, sondern sind auch weiß gefärbt. Auch Aluminium, häufig stranggepresst und eloxiert, als Verkehrstafelmasten oder Geländer wirkt hell, solange es nicht durch Salz stark korrodiert ist.

Bei Fahrbahnmarkierungen werden Glaskugeln ("d" 

1/f²-Rauschen

1/f²-Rauschen (auch „Brownsches-“, „Brown-“ oder „rotes Rauschen“ genannt) bezeichnet ein Rauschen, bei dem sich die Leistungsdichte umgekehrt proportional zum Quadrat der Frequenz (~ 1/f²) verhält. Die Rauschleistungsdichte sinkt also um einen Faktor vier oder 6 dB, wenn sich die Frequenz verdoppelt (Oktave) bzw. um 20 dB pro Dekade.
Das ähnliche 1/f-Rauschen weist dagegen einen Abfall der Rauschleistungsdichte von 3 dB je Oktave bzw. 10 dB pro Dekade auf.

Die Bezeichnungen "Brown" und "Brownsches" für 1/f²-Rauschen beziehen sich auf den schottischen Botaniker und Namensgeber der Brownschen Molekularbewegung, Robert Brown, nicht auf die Farbe „braun“ (). Die Brownsche Molekularbewegung entspricht zum Beispiel einem 1/f²-Rauschen. Trotzdem ist, da auch andere Arten von Rauschen mit Farben bezeichnet werden („weißes Rauschen“ oder 1/f-Rauschen als „rosa Rauschen“), auch die Bezeichnung braunes Rauschen verbreitet.

Die Brownsche Molekularbewegung kann als stochastischer Prozess formula_1 im Rahmen des Wiener-Prozess als das Integral vom weißen Rauschen formula_2 beschrieben werden:

Weißes Rauschen weist eine konstante Leistungsdichte auf:

mit der Fouriertransformation formula_5. Eine Eigenschaft der Fouriertransformation ist, dass sich die auftretende Ableitung als Produkt ausdrücken lässt als:

mit formula_7 als die imaginäre Einheit und der Kreisfrequenz formula_8.

Daraus ergibt sich der Betrag des Leistungsdichtespektrums für 1/f²-Rauschen aus dem konstanten Betragsleistungsdichtespektrum formula_9 für weißes Rauschen zu:

Anschaulich kann 1/f²-Rauschen durch Filterung von weißem Rauschen mit einem Tiefpassfilter zweiter Ordnung mit einer Grenzfrequenz von 0 Hz erzeugt werden.

"1/f²-Rauschen" kann auch hörbar gemacht werden, allerdings ist der Frequenzanteil durch den starken Abfall des Leistungsdichtespektrums von 20 dB pro Dekade auf niederfrequente Signalanteile beschränkt, so dass primär für den Menschen nicht oder nur schwer wahrnehmbarer Infraschall auftritt.

"1/f²-Rauschen" kann visualisiert werden, indem eine diskrete zweidimensionale komplexe Funktion mit bihyperbolisch abfallender Amplitude und zufälliger Phase invers fourier-transformiert wird. Der Betrag der komplexwertigen Fourier-Rücktransformierten kann sowohl einfarbig (Graustufen) als auch getrennt für die drei Farbkanäle als RGB-Signal ausgegeben werden.

"1/f²-Rauschen" kann theoretisch hörbar gemacht werden, indem eine diskrete eindimensionale komplexe Funktion mit einer mit bihyperbolisch abfallenden Amplitude und zufälliger Phase invers fourier-transformiert wird. Allerdings ist der Frequenzanteil auf sehr niederfrequente Signale beschränkt, so dass der Infraschall für den Menschen nicht hörbar ist.

Der Begriff Rotes Rauschen wurde mit einer vergleichbaren Farbanalogie wie die Begriffe Weißes Rauschen und Rosa Rauschen gebildet. Da im Leistungsdichtespektrum von Rotem Rauschen die niedrigeren Frequenzen noch stärker dominieren als beim Rosa Rauschen, entspricht der daraus – im übertragenen Sinne – entstehende Farbeindruck etwas, das röter ist, als rosa.





Zweizustandssystem

Ein Zwei-Zustands-System oder auch Zwei-Niveau-System in der Quantenmechanik ist ein einfaches, aber wichtiges Modellsystem, das zur Beschreibung vieler Situationen herangezogen werden kann. Das System kann sich nur in einem von zwei möglichen Zuständen, formula_1 und formula_2 benannt, oder in einer Superposition dieser zwei Zustände befinden (Bra-Ket-Notation). Diese zwei Zustände haben dabei üblicherweise unterschiedliche Energien formula_3 und formula_4. Ein Beispiel ist etwa ein an ein Atom gebundenes Elektron, das eines von zwei Niveaus des Atomspektrums besetzen kann (Grundzustand, angeregter Zustand, siehe Abbildung rechts). Oft wird auch das Modellsystem eines quantenmechanischen Spins-1/2 (Drehimpulses) benutzt, der sich nur in zwei Einstellungen befinden kann. Zwischen den Niveaus existiert ein Übergang (z. B. ein optischer Übergang, der durch sichtbares Licht angeregt werden kann). Befindet sich das System einmal in einem der beiden Zustände, so bleibt es für immer dort, zumindest solange man das System nicht stört. Wird eine Störung in dem System eingeschaltet, so kann man beobachten, dass die Zustände ineinander übergehen können:

Befindet sich z. B. ein Elektron im Zustand formula_1 (der energetisch niedriger liege als formula_2), so kann es durch einen resonant eingestrahlten Laser-Puls in den Zustand formula_2 übergehen. Ein Elektron im Zustand formula_2 kann durch Emission eines Photons, das die Differenzenergie formula_9 zwischen den Zuständen trägt, in den Zustand formula_1 zurückfallen. Die nebenstehende Abbildung zeigt das schematisch.

Liegt die Störung längere Zeit an, so oszilliert die Wahrscheinlichkeit, das Atom in einem der Zustände zu finden. Nach einer halben Oszillationsdauer ist die Wahrscheinlichkeit hoch, das Atom im angeregten Zustand vorzufinden, nach einer ganzen Dauer ist es höchstwahrscheinlich wieder im Grundzustand usw. Dieses Phänomen entspricht den Rabi-Oszillationen.

Zum gegebenen System gehört ein Hamiltonoperator formula_11. Die Zustände formula_12 sind Eigenzustände dieses Hamiltonians zu den Eigenwerten formula_13:

Wird zusätzlich zu formula_11 eine hermitesche Störung formula_16 eingeschaltet, so sind formula_12 nicht mehr die Eigenzustände des neuen Hamiltonians formula_18. Die neuen Eigenzustände seien mit formula_19 und die neuen Eigenenergien mit formula_20 bezeichnet. Man erhält in der formula_21-Basis folgende Darstellung für formula_22:


Wird das System zum Zeitpunkt formula_34 im Eigenzustand formula_1 präpariert, so bleibt es für alle Zeiten in diesem Zustand. Wird nun aber die Störung formula_16 (mit nichtverschwindenden Nebendiagonal-Elementen) zugeschaltet, so ist die Wahrscheinlichkeit formula_37, das System zum Zeitpunkt "t" im Zustand formula_2 zu finden, nicht mehr 0. Dies ist im Wesentlichen darauf zurückzuführen, dass die Zustände formula_1 und formula_2 keine Eigenzustände des Systems mehr sind. Aus der etwas umfangreichen Rechnung erhält man:

Diese Oszillationen zwischen den Zuständen, wie sie auch die nebenstehende Abbildung zeigt, werden auch als Rabioszillationen bzw. als Rabiflops bezeichnet.







Friedrich Rinne

Friedrich Wilhelm Berthold Rinne (* 16. März 1863 in Osterode am Harz; † 12. März 1933 in Freiburg im Breisgau) war ein deutscher Mineraloge, Kristallograph und Petrograph. Er gilt als Begründer der Salzpetrographie.

Nach dem Besuch des Gymnasiums in Rinteln studierte Friedrich Rinne an der Georg-August-Universität Göttingen, wo er 1883 promoviert wurde. Von 1883 bis 1886 war er Assistent am Mineralogisch-Petrographischen Institut der Universität Göttingen. 1885 wurde er Privatdozent für Mineralogie und Petrographie. Von 1887 bis 1894 war er Assistent am Mineralogisch-Petrographischen Institut der Friedrich-Wilhelms-Universität Berlin. Danach arbeitete als Hochschullehrer: 1894 als Professor an der Technischen Hochschule Hannover, 1904 an der Universität Gießen, 1908 an der Universität Kiel, 1908 an der Universität Königsberg sowie 1909 an der Universität Leipzig. 1909 wurde das Mineral "Rinneit" (auch "Rinneita") nach ihm benannt. 1928 wurde er Professor an der Albert-Ludwigs-Universität Freiburg. Im Ersten Weltkrieg war Rinne von 1917 bis 1918 Soldat.








GEOS (NASA)

GEOS (Geodetic Earth Orbiting Satellite) war eine Serie von drei US-amerikanischen geodätischen Forschungssatelliten der NASA.

Die ersten beiden GEOS-Satelliten (gestartet 1965 und 1968) wurden unter dem Explorer-Programm entwickelt. Sie verfügten über eine Lagestabilisierung mittels Schweregradienten und eine Energieversorgung über Solarzellen. Die GEOS-Satelliten waren die ersten Missionen im National Geodetic Satellite Program.

Die Ausrüstung bestand aus einer Anzahl von optischen und Funk-Systemen (darunter auch jeweils einen Transponder für das SECOR-System) zur exakten Bahnverfolgung sowie Laser-Reflektoren. Als "aktive Satelliten" waren sie eine Weiterentwicklung der ANNA 1B-Sonde und konnten kurze Serien von sechs bis acht hellen Lichtblitzen aussenden. Diese wurden von Bodenstationen synchron vor dem Sternhintergrund fotografiert (Stellartriangulation), um durch genaue Richtungsmessung großräumige Vermessungsnetze aufzubauen.

Ziel der Missionen war die Vermessung von ausgewählten Beobachtungspunkten mit einer Genauigkeit von zehn Metern in einem dreidimensionalen Koordinatensystem mit dem Masseschwerpunkt der Erde als Mittelpunkt. Weiterhin wurde die Struktur des irregulären Erdgravitationsfelds sowie die Lage der großen Gravitationsanomalien bestimmt.

Der GEOS-3-Satellit (auf Grund der erweiterten Aufgaben auch Geodynamics Experimental Ocean Satellite) war eine erweiterte Version der beiden vorangegangenen Satelliten. Dieser Satellit war nicht mehr Bestandteil des Explorer-Programms. Zusätzlich zur Aufgabenstellung der Vorgänger war durch ein Radar-Altimeter eine präzise Messung der Ozeanhöhen möglich, was eine weitere Verfeinerung des Gravitationsmodells der Erde ermöglichte. Zusätzlich wurde ein Bahnverfolgungsexperiment mit dem experimentellen Kommunikationssatelliten ATS 6 durchgeführt. Eine verbesserte Version des GEOS-3-Satelliten wurde von der US Navy unter der Bezeichnung Geosat gestartet.






Detached-Eddy Simulation

Die Detached-Eddy Simulation (DES) ist ein Verfahren zur Berechnung von turbulenten Strömungen mit Hilfe von Computern. Da Turbulenz sowohl räumlich als auch zeitlich auf sehr unterschiedlichen und vor allem auch sehr kleinen Skalen stattfindet, werden zur korrekten Auflösung aller Phänomene extrem feine Rechengitter und Zeitschritte benötigt (Direkte Numerische Simulation). In der Praxis sind diese Bedingungen oft durch begrenzte Rechenleistung nicht zu erfüllen. Turbulenzmodelle wie DES dienen der Reduktion des Rechenaufwandes.

Bei DES handelt es sich um eine Kombination der gängigen Large Eddy (LES) und Reynolds Averaged Navier Stokes (RANS) Modelle, wobei je nach Gitterauflösung und Abstand von Wänden dynamisch zwischen beiden Modellen gewechselt wird. Die Motivation hierzu liegt im geringeren Rechenaufwand von RANS und der höheren Genauigkeit von LES. Wänden, das heißt Oberflächen an denen die Strömungsgeschwindigkeit durch die Haftbedingung null wird, kommt eine besondere Rolle zu, da hier sehr dünne Grenzschichten auftreten die einen großen Einfluss auf die turbulente Strömung ausüben.

Die Detached Eddy Simulation (DES) wurde erstmals 1997 von Philippe Spalart veröffentlicht. Sie basiert in ihrer ursprünglichen Form auf dem Turbulenzmodell von Spalart-Allmaras (eine Transportgleichung), es wird aber auch an der Anwendung in Verbindung mit anderen Modellen geforscht.

Die DES ersetzt den Wandabstand, der als Variable im Spalart-Allmaras-Modell vorkommt, in wandfernen Bereichen durch die größte Weite einer Gitterzelle. Durch diese Formulierung lässt sich in den wandfernen Bereichen ein LES-ähnliches Verhalten der Rechnung erreichen. De facto erhält man so also eine RANS-Formulierung in der Grenzschicht an Wänden und eine LES Formulierung in der freien Strömung, also das im jeweiligen Bereich am besten geeignete Verfahren (bez. Genauigkeit und Rechenaufwand).

Da RANS und LES unterschiedliche Anforderungen an das Gitter stellen, hat das Erstellen eines geeigneten, in entsprechende Zonen unterteilten Gitters einen großen Einfluss auf den Erfolg der Rechnung. Dasselbe gilt für die verwendeten numerischen Methoden. Diese sind aber meist gezwungenermaßen im gesamten Rechengebiet dieselben, was teilweise zu Kompromissen bezüglich der Genauigkeit führt.





Staupunkt

Als Staupunkt (engl. "stagnation point") wird der Punkt auf der Oberfläche eines angeströmten Körpers (Profil) bezeichnet, an dem das strömende Fluid theoretisch senkrecht auftrifft. Die Geschwindigkeit der Strömung verschwindet im Staupunkt, so dass die kinetische Energie (im idealisierten Fall vollständig) in Druckenergie umgewandelt wird. Der Druck im Staupunkt ist daher, verglichen mit anderen Stellen auf der Körperoberfläche, am größten und wird als Gesamtdruck bezeichnet. Diejenige Stromlinie, die als einzige im Staupunkt endet, wird meist als "Staupunktsstromlinie" bezeichnet.

Bei Tragflächen befindet sich der Staupunkt an der Vorderkante des Profils. Mit Änderung des Anstellwinkels ändert sich auch die Lage des Staupunktes: mit Erhöhung des Anstellwinkel wandert der Staupunkt ein wenig auf die Tragflächenunterseite (vgl. Abbildung). Folglich muss die anströmende Luft, die unmittelbar oberhalb des Staupunktes auf die Vorderkante trifft, eine kurze Strecke entgegengesetzt der anströmenden Luft zurücklegen, um die Profiloberseite zu erreichen.





Röntgen-Nahkanten-Absorptions-Spektroskopie

Die Röntgen-Nahkanten-Absorptions-Spektroskopie, englisch oder , kurz NEXAFS- oder XANES-Spektroskopie, ist ein zur Röntgenabsorptionsspektroskopie gehörendes Spektroskopie-Verfahren zur Untersuchung von Festkörperoberflächen. Es erfasst die unbesetzten Elektronen-Zustände und kann die Häufigkeit sowie die räumliche Lage von Atomen oder Molekülen auf der Oberfläche untersuchen.

Bei NEXAFS werden mit Röntgenstrahlung kernnahe, stark gebundene Elektronen in einen unbesetzten Zustand im Valenzband oder in ein unbesetztes Atom- oder Molekülorbital angehoben.

Grundlage dieser Technik ist das Auftreten von Röntgen-Absorptions-Kanten: Ein Röntgen-Quant kann nur dann ein Elektron aus einem stark gebundenen Zustand (d. h. kernnahen Orbital) herausschlagen, wenn die Energie ausreicht, damit das Elektron einen unbesetzten Zustand (oder das Kontinuum) erreicht. Erhöht man die Energie der Röntgenstrahlung von einem Wert, bei dem das (noch) nicht möglich ist, zu einem Wert, bei dem so ein Prozess möglich ist, steigt schlagartig die Röntgenabsorption eines Materials. Sobald die Energie hoch genug ist, damit das Elektron das Kontinuum erreichen kann, bleibt die Absorption auf einem hohen Wert, der nur langsam bei weiter zunehmender Energie sinkt. Die plötzliche Zunahme der Absorption wird als Absorptionskante bezeichnet; und im Bereich der Absorptionskante hängt die Absorption davon ab, wie viele unbesetzte Zustände einer bestimmten Energie für das Elektron zur Verfügung stehen.
Die nebenstehenden Abbildungen zeigen die Entstehung der Röntgenfeinstruktur bei der Absorption einer einfallenden ebenen Welle von einem Atom (linke Abbildung). Nach der Absorption emittiert das absorbierende Atom seinerseits eine sphärische Photoelektronenwelle (mittlere Abbildung), die anschließend an umliegenden Atomen gestreut wird (rechte Abbildung). Es kommt dabei einerseits (blau) zu Einfachstreuprozessen, die die Ursache für die EXAFS-Strukturen sind, andererseits (orange) aber auch zu Mehrfachstreuprozessen, die die Ursache der NEXAFS/XANES-Strukturen im detektierten Spektrum sind.

In beiden Fällen kommt es zu Interferenzen (Pfeile), die die Grundlage für quantitative Untersuchungen bei NEXAFS/XANES- bzw. EXAFS-Messungen liefern.

Um Missverständnisse zu vermeiden, muss jedoch klargestellt werden, dass der XANES-Bereich nicht ausschließlich durch die oben beschriebene Mehrfachstreuung zu beschreiben ist. Während der EXAFS-Bereich relativ fern der Absorptionskante im Wesentlichen durch die Interferenz der emittierten Photoelektronen dominiert wird, so kann der Bereich um ±100 eV um die Absorptionskante zum Teil sehr stark durch inner-atomare Absorptionsbänder bestimmt sein. (Anregung kernnaher Elektronen in unbesetzte Zustände die energetisch knapp unterhalb der Photoionisationsenergie liegen.) Der Anteil der oben beschriebenen Mehrfachstreuung der Photoelektronen im XANES-Bereich kann also in den Hintergrund treten. Insbesondere der sogenannte „Pre-Peak“ wird häufig einem Quadrupol-Übergang zugeordnet, die direkte Emission eines Photoelektrons ist in diesem Energiebereich im klassischen Sinne noch nicht möglich.

Zur Messung eines NEXAFS-Spektrums muss die Röntgenenergie variiert werden, dafür wird aus Synchrotronstrahlung (großer Wellenlängenbereich) monochromatische Röntgenstrahlung herausgefiltert. Die Wellenlänge wird mit dem Monochromator in einem kleinen Bereich (einige eV) um die Absorptionskante variiert.

Die Absorption wird meistens nicht direkt über die Abschwächung des Röntgenstrahls bestimmt, sondern es wird das „Auffüllen“ des freigewordenen kernnahen Orbitals beobachtet. Dabei werden die Augerelektronen freigesetzt, diese und/oder die von ihnen erzeugten Sekundärelektronen können mit einem Sekundärelektronenvervielfacher, zum Beispiel einem Channeltron detektiert werden und sind ein Maß für die Röntgenabsorption. Diese Detektionsmethode ist im Englischen unter (Augerelektronenausbeute) bzw. (Gesamtelektronenausbeute) bekannt. Da die Elektronen im Festkörper nur kurze Strecken zurücklegen können (Augerelektronen ca. 1 nm, bis sie Energie verlieren; niederenergetische Elektronen einige Nanometer), ist diese Methode nur auf den Bereich nahe der Oberfläche empfindlich.

Eine andere Methode, die Absorption zu bestimmen, ist die Messung der Röntgenstrahlung, die beim Auffüllen des freigewordenen kernnahen Orbitals entstehen kann. Es wird also die Röntgenfluoreszenz gemessen (Fluoreszenzausbeute, engl. ). Da bei den mit NEXAFS meist untersuchten leichten Elementen beim Auffüllen des kernnahen Orbitals nur mit sehr geringer Wahrscheinlichkeit (einige Promille) Röntgenquanten emittiert werden und normalerweise stattdessen ein Augerprozess stattfindet, ist die Röntgenausbeute geringer als die Elektronenausbeute, allerdings kann die weiche Röntgenstrahlung dickere Materialschichten durchdringen als Elektronen. Es können mit Detektion der Röntgenstrahlung also tiefere Schichten erfasst werden.


Mit NEXAFS werden meist die Absorptionskanten von Kohlenstoff (Röntgenenergie = 285 eV), Stickstoff (400 eV) oder Sauerstoff (530 eV) gemessen.








Radon-Zerfallsprodukte

Radon-Zerfallsprodukte entstehen aus dem radioaktiven Edelgas Radon durch Kernzerfall. Die Atemluft im Freien, in Wohnungen und besonders in Erdhöhlen enthält ein Gemisch aus Radon und seinen Zerfallsprodukten. Für ungefähr 10 % aller Bronchialkarzinome ist Radon verantwortlich. Die Ursache sind seine kurzlebigen Zerfallsprodukte.

Vor einigen hundert Jahren starben speziell im Bergbaugebiet von Schneeberg und Joachimstal die meisten Bergleute an der Schneeberger Krankheit, die später als Lungenkrebs identifiziert wurde. Zu Beginn des 20. Jahrhunderts fiel der hohe Radongehalt der Luft in diesen Bergwerken auf und ließ einen Zusammenhang mit der Krankheit vermuten. Erst in den 50er Jahren erkannten Strahlenschützer, dass die Inhalation der Radon-Zerfallsprodukte zu einer hohen Alpha-Strahlendosis im Bronchialepithel und auf diese Weise zu Lungenkrebs führt. Der professionelle Strahlenschutz in Uran- und anderen Bergwerken begann.

Von radiologischer Bedeutung sind die Isotope Radon-222 und Radon-220. Während der Inhalation scheiden sich deren Zerfallsprodukte im Atemtrakt ab und reichern sich dort an. Wichtig sind nur die kurzlebigen Isotope der jeweiligen Zerfallsreihe. Der Organismus scheidet die ebenfalls vorhandenen langlebigen Isotope aus, so dass deren Strahlung kaum wirksam ist. Die biologisch besonders wirksame Alphastrahlung stammt größtenteils von den Polonium-Isotopen unter den Zerfallsprodukten.

Im allgemeinen Sprachgebrauch bezeichnet Radon das Radon-222 einschließlich seiner Zerfallsprodukte. Auch Grenzwerte für Radongas schließen immer die Wirkung der Zerfallsprodukte ein. Sind allein Radongas oder die Zerfallsprodukte gemeint, wird meistens ausdrücklich darauf hingewiesen. Radon-220 hat auch den historischen Namen "Thoron". Oft treten Radon und Thoron gemeinsam auf. Die Strahlendosis durch Thoron ist meistens um den Faktor 10 niedriger als die durch Radon.

Radon-222 zerfällt der Reihe nach in die in der Tabelle eingetragenen Nuklide. Dabei handelt es sich um den vereinfachten letzten Teil der Uran-Radium-Reihe. Die für den Strahlenschutz wichtigsten Zahlen sind in Fettschrift angegeben. Die Daten der letzten drei Spalten sind für die Berechnung der Potentiellen Alphaenergie (PAE) nützlich (siehe weiter unten).

HWZ: Halbwertszeit (d Tage, a Jahre)
PAE/Atom: Potentielle Alphaenergie pro Atom
Atome/Bq: Anzahl der Atome je Aktivitätseinheit Becquerel
PAE/Bq: Potentielle Alphaenergie je Aktivitätseinheit Becquerel

Radon-220 zerfällt der Reihe nach in die in der Tabelle eingetragenen Nuklide. Dabei handelt es sich um den vereinfachten letzten Teil der Thorium-Reihe. Die für den Strahlenschutz wichtigsten Zahlen sind in Fettschrift angegeben. Die Daten der letzten drei Spalten sind für die Berechnung der Potentiellen Alphaenergie (PAE) nützlich (siehe weiter unten).

HWZ: Halbwertszeit (d Tage, a Jahre)
PAE/Atom: Potentielle Alphaenergie pro Atom
Atome/Bq: Anzahl der Atome je Aktivitätseinheit Becquerel
PAE/Bq: Potentielle Alphaenergie je Aktivitätseinheit Becquerel

Die PAE/Atom des Bi-212 errechnet sich entsprechend den Verzweigungswahrscheinlichkeiten in Tl-208 und Po-212:

In der Luft entstehen einzelne Zerfallsproduktatome durch Kernumwandlung aus den dort vorhandenen Radonatomen oder bereits vorher entstandenen anderen Zerfallsproduktatomen. Trifft ein Zerfallsproduktatom auf ein Hindernis, lagert es sich daran an und bleibt auch dort. Hindernisse sind meistens Aerosolteilchen in der Luft, aber auch Wände oder Möbel. Weil sich einzelne Zerfallsproduktatome durch Diffusion sehr schnell in der Luft bewegen und es meistens viele Staub- oder Aerosolteilchen gibt, treffen die Zerfallsproduktatome recht schnell auf ein Teilchen und kleben daran fest. Es ist dann ein "angelagertes Zerfallsprodukt", und die Aerosolteilchen bestimmen sein weiteres Verhalten. Dazu gehören die Bewegung in der Luft und die Abscheidung an Gegenständen und im Atemtrakt.

Im Gegensatz zu inaktiven Schwermetallatomen können sich radioaktive Zerfallsproduktatome wieder von einem Staubteilchen lösen. Das geschieht bei der Kernumwandlung durch den Rückstoß beim Aussenden eines Strahlungsteilchens.

Nicht an Aerosolteilchen angelagerte Zerfallsprodukte heißen "freie Zerfallsprodukte". Der Anteil an freien Zerfallsprodukten beträgt in normaler Luft 1 % oder weniger. Er kann jedoch in besonders sauberer Luft oder bei frisch hinzugefügtem Radongas weit größer sein.

Die Abscheidewahrscheinlichkeit von Aerosolteilchen und angelagerten Zerfallsprodukten im Atemtrakt beträgt ungefähr 10 %. Der Mensch atmet den Rest wieder aus. Freie Zerfallsprodukte scheiden sich wegen ihrer schnellen Diffusion zu 100 % im Atemtrakt ab. Deshalb sind freie Zerfallsprodukte gefährlicher als angelagerte. Gegenüber Atemluft ohne freie Zerfallsprodukte erzeugt zum Beispiel Luft mit 10 % freien Zerfallsprodukten ungefähr die doppelte Inhalationsdosis.

Manche Filtergeräte versprechen eine Reduzierung der Strahlendosis in Aufenthaltsräumen, weil sie die schädlichen Zerfallsprodukte aus der Atemluft entfernen. Sie entfernen allerdings ebenso die Aerosolteilchen. Aus dem Filtergerät strömt zunächst staubarme, zerfallsproduktfreie Luft. Weil das Radongas noch vorhanden ist, bilden sich schnell neue Zerfallsprodukte. Damit entsteht zwar ein Gemisch mit weniger Radioaktivität aber einem höheren Anteil freier Zerfallsprodukte. Dadurch reduziert sich die Strahlendosis meistens nur unwesentlich. Unter ungünstigen Bedingungen kann ein Filtergerät die Strahlendosis sogar erhöhen.

Der Berufliche Strahlenschutz verwendet Arbeitshelme mit eingebauten Ventilatoren und Filtern. Hier funktioniert das Verfahren, weil der Helm gefilterte Luft direkt vor das Gesicht bläst. In der kurzen verfügbaren Zeit bilden sich kaum Zerfallsprodukte.

Die Konzentration der Radon-Zerfallsprodukte in Form der Potentiellen Alphaenergiekonzentration (PAEK) in der Atemluft ist ein Maß für die schädigende Wirkung eines Gemisches aus dem radioaktiven Edelgas Radon und seinen ebenfalls radioaktiven Zerfallsprodukten. Vorschriften des beruflichen Strahlenschutzes begrenzen die PAEK. Ein Vorteil dieses Konzepts ist, dass zur Beurteilung der Atemluft nur ein Zahlenwert erforderlich ist und nicht die Aktivitäten aller vorkommenden Zerfallsprodukte.

Die Radongas-Konzentration wird meistens dann berücksichtigt, wenn vorzugsweise preisgünstige Messverfahren verwendet werden, die nur die Konzentration des Radongases und nicht die der Zerfallsprodukte erfassen können. Gesetzliche oder empfohlene Grenzwerte für Radongas bewerten grundsätzlich die gemeinsam damit auftretenden Zerfallsprodukte.

Der Strahlenschutz berücksichtigt im Zusammenhang mit der Radoninhalation allein die Alphastrahlen aller Atome, die sich im Atemtrakt abscheiden können. Dazu gehören auch Atome, die selbst keine Alphastrahler sind, wenn sie sich später in einen Alphastrahler umwandeln werden. Radongas gehört nicht dazu, weil es zum größten Teil wieder ausgeatmet wird. Der im Körper bleibende restliche Gasanteil verlässt den Atemtrakt sehr schnell und verteilt sich über das Blut in den gesamten Organismus. Auch werden nur die kurzlebigen Zerfallsprodukte mit Halbwertszeiten bis zu einigen Stunden berücksichtigt, weil der Organismus langlebige ausscheidet, bevor eine nennenswerte Strahlenwirkung eintreten kann.

Inhaliert der Mensch ein Zerfallsproduktatom, so ist seine später irgendwann abgegebene Alpha-Energie die im Strahlenschutz relevante Größe. Diese wird als Potentielle Alphaenergie (PAE) bezeichnet. Jedem Zerfallsproduktatom wird die PAE zugeordnet, die es bis zum Ende des kurzlebigen Teils der Radon-Zerfallsreihe abgeben wird. Die Tabellen 1 und 2 enthalten die PAE aller in Frage kommenden Zerfallsprodukt-Atome (PAE/Atom).

Die in einem Luftvolumen enthaltene Potentielle Alphaenergie ist die Summe der PAE aller Atome, die sich in diesem Volumen aufhalten. Die Radioaktivität lässt sich als Liste der Aktivitäten aller vorhandenen Radionuklide angeben. Aus diesen Aktivitäten und den Halbwertszeiten der Nuklide lässt sich mit dem Zerfallsgesetz für jedes Nuklid berechnen, wie viele Atome vorhanden sind. Die Tabellen 1 und 2 enthalten die Anzahlen der Atome jeweils für die Aktivitätseinheit 1 Bq.

Die Potentielle Alphaenergiekonzentration lässt sich mit den Tabellenangaben sofort berechnen, wenn die Aktivitätskonzentrationen aller kurzlebigen Zerfallsprodukte bekannt sind. In der Praxis ist es nicht erforderlich, zunächst alle einzelnen Aktivitätskonzentrationen zu bestimmen, um daraus die PAEK zu berechnen. Es gibt auch Verfahren, die mit ausreichender Genauigkeit ohne diesen Umweg die PAEK messen können.

Wenn als einzige Quelle und Senke für die Aktivität von Radon bzw. Thoron und ihrer Zerfallsprodukte der radioaktive Zerfall vorliegt, d. h. wenn andere Quellen oder Senken wie Ventilation, Deposition oder Filterung der Luft vernachlässigbar sind, stellt sich ein radioaktives Gleichgewicht zwischen den Nukliden ein. Dabei beträgt die Summe der Potentiellen Alphaenergiekonzentrationen aller Folgeprodukte 5,5 nJ pro Bq Radon und 75 nJ pro Bq Thoron für die jeweiligen Folgeprodukte; für Radon werden hier nur die strahlenschutzrelevanten kurzlebigen Folgeprodukte betrachtet. Das Verhältnis der PAEK der tatsächlich in einer Probe vorhandenen Folgeprodukte zu diesem Wert wird als Gleichgewichtsfaktor "F" bezeichnet. Überwiegen Senken für Folgeprodukte, zum Beispiel bei Luftproben in Form von Deposition an Oberflächen, ist "F"  1.





Wechselwirkungsbild

Das Wechselwirkungsbild (auch bezeichnet als Wechselwirkungsdarstellung bzw. nach Paul Dirac als Dirac-Bild oder Dirac-Darstellung) ist in der Quantenmechanik ein Modell für den Umgang mit zeitabhängigen Problemen unter Berücksichtigung von Wechselwirkungen.

Es ist dem Heisenberg- und dem Schrödinger-Bild weitgehend äquivalent, d. h. alle physikalisch relevanten Größen (Skalarprodukte, Eigenwerte usw.) bleiben die gleichen (siehe auch Mathematische Struktur der Quantenmechanik). 

Zur Kennzeichnung, dass man das Wechselwirkungsbild verwendet, werden Zustände und Operatoren gelegentlich mit dem Index „I“ (wie engl. "interaction") oder „D“ (wie "Dirac-Bild") versehen: formula_1 bzw. formula_2

Das Wechselwirkungsbild wurde 1926 von Paul Dirac in die Quantenmechanik eingeführt. In Zusammenhang mit Quantenelektrodynamik wurde das Wechselwirkungsbild auch von Tomonaga, Dirac und (in einer unveröffentlichten Arbeit als Student am City College of New York) von Julian Schwinger (1934) eingeführt. Die Behandlung der relativistischen Quantenfeldtheorie im Wechselwirkungsbild mit Zweiter Quantisierung fand danach Eingang in die Standardlehrbücher.

Im Wechselwirkungsbild gelten folgende Annahmen:

Der Grundgedanke des Wechselwirkungsbildes besteht darin, die zeitliche Entwicklung des Systems, die von formula_8 verursacht wird, in die zeitliche Abhängigkeit der Operatoren zu stecken, während die von formula_5 verursachte Zeitabhängigkeit in die Entwicklung des Zustandes eingeht.

Dazu werden zwei Zeitentwicklungsoperatoren definiert:


Der Erwartungswert "a" des Operators formula_15 muss in allen drei Bildern (Heisenberg-Bild: Index formula_16, Schrödinger-Bild: Index formula_17, Dirac) gleich sein:

Der zeitabhängige Operator formula_20 ist wie im Heisenberg-Bild gegeben durch:

Der zeitabhängige Zustand formula_1 kann nur indirekt – über die Reduktion des (im Schrödinger-Bild) vollständig die Dynamik beschreibenden Zustandes formula_23 um den von formula_8 verursachten Anteil seiner Zeitentwicklung – definiert werden:

Damit lässt sich der Operator formula_26 definieren:

Der zeitlich unabhängige Anteil des Hamiltonoperators formula_8 ist im Wechselwirkungsbild identisch mit dem im Schrödinger-Bild:

Die Dynamik der Zustände wird (ähnlich dem Schrödinger-Bild) beschrieben durch die Gleichung:

Die Dynamik der Operatoren wird (wie im Heisenberg-Bild) beschrieben durch die Heisenbergsche Bewegungsgleichung, mit dem nicht zeitabhängigen Hamilton-Operator formula_8, der das ungestörte System beschreibt:

Mit formula_33 geht das Dirac-Bild in das Heisenberg-Bild über.

Zum Zeitpunkt formula_34 stimmen alle drei Bilder überein:

Zur Vorbereitung werden die zeitlichen Ableitungen von formula_37 und formula_38 ermittelt:

Bewegungsgleichung für die Zustände:

Bewegungsgleichung für die Operatoren:





Bubnoff-Einheit


Die Bubnoff-Einheit ist eine bisweilen verwendete Maßeinheit für die (sehr geringe) „Geschwindigkeit“ geologischer Vorgänge. Sie ist nach dem deutsch-russischen Geotektoniker Serge von Bubnoff benannt und wird mit „B“ abgekürzt. Eine Bubnoff-Einheit entspricht einem Millimeter pro 1000 Jahre oder einem Meter pro Jahrmillion.






Long Duration Exposure Facility

Die Long Duration Exposure Facility (LDEF) war ein Satellit, der als Träger für technische und naturwissenschaftliche Experimente im Weltraum diente. Er kam nur ein einziges Mal zum Einsatz.

LDEF bestand aus einer zylindrischen Aluminiumstruktur, an deren Außenseiten 86 Experimententräger angebracht waren: Zwölf Reihen zu je sechs Einheiten befanden sich im Mantel und je sieben an den beiden Enden. Der Satellit hatte eine Länge von 9,20 Meter und einen Durchmesser von 4,29 Meter. Die Struktur hatte eine Masse von 3,6 Tonnen; voll bestückt lag sie bei 9,7 Tonnen.
Am 6. April 1984 startete die Raumfähre Challenger mit der Mission STS-41-C und brachte die Forschungsplattform LDEF in eine fast kreisförmige Erdumlaufbahn mit 477 km Bahnhöhe.

Die Lage im Orbit war in der Weise stabilisiert, dass die Längsachse von LDEF zum Erdmittelpunkt ausgerichtet war.

Ursprünglich sollte LDEF nach zehn Monaten mit der Mission STS-51-D im Februar 1985 geborgen werden. Durch Terminverschiebungen des Space-Shuttle-Programms rückte der Bergungstermin in das Jahr 1986. Durch die Katastrophe der Challenger (STS-51-L) am 28. Januar 1986 wurde das unmöglich. Das Shuttle-Programm war für über zwei Jahre unterbrochen.

Die Plattform konnte erst im Januar 1990 nach fast sechs Jahren Flug mit der Mission STS-32 geborgen werden. Die Bahnhöhe war inzwischen auf 335 km gesunken und LDEF wäre wohl im folgenden Monat in die Erdatmosphäre eingetreten. Die Columbia näherte sich LDEF dabei auf einer Flugbahn, die die Experimente möglichst wenig kontaminieren konnte. LDEF blieb nach dem Einfangen noch für viereinhalb Stunden am Robotarm, um Bilder von allen Experimententrägern zu machen. Am 20. Januar landete die Columbia auf der Edwards Air Force Base und wurde sechs Tage später huckepack auf dem Shuttle Carrier Aircraft (mit LDEF in der Ladebucht) zum Kennedy Space Center transportiert.

Die Untersuchung des Satelliten brachte unter anderem sehr viel Informationen über Weltraummüll und Mikrometeoriten.






Prädissoziation

Der Begriff Prädissoziation bezeichnet die Zersetzung (Dissoziation) eines Moleküls in meist zwei Bausteine, wobei nicht die volle Dissoziationsenergie aufgebracht werden muss, da es einen elektronisch angeregten Zustand B gibt, dessen Dissoziationsenergie unterhalb der des aktuellen bindenden Molekülzustandes A liegt.
Regt man im bindenden Ausgangszustand A ein vibronisches Niveau an, von dem ein Übergang zu B möglich ist, und liegt dessen Energie oberhalb der Dissoziationsenergie von B, so kann das Molekül spontan aufspalten.






Spektroskopischer Doppelstern

Spektroskopische Doppelsterne sind Sternsysteme, die aus zwei eng umeinander kreisenden Sonnen bestehen. Sie haben einen so geringen Abstand, dass das Auflösungsvermögen auch der größten Teleskope nicht ausreicht, um sie direkt zu unterscheiden. Dies ist meist nur indirekt durch Spektroskopie möglich, wie der Name sagt.

Nachweisbar sind solche Doppelsterne durch einen periodischen Dopplereffekt im gemeinsamen Linienspektrum bzw. manchmal durch andere spektrale Anomalien. Bei nicht allzu schwachen Sternen können auch beide Radialgeschwindigkeiten bestimmt werden, woraus das Massenverhältnis folgt. Aus den Umlaufbahnen erhält man die Massensumme und damit auch die zwei Einzelmassen. Diese für die Astrophysik wesentlichen Daten und Messmethoden sind auch bei anderen (insbesondere den vielen teleskopischen) Doppelsternen möglich.

Die hier behandelten Doppelsterne verraten sich durch ihren periodischen Dopplereffekt im Spektrum bzw. durch andere spektrale Anomalien. Die Umlaufbewegungen bewirken eine regelmäßige Verschiebung der Spektrallinien, die vielfach gemeinsamen Spektrallinien spalten sich dadurch auf ("Zweispektrensystem"): wenn sich der eine Stern von uns entfernt, schiebt der Dopplereffekt seine Spektrallinien gegen Rot(Rotverschiebung), während gleichzeitig die Spektrallinien des anderen Sterns, der sich auf uns zubewegt, gegen Blau verschoben werden. Aus der Periode der Verschiebung kann die Umlaufzeit bestimmt werden, zusammen mit der Größe der Verschiebung zusätzlich die Umlaufgeschwindigkeit und der Abstand der Sterne. Dabei ist noch die Bahnneigung gegen die Sichtlinie zu berücksichtigen.

Bei ähnlicher Helligkeit überlagern sich die beiden Farbbänder zu einem gemischten Spektraltyp. Ist jedoch der Helligkeitsunterschied größer als eine Magnitude, so überstrahlt das Spektrum des Hauptsterns dasjenige des Begleiters, und die Linienverschiebung ist nur nach einer Seite feststellbar. In solchen "Einspektrensystemen" bleiben die Bahnneigung und das Massenverhältnis der zwei Sterne unbekannt.

Die größten gemessenen Radialgeschwindigkeiten sehr enger Systeme liegen bei 1500 km/s. Kleinere als 1 km/s sind kaum nachweisbar; sie treten in Systemen extrem langer Umlaufzeit auf, oder wenn die Bahnebene quer zur Sichtlinie liegt.

Viele spektroskopische Doppelsterne gehören zur Gruppe der Riesensterne; die erste spektroskopische Entdeckung war 1889 der helle Stern Mizar in der Deichsel des Großen Wagens, der sogar ein Vierfachsystem (2 Sternpaare) darstellt. Sein ferner Begleiter Alkor (der sog. Augenprüfer) ist ebenfalls ein spektroskopischer Doppelstern.

Weil die Spektrallinien bei hellen Sternen besonders gut messbar sind, konnten 1889 auch Spica (Hauptstern in der Jungfrau), Algol im Perseus und Beta Aurigae als enge Sternpaare enttarnt werden sowie in den Folgejahren auch tausende von schwächeren Sternen. 

Bei nicht zu schwachen Sternen kann die Interferometrie zusätzliche Messungen und Aussagen über die Komponenten des Doppelsterns ermöglichen.

Während "teleskopische" Doppelsterne für eine gegenseitige Umkreisung mehrere Jahre bis Jahrhunderte brauchen, liegt die Periode der spektroskopischen Systeme bei einigen Stunden bis Wochen. Die Extremwerte sind 1-5 Jahre (bei weit entfernten Sternen, deren Winkelabstand für teleskopische Entdeckung zu klein ist) bzw. 80 Minuten bei WZ Sagittae. Dieser ist gleichzeitig ein Bedeckungsveränderlicher – so eng, dass Materie zum kompakten Begleiter überströmt und ihn zur wiederkehrenden Nova macht.

Alle Spektralklassen sind bei spektroskopischen Doppelsternen vertreten. Besonders enge Komponenten gehören meist den Spektralklassen O, B, A und F an, während bei längeren Umlaufzeiten oft auch Riesensterne der Klassen G, K und M vorkommen.






Totalenthalpie

Die Totalenthalpie formula_1, auch Gesamtenthalpie, Stagnationsenthalpie oder Ruheenthalpie genannt, ist eine thermodynamische Zustandsgröße, die in der Strömungslehre zur Beschreibung kompressibler strömender Medien benötigt wird, insbesondere bei Berechnungen für Wärmekraftmaschinen wie Dampfturbinen und Raketentriebwerke.

Die Totalenthalpie ist ein Maß dafür, wie viel „Arbeitsfähigkeit“ einem Medium, beispielsweise Dampf, an einer bestimmten Stelle einer Dampfturbine, noch innewohnt, unabhängig davon, ob diese Energie als thermische Energie, Druck (diese beiden machen die Enthalpie aus) oder kinetische Energie vorhanden ist.

Die Totalenthalpie ist definiert als Summe aus der Enthalpie formula_2 und der kinetischen Energie formula_3 eines strömenden Teilchens:

wobei

Da bei einer Strömung die Masse erhalten bleibt, wird oft die spezifische Totalenthalpie formula_7, also die Totalenthalpie je Masseneinheit, verwendet:

Enthalpie kann in kinetische Energie des strömenden Mediums übergeführt werden (in diesem Fall kommt es zur Expansion und meist zur Temperaturerniedrigung), und umgekehrt kinetische Energie in Enthalpie (Staudruck, also Kompression, damit einhergehend meist Temperaturerhöhung). Die Totalenthalpie ist also ein Maß dafür, wie viel „Arbeitsfähigkeit“ einem Medium, beispielsweise Dampf, an einer bestimmten Stelle einer Dampfturbine, noch innewohnt, unabhängig davon, ob diese Energie als thermische Energie, Druck (diese beiden machen die Enthalpie aus) oder kinetische Energie vorhanden ist.

Aus der Totalenthalpie lässt sich die Stagnations- oder Totaltemperatur formula_9 ableiten:

wobei formula_11 die spezifische Wärme bei konstantem Druck ist (Hier wurde angenommen, dass formula_11 zwischen formula_13 und formula_9 nicht von der Temperatur abhängt).

Wird das strömende Medium auf die Geschwindigkeit formula_15 abgebremst (beispielsweise durch ein Hindernis), so erhöht sich seine Enthalpie auf die Totalenthalpie:

und seine Temperatur von formula_13 auf die Totaltemperatur:

Die Stagnationstemperatur ist im Raketen- und Flugzeugbau von Bedeutung für die thermische Belastung von Oberflächen bei Überschallströmungen.

Die Totalenthalpie (und damit bei idealen Gasen auch formula_9) bleibt in Strömungen konstant, solange weder mechanische Arbeit geleistet wird noch Wärme zwischen dem strömenden Medium und der Umgebung fließt (vgl. unten: erster Hauptsatz); dies gilt auch für die Stoßwellen einer Überschallströmung.

Weitere Energieterme können hinzugefügt werden (etwa für chemische Reaktionen, Phasenänderungen, Bewegung in Feldern).

So kann im Gravitationsfeld die potentielle Energie berücksichtigt werden:

mit

Beispielsweise lässt sich der Erste Hauptsatz der Thermodynamik (für ein offenes System) dann so anschreiben:

Hier sind




Qutb ad-Din asch-Schirazi

Qutb ad-Din Mahmud ibn Masud asch-Schirazi, unter anderem als "Qotboddin Shirāzi" transliteriert (; * Oktober 1236 in Schiras, Iran; † 7. Februar 1311 in Täbris) war ein persischer Wissenschaftler, der vor allem in Schiras, Maragha, Dschuvain, Anatolien (Konya, Sivas, Malatya) und Syrien wirkte. Bekannt ist er unter anderem für die mit seinem Lehrer Nasir ad-Din at-Tusi verfassten Kritiken am "Almagest" des Ptolemäus, die Fortführungen der optischen Versuche des Alhazen und für die erste richtige Erklärung für den Aufbau des Regenbogens. Neben der Astronomie und Physik beschäftigte sich Quṭb ad-Din auch ausgiebig mit Medizin und Mathematik, zu der auch die Musik gehörte, sowie mit Theologie und Philosophie.

Quṭb ad-Dins bekannteste Werke zur Astronomie sind das 1281 fertiggestellte "Nihāyat al-idrāk fī dirāyat al-aflāk" („Die Grenzen der Durchdringung des Wissens über den Himmel“) und "At-Tuḥfatu-š-šāhīya" („Das königliche Geschenk“) aus dem Jahre 1284. Beide behandeln sein Modell der Planetenbewegung, das eine Verbesserung des ptolemäischen Epizyklenmodells darstellte.

In der Mathematik schrieb er eine Abhandlung mit dem Titel "Fī ḥarakāt ad-daḥrağa wa-n-nisba baina-l-mustawā wa-l-munḥanā" („Über Abrollbewegungen und das Verhältnis zwischen Ebene und Kurve“), vermutlich über elementare Zykloidengeometrie.

Quṭb ad-Din stammt aus einer Familie, die viele Sufis (islamische Mystiker) hervorbrachte, und so wirkte er auch selber als solcher. In dieser Rolle schrieb er einen berühmten Kommentar zum "Ḥikmatu-l-išrāq" des Schahab ad-Din Yahya Suhrawardi, einem einflussreichen Werk innerhalb des Sufismus. Qutb ad-Din asch-Schirazis bekannteste Schrift ist aber das in persisch geschriebene Werk "Durrat al-tāğ li-Ġurrat ad-dībāğ" („Perlenkrone“) von 1306, bei dem es sich um einen umfangreichen Traktat über Musik handelt und das neben eigenen Beiträgen auch die Theorien von al-Fārābī und seines Lehrers Safi ad-Din al-Urmawi vermittelt sowie ein komplexes Notationssystem enthält. Die Kenntnis der spekulativen Mystik (arab. "ʿirfān") erwarb er durch das Studium bei Ṣadr ad-Dīn al-Qūnawī, der wiederum als einer der besten Schüler von Ibn Arabi gilt.

Auch einen Korankommentar hat er verfasst.





Columbia Non-Neutral Torus

Der Columbia Non-neutral Torus (CNT) ist ein kleiner Stellarator am Labor für Plasmaphysik der Columbia University.

Er wurde von Thomas Sunn Pedersen, Wayne Reiersen und Fred Dahlgren vom Princeton Plasma Physics Laboratory entwickelt, um geladene Plasmen, eingeschlossen zwischen magnetischen Flächen, zu untersuchen. Das Experiment lief im November 2004 an und wird von der National Science Foundation und dem United States Department of Energy unter anderem mittels eines „Faculty Early Career Development (CAREER)“-Preises über 800.000 US-Dollar, der an Pedersen verliehen wurde, finanziert.

Zur wissenschaftlichen Leitung gehört auch Allen Boozer.

Der CNT befindet sich innerhalb einer zylindrischen Vakuumkammer aus Edelstahl und hat etwa 1,5 m Durchmesser und 1,9 m Höhe. Die Vakuumkammer erreicht ein Vakuum, das einem Druck von etwa 1,3·10 Pa entspricht. Der CNT nutzt lediglich vier Magnetspulen, zwei parallele Spulen außerhalb und zwei poloidal-Feldspulen innerhalb der Stahlkammer. Sie werden mit bis zu 200 kW elektrischer Leistung versorgt und erzeugen Magnetfelder im Bereich von 0,01 bis 0,2 T. Der Aufbau hat ein Aspektverhältnis von kleiner 1,9.

Die Forschungen am CNT dienen der experimentellen Untersuchung an geladenen Plasmen: Deren Gleichgewicht Transportphänomene, Eindämmung und Ionen-bedingte Instabilitäten.

Die ersten Untersuchungen zeigten, dass magnetische Flächen mit dem einfachen Vier-Spulen-Aufbau erzeugt werden können. Bei ausreichend geringem Druck und hohen Magnetfeld lässt sich ein im Wesentlichen reines Elektronenplasma für bis zu 20 Millisekunden stabil festhalten. Der Transport wird durch Stöße mit neutralen Teilchen angetrieben. Außerdem gibt es einen Drift (Kraft auf elektrische Ladung in Magnetfeld) entlang isolierender Stäbe, die in das Plasma eingeführt werden. Bei einem Druck über 10 Pa wurde eine Instabilität mit einer Frequenz im Bereich von 10 bis 50 kHz und einer poloidalen (in Feldrichtung) Modenzahl "m"=1 beobachtet.





Particle Data Group

Die Particle Data Group ist eine internationale Kollaboration von Teilchenphysikern, die publizierte Resultate, die Eigenschaften von Elementarteilchen und den Grundkräften der Physik behandeln, zusammenstellt und nochmals analysiert. Die Gruppe veröffentlicht auch Reviews von theoretischen Resultaten, die für die Phänomenologie und verwandte Felder wie die Kosmologie interessant sind.
Die Particle Data Group veröffentlicht momentan den "Review of Particle Physics" und seine Taschenausgabe, das "Particle Physics Booklet", die jährlich im Netz aktualisiert und alle zwei Jahre in Buchform herausgegeben werden.

Die Particle Data Group veröffentlicht auch das "Pocket Diary for Physicists", einen Kalender mit den Daten von wichtigen internationalen Konferenzen und Kontaktinformationen von großen Hochenergiephysikinstitutionen. Die Particle Data Group verwaltet darüber hinaus in Zusammenarbeit mit den Autoren von Event-Generatoren (Programmen, die mittels Monte-Carlo-Berechnungen Teilchenkollisionen simulieren) das standardisierte Nummernschema für Teilchen in solchen Simulationen.

Der Review of Particle Physics (früher Review of Particle Properties, Data on Particles and Resonant States, und Data on Elementary Particles and Resonant States) ist ein umfangreiches, 1200 Seiten starkes Referenzwerk, das Teilcheneigenschaften zusammenfasst und einen Überblick über den aktuellen Stand der Elementarteilchenphysik, Allgemeinen Relativitätstheorie und Big Bang Kosmologie gibt.
Es ist mit mehr als 3000 Zitationen jährlich momentan das am häufigsten zitierte Dokument in der Hochenergiephysik.

Der Review ist im Moment in 3 Bereiche aufgeteilt:

Eine gekürzte Version des "Reviews", mit den "Summary Tables", einem stark gekürzten Abschnitt "Reviews, Tables and Plots", und ohne den Abschnitt "Particle Listings" ist als ein 300 Seiten starkes Particle Data Booklet im Taschenbuchformat verfügbar.

Die Geschichte des "Review of Particle Physics" kann auf den 1957 erschienenen Artikel "Hyperons and Heavy Mesons (Systematics and Decay)" von Murray Gell-Mann und Arthur H. Rosenfeld und die Aktualisierung "Data for Elementary Particle Physics" (University of California Radiation Laboratory Report UCRL-8030) zurückverfolgt werden. 1963 veröffentlichte Matts Ross unabhängig davon eine Zusammenstellung "Data on Elementary Particles and Resonant States". Auf seinen Vorschlag hin wurden die beiden Veröffentlichungen ein Jahr später im "Data on Elementary Particles and Resonant States" von 1964 zusammengeführt.

Der Name der Veröffentlichung wurde danach noch dreimal gewechselt: 1965 in "Data on Particles and Resonant States", 1970 in "Review of Particle Properties", und 1996 in die aktuelle Form des "Review of Particle Physics".






Anströmgeschwindigkeit

Die Anströmgeschwindigkeit ist die Geschwindigkeit, mit der ein Gas oder eine Flüssigkeit auf einen festen Körper zuströmt, bevor dieser die Strömung beeinflusst.

Der dynamische Auftrieb und der Strömungswiderstand sind über einen weiten Geschwindigkeitsbereich proportional zum Quadrat der Anströmgeschwindigkeit. Richtungsangaben der Anströmung (Anströmwinkel) beziehen sich auf eine ausgezeichnete Achse bzw. Ebene des Körpers, beim Tragflügel z. B. auf die Sehne eines Profils:
Bei PKW bezieht sich der Schiebewinkel, der z. B. durch Seitenwind entsteht, auf die Mittelebene des Fahrzeugs. Im Windkanal werden in Abhängigkeit vom Anströmwinkel sechs Komponenten gemessen:
Aus den Kräften und Momenten werden jeweils dimensionslose Beiwerte gebildet. Bezugsgröße für die Kräfte ist die Stirnfläche, für die Momente das Produkt aus Stirnfläche und Radstand. Der Bezugspunkt für die Momente liegt auf der Fahrbahn und mittig bezüglich Radstand und Spurweite. 





Prandtlsche Mischungsweghypothese

Die Prandtlschen Mischungsweghypothese gehört zu den wichtigsten Schließungsansätzen im Zusammenhang der Turbulenzmodelle, um die zusätzlichen Unbekannten, die in den Reynolds-Gleichungen auftreten, zu berechnen.

In der Strömungsmechanik lässt sich nach der Prandtlschen Mischungsweghypothese die Wirbelzähigkeit als Produkt einer charakteristischen Geschwindigkeit und einer charakteristischen Länge, der sog. Mischungsweglänge, darstellen. Sie wurde 1925 von Ludwig Prandtl aufgestellt. Die Mischungsweglänge kann dabei maximal so lang sein wie zum Beispiel die Dicke der Scherschicht. Sie wird auch als Kohärenzlänge bezeichnet, da man sie auch als Weg deuten kann, den ein Turbulenzballen durchschnittlich zurücklegt, bevor er seine Individualität einbüßt.

Die Mischungsweglänge lässt sich darstellen als:

Auf diese Weise erhält man die 1. Prandtlsche Mischungswegformel für die turbulente Schubspannung formula_2:

Geht man davon aus, dass die Mischungsweglänge konstant ist, so ändert sich die Reynoldsspannung proportional zum Quadrat der gemittelten Strömungsgeschwindigkeit.

Man erhält schließlich für die Wirbelzähigkeit:





Hurst-Exponent

Der Hurst-Exponent formula_1 ist eine Kennzahl aus der Chaostheorie bzw. aus der Fraktalgeometrie, die von Benoît Mandelbrot sowohl nach Harold Edwin Hurst als auch nach Otto Ludwig Hölder benannt wurde. Sie stellt einen Abhängigkeitsindex zwischen verschiedenen Größen dar. Zudem kann sie als relative Tendenz einer Zeitreihe gesehen werden.

Angewandt auf fraktale Oberflächen stellt sie einen Rauhigkeitskoeffizient dar, der direkt mit der fraktalen Dimension "D" in Verbindung steht:

Der Hurst-Exponent variiert zwischen Null und Eins, wobei größere Werte weichere Formen erzeugen:




Eric Betzig

Robert Eric Betzig (* 13. Januar 1960 in Ann Arbor) ist ein US-amerikanischer Physiker. 2014 erhielt er gemeinsam mit Stefan W. Hell und William E. Moerner für die Entwicklung superauflösender Fluoreszenzmikroskopie den Nobelpreis für Chemie.

Eric Betzig erwarb den Bachelor of Science in Physik vom California Institute of Technology (Juni 1983), den Master of Sciences in Angewandter Physik an der Cornell University (Januar 1985) und den Ph.D. für angewandte Physik von der Cornell University (August 1988) bei Michael Isaacson. Er arbeitete von 1988 bis 1994 in den Bell Laboratories auf dem Gebiet der optischen Nahfeldmikroskopie und gilt mit Dieter Pohl als einer der Begründer der NSOM-Technologie (in Europa SNOM abgekürzt). Seine Forschungen steigerten das Auflösungsvermögen bis zu einem Vierzigstel der optischen Auflösungsgrenze. Von 1994 bis 1996 war er als Forscher und Berater für sein Unternehmen "NSOM Enterprises" tätig. Anschließend kehrte er der optischen Forschung für fast zehn Jahre den Rücken und arbeitete im Bereich Maschinenbau für die familieneigene Ann Arbor Machine Company (1996–2002). Er ist Inhaber von "New Millennium Research", für die er von 2002 bis 2005 als Forscher und Berater arbeitete.

Seit Dezember 2005 ist Betzig Leiter einer Forschungsgruppe am Janelia Farm Research Campus des Howard Hughes Medical Institute. Im Jahr 2007 erhielten die deutsche Firma Carl Zeiss und Eric Betzig zusammen mit Harald Hess die Exklusivrechte der Vermarktung des PALM-Verfahrens in der Fluoreszenzspektroskopie, das die Beobachtung von Zellstrukturen ab einer Größe von 20 nm ermöglicht.

1992 erhielt Betzig den "William L. McMillan Award" und den 1993 den William O. Baker Award for Initiatives in Research. 2010 wurde ihm der Max Delbruck Prize verliehen, er lehnte jedoch die Annahme des Preises ab. 2014 wurde er für die Entwicklung der hoch auflösenden („Superresolution“) Fluoreszenzmikroskopie zusammen mit Stefan Hell und William E. Moerner mit dem Nobelpreis für Chemie ausgezeichnet. 2015 wurde Betzig in die National Academy of Sciences gewählt.







Nuklidkarte/Ordnungszahl 61 bis 80




Atominstitut

Das Atominstitut, früher Atominstitut der österreichischen Universitäten, ist ein Institut der Fakultät für Physik der Technischen Universität Wien.
Mit seinem 1962 in Betrieb genommenen Forschungsreaktor TRIGA Mark II ist es heute die einzige in Österreich verbliebene Einrichtung, die über einen laufenden Kernspaltungsreaktor verfügt (siehe auch Liste der Kernreaktoren in Österreich). Seit Ende 2017 wird das Institut von Thorsten Schumm als Institutsvorstand geführt. Das Institut befindet sich in Wien 2., Stadionallee 2, unmittelbar neben der Stadionbrücke über den Donaukanal. Der Haupteingang für Fußgänger trägt die Adresse Schüttelstraße 115.

Im Herbst 1895 wurde an dieser Stelle das von der Bürogemeinschaft der Architekten Gustav Orglmeister (1861–1953) und Franz Kupka (1855–1924) ausgeführte "k.u.k. Hof-Fourage-Magazin" in Betrieb genommen. Die Anlage bestand aus einem sechsstöckigen Getreidespeicher (Hafer), zwei Scheunen (jeweils 1.100 m²), einem Stallgebäude sowie einem Verwaltungsgebäude (samt Wohnungen). Nach dem Ersten Weltkrieg diente das ehemalige "Fourage-Depôt" ab 1919 als Bundesfuhrwerksbetrieb und ab Ende 1925 den Bundestheatern als Kulissendepot, das bis dahin in der Westgalerie der Rotunde untergebracht gewesen war. Während des Zweiten Weltkriegs schwer beschädigt, wurden die Gebäude nach Kriegsende abgetragen.

Als interuniversitäres Institut 1958 unter dem Namen "Atominstitut der österreichischen Hochschulen" gegründet, wurde es bereits 1962 der Technischen Universität Wien administrativ zugeordnet. Es behielt mehr als drei Jahrzehnte seinen Sonderstatus als interuniversitäres Institut bei und wurde unter der Bezeichnung "Atominstitut der österreichischen Universitäten" geführt. 2009 wurde das Institut in "Atominstitut" umbenannt. Zeitgleich wurde auch die oftmals im englischen gebrauchte Form "Atomic Institute" oder "Atomic Institute of the Austrian Universities" umbenannt. Somit lautet der offizielle Name des Instituts im englischen Institute of Atomic and Subatomic Physics.
Heute ist das Atominstitut ein Institut der Technischen Universität Wien und bildet zusammen mit den Instituten für Theoretische, Angewandte sowie Festkörperphysik die Fakultät für Physik dieser Universität.

Während seines mehr als 40-jährigen Bestehens hat das Institut einen großen Wandel vollzogen. Ursprünglich der Forschung und Ausbildung auf den Gebieten der Atom-, Kern- und Reaktorphysik, der Strahlenphysik und des Strahlenschutzes, der Umweltanalytik und Radiochemie sowie der nuklearen Messtechnik gewidmet, sind über die Jahre weitere Forschungsbereiche dazugekommen.

Heute liegt der Schwerpunkt der Forschung im Bereich der Strahlenphysik, Neutronen- und Quantenphysik, Angewandten Quantenphysik, Quantenoptik, sowie der Tieftemperaturphysik und Supraleitung.

Die Berufungen der Jahre 2006 und 2009 (Arbeitsbereiche Quantenoptik und Angewandte Quantenphysik) ergänzen die kern-, reaktor-, neutronen und strahlenphysikalischen Arbeitsschwerpunkte des Atominstituts. Das Atominstitut deckt somit wesentliche wissenschaftliche Bereiche der Kern-, Strahlen-, Atom- und Reaktorphysik fachlich ab. Die Umbauarbeiten der vergangenen Jahre haben die Labors auf den Stand der Technik zurückgebracht.

Das Atominstitut ist organisatorisch in zentrale Einrichtungen, insbesondere Forschungsreaktor und betrieblicher Strahlenschutz, sowie sechs Forschungsbereiche unterteilt: Angewandte Quantenphysik, Atomphysik und Quantenoptik, Kern- und Teilchenphysik, Neutronen- und Quantenphysik, Strahlenphysik sowie Tieftemperaturphysik und Supraleitung.

Der Forschungsreaktor wird im Rahmen des Safeguards-Programms der Internationalen Atomenergiebehörde (IAEA) der Vereinten Nationen zur Trainings- und Ausbildungsarbeit von Fachinspektoren genützt.


Eine Besonderheit ist die einzigartige geographische Lage des Reaktors in der Nähe des Zentrums einer Großstadt – von Wien. Die Entfernung Stephansdom – Kernreaktor beträgt etwa 3,2 km. Die Forschungsstätte liegt am Rand des Praters und wird daher volkstümlich "Praterreaktor" genannt.

In den Jahren 2005 bis 2010 haben mehr als 16.500 Personen, hauptsächlich Schüler, an öffentlichen Führungen am Atominstitut teilgenommen.





Messeffekt

Ein Messeffekt ist eine physikalische Erscheinung, die bei einer Messung verwendet wird. Es gibt nur wenige Fälle, bei denen die Messgröße direkt mit einem bekannten Wert dieser Größe verglichen werden kann, etwa indem man einen Maßstab anlegt. Bei der Mehrzahl der zu messenden Größen ist ein physikalisches Gesetz auszunutzen, das eindeutig und umkehrbar auf eine andere physikalische Größe führt, die der Messung leicht zugänglich ist.

Während in der Frühzeit der Entwicklung der Messtechnik vorzugsweise Erscheinungen ausgenutzt wurden, die sichtbar und dann auch ablesbar sind, werden jetzt vorzugsweise Erscheinungen ausgenutzt, die auf ein elektrisches Signal führen.
Der Vorteil der elektrischen Signale liegt in der leichten Umformbarkeit (in robuste störunempfindliche Analog- oder Digitalsignale), der leichten Übertragbarkeit an entfernte Registrierungs-Stellen (Anzeige, Protokollierung) und in der leichten Bewertbarkeit in Automatisierungsaufgaben (etwa Regelungstechnik).

Messprinzip




Synchrophasotron

Das Synchrophasotron war ein Synchrotron-Teilchenbeschleuniger mit 208 m Umfang am Vereinigten Institut für Kernforschung in Dubna.

Im Jahr 1944 erarbeitete Wladimir Iossifowitsch Weksler mit dem Prinzip der Phasenfokussierung die Grundlage zum Bau von Synchrotronbeschleunigern. Unter seiner Leitung wurden in den Jahren 1949 und 1950 die Pläne zum Synchrophasotron erarbeitet. Das Synchrophasotron ging im April 1957 in Betrieb und war mit einer Protonenenergie von 10 GeV zu diesem Zeitpunkt der stärkste Teilchenbeschleuniger weltweit. Im Jahr 1971 wurden erstmals Deuteriumkerne auf eine Energie von 10 GeV beschleunigt.

In einem Tunnel 3,7 m direkt unterhalb des Synchrophasotrons wurde in den Jahren 1987 bis 1992 das supraleitende Nuclotron errichtet, welches im März 1992 erstmals in Betrieb ging.
Beide Anlagen teilten sich die Vorbeschleuniger und konnten die angegliederten Experimentierplätze versorgen.

Ab dem Jahr 1991 musste der Betrieb des Synchrophasotrons infolge hoher Stromkosten und geringem finanziellem Budget erheblich eingeschränkt werden.
Der Betrieb wurde im Jahr 2003 eingestellt.

Beim Synchrophasotron handelt es sich um den letzten Beschleuniger der ersten Synchrotron-Generation. Diese Bauart wird auch als „constant Gradient“- oder „weak Focusing“- Synchrotron bezeichnet und weist einen bauartbedingten großen Strahlrohr-Querschnitt auf. Der Luftspalt der Elektromagnete hatte die Abmessungen von 40 cm × 2 m, das Gewicht der Eisenkerne der Elektromagnete betrug 36000 Tonnen.
Später gebaute Synchrotrone wie das Proton Synchrotron oder das Alternating Gradient Synchrotron kamen durch das „alternating Gradient“- oder „strong Focusing“-Verfahren mit wesentlich kleineren Strahlrohr-Querschnitten, und damit kleineren Magneten aus.

Im Jahr 1988 wurde im Synchrophasotron eine „alternating Gradient“-Strahlfokussierung nachgerüstet, dazu wurden in den Luftspalt der Elektromagnete unsymmetrisch angeordnete keilförmige Zwischenstücke eingesetzt, um so das Magnetfeld gemäß vorherigen Berechnungen zu verformen. Dadurch konnte der Strahl auf etwa 6×10 cm eingeengt werden.

Das Synchrophasotron war erst zur Beschleunigung von Protonen ausgelegt. Später wurden auch Kerne leichter Elemente wie Deuterium, Helium, Lithium, Kohlenstoff, Sauerstoff, Neon, Magnesium, Silizium, Schwefel und Xenon beschleunigt. Die Beschleunigung schwerer Ionen erwies sich jedoch bedingt durch mäßige Vakuumqualität und eine schwach gekoppelte Hochfrequenzbeschleunigung als ineffektiv.

Als Steuerungsrechner kamen in den 70er Jahren die Modelle EC1010 und VT1010B des Herstellers Videoton zum Einsatz.





Oskar-Klein-Medaille

Die Oskar-Klein-Gedächtnisvorlesung und die damit verbundene Oskar-Klein-Medaille ist ein schwedischer Preis in theoretischer Physik, der gemeinsam von der Universität Stockholm und dem Nobelkomitee der Königlich Schwedischen Akademie der Wissenschaften vergeben wird. Er ist nach Oskar Klein benannt.

Mit dem Titel der Vorlesung




Bent Herskind

Bent Herskind (* 14. Dezember 1931 in Kopenhagen) ist ein dänischer experimenteller Kernphysiker.

Herskind war seit 1956 am Niels-Bohr-Institut (NBI) in Kopenhagen als Forschungsassistent. Er entwickelte dort Elektronik für Experimente am Van-de-Graaff-Beschleuniger und danach am Tandem Beschleuniger (TAL, dann NBITAL). 1966 wurde er leitender Wissenschaftler (Amanuensis) am NBI. 1966/67 war er zu einem Forschungsaufenthalt an der University of Wisconsin–Madison. 1990 wurde er Dozent am NBI und 2002 ging er beim NBI offiziell in den Ruhestand, blieb aber weiter wissenschaftlich aktiv.

Er entwickelte mit Peter Twin den TESSA Detektor (Escape Suppression Detector), verwendet zur Beobachtung der Gammastrahl Kaskaden hochangeregter, schnell rotierender Kerne. Dazu müssen die abgegebenen Gammaquanten möglichst vollständig registriert werde, wozu möglichst viele kugelförmig um das Target angeordnete Detektoren nötig sind (Ballanordnung). TESSA war ab 1980 am NBI und die Fortentwicklungen TESSA 1 bis 3 in Daresbury. Später war er an NORDBALL (1985–1998, NBI) und EUROBALL (EB) beteiligt und war 1989 bis 1992 und 1997 Vorstand von dessen Leitungsgremium und 1993 bis 1996 von der Gruppe zur Datenanalyse der EB Experimente.

Er untersuchte unter anderem die Dipol-Riesenresonanz in heißen (das heißt hoch angeregten) Kernen, Dämpfung von Rotations- und Kollektiv-Anregungen in warmen Kernen, Superdeformierten Kernen und Kernen mit noch exotischeren Formen.

1971 wurde er Professor am NBI. 1974 wurde er an der Universität Kopenhagen promoviert (Ph. D.). 1990 wurde er Ehrendoktor der Universität Lund. 1990 wurde er Mitglied der Dänischen Akademie für Naturwissenschaft.

Er war Mitherausgeber von Hyperfine Interaction, Zeitschrift für Physik A (von 1985 bis 1995) und dessen Nachfolger European Physics Journal A (bis 1997). In den 1980er Jahren war er Berater am Lawrence Berkeley National Laboratory. Er war 1975 Gastprofessor an den Laboratorien in Chalk River (Kanada) und 1982 und 1987 am Oak Ridge National Laboratory.

2004 erhielt er mit Peter J. Twin den Lise-Meitner-Preis.




Piet Hartman

Piet Hartman (* 11. April 1922) ist ein niederländischer Kristallograph.

Hartman studierte Chemie an der Reichsuniversität Groningen und promovierte dort 1953 unter der Betreuung von Pieter Terpstra. Danach wurde er Professor an der geologischen Fakultät in Leiden. Nach dem Zusammenschluss der geologischen Fakultäten von Leiden und Utrecht wurde er 1978 Professor an der Universität Utrecht, wo er bis zu seinem Emeritat 1987 verblieb. 1981 wurde er zum Mitglied der Leopoldina gewählt.

Hartman beschäftigte sich mit der Beziehung von Kristallstruktur und Kristallmorphologie. Erste Theorien dazu gehen auf Auguste Bravais zurück. Nach Bravais bilden sich im Kristall diejenigen Flächen aus, die die größte Dichte an Gitterpunkten besitzen. Danach hängt die Kristallmorphologie also nur von der Gittersymmetrie (Bravais-Gitter) und den Gitterparametern ab. Spätere Theorien gaben an, dass auch Schraubenachsen und Gleitspiegelebenen einen Einfluss haben, also die gesamte Symmetrie der Raumgruppe.

Hartman und Perdok fanden heraus, dass die Kristallmorphologie von den intermolekularen Bindungen zwischen den Kristallbausteinen (Moleküle, Ionen) abhängt. Eine ununterbrochene Kette von starken intermolekularen Bindungen wird als Periodic Bond Chain bezeichnet und hat die größte Wachstumsrate. Kristallrichtungen, die am schnellsten wachsen, wachsen aus dem Kristall und sind somit verschwunden. Fast alle heutigen Programme zur Vorhersage der Kristallmorphologie beruhen auf der Hartman-Perdok-Theorie (Periodic-Bond-Chain-Theorie) oder Abwandlungen davon.






Stephan Grill

Stephan Wolfgang Grill (* 5. Mai 1974 in Heidelberg) ist ein deutscher Biophysiker.

Grill studierte an der Universität Heidelberg Physik und arbeitete dann am European Molecular Biology Laboratory (EMBL) in Heidelberg bei Anthony Hyman und Ernst Stelzer, was zu seiner Promotion über das Thema "The mechanics of asymmetric spindle positioning in the Caenorhabditis elegans embryo" 2002 führte (Promotion offiziell an der TU München). Als Post-Doktorand war er am Max-Planck-Institut für molekulare Zellbiologie und Genetik in Dresden und bei Carlos Bustamante am Lawrence Berkeley National Laboratory (und der Universität Berkeley). Ab 2006 war er Nachwuchsgruppenleiter am MPI für molekulare Zellbiologie und Genetik und am Max-Planck-Institut für Physik komplexer Systeme in Dresden. 2009 hatte er eine Vertretungsprofessur an der Universität Leipzig. 

Grill ist seit 2013 Professor für Biophysik und Biotechnologie an der Technischen Universität Dresden. Seit September 2018 ist er Wissenschaftliches Mitglied der Max-Planck-Gesellschaft und Direktor am Max-Planck-Institut für molekulare Zellbiologie und Genetik in Dresden.

Grill studiert mit seiner Gruppe die Rolle mechanischer Prozesse bei Zelldifferentiation (Meiose, studiert bei C. elegans – Verteilung der Chromosomen bei der Teilung über die Ausbildung des Spindelapparats und Aufteilung der Zellbestandteile über die Kontraktion eines feinen Netzwerks von Molekülen ähnlich denen beim Muskel) und ihr Zusammenhang mit molekularen Prozessen (z. B. über chemische Stoffgradienten gegeben). Dazu entwickelte er ein Verfahren, durch gezielte Durchtrennung zellulärer Strukturen mit Lasern mechanische Spannungen in der Zelle aufzuzeigen (lasergestützte nicht-invasive zelluläre Mikrochirurgie). Das Verfahren entwickelte er schon am EMBL. Im Fall der Untersuchung der Zellteilung wurden dazu mit gentechnischen Verfahren gezielt Teile des kontraktilen Netzes der Zelle gefärbt und so sichtbar und für den Laser gezielt durchtrennbar gemacht. Das Ausmaß des Auseinanderklaffens nach der Durchtrennung war ein Maß der Spannung.

In Berkeley begann er sein zweites Forschungsfeld, die direkte Beobachtung von Molekülen bei der Transkription von DNA in RNA. Dabei konnte er die Bewegung der dafür verantwortlichen RNA-Polymerase II am DNA-Strang verfolgen, die sich dabei nicht nur in eine Richtung bewegt, sondern auch vor- und zurückspringt. Das spiegelt Kontrollprozesse und Korrekturprozesse bei der Transkription wider, die die Fehlerquote senken.

2010 erhielt Grill den "EMBO Young Investigator Award". 2011 erhielt er den Paul-Ehrlich-und-Ludwig-Darmstaedter-Preis für Nachwuchswissenschaftler. 2015 wurde Grill mit dem Sackler-Preis für Biophysik ausgezeichnet, 2017 wurde er in die European Molecular Biology Organization gewählt.





Versorium

Das Versorium stellt eines der ersten neuzeitlichen elektrischen Messgeräte dar. Es ist eine sehr einfache Bauform eines Elektroskops und dient zur groben Bestimmung der elektrostatischen Feldverteilung. Es ist mechanisch ähnlich aufgebaut wie eine Kompassnadel, besteht aber im Gegensatz dazu nicht aus einer magnetischen Nadel, sondern reagiert durch Auslenkung einer metallischen Nadel auf das statische elektrische Feld. Es ist damit möglich, nahe an einem Körper zu unterscheiden, ob er statisch elektrisch aufgeladen ist oder nicht.

Konstruiert wurde das Versorium um das Jahr 1600 von dem englischen Arzt und Physiker William Gilbert, nachdem er sich mit den Schriften des italienischen Arztes Girolamo Fracastoro beschäftigt hatte.

Das Versorium besteht aus einer dünnen, unmagnetischen Nadel, ähnlich gestaltet wie eine Kompassnadel, welche zentrisch gelagert ist und sich um den Auflagepunkt sehr leicht drehen kann. Durch die Kraftwirkung auf elektrische Ladungen kommt es zu Influenz, also zur Verschiebung der elektrischen Ladungen in der metallischen Nadel. Aufgrund der elektrischen Kräfte, wie sie das Coulombsche Gesetz ausdrückt, kommt es bei Anwesenheit von elektrostatisch aufgeladenen Körpern somit zu einer Drehbewegung der Nadel in Richtung des elektrostatisch geladenen Körpers. Bei elektrisch ungeladenen Körpern in der näheren Umgebung kommt es zu keiner Kraftwirkung auf die Nadel und sie verbleibt in der bestehenden Position.

Die Symmetrie der Ladungsverschiebungen in der Nadel macht beide Nadelenden als Zeiger verwendbar. Eine Unterscheidung zwischen elektrisch positiven oder negativen geladenen Körpern ist mit dieser Anordnung nicht möglich.

Wegen der einfachen und anschaulichen Gestaltung werden Aufbauten ähnlich dem Versorium im schulischen Physikunterricht als Experimentier- und Bastelobjekt verwendet.

William Gilbert beschreibt in seiner Originalarbeit auch ein im Aufbau ähnliches sogenanntes magnetisches Versorium, dessen Nadel aus einem hartmagnetischen Werkstoff besteht und mit der Funktion einer Kompassnadel identisch ist. Die Kompassnadel oder das magnetische Versorium richtet sich nach einem äußeren Magnetfeld aus.

Durch die Verwendung des unmagnetischen und des magnetischen Versoriums unterschied Gilbert in seinem Werk eindeutig und als einer der ersten zwischen den Wirkungen des Magnetismus und den Wirkungen der Elektrostatik.





Piet Gros

Piet Gros (* 31. Juli 1962 in Dokkum) ist ein niederländischer Chemiker und Professor für biomolekulare Kristallographie an der Universität Utrecht. Er befasst sich mit Protein-Kristallographie mit Anwendungen auf große Moleküle in Zellmembranen und im Blutplasma und von Rezeptoren an Zelloberflächen.

Gros studierte Chemie an der Universität Groningen mit dem Diplom 1985 und der Promotion 1990 bei W. G. J. Hol über Protein-Kristallographie ("Studies in protein crystallography & dynamics : on membrane protein crystallization, the structure of thermitase-Eglin and the application of molecular dynamics"). Als Post-Doktorand war er an der ETH Zürich (bei F. W. van Gunsteren) und der Yale University (bei A. T. Brunger) und ab 1994 an der Universität Utrecht, an der er 2002 eine volle Professur erhielt. 2007 bis 2010 leitete er die Chemie-Fakultät. 2012 wurde er wissenschaftlicher Direktor des Bijvoet Center for Biomolecular Research.

Er klärte die dreidimensionale Struktur des Komplementfaktors C3. Damit konnte er auch die Wirkungsweise dieses Komplementfaktors des angeborenen Immunsystems erforschen und er erforschte auch den Wirkmechanismus des Komplementfaktors C8. Er erforscht auch verschiedene bakterielle Zelloberflächen-Proteine, die für deren Pathogenität wichtig sind, und Blutgerinnungsfaktoren (wie den Willebrand-Faktor).

2010 erhielt er den Spinoza-Preis und 2008 einen Advanced Grant des European Research Council. Er ist Mitglied der Niederländischen Akademie der Wissenschaften (2010) und wurde 2013 Ritter vom Orden des Niederländischen Löwen. Seit 2013 ist er EMBO Mitglied. Für 2018 wurde Gros der Gregori-Aminoff-Preis zugesprochen.






Konvektive Koordinaten

Konvektive Koordinaten sind krummlinige Koordinatensysteme auf dem euklidischen Raum formula_1, die an einen Träger gebunden sind und von allen Transformationen, die der Träger erfährt, mitgeführt werden, daher die Bezeichnung konvektiv. In der Kontinuumsmechanik ergeben sich konvektive Koordinaten auf natürliche Weise, wenn die Koordinatenlinien materielle Linien sind, die dann von allen Bewegungen und Deformationen des materiellen Körpers mittransponiert werden. Bildlich kann man sich ein Koordinatennetz auf eine Gummihaut aufgemalt denken, die dann gedehnt wird und das Koordinatennetz mitnimmt, siehe Abbildung rechts.

Praktische Bedeutung haben konvektive Koordinatensysteme in der Kinematik schlanker Strukturen (Stäbe, Balken) und dünnwandiger Strukturen (Schalen und Membranen), wo die Spannungen und Dehnungen parallel zu den Vorzugsrichtungen der Struktur interessieren. Außerdem können materielle Vorzugsrichtungen nicht isotroper Materialien, wie z. B. von Holz, in konvektiven Koordinaten beschrieben werden. In der Kinematik deformierbarer Körper bekommen die in der Kontinuumsmechanik benutzten Tensoren in konvektiven Koordinaten ausgedrückt besonders einfache Darstellungen.

Die Methode der konvektiven Koordinaten ist ein Spezialfall adaptiver Finite-Elemente-Methoden und wird wie diese in der numerischen Lösung von Advektions-Diffusions-Problemen verwendet (z. B. Schadstoffausbreitung in der Atmosphäre oder im Grundwasser).

Betrachtet wird ein deformierbarer Körper wie im Bild, der mittels Konfigurationen in einen euklidischen Vektorraum formula_2 abgebildet wird. Die konvektiven Koordinaten eines materiellen Punktes formula_3 werden durch die Referenzkonfiguration formula_4 zugewiesen. Für jedes Partikel formula_3 eines Körpers formula_6 sind seine konvektiven Koordinaten gegeben durch:

Diese Zuordnung ist vom gewählten Bezugssystem des Beobachters, von der Zeit und vom physikalischen Raum unserer Anschauung unabhängig. Für den viereckigen Körper im Bild eignet sich z. B. das Einheitsquadrat formula_8 als Bildbereich. formula_9 ist ein-eindeutig (bijektiv), so dass formula_10 auch der Benennung des Partikels formula_3 dienen kann. Weil die Koordinaten formula_10 an das Partikel gebunden sind, werden sie von jeder Bewegung des Partikels mitgenommen.

Die Bewegungsfunktion formula_13 beschreibt die Bewegung des Partikels formula_10 durch den Raum unserer Anschauung und liefert uns ein Objekt unserer Anschauung, weil diese Positionen vom Körper einmal eingenommen wurden. Die Bewegung startet zu einem bestimmten Zeitpunkt formula_15, in dem sich der Körper in der Ausgangskonfiguration befindet. Die Funktion

ordnet den Koordinaten formula_10 ein-eindeutig (bijektiv) einen Punkt formula_18 im Raum zu, den das Partikel zum Zeitpunkt formula_15 eingenommen hat. Der Vektor formula_18 hat materielle Koordinaten formula_21 bezüglich der Standardbasis formula_22. Wegen der Bijektivität kann

geschrieben werden. Variiert im Vektor formula_10 nur eine Koordinate formula_25, dann fährt formula_26 eine materielle Koordinatenlinie ab, die im allgemeinen Fall eine Kurve im Raum ist, siehe obere Abbildung rechts. Die Tangentenvektoren

an diese Kurven werden "kovariante" Basisvektoren des krummlinigen Koordinatensystems genannt. Die Richtung, in der sich die Koordinate formula_25 am stärksten ändert, sind die Gradienten

die die Basisvektoren formula_30 in einem materiellen Punkt darstellen. Wegen

sind die ko- und kontravarianten Basisvektoren dual zueinander und die kontravarianten Basisvektoren können aus

berechnet werden. Darin wurde das dyadische Produkt "formula_33" benutzt. In der Jacobi-Matrix formula_34 sind die kovarianten Basisvektoren formula_35 spaltenweise eingetragen und die kontravarianten Basisvektoren formula_30 finden sich in den Zeilen der Inversen formula_37.

Die ko- und kontravarianten Basisvektoren werden nur lokal (in den Tangentialräumen) im Punkt formula_18 als Basissystem für Vektor- und Tensorfelder, nicht aber für Ortsvektoren, benutzt: Die kovarianten Basisvektoren formula_35 bilden eine Basis des Tangentialraumes formula_40 und die kontravarianten Basisvektoren formula_30 bilden eine Basis des Kotangentialraumes formula_42 im Punkt formula_18, siehe untere Abbildung rechts.

Im Zuge der Bewegung entsteht in jedem Punkt und zu jedem Zeitpunkt formula_44 einen Satz kovarianter Basisvektoren formula_45 und kontravarianter Basisvektoren formula_46, die die Tangenten bzw. Gradienten der materiellen Koordinatenlinien im deformierten Körper zur Zeit formula_47 sind. Sie sind mithin Basen der Tangentialräume formula_48 bzw. formula_49.

Die Differentialoperatoren Gradient (grad), Divergenz (div) und Rotation (rot) aus der Vektoranalysis können mit dem Nabla-Operator formula_50 definiert werden. In konvektiven Koordinaten hat der Nabla-Operator in der Lagrange’schen Fassung die Form:

Die Gradienten von Skalar- und Vektorfeldern werden mit ihm wie folgt dargestellt:

Die Divergenzen werden aus dem Skalarprodukt mit formula_52 erhalten:

Die Rotation eines Vektorfeldes entsteht mit dem Kreuzprodukt:

Entsprechende Operatoren formula_54, formula_55 und formula_56 für Felder in der Euler’schen Fassung liefert der Nabla-Operator

Der Einheitstensor formula_58 bildet jeden Vektor auf sich selbst ab. Bezüglich der ko- und kontravarianten Basisvektoren lauten seine Darstellungen:

Die Skalarprodukte der kovarianten Basisvektoren

heißen "kovariante Metrikkoeffizienten" (des Tangentialraumes formula_40). Entsprechend sind die Skalarprodukte der kontravarianten Basisvektoren

"kontravariante Metrikkoeffizienten" (des Kotangentialraumes formula_42).

In der Euler’schen Betrachtungsweise ist entsprechend

mit den ko- und kontravarianten Metrikkoeffizienten formula_65 bzw. formula_66 (des Tangentialraumes formula_48 bzw. Kotangentialraumes formula_49).

In konvektiven Koordinaten ausgedrückt bekommt der Deformationsgradient formula_69 eine besonders einfache Form. Der Deformationsgradient bildet gemäß seiner Definition die Tangentenvektoren an materielle Linien in der Ausgangskonfiguration auf die in der Momentankonfiguration ab und diese Tangentenvektoren sind gerade die kovarianten Basisvektoren formula_35 bzw. formula_45. Also ist

Das ergibt sich auch aus der Ableitung der Bewegungsfunktion formula_73 :

In dieser Darstellung lässt sich auch sofort mit

die Inverse des Deformationsgradienten angeben. Der transponiert inverse Deformationensgradient bildet die kontravarianten Basisvektoren aufeinander ab:

Die materielle Zeitableitung des Deformationsgradienten ist der "materielle Geschwindigkeitsgradient"

denn die Ausgangskonfiguration hängt nicht von der Zeit ab und das gilt dann auch für die Basisvektoren formula_35 und formula_30. Der "räumliche Geschwindigkeitsgradient" formula_80 bekommt in konvektiven Koordinaten die einfache Form

worin formula_82 die Geschwindigkeit eines Partikels am Ort formula_83 zur Zeit formula_47 ist. Der räumliche Geschwindigkeitsgradient transformiert die Basisvektoren in ihre Raten:

Die folgenden Tensoren treten in der Kontinuumsmechanik auf. Ihre Darstellung in konvektiven Koordinaten ist in der Tabelle zusammengestellt.

Weil der rechte Cauchy-Green Tensor formula_86, der Green-Lagrange-Verzerrungstensor formula_87 und der Euler-Almansi-Tensor formula_88 in ihrer (hier angegebenen) natürlichen Form mit den kovarianten Komponenten formula_89 bzw. formula_90 gebildet werden, werden diese Tensoren üblicher Weise als "kovariante Tensoren" bezeichnet. Die Spannungstensoren formula_91 und formula_92 sind entsprechend "kontravariante Tensoren".

Objektive Größen sind solche, die von bewegten Beobachtern in gleicher Weise wahrgenommen werden. Die Zeitableitung von Tensoren ist im allgemeinen nicht objektiv. Die konvektiven ko- bzw. kontravarianten Oldroyd-Ableitungen objektiver Tensoren sind jedoch objektiv. Sie sind definiert über

Kovariante Oldroyd-Ableitung, z. B. von formula_93:

Kontravariante Oldroyd-Ableitung, z. B. von formula_95:

Daraus leiten sich auch die Bezeichnungen "konvektiv kovariant" bzw. "konvektiv kontravariant" der Oldroyd-Ableitungen ab. Bemerkenswert sind die übereinstimmenden Transformationseigenschaften der kovarianten Tensoren

sowie der kontravarianten Tensoren

Siehe auch den Abschnitt Objektive Zeitableitungen im Artikel zum Geschwindigkeitsgradient.

Ein Parallelogramm mit Grundseite und Höhe formula_101 und Neigungswinkel formula_102 wird zu einem flächengleichen Quadrat verformt, siehe Bild. Als Referenzkonfiguration eignet sich das Einheitsquadrat

In der Ausgangskonfiguration haben die Punkte des Parallelogramms die Koordinaten:

Die kovarianten Basisvektoren sind

Sie stehen spaltenweise in der Jacobimatrix formula_34 und die kontravarianten Basisvektoren entspringen den Zeilen der Inversen der Jacobimatrix:

In der Momentankonfiguration ist formula_109:

und die konvektiven ko- und kontravarianten Basisvektoren bilden die Standardbasis

Der Deformationsgradient

ist ortsunabhängig und hat die Determinante eins, was die Erhaltung des Flächeninhalts differentialgeometrisch nachweist. Die kovarianten Metrikkoeffizienten lauten

Damit lautet der Green-Lagrange-Verzerrungstensor:

Kontinuumsmechanik:

Mathematik:





German Young Physicists’ Tournament

Das German Young Physicists’ Tournament (GYPT) ist ein bundesweiter Physik-Wettbewerb, bei dem Teams gegeneinander antreten und ihre Lösung zu vorgegebenen, physikalischen Problemen auf Englisch präsentieren. Anschließend muss die eigene Lösung und Bearbeitung in einer zeitlich begrenzten Diskussion jeweils gegen ein gegnerisches Team verteidigt werden.

Das GYPT ist der nationale Abkömmling des International Young Physicists’ Tournament (IYPT),

Beim jährlich stattfindenden nationalen Wettbewerb wird ein Gewinnerteam gekürt. Unabhängig davon werden je zehn Workshop-Teilnehmer ermittelt, von denen in einer weiteren Workshoprunde fünf Teilnehmer ausgewählt werden, die als Team Deutschland beim IYPT vertreten. Im Vorfeld des Turniers erhalten die Schüler Betreuung und Unterstützung an einem der deutschlandweiten GYPT-Zentren.

Seit dem Jahr 2014 wird das GYPT in Deutschland jährlich durchgeführt. Zu Beginn wurde das GYPT maßgeblich vom Schülerforschungszentrum Südwürttemberg organinisert. Mittlerweile sind Träger und Organisator die Universität Ulm, die Deutsche Physikalische Gesellschaft und die Wilhelm und Else Heraeus-Stiftung.

Ablauf und Dauer sind im Vergleich zum IYPT deutlich abgespeckt. Vom internationalen "Physics-Fight" wurden der "Report" und die "Opposition" übernommen. Bepunktet werden die Teilnehmer wie beim IYPT von einer mehrköpfigen Jury bestehend aus Wissenschaftlern, Lehrern, Studenten und ehemaligen Teilnehmern. Üblicherweise findet das GYPT an einem Wochenende statt, während die Fights am Samstag und das Finale sonntags ausgetragen werden. Die drei besten Teams (Punkte der Einzelteilnehmer mit Streichergebnis gemittelt) treten im Finale gegeneinander um den Titel des "Deutschen Physik-Meisters" an. Separat zum Teamwettbewerb werden die besten zehn Teilnehmer ermittelt und zu einem Workshop eingeladen. Dort wird unter Anleitung ausgewählter Betreuer an neuen Projekten gearbeitet und die Schüler werden auf physikalisches Wissen, Team- und Kommunikationsfähigkeit getestet. Die besten Fünf fahren anschließend zum IYPT, während die Anderen die Chance zur Teilnahme am Austrian Young Physicists’ Tournament (AYPT) erhalten. Dieses Turnier konnte bei der ersten Teilnahme im Jahr 2015 gewonnen werden Im Jahr darauf konnte dieser Titel verteidigt werden.
Das komplette Regelwerk sowie eine detaillierte Beschreibung des Ablaufs lassen sich auf der Website finden.

Im Jahr 2014 fand das Turnier am Schülerforschungszentrum Nordhessen in Kassel und 2015 am Josef-Effner-Gymnasium Dachau statt. Die Final- und Workshopteilnehmer durften im Januar 2016 als Preis das CERN besuchen.
Das 3. GYPT 2016 am Physikzentrum Bad Honnef war mit 86 Teilnehmern bis zu diesem Zeitpunkt das größte seiner Art.

2017 und 2018 wurden die Anmelderekorde jeweils von Neuem geknackt, sodass für den Wettbewerb 2017 erstmals ein Vorentscheid in Regionalwettbewerben sowie eine Online-Wettbewerbsauswahl eingeführt wurde, die beide seitdem weiter bestehen. Die Teams müssen sich also zunächst auf regionaler Ebene für die nationale Wettbewerbsrunde qualifizieren.

Bei den 17 Problemen handelt es sich um die international festgelegten Aufgaben des IYPT. Für das GYPT sucht sich jeder Teilnehmer eines dieser Probleme aus, welches er am Turnier präsentieren wird. Pro Diskussion treten jeweils zwei Dreierteams zufällig gegeneinander an. Daher wird erst am Abend vor dem Turnier bekanntgegeben, welches Team welches Problem und welche gegnerische Gruppe opponieren wird.

Die Schüler schließen sich üblicherweise im Vorfeld zu Teams aus zwei oder drei Personen zusammen, jedoch werden auch Einzelteilnehmer akzeptiert, welche unvollständigen Teams zugeordnet werden. Das Alter der Teilnehmer liegt aufgrund der komplexen Probleme und der Präsentation auf Englisch zwischen 15 und 18 Jahren. Jedoch gibt es regelmäßig Teilnehmer, die deutlich jünger erfolgreich am Wettbewerb mitmachen
. Seit dem Wettbewerb 2018 existiert ein Mindestalter von 14 Jahren.

Die Anzahl der sich bewerbenden Teams steigt stetig an.






Mychajlo Bilyj

Mychajlo Uljanowytsch Bilyj (* 12. November 1922 in Moskali (), Gouvernement Tschernigow, Ukrainische SSR; † 5. August 2001 in Kiew, Ukraine) war ein ukrainisch-sowjetischer Politiker, Physiker und Rektor der Taras-Schewtschenko-Universität Kiew.

Mychajlo Bilyj nahm als Soldat der Roten Armee am Zweiten Weltkrieg teil, studierte anschließend Physik an der Taras-Schewtschenko-Universität, wurde 1964 Doktor der physikalisch-mathematischen Wissenschaften und ein Jahr später Professor. Als Spezialist der Experimentalphysik, Optik und Spektroskopie, insbesondere der Lumineszenz anorganischer Substanzen leitete er von 1963 bis 1993 die Abteilung für Experimentalphysik, Festkörperoptik und Optik, von 1962 bis 1970 war er Dekan der Physikalischen Fakultät und von Dezember 1970 bis Dezember 1985 Rektor der Taras-Schewtschenko-Universität in Kiew. Von 1969 an war Bilyj korrespondierendes Mitglied der Ukrainischen Akademie der Wissenschaften. Er hat mehr als 350 wissenschaftliche Arbeiten veröffentlicht.

Bilyj war auch als Politiker tätig und als solcher Mitglied des Zentralkomitees der KPU, Abgeordneter der Werchowna Rada der Ukrainischen SSR sowie zwischen 1972 und 1980 deren Vorsitzender.
Mychajlo Bilyj starb in Kiew und wurde auf dem dortigen Baikowe-Friedhof beerdigt.

Mychajlo Bilyj erhielt zahlreiche Titel, Orden und Ehrungen. Darunter:






Fast Auroral Snapshot Explorer

Der Fast Auroral Snapshot Explorer (FAST) ist ein Forschungssatellit der NASA, der die Plasmaphysik der an den beiden Polen der Erde auftretenden Polarlichter erforscht hat. Der Satellit wurde im Goddard Space Flight Center der NASA gebaut und am 21. August 1996 mit einer flugzeuggestützten Pegasus-XL-Rakete gestartet. Da der Satellit zum Small-Class-Explorer-Programm gehört, wird er auch als SMEX 2 bzw. Explorer 70 bezeichnet.

Nach dem Start trat FAST in einen elliptischen Erdorbit mit einem Perigäum von 350 km und einem Apogäum von 4175 km und einer Bahnneigung von 83° ein. Für eine Erdumrundung braucht FAST 133 min. Der Satellit durchfliegt die Polarlichtzone, die etwa bei 65° nördlicher und südlicher Breite liegt, vier mal pro Orbit und sammelte nur währenddessen Daten. FAST ist 0,93 m hoch, hat einen Durchmesser von 1,02 m und eine Masse von 191 kg, wovon 65 kg auf die wissenschaftlichen Instrumente entfallen. Er ist spinstabilisiert mit 12 Umdrehungen in der Minute. Der inaktive Satellit befindet sich weiterhin in der Umlaufbahn. (Stand: Mai 2018)





Born-Koordinaten

Die Born-Koordinaten beschreiben in der relativistischen Physik eine Karte für einen Teil des flachen Minkowski-Raumes der speziellen Relativitätstheorie, den räumlichen Zylinder mit formula_1 Das entsprechende Linienelement mit der Signatur -2, also formula_2 den natürlichen Einheiten formula_3 und der Winkelgeschwindigkeit formula_4 ist
Die Born-Koordinaten werden für die mathematische Analyse der Physik von sogenannten Langevin-Beobachtern benutzt, die auf einem Ring mit konstantem Abstand zum Drehmittelpunkt rotierender starrer Scheiben ruhen (siehe Ehrenfestsches Paradoxon). Die erstmalige Beschreibung dieser Koordinaten erfolgte im Zusammenhang mit Max Borns (1909) relativistischer Physik der starren Körper, die für rotierende Körper unter anderem von Gustav Herglotz (1909) weiterentwickelt wurde. Für einen allgemeinen Überblick zu Beschleunigungen in der Minkowski-Raumzeit, siehe Beschleunigung (Spezielle Relativitätstheorie).

Ideale ruhende Uhren auf der rotierenden Scheibe lassen sich nicht widerspruchsfrei synchronisieren. Nicht auf einem Ring mit konstantem Durchmesser auf dem die Uhren zumindest gleich schnell laufen, schon gar nicht auf der ganzen Scheibe, auf der die Uhren, je weiter sie von der Drehachse entfernt sind, um den Faktor formula_7 langsamer laufen (Zeitdilatation). Das heißt, dass die Uhren entlang eines Radius mit formula_8 nicht gleich schnell laufen, sondern im Vergleich mit einer Referenzuhr im Drehmittelpunkt formula_9 mit wachsendem formula_10 langsamer werden.

Da die Born-Koordinaten einen Teil des flachen Minkowski-Raums beschreiben, lassen sich die Uhren wie dort synchronisieren, indem die Uhr im Drehmittelpunkt (Uhr formula_11 im Rotationszentrum) als Referenz dient und z. B. im Sekundentakt ein Lichtsignal aussendet, mit dem alle Uhren auf der rotierenden Scheibe synchronisiert werden (siehe Abb. 2). Die Empfängeruhren formula_12, formula_13 und formula_14 kennen die jeweilige Laufzeit des Signals vom Drehmittelpunkt und justieren ihre Uhrzeit entsprechend. Das heißt, dass alle Beobachter, die sich auf der Drehachse befinden, Lichtsignale, die von den Uhren auf der Scheibe im "Sekunden"takt abgesendet werden, im Sekundentakt empfangen werden. Werden die Signale von Uhren auf einem Ring mit konstantem Radius gesendet, werden sie darüber hinaus "gleichzeitig" empfangen. Solcherart synchronisiert, sind die Uhren in formula_13, formula_14, formula_12 und formula_11 synchron mit der Koordinatenzeit formula_19 des Inertialsystems, in dem die Drehachse der rotierenden Scheibe ruht.

Für den Langevin-Beobachter (Uhr formula_12) ist die Situation jedoch nicht zufriedenstellend. Für ihn ergeben sich zwei Probleme. Erstens läuft seine so synchronisierte Uhr zu schnell im Vergleich zu einer Uhr, die er auf die Standard-SI-Sekunde geeicht hat. Zweitens macht er die irritierende Feststellung, dass zwei Lichtsignale, die von den beiden gleich weit entfernten Uhren formula_13 und formula_14 auf seinem Ring "gleichzeitig" ausgesendet werden, "nicht gleichzeitig" bei ihm eintreffen. Da er weiß, dass die Uhren formula_13 und formula_14 gleich weit von seiner Uhr formula_12 entfernt sind, gibt es nur zwei Möglichkeiten. Entweder ist die Lichtausbreitung nicht isotrop – Licht von formula_13 in Drehrichtung ist früher bei ihm (die Geschwindigkeit des Lichts wäre daher schneller als formula_27), als Licht von formula_14 entgegen der Drehrichtung (die Geschwindigkeit des Lichts wäre daher langsamer als formula_27). Oder die Uhren formula_13 und formula_14 sind mit seiner Uhr formula_12 nicht synchron.

Um sie aus seiner Sicht zu synchronisieren geht er wie im Inertialsystem vor. Er eicht die Uhren in formula_13, formula_14 und formula_12 auf die SI-Sekunde und synchronisiert die Uhren formula_13 und formula_14 mit seiner Uhr formula_12 mittels Einstein-Synchronisation (eine interne Synchronisation). Er stellt fest, dass er die Uhr formula_13, die in Drehrichtung liegt, zurückstellen muss und die Uhr formula_14, die entgegen der Drehrichtung liegt, vorstellen muss. Zwei Lichtsignale, die jetzt "gleichzeitig" von formula_13 und formula_14 ausgesendet werden, treffen "gleichzeitig" bei formula_12 ein (sie treffen aber "nicht gleichzeitig" bei der Uhr formula_11 ein). Synchronisiert man alle Uhren entlang des Ringes auf diese Art und Weise, ergibt sich neuerlich ein Problem. Zumindest zwei benachbarte Uhren zeigen nicht die gleiche Zeit. In Summe weisen die Uhren entlang des Rings mit dem Radius "r" eine Zeitdifferenz formula_45 auf.

Außerdem stellt der Langevin-Beobachter fest, dass, egal wie er die Distanzen seiner lokalen Umgebung vermisst, die Geometrie nicht-euklidisch ist und am besten durch eine Riemannsche Metrik, die sogenannte Langevin-Landau-Lifschitz Metrik beschrieben wird. Diese Metrik wiederum wird sehr gut durch die Metrik der hyperbolischen Ebene angenähert. Bei der Vermessung großer Distanzen über die lokale Umgebung hinaus hängen die Ergebnisse von der Messmethode ab, die sich nicht mit den Eigenschaften einer Riemann Metrik beschreiben lassen. Die mit der Radarmethode gemessenen großen Distanzen sind nicht einmal symmetrisch. Die gemessene Entfernung von formula_12 nach formula_11 ist nicht gleich der gemessenen Entfernung von formula_11 nach formula_12. Das heißt, dass die Geometrie der rotierenden Scheibe weder eine euklidische noch eine riemannsche Geometrie ist.

Die rotierende Scheibe ist kein Paradoxon. Egal welche Methode der Langevin-Beobachter verwendet um seine lokale Umgebung zu analysieren: er stellt fest, dass er sich in einem rotierenden Bezugssystem befindet und nicht in einem Inertialsystem.

Für die Herleitung der Born-Koordinaten ist es sinnvoll, die Langevin-Beobachter zuerst in Zylinderkoordinaten darzustellen. Ihre Weltlinien bilden eine , die als "starr" angesehen werden kann, da der entsprechende verschwindet (siehe unten). Die Langevin-Beobachter rotieren um die Symmetrieachse des Zylinders.

Aus dem Linienelement des Minkowskiraumes in Zylinderkoordinaten
können unmittelbar die (Vektorfelder der Basisvektoren) des lokalen Bezugssystems für stationäre Beobachter "abgelesen" werden:
formula_52 ist ein zeitartiges Vektorfeld. formula_53, formula_54 und formula_55 sind raumartige Vektorfelder.

Mit einem Lorentz-Boost der Vektorfelder der Basisvektoren mit formula_56 in Richtung formula_55 (Anwendung der Lorentz-Transformation auf formula_55 und formula_52) ergeben sich die Vektorfelder der Basisvektoren für die Langevin-Beobachter mit
Diese Vektorfelder wurden erstmals (implizit) von Paul Langevin 1935 verwendet. Eine ausführliche Beschreibung erfolgte durch Thomas A. Weber erst 1997. Definiert sind diese Vektorfelder im Gegensatz zu den Zylinderkoordinaten mit formula_63 im Bereich formula_64. Diese Einschränkung ist fundamental, da sich die Geschwindigkeit der Langevin-Beobachter nahe der Grenze formula_65 der Lichtgeschwindigkeit nähert.
Jede integrale Kurve des zeitartigen Vektorfeldes formula_66 wird in Zylinderkoordinaten als wendelförmige Kurve mit konstantem Radius abgebildet (siehe die rote Kurven in Abb. 1 und Abb. 3) und repräsentiert einen in Bezug auf die rotierende Scheibe ruhenden Langevin-Beobachter. Weitere Langevin-Beobachter, die sich mit dem ersten auf einem Ring mit dem Radius formula_67 und der Winkelgeschwindigkeit formula_4 befinden, sollten auf der integralen Kurve des Vektorfeldes formula_69 (siehe die blaue Kurve in Abb. 3) "gleichzeitig" (synchron) sein. Jedoch zeigt sich, dass sich ideale Uhren entlang der blauen Kurve mittels Einstein-Synchronisation nicht synchronisieren lassen, da sich die blaue Kurve an unterschiedlichen Stellen mit der roten Kurve schneidet. Zumindest zwei benachbarte Uhren zeigen unterschiedliche Zeiten, da die blaue Kurve "gleichzeitig" ist, zwei unterschiedliche Stellen auf der roten Kurve aber einen zeitlichen Abstand haben – die blaue Kurve ist keine geschlossene Kurve in der Raumzeit und die benachbarten Uhren sie sind nicht synchron. Das ist der erste Hinweis darauf, dass sich für die rotierende Scheibe keine zufriedenstellende Beschreibung einer "räumlichen Geometrie" finden lässt, auf der alle Ereignisse gleichzeitig sind. Weder für die ganze rotierende Scheibe, noch für einen rotierenden Ring mit Radius formula_67.

Im Gegensatz dazu, ist die Projektion der wendelförmigen Weltlinie eines Langevin-Beobachters auf die räumliche Hyperebene formula_71 ein Kreis – also eine geschlossene Kurve in der Raumzeit. Darüber hinaus ist das Vektorfeld der Basisvektoren formula_72 ein raumartiges Killingvektorfeld, dessen integrale Kurven geschlossene Kurven im Raum sind (de-facto Kreise). Sie degenerieren für formula_73 zu Kurven mit der Länge Null. Das entspricht der Vorstellung der zylindrischen Symmetrie und der Vorstellung eines rotierenden Langevin-Beobachters. Diese Vorstellung basierend auf formula_71 lässt sich jedoch nicht realisieren, da die idealen Uhren auf dem Ring nicht widerspruchsfrei synchronisiert werden können und damit formula_71 (Gleichzeitigkeit) keinen Sinn ergibt, wenn keine eindeutige Synchronisation der Uhren möglich ist.

Die des zeitartigen Vektorfeldes formula_66 ist
Sie zeigt radial zur Drehachse und hängt nur vom konstanten Radius formula_67 und der konstanten Winkelgeschwindigkeit formula_4 der wendelförmigen Weltlinie des Langevin-Beobachters ab (Zentripetalbeschleunigung). Mit der kinematischen Dekomposition (siehe Viererbeschleunigung oben) ergeben sich der Expansionstensor und der Wirbelvektor. Der Expansionstensor ist Null. Das heißt, dass die Abstände der Langevin-Beobachter zueinander und zur Drehachse konstant bleiben ("starren" Scheibe). Der Wirbelvektor ergibt sich mit
und ist parallel zur Drehachse. Das heißt, dass die Langevin-Beobachter um die Drehachse rotieren. Das heißt aber auch, dass die Langevin-Beobachter um ihre "eigene" Achse rotieren. Das entspricht der Vorstellung einer "lokalen Verwirbelung". In Abb. 4 zeigt die cyanfarbene Kurve, wie sich die raumartigen Vektorfelder formula_81 um das Vektorfeld formula_82 "winden" (formula_82 – die formula_84-Koordinate – ist in der Abb. 4 durch formula_52 – die Zeit – ersetzt, da die formula_84-Koordinate hier keine Rolle spielt). Die lokalen Bezugssysteme der Langevin-Beobachter rotieren also auch und sind keine Inertialsysteme und ihre formula_87 sind mit dem radial Basisvektor formula_88 ausgerichtet. Diese "lokale" Rotation von formula_89 und formula_69 um die Achse formula_82 kann durch eine konstante lokale Gegenrotation aufgehoben werden.

Um die Born-Koordinaten zu erhalten, werden die wendelförmigen Weltlinien der Langevin-Beobachter mittels der Transformation
„gestreckt“ und es ergibt sich das neue Linienelement
Wegen des Mischterms formula_95 sind die Born-Koordinaten kein .

Die Weltlinien der Langevin-Beobachter sind jetzt gerade vertikale Linien. In Born-Koordinaten sind die Vektorfelder der Basisvektoren der Langevin-Beobachter

Die wendelförmigen Weltlinien der Langevin-Beobachter in Zylinderkoordinaten sind in Born-Koordinaten gerade Linien. Jedoch sind die geraden Weltlinien der statischen Beobachter in Zylinderkoordinaten, die neben der rotierenden Scheibe ruhen, in Born-Koordinaten wendelförmige Weltlinien. Im Gegensatz zu den Zylinderkoordinaten, sind nicht nur die Vektorfelder der Basisvektoren der Langevin-Beobachter auf den Bereich formula_99 beschränkt, sondern die Born-Koordinaten insgesamt.

Die kinematische Dekomposition der zeitartigen Kongruenz formula_100 liefert die gleichen Ergebnisse wie zuvor, jedoch mit den neuen Born-Koordinaten. Der Beschleunigungsvektor ist
der Expansionstensor ist Null und der Wirbelvektor ist

Das zu einem zeitartigen Einheitsvektorfeld duale Kovektorfeld definiert in jedem lokalen Bezugssystem eine infinitesimale räumliche Hyperebene. Ob diese infinitesimalen räumlichen Hyperebenen zu einer globalen räumlichen Hyperebene "verbunden" werden können, die überall zur Kongruenz der Weltlinien orthogonal ist, wird durch die Bedingung für die vollständige Integrierbarkeit bestimmt. Es zeigt sich, dass diese Bedingung genau dann und nur dann erfüllt ist, wenn der Wirbeltensor Null ist. Für den statischen Beobachter in Zylinderkoordinaten existieren solche räumliche Hyperflächen für die formula_71 gilt, für den Langevin-Beobachter jedoch nicht. Insbesondere sind die räumlichen Hyperflächen formula_104 in Born-Koordinaten orthogonal zu den Weltlinien der statischen Beobachter, nicht zu den Weltlinien der Langevin-Beobachter.

Dies wird deutlich, wenn die Integralkurven des Langevin-Vektorfeldes
die durch den Radius formula_106 verlaufen, dargestellt werden. Diese Kurven liegen in der Fläche
(siehe Abb. 5). Die für die Darstellung unwesentliche Koordinate formula_108 wurde durch die Koordinate formula_19 ersetzt. Es liegt nahe, diese Fläche als räumliche „Gleichzeitigkeitsfläche“ für Langevin-Beobachter zu interpretieren. Dies ist aus zwei Gründen nicht möglich. Erstens ist die Bedingung für die vollständige Integrierbarkeit nicht erfüllt, sodass es keine räumliche Hyperfläche gibt zu der die Vektorfelder formula_110 tangential sind. Das Vektorfeld formula_89 ist nur für formula_112 tangential. Das ist ein infinitesimales (lokales) Problem. Zweitens wird in Abb. 5 ersichtlich, dass die so konstruierte räumliche Hyperebene eine Unstetigkeit aufweist, einen "Sprung" (siehe die blauen Gitterlinien Abb. 5). Das ist ein globales Problem und eine Konsequenz daraus, dass sich die Uhren der Langevin-Beobachter nicht widerspruchsfrei synchronisieren lassen. Weder auf der ganzen rotierenden Scheibe, noch auf einem Ring mit konstantem formula_113.

Die Nullgeodäten in Zylinderkoordinaten ergeben sich aus den Geodätengleichungen
Die ersten Integrale für formula_115 lassen sich unmittelbar angeben mit
Wird das in den Ausdruck für das Linienelement der Zylinderkoordinaten eingesetzt und wird außerdem formula_117 (für die Nullgeodäte) gesetzt, so ergibt sich
Daraus ergibt sich für den minimalen Radius "R" einer Nullgeodäte
und weiter
Damit ist für formula_122 das erste Integral ebenfalls gefunden.

Die Lösung der Nullgeodäten als Kurven mit dem affinen Parameter "λ" ist (siehe Abb. 6)

Die Trajektorien der Nullgeodäten, also die Spuren ihrer „Projektion“ in die räumliche Hyperebene formula_71 sind im Minkowski-Raum mit Zylinderkoordinaten natürlich Geraden und gegeben durch

Für "radiale Nullgeodäten" ist formula_126. Außerdem gilt mit formula_127 in die Gleichungen oben eingesetzt für auslaufende radiale Nullgeodäten
formula_130 ist der Abstand des Langevin-Beobachters vom Rotationszentrum (siehe Abb. 7). Werden diese Gleichungen in Born-Koordinaten transformiert, ergibt sich für den auslaufenden Lichtstrahl
In Born-Koordinaten ist diese Trajektorie keine Gerade (siehe die grüne Kurve in der Abb. 7). Wie im Abschnitt Transformation in Born-Koordinaten gezeigt, handelt es sich in Born-Koordinaten bei diesen Trajektorien streng genommen nicht um eine Projektion in eine räumliche Hyperebene, da eine solche für formula_104 nicht definiert ist (siehe Abb. 5).

Für einlaufende radiale Nullgeodäten ergibt sich
dargestellt als rote Kurve in Abb. 7.

Um einen Laserimpuls zum stationären Beobachter S bei "r" = 0 zu senden, muss der Langevine-Beobachter bei "r" = "R" "voraus" zielen (für ihn bewegt sich S gegen den Uhrzeigersinn von rechts nach links). Das gleiche gilt für den Beobachter S, der einen Laserimpuls zum Beobachter L schicken will. Für "ω" > 0 sind die ein- und auslaufenden Nullgeodäten unterschiedliche Kurven in der Raumzeit mit unterschiedlichen Trajektorien.

Nullgeodäten zwischen Langevin-Beobachtern auf dem Ring mit "r" = "R" sind für "ω" > 0 ebenfalls nach innen oder nach außen gebogen. Um dies zu sehen, werden die Gleichungen für die in Rotationsrichtung laufenden Nullgeodäten in Zylinderkoordinaten in der Form
geschrieben. Die Transformation in Born-Koordinaten ergibt
oder nach "r" und "ϕ" aufgelöst
Diese Trajektorie ist für "ω" > 0 tatsächlich nach innen gebogen (siehe die grüne Kurve in Abb. 7).
Für Nullgeodäten gegen die Rotationsrichtung (siehe die rote Kurve in Abb. 7) ergeben sich die Kurven
deren Trajektorien nach außen gebogen sind. Für die Langevin-Beobachter gilt, dass sie um sich Laserimpulse zu senden "vorhalten" müssen.

Damit sind die Betrachtungen zu Nullgeodäten abgeschlossen, denn eine Nullgeodäte ist entweder radial oder hat einen minimalen Radius "r".

Für beschleunigte Beobachter gibt es selbst im einfachsten Fall des flachen Minkowski-Raumes verschiedene Möglichkeiten zur Entfernungsmessung, die sich als operational sinnvoll erweisen. Von diesen ist die Radardistanz die einfachste. Misst ein stationärer Beobachter C im Drehzentrum der Scheibe "R" = "r" = 0 die Laufzeit zu einem Langevin-Beobachter A auf einem Ring mit dem Radius formula_142, so erhält er als Ergebnis formula_142. Für den Langevin-Beobachter A ist die Situation anders. Er misst eine etwas kürzere Laufzeit, da seine Uhr im Vergleich zur Uhr des stationären Beobachters C um den Faktor formula_144 zeitdilatiert ist. Er erhält als Ergebnis seiner Messung formula_145. Bereits dieser einfachen Fall zeigt sich schwierig. Die Beobachter A und C sind sich über ihren Abstand nicht einig. Die Radarmessung liefert kein symmetrisches Ergebnis.

Im Vergleich dazu liefert die Radarmessung zwischen zwei Langevin-Beobachtern A und B auf dem Ring mit dem Radius formula_142 ein symmetrisches Ergebnis. Der Beobachter A misst den Abstand zu B gleich wie der Beobachter B den Abstand zu A (siehe Abb. 9). Die Laufzeiten zwischen A und B können mit den Gleichungen für die allgemeinen Nullgeodäten ermittelt werden. Mit dem Winkel ΔΦ zwischen A und B ergibt sich für die Laufzeit Δ"t" (in Koordinatenzeit) von A nach B
und für die Laufzeit Δ"t" (in Koordinatenzeit) von B nach A
Die beiden Laufzeiten können mit diesen nichtlinearen Gleichungen numerisch berechnet werden.
Die Radardistanz unter Berücksichtigung der Zeitdilatation ist damit

Trotz der dargestellten Schwierigkeiten bei der Bestimmung von Radardistanzen im Großen lässt sich beispielsweise mit Märzke-Wheeler-Koordinaten für einen bestimmten Langevin-Beobachter eine Gleichzeitigkeitsfläche konstruieren, mit deren Hilfe sich große Radardistanzen messen lassen.

Wie im Abschnitt Transformation in Born-Koordinaten gezeigt, lassen sich mit Born-Koordinaten keine „Gleichzeitigkeitsflächen“ festlegen. Jedoch ist die zeitartige Kongruenz der Langevin-Beobachter stationär und ihre Weltlinien können durch Punkte ersetzt werden. Dadurch wird ein Quotientenraum des Bereichs formula_64 des Minkowskiraums gebildet. Dieser Raum ist eine dreidimensionale topologische Mannigfaltigkeit, die sich durch eine Riemannsche Metrik in eine dreidimensionale Riemannsche Mannigfaltigkeit wandeln lässt, mit der sich einfach operational Abstände finden lassen.

Ausgangspunkt ist das Linienelement in Born-Koordinaten
Wird formula_153 gesetzt und nach formula_154 aufgelöst, ergeben sich die Lichtlaufzeiten hin und zurück
Und damit die lokale (infinitesimale) Radardistanz als arithmetischer Mittelwert der Lichtlaufeigenzeit
Somit hat der Quotientenraum das Riemannsche Linienelement
das dem Abstand zweier benachbarter Langevin-Beobachter mit infinitesimalem Abstand entspricht. Diese Metrik wird "Langevin-Landau-Lifschitz-Metrik" genannt und stellt die „Radardistanz im Kleinen“ dar. Diese Metrik wurde von Langevin eingeführt und von Lifschitz und Landau als „Radardistanz im Kleinen“ für durch beliebige "stationäre" zeitartige Kongruenzen gebildete Quotientenräumen von Lorentzmannigfaltigkeiten verallgemeinert.

Für den Quotientenraum der Langevin-Landau-Lifschitz-Metrik berechnet sich der Krümmungsskalar mit
Dieser ist bis Größen vierter Ordnung in formula_4 identisch mit der konstanten negativen Krümmung des Hyperbolischen Raums mit dem Linienelement
und dem Krümmungsskalar
In diesem Sinne ist die „Geometrie der rotierenden Scheibe“ tatsächlich gekrümmt und entspricht in der Näherung dem Hyperbolischen Raum, wie Theodor Kaluza bereits 1910 (ohne Beweis) vermutet hat. Wie jedoch oben gezeigt wurde, gibt es unterschiedliche Möglichkeiten Abstände auf der rotierenden Scheibe zu messen, die sehr unterschiedliche Ergebnisse liefern. Mit der Langevin-Landau-Lifschitz-Metrik lässt sich ebenso wie mit der Radardistanz im Großen, der radiale Abstand eines Langevin-Beobachters am Ring mit Radius formula_142 vom Rotationszentrum ermitteln. Dazu wird das entsprechende Linienelement für die oben angegebene Nullgeodäte integriert.
in das Linienelement eingesetzt ergibt
und weiter
Dieser Abstand ist größer als formula_142, während die „Radardistanz im Großen“ gleich oder kleiner als formula_142 ist.

Da die zugrunde liegende Langevin-Landau-Lifschitz-Metrik eine Riemann-Metrik ist, ist dieser Abstand im Gegensatz zur „radialen Radardistanz im Großen“ symmetrisch. Der Riemannsche Krümmungstensor der (gekrümmten) Langevin-Landau-Lifschitz-Metrik ist operational signifikant. Wie Nathan Rosen festgestellt hat, stimmen für benachbarte Langevin-Beobachter die gemessenen lokalen (infinitesimalen) Abständen mit jenen überein, die von einem inertialen Beobachter gemessen werden, der sich in einem bestimmten Augenblick parallel und synchron zu ihnen bewegt.







Lars Hernquist

Lars E. Hernquist (* 14. Dezember 1954 in Princeton (New Jersey)) ist ein US-amerikanischer theoretischer Astronom und Astrophysiker. Er ist Mallinckrodt Professor am Harvard-Smithsonian Center for Astrophysics.

Hernquist, der Sohn des Physikers Karl Gerhard Hernquist, studierte an der Cornell University mit dem Bachelor-Abschluss 1975 und wurde 1985 bei Roger Blandford am Caltech promoviert. Als Post-Doktorand war er an der University of California, Berkeley und 1987 bis 1990 am Institute for Advanced Study und 1990/91 an der Princeton University. Ab 1991 war er Assistant Professor für Astronomie am Lick Observatory der University of California, Santa Cruz. Seit 1998 ist er Professor am Harvard-Smithsonian Center for Astrophysics.

Er ist bekannt für seine Untersuchungen der dynamischen Entwicklung von Galaxien und der Erklärung der Entstehung ihrer Formen aus der aus Prozessen wie der Verschmelzung mit anderen Galaxien in der Vergangenheit. Dabei simuliert er deren Entwicklung auf Supercomputern. Ein analytisches Modell der Verteilung dunkler Materie in Galaxien (Herquist Profil) ist nach ihm benannt. Er untersucht auch stark magnetisierte Neutronensterne und andere kompakte Objekte.

1987 bis 1990 war er am Institute for Advanced Study. Er war Sloan Research Fellow (1991–1993) und ist Fellow der American Academy of Arts and Sciences sowie seit 2008 Mitglied der National Academy of Sciences.






Nazzareno Mandolesi

Nazzareno Mandolesi (* 31. August 1944) ist ein italienischer Astrophysiker.

Mandolesi machte 1969 seinen Laurea-Abschluss an der Universität Bologna. Er war Professor an der Universität Ferrara und Forschungsdirektor am Istituto Nazionale di Astrofisica (INAF).

Mandolesi war leitender Wissenschaftler beim Planck-Weltraumteleskop.

2014 erhielt er die Amaldi-Medaille und 2015 den Edison-Volta-Preis. 2018 erhielt er mit Jean-Loup Puget und dem Planck-Team den Gruber-Preis für Kosmologie. 




Daniil Iljitsch Chomski

Daniil Iljitsch Chomski (; * 13. September 1938 in Leningrad) ist ein russischer Physiker.

Chomski studierte bis 1962 an der Lomonossow-Universität Moskau. Ab 1965 arbeitete er in der theoretischen Abteilung des Lebedew-Instituts in Moskau, an der er 1969 seinen PhD erhielt und 1980 habilitierte. Ab 1992 war er Professor an der Universität Groningen in den Niederlanden. Nach seiner Emeritierung im Jahr 2003 wurde er Gastprofessor des Physikalischen Instituts der Universität zu Köln. Er ist seit 2008 Fellow der American Physical Society.

Chomski forscht an Quantenmaterialien, an Metall-Isolator-Übergängen, an Magnetismus, an der Supraleitung und an der Wechselwirkung zwischen Spin- und Orbitalfreiheitsgraden in komplexen Materialien. Um letztere zu beschreiben, entwickelte er zusammen mit dem russischen Physiker Kliment I. Kugel das „Kugel-Khomskii-Modell“ – ein Modell, das eine dynamische Kopplung beider Freiheitsgrade beinhaltet. Später arbeitete Chomski an den magnetischen Eigenschaften und der Orbitalordnung in Manganaten sowie an Ferroelektrika und Multiferroika. Besondere Bekanntheit erlangte er auch für die Zusammenarbeit mit einer Vielzahl experimentalphysikalischer Arbeitsgruppen.





Martin Teucher

Martin W. Teucher (* 1921; † 26. Dezember 1978) war ein deutscher Experimentalphysiker im Bereich der Teilchenphysik und Hochenergiephysik.

Martin Teucher begann sein Studium in Leipzig und setzte es nach dem Zweiten Weltkrieg in Göttingen am Institut von Hans Kopfermann fort. Seine Doktorarbeit machte er bei Friedrich Georg Houtermans zu einem Thema der experimentellen Kernphysik, nämlich der Untersuchung von (n,2n)-Kernreaktionen unter Verwendung einer Radium-Alpha-Beryllium-Neutronenquelle.

Nach der Promotion arbeitete er 1949 an dem von Werner Heisenberg geleiteten Max-Planck-Institut für Physik. 1952 folgte er seinem Doktorvater Houtermans, der eine Professur an der Universität Bern angetreten hatte, dorthin; 1956 habilitierte er sich in Bern. Anschließend verbrachte er einige Jahre in den Vereinigten Staaten, zunächst ein Jahr an der ', anschließend im Labor von Marcel Schein an der '.

1960 wurde er als Professor an die Universität Hamburg berufen und kehrte damit nach Deutschland zurück. Ab 1962 war er als Mitglied des Direktoriums des Deutschen Elektronen-Synchrotrons DESY für die Koordination administrativer und technischer Aufgaben verantwortlich.

Am 26. Dezember 1978 verstarb Martin Teucher im Alter von 57 Jahren.

Nach seiner Doktorarbeit zur Kernphysik beschäftigte sich Teucher nach dem Kriege zunächst am Max-Planck-Institut für Physik mit Erzeugung und Nachweis der kurz zuvor entdeckten Pionen, wobei er als Teilchenquelle die kosmische Strahlung und als Nachweistechnik die bereits vom Pionen-Entdecker Cecil Powell verwendete Kernemulsions-Methode benutzte.

Die Emulsionstechnik brachte er auch mit nach Bern, als er seinem Doktorvater Houtermans an dessen neues Institut dorthin folgte. Houtermans betraute Teucher mit dem Aufbau einer Hochenergiephysik-Gruppe unter Verwendung der Methode. Die Gruppe erbrachte damit wichtige Beiträge zur Physik der Kaonen.

In Chicago konnte Teucher Ende der 1950er Jahre – auch dank der guten Ausstattung des Labors von Marcel Schein – Teilchenreaktionen höchster Energien, von 100 bis 10.000 GeV, in der kosmischen Strahlung erforschen. Seine Arbeiten aus dieser Zeit waren auch dreißig Jahre später noch für Fragestellungen zu Stößen schwerer Kerne mit mehreren hundert erzeugten Teilchen von aktueller Bedeutung.

Nach dem Wechsel an die Universität Hamburg eröffnete er dort das Arbeitsgebiet der Hochenergiephysik am europäischen Teilchenbeschleunigerlabor CERN. Die von ihm gegründete Blasenkammer-Forschungsgruppe war beteiligt an der Entdeckung mehrerer Mesonen aus Daten der 80-cm-Blasenkammer bei CERN. Für das in Bau befindliche DESY erwirkte er den Bau einer Blasenkammer zum dortigen Einsatz, die vom französischen CEA in Saclay gebaut wurde und nach der Fertigstellung von DESY in Hamburg dort für systematische Untersuchungen zur Photoproduktion im GeV-Energiebereich zum Einsatz kam.

Als Mitglied des DESY-Direktoriums hatte er maßgeblichen Anteil an den strategischen Weichenstellungen für die weitere Entwicklung der Einrichtung und setzte sich für die Errichtung eines Elektron-Positron-Speicherrings für Kollisionsexperimente ein und war dann auch für die Realisierung dieses Projekts in Form des Doppelringspeichers DORIS verantwortlich.

Im Lauf der 1960er Jahre hatte Teucher erkannt, dass die wesentlichen künftigen Anwendungen der Blasenkammertechnik im Bereich der Neutrino-Physik liegen würden. 1966 und 1967 war er Vorsitzender des Blasenkammer-Komitees bei CERN und förderte in dieser Funktion das Projekt der "" BEBC, das als multinationale Kollaboration realisiert wurde, 1970/71 in Betrieb ging und zu einem wichtigen Instrument in diesem Bereich wurde.




3j-Symbol

3j-Symbole sind eine Notation zur Kopplung von zwei Drehimpulsen in der Quantenmechanik und wurden von Eugene Wigner eingeführt. Mit ihnen lassen sich Zustände zwischen der gekoppelten und ungekoppelten Basis transformieren. Die 3j-Symbole sind eine Alternative zu den Clebsch-Gordan-Koeffizienten.

Es gibt auch 6j-Symbole nach Wigner entsprechend der Kopplung von drei Drehimpulsen und 9j-Symbole bei Kopplung von vier Drehimpulsen.

Um den Zustand eines aus zwei Bestandteilen mit Drehimpuls formula_1 und formula_2 bestehenden Gesamtsystems zu schreiben, sind in der Quantenmechanik zwei Orthonormalbasen gebräuchlich, die jeweils Eigenbasis einer vollständigen Menge kommutierender Observablen sind. Zum einen die Eigenbasis der Operatoren der beiden Teilsysteme: das Betragsquadrat der beiden Drehimpulsvektoren formula_3 und die jeweiligen formula_4-Komponenten formula_5 (formula_6); die jeweiligen Eigenwerte werden mit formula_7 bezeichnet und die entsprechenden Basiszustände werden als formula_8 geschrieben. Zum anderen der Drehimpuls des Gesamtsystems, d. h., formula_9 und formula_10 (die entsprechenden Quantenzahlen werden mit formula_11 und formula_12 bezeichnet) zusätzlich zu den Drehimpulsen der Teilsysteme formula_3 (aber "nicht" den formula_5); hier schreibt man die Eigenzustände als formula_15.

Dann lässt sich die formula_16-Komponente des Zustands formula_15 mit dem 3j-Symbol wie folgt schreiben:

Die linke Seite der Gleichung wird auch als "Clebsch-Gordan-Koeffizient" bezeichnet. Verglichen mit diesen ist die Kopplung mit 3j-Symbolen symmetrischer formuliert und die Symmetrieeigenschaften der 3j-Symbole lassen sich daher einfacher formulieren.

Als Funktion der Clebsch-Gordan-Koeffizienten ergibt sich für die 3j-Symbole der folgende Ausdruck:
Dabei stehen j und m für die Drehimpulsquantenzahlen.

Der Addition zweier Drehimpulse mit Clebsch-Gordan-Koeffizienten
entspricht bei den 3j-Symbolen die Formulierung als Addition dreier Drehimpulse zu Null:
Der Zustand formula_22 entspricht verschwindenden Drehimpulsquantenzahlen(formula_23). Da die 3j-Symbole alle Drehimpulse auf gleicher Stufe behandeln ist die Formulierung symmetrischer als mit Clebsch-Gordan-Koeffizienten und manifest rotationsinvariant.

Die 3j-Symbole verschwinden außer für:

Das 3j-Symbol ist invariant unter gerader Permutation der Spalten:
Bei ungerader Permutation gibt es einen Phasenfaktor:
Änderung des Vorzeichens der Quantenzahlen formula_12 (entsprechend einer Zeitumkehr) gibt ebenfalls einen Phasenfaktor:
Weiter gibt es sogenannte Regge-Symmetrien:
Insgesamt gibt es 72 Symmetrien, die durch ein "Regge-Symbol" dargestellt werden können:

Die 72 Symmetrien entsprechen der Vertauschung von Reihen und Spalten untereinander und der Transposition der Matrix.

Die Orthogonalitätsrelationen folgen daraus, dass die 3j-Symbol eine unitäre Transformation der verschiedenen Drehimpulsbasen sind (der Basen zu den Drehimpulsen j1, j2 und der des gekoppelten Systems mit Drehimpuls j3).

Dabei ist das "trianguläre Delta" formula_34 gleich 1 falls die Dreiecksbedingung erfüllt ist und 0 sonst. Die Dreiecksbedingung lautet, dass formula_35 einen der Werte formula_36 annimmt.

Die 3j-Symbole sind das Integral des Produkts von drei Kugelflächenfunktionen:
wobei formula_38, formula_39 and formula_40 ganze Zahlen sind.

Analog gilt mit spin-gewichteten Kugelflächenfunktionen und bei halbzahligem Drehimpuls für formula_41:

Für formula_44 gilt für ein nicht-verschwindendes 3j-Symbol (A. R. Edmonds):
mit formula_46 und der Wignerschen kleine D-Matrix formula_47. Eine bessere Näherung, die die Regge-Symmetrie erfüllt, ist:
mit formula_49.

Die folgende Größe spielt die Rolle eines metrischen Tensors in der Theorie und wird auch "Wigner 1-jm symbol" genannt:
Es dient dazu Zeitumkehr bei Drehimpulsen auszudrücken.

mit der Legendrefunktion formula_53.

Die Beziehung zu den Racah-V-Koeffizienten ist ein einfacher Phasenfaktor:






Gestörte Gamma-Gamma-Winkelkorrelation

Die gestörte γ-γ-Winkelkorrelation, kurz PAC () oder PAC-Spektroskopie, ist eine Methode der nuklearen Festkörperphysik, mit der magnetische und elektrische Felder in Kristallstrukturen gemessen werden können. Dabei werden elektrische Feldgradienten und die Larmorfrequenz in Magnetfeldern sowie dynamische Effekte bestimmt. Mit dieser sehr sensitiven Methode, die nur ca. 10–1000 Milliarden Atome eines radioaktiven Isotops pro Messung benötigt, können Materialeigenschaften in der lokalen Struktur, Phasenübergänge, Magnetismus und Diffusion untersucht werden. Die PAC-Methode ist verwandt mit der Kernspinresonanz und dem Mößbauer-Effekt, jedoch zeigt sie keine Signalabschwächung bei sehr hohen Temperaturen. Heute wird hautsächlich die zeit-differenzielle PAC (TDPAC, ) verwendet. 

PAC geht auf eine theoretische Arbeit von Donald R. Hamilton aus dem Jahr 1940 zurück. Das erste erfolgreiche Experiment wurde von Brady und Deutsch 1947 durchgeführt. Bei diesen ersten PAC-Experimenten wurden im Wesentlichen Spin und Parität von Kernspins untersucht. Es wurde jedoch früh erkannt, dass elektrische und magnetische Felder mit dem Kernmoment wechselwirken, was die Grundlage für eine neue Form der Materialuntersuchungen lieferte: Die nukleare Festkörperspektroskopie.

Schritt für Schritt entwickelt sich die Theorie.
Nachdem 1953 Abragam und Pound ihre Arbeiten über die Theorie von PAC veröffentlichten, die extranukleare Felder in der Theorie berücksichtigen, wurden danach viele Untersuchungen mit PAC durchgeführt.

In den 1960er und 1970er Jahren stieg das Interesse an PAC-Experimenten stark an, deren Fokus hauptsächlich magnetische und elektrische Felder in Kristallen waren, in die die Sondenkerne eingebracht wurden. Mitte der 1960er Jahre wurde die Ionenimplantation entdeckt, die neue Möglichkeiten zur Probenherstellung ermöglichte. Die rasante elektronische Entwicklung der 1970er Jahre brachte deutliche Verbesserungen in der Signalverarbeitung. Von den 1980ern bis heute hat sich PAC als eine wichtige Methode zur Untersuchung und Charakterisierung von Materialien entwickelt, z. B. für die Untersuchung von Halbleitermaterialien, zwischenmetallischen Verbindungen, Oberflächen und Grenzflächen. Lars Hemmingsen et al. wendeten PAC zuletzt auch in biologischen Systemen an.

Während bis ca. 2008 PAC-Instrumente konventionelle Hochfrequenzelektronik der 1970er Jahre verwendeten, wurde in 2008 durch Christian Herden und Jens Röder et al. das erste voll-digitalisierte PAC-Instrument entwickelt, dass umfangreiche Datenanalysen sowie den parallelen Einsatz mehrerer Sonden ermöglicht. Nachbauten und weitere Entwicklungen folgten.

PAC nutzt radioaktive Sonden, die beim Zerfall einen Zwischenzustand mit Lebensdauern von 2 ns bis ca. 10 µs besitzen, siehe Beispiel In im Bild rechts. Nach dem Elektroneneinfang (EC, ) transmutiert Indium zu Cadmium. Unmittelbar danach befindet sich der Cadmium-Kern überwiegend im angeregten 7/2-Kernspin und nur zur einem ganz kleinen Teil im 11/2-Kernspin, letzterer soll nicht weiter betrachtet werden. Der 7/2 angeregte Zustand geht durch Aussenden eines γ-Quants mit 171 keV in den 5/2-Zwischenzustand über, der eine Lebensdauer von 84,5 ns besitzt und der sensitive Zustand für die PAC ist. Dieser Zustand wiederum zerfällt in den 1/2-Grundzustand durch aussenden eines γ-Quants mit 245 keV. PAC detektiert nun beide γ-Quanten und wertet das erste als Start-Signal, das zweite als Stop-Signal.
Nun misst man die Zeit zwischen Start und Stopp für jedes Ereignis. Man spricht hier von einer Koinzidenz, wenn ein Start- und Stopp-Paar gefunden wurde. Da der Zwischenzustand nach den Gesetzen des radioaktiven Zerfalls zerfällt, erhält man nach dem Auftragen der Häufigkeit über der Zeit eine exponentielle Kurve mit der Lebensdauer dieses Zwischenzustandes. Aufgrund der nicht-kugelsymmetrischen Ausstrahlung des zweiten γ-Quants, die sogenannte Anisotropie, die eine intrinsische Eigenschaft des Kerns in diesem Übergang ist, kommt es mit den ihm umgebenden elektrischen oder/und magnetischen Feldern zu einer periodischen Störung (Hyperfeinwechselwirkung). Die Abbildung der Einzelspektren rechts zeigt den Effekt dieser Störung als Wellenverlauf auf dem exponentiellen Zerfall von je zwei Detektoren, ein Paar in 90° und eins in 180° zueinander. Die Wellenverläufe zu beiden Detektorpaaren sind gegeneinander verschoben. Sehr vereinfacht kann man sich vorstellen, dass ein fest stehender Beobachter einen Leuchtturm betrachtet, dessen Lichtintensität periodisch heller und dunkler wird. Entsprechend „sieht“ eine Detektoranordnung, meist 4 Detektoren in planarer 90°-Anordnung oder 6 Detektoren in oktaedrischer Anordnung, die Rotation des Kerns in Größenordnungen von MHz bis GHz.

Nach der Anzahl n der Detektoren, ergibt sich die Anzahl der Einzelspektren ("z") nach "z"="n"²−"n", für "n"=4 daher 12 und für "n"=6 somit 30. Um ein PAC-Spektrum zu erhalten, werden die 90°- und 180°-Einzelspektren so miteinander verrechnet, dass die exponentiellen Funktionen sich aufheben und zusätzlich die unterschiedlichen Detektoreigenschaften sich herauskürzen. Es bleibt die reine Störfunktion übrig, wie in dem Beispiel eines komplexen PAC-Spektrums gezeigt ist. Seine Fouriertransformation ergibt die Übergangsfrequenzen als Peaks.

formula_1, das Zählratenverhältnis wird wie folgt berechnet: 

Je nach Spin des Zwischenzustandes zeigen sich eine unterschiedliche Anzahl von Übergangsfrequenzen. Für 5/2 Spin sind 3 Übergangsfrequenzen zu beobachten mit dem Verhältnis ω+ω=ω. Für jeden zugehörigen Gitterplatz der Einheitszelle ist in der Regel eine andere Kombination von 3 Frequenzen zu beobachten.

PAC ist eine statistische Methode: Jedes radioaktive Sondenatom sitzt in seiner eigenen Umgebung. In Kristallen sind aufgrund der hohen Regelmäßigkeit der Anordnung der Atome oder Ionen die Umgebungen identisch oder sehr ähnlich, so dass Sonden auf identischen Gitterplätzen die gleiche Störgröße erfahren, die dann erst in einem PAC-Spektrum messbar wird. Befinden sich die Sonden hingegen in sehr unterschiedlichen Umgebungen, wie z. B. in amorphen Materialien, ist in der Regel eine breite Frequenzverteilung oder gar keine erkennbar und das PAC-Spektrum erscheint flach, ohne Frequenzverlauf. Bei Einkristallen können je nach Orientierung des Kristalls zu den Detektoren bestimmte Übergangsfrequenzen vermindert oder ausgelöscht werden, wie im Beispiel des PAC-Spektrums von Zinkoxid (ZnO) zu sehen ist.

Beim typischen PAC-Spektrometer sind 4 Detektoren in planarer 90°- und 180°-Anordnung oder 6 Detektoren in oktaedrischer Anordnung um die Probe mit radioaktiver Quelle platziert. Als Detektoren werden Szintiallationskristalle aus BaF oder NaI verwendet. Bei modernen Instrumenten kommen heute überwiegend LaBr:Ce oder CeBr zum Einsatz. Photomultiplier wandeln die schwachen Lichtblitze in elektrische Signale um, die im Szintillator durch Gammastrahlung erzeugt wurden. In klassischen Instrumenten werden diese Signale verstärkt und in logischen UND/ODER Schaltungen in Kombination mit Zeitfenstern den verschiedenen Detektorkombinationen (für 4 Detektoren: 12, 13, 14, 21, 23, 24, 31, 32, 34, 41, 42, 43) zugeordnet und gezählt. Moderne digitale Spektrometer verwenden Digitizer-Karten, die das Signal direkt verwenden und in Energie- und Zeitwerte umwandeln und auf Festplatten speichern. Diese werden dann per Software nach Koinzidenzen durchsucht.
Während bei klassischen Instrumenten für die jeweiligen γ-Energien begrenzende „Fenster“ vor der Verarbeitung gesetzt werden müssen, ist dies bei der digitalen PAC während des Aufzeichnens der Messung nicht notwendig und erfolgt erst im Analyseschritt. Bei Sonden mit komplexen Kaskaden ist es dadurch möglich, eine Datenoptimierung vorzunehmen oder mehrere Kaskaden parallel auszuwerten, sowie verschiedene Sonden gleichzeitig zu messen. Die dabei anfallenden Datenmengen können pro Messung zwischen 60 und 300 GB betragen.

Als Materialien für die Untersuchung (Proben) eigenen sich prinzipiell alle Materialien, die fest und flüssig sein können. Je nach Fragestellung und Ziel der Untersuchung ergeben sich bestimmte Rahmenbedingungen. Für die Beobachtung klarer Störfrequenzen ist es aufgrund der statistische Methode notwendig, dass ein gewisser Anteil der Sondenatome sich in einer ähnlichen Umgebung befindet und z. B. denselben elektrischen Feldgradienten erfährt. Ferner darf sich während des Zeitfensters zwischen dem Start und Stop, oder grob ca. 5 Halbwertszeiten des Zwischenzustandes, die Richtung des elektrischen Feldgradienten nicht ändern. In Flüssigkeiten wird deshalb infolge der häufigen Stöße keine Störfrequenz messbar, es sei denn, die Sonde befindet sich komplexiert in großen Molekülen, wie z. B. in Proteinen. Die Proben mit Proteinen oder Peptiden werden zur Verbesserung der Messung meist eingefroren.

Die am meisten untersuchten Materialien mit PAC sind Festkörper wie Halbleiter, Metalle, Isolatoren und verschiedene Arten funktioneller Materialien. Für die Untersuchungen liegen diese meist kristallin vor. Amorphe Materialien besitzen keine hochgeordneten Strukturen. Sie besitzen jedoch eine Nahordnung, die sich in der PAC-Spektroskopie als breite Verteilung von Frequenzen zeigen kann. Nano-Materialien haben nach dem Core-Shell-Modell einen kristallinen Kern und eine Hülle, die eine eher amorphe Struktur besitzt. Je kleiner das Nanoteilchen wird, um so größer wird der Volumenanteil dieses amorphen Anteils. In PAC-Messungen zeigt sich dies mit der Abnahme des kristallinen Frequenzanteils in einer Verringerung der Amplitude (Dämpfung).

Die für eine Messung benötigte Menge an geeigneten PAC-Isotopen liegt zwischen ca. 10 bis 1000 Milliarden Atomen (10–10). Die richtige Menge hängt von den jeweiligen Eigenschaften des Isotops ab. 10 Milliarden Atome sind eine sehr kleine Stoffmenge. Zum Vergleich enthält ein Mol ca. 6,22·10 Teilchen. 10 Atome in einem Kubikzentimeter Beryllium ergeben eine Konzentration von ca. 8 nmol/L (Nanomol = 10 mol). Die radioaktiven Proben haben je eine Aktivität von 0,1–5 MBq, was in der Größenordnung der Freigrenze für das jeweilige Isotop liegt.

Wie die PAC-Isotope in die zu untersuchende Probe gebracht werden, obliegt dem Experimentator und den technischen Möglichkeiten. Es sind folgende Methoden üblich:

Bei der Implantation wird ein radioaktiver Ionenstrahl erzeugt, der auf das Probenmaterial gerichtet ist, wie z. B. an ISOLDE. Durch die kinetische Energie der Ionen (1–500 keV) fliegen diese in das Kristallgitter und werden durch Stöße abgebremst. Sie kommen entweder auf Zwischengitterplätzen zum Stehen oder stoßen ein Gitteratom von seinem Platz und ersetzen dieses. Dies führt zu einer Störung der Kristallstruktur. Diese Störungen können mit PAC untersucht werden. Durch Temperieren können diese Störungen ausgeheilt werden. Sollen hingegen Strahlendefekte im Kristall und deren Ausheilung untersucht werden, misst man unausgeheilte Proben, die dann schrittweise ausgeheilt werden.

Die Implantation ist meist die Methode der Wahl, weil mit ihr sehr gut definierte Proben hergestellt werden können.

Im Vakuum kann die PAC-Sonde auf die Probe aufgedampft werden. Die radioaktive Sonde wird dazu auf einer Heizplatte oder Glühlwendel aufgetragen, dort auf die Verdampfungstemperatur gebracht und auf dem gegenüberliegenden Probenmaterial kondensiert. Mit dieser Methode können z. B. Oberflächen untersucht werden. Weiterhin können durch Aufdampfen weiterer Materialien Grenzflächen hergestellt werden, deren Verhalten beim Temperieren mit PAC studiert werden kann. Ebenso kann die PAC-Sonde bei Sputtern übertragen werden mit Hilfe eines Plasmas.

Bei der Diffusionsmethode wird die radioaktive Sonde meist in einem Lösungsmittel verdünnt auf die Probe aufgebracht, eingetrocknet und sie wird durch Temperieren in das Material eindiffundiert. Die Lösung mit der radioaktiven Sonde sollte dabei möglichst rein sein, da auch alle anderen Substanzen mit in die Probe eindiffundieren können und dadurch das Messergebnis beeinflusst wird. Die Probe sollte ausreichend in der Probe verdünnt sein. Daher sollte der Diffusionsvorgang sollte so geplant sein, dass eine möglichst gleichmäßige Verteilung oder ausreichende Eindringtiefe erreicht wird.

PAC-Sonden können auch während der Synthese von Probenmaterialien beigegeben werden, um eine möglichst gleichmäßige Verteilung in der Probe zu erreichen. Diese Methode ist besonders gut geeignet, wenn beispielsweise die PAC-Sonde nur schlecht im Material diffundiert und eine höhere Konzentration an Korngrenzen zu erwarten ist. Da bei PAC nur sehr kleine Proben notwendig sind (ca. 5 mm), können Mikro-Ansätze verwendet werden. Ideal wird die Sonde der flüssigen Phase des Sol-Gel-Prozesses beigegeben oder einer der späteren Präkursor-Phasen.

Bei der Neutronenaktivierung wird die Sonde direkt aus dem Probenmaterial hergestellt, indem durch Neutroneneinfang in sehr geringer Teil eines der Elemente des Probenmaterials in die gewünschte PAC-Sonde oder sein Mutterisotop umgewandelt wird. Wie auch bei der Implantation müssen Strahlenschäden ausgeheilt werden. Diese Methode beschränkt sich auf Probenmaterialien, die Elemente enthalten, aus denen durch Neutroneneinfang PAC-Sonden hergestellt werden können. Ferner können Proben mit solchen Elementen gezielt verunreinigt werden, das aktiviert werden soll. Beispielsweise eignet sich Hafnium für die Aktivierung ausgezeichnet wegen seines großen Einfangquerschnitts für Neutronen.

Selten verwendet werden direkte Kernreaktionen, bei denen durch Beschuss durch hochenergetischen Elementarteilchen oder Protonen Kerne in PAC-Sonden umgewandelt werden. Hierbei treten große Strahlenschäden auf, die ausgeheilt werden müssen. Diese Methode wird bei PAD verwendet, die zu den PAC-Methoden gehört.

Das aktuell weltweit größte PAC-Labor befindet sich an der ISOLDE im CERN mit ca. 10 Instrumenten, das wesentlich vom BMBF gefördert wird. An der ISOLDE werden radioaktive Ionenstrahlen hergestellt, indem Protonen aus dem Booster auf Target-Materialien (Urancarbid, flüssiges Zinn usw.) geschossen werden und die Spallationsprodukte bei hohen Temperaturen verdampft (bis zu 2000 °C), dann ionisiert und anschließend beschleunigt werden. Mit der anschließenden Massenseparation können meist sehr reine Isotopenstrahlen hergestellt werden, die in PAC-Proben implantiert werden können. Vom besonderem Interesse für die PAC sind dort kurzlebige Sonden wie: Cd, Hg, Pb, sowie verschiedene Sonden der seltenen Erden.

Das erste formula_3-Quant (formula_4) wird isotop ausgestrahlt. Durch die Detektion dieses Quants in einem Detektor wird aus den vielen möglichen Richtungen eine Teilmenge herausgesucht, die eine gegebene Orientierung hat. Das zweite formula_3-Quant (formula_6) wird anisotop ausgestrahlt und zeigt den Effekt der Winkelkorrelation. Das Ziel ist relative Wahrscheinlichkeit formula_7 mit der Detektion von formula_8 im feststehenden Winkel formula_9 in Bezug zu formula_10 zu bestimmen (Störungstheorie). Die Wahrscheinlichkeit ist gegeben mit der Winkelkorrelation:

Für eine formula_3-formula_3-Kaskade ist formula_14 gerade aufgrund der Konservierung der Parität sowie:
Dabei ist formula_16 der Spin des Zwischenzustandes und formula_17 mit formula_18 die Multipolarität der zwei Übergänge. Für reine Multipolübergänge ist formula_19.

formula_20 ist der Anisotropiekoeffizient, der abhängig ist vom Drehimpuls des Zwischenzustands und den Multipolaritäten des Überganges.

Der radioaktive Kern ist für Untersuchungen im Probenmaterial eingebaut und sendet beim Zerfall zwei formula_3-Quanten aus. Während der Lebensdauer des Zwischenzustandes, also der Zeit zwischen formula_10 und formula_8, erfährt der Kern aufgrund der Hyperfeinwechselwirkung durch seine elektrische und magnetische Umgebung eine Störung. Durch diese Störung ändert sich die Winkelkorrelation nach:

formula_25 ist der Störfaktor. Aufgrund der elektrischen und magnetischen Wechselwirkung erfährt der Drehimpuls des Zwischenzustandes formula_17 um seine Symmetrieachse ein Drehmoment. Quantenmechanisch bedeutet dies, dass die Wechselwirkung zu Übergängen zwischen dem M-Zuständen führt. Das zweite formula_3-Quant (formula_8) wird dann von einem Niveau mit geänderter Population ausgesandt. Diese Populationsänderung ist der Grund für die Dämpfung der Korrelation.

Die Wechselwirkung findet zwischen dem magnetischen Kerndipolmoment formula_29 und dem Zwischenzustand formula_16 oder/und einem äußeren magnetischen Feld formula_31 statt. Die Wechselwirkung findet auch statt zwischen Kernquadrupolmoment und dem außerkernischen elektrischen Feldgradienten formula_32.

Für die magnetische Dipolwechselwirkung ist die Frequenz der Präzession des Kernspins um die Achse des magnetischen Felds formula_31:

formula_36 ist der Landé-Faktor und formula_37 ist das Kernmagneton.

Mit formula_38 ergibt sich:

Aus der allgemeinen Theorie wird dann erhalten:

Für die magnetische Wechselwirkung ergibt sich dann:

Die Energie der elektrischen Hyperfeinwechselwirkung zwischen der Ladungsverteilung des Kerns und dem extranuklearen statischen elektrischen Feld kann zu Multipolen erweitert werden. Der Monopolterm bewirkt lediglich eine Energieverschiebung und der Dipolterm verschwindet, sodass der erste relevante Expansionsterm der Quadrupolterm ist:

Dieser kann als Produkt des Quadrupolmomentes formula_43 und des elektrischen Feldgradienten formula_44 geschrieben werden. Beide Tensoren sind von zweiter Ordnung. Höhere Ordnungen haben einen zu kleinen Effekt, um mit PAC gemessen werden zu können.

Der elektrische Feldgradient ist die zweite Ableitung des elektrischen Potentials formula_45 am Kern:

formula_44 wird so diagonalisiert, dass:

Die Matrix ist spurenfrei im Hauptachsensystem (Laplace-Gleichung):

Üblicherweise wird der elektrische Feldgradient mit dem größten Anteil formula_32 und formula_51 definiert:

In kubischen Kristallen sind die Achsenparameter der Elementarzelle x,y,z gleich lang. Daher ist auch formula_54 und formula_55 In axialsymmetrischen Systemen ist ebenfalls formula_55.

Für axialsymmetrische elektrische Feldgradienten nimmt die Energie der Unterzustände die Werte an:

Die Energiedifferenz zwischen zwei Unterzuständen, formula_58 und formula_59, ist gegeben nach:

Die Quadrupolfreuquenz formula_61 wird eingeführt.
Die Formeln in den farbigen Rahmen sind wichtig für die Auswertung:

In den Veröffentlichungen ist überwiegend formula_64 angegeben. formula_65 als Elementarladung und formula_66 als Planck-Konstante sind gut bekannt oder fest definiert. Das Kernquadrupolmoment formula_67 ist häufig nur sehr ungenau bestimmt (oft nur mit 2–3 Stellen). Da formula_64 viel genauer bestimmt werden kann als formula_67 ist aufgrund der Fehlerfortpflanzung es nicht sinnvoll, nur formula_32 anzugeben. Zudem ist formula_64 unabhängig vom Spin! Das Bedeutet, dass Messungen von zwei Isotopen desselben Elements miteinander vergleichen werden können, wie z. B. Hg(5/2−), Hg(5/2−) und Hg(9/2−). Weiterhin kann formula_64 als Fingerprint-Methode eingesetzt werden.

Die Energiedifferenz ergibt sich dann nach:

Wenn formula_55 ist, gilt:
mit:
Für ganzzahlige Spins gilt:
Für halbe Spins gilt:

Für den Störfaktor ergibt sich:

mit dem Faktor für die Gewichtung der beobachteten Frequenzen:

Was die magnetische Dipolwechselwirkung betrifft, induziert auch die elektrische Quadrupolwechselwirkung eine Präzision der Winkelkorrelation in der Zeit und dies moduliert die Quadrupolwechselwirkungsfrequenz. Diese Frequenz ist eine Überlappung der verschiedenen Übergangsfrequenzen formula_83. Die relativen Amplituden der verschiedenen Komponenten hängen von der Ausrichtung des elektrischen Feldgradienten relativ zu den Detektoren (Symmetrieachse) und vom Asymmetrieparameter formula_51 ab. Für eine Untersuchung mit verschiedenen Kernen benötigt man einen Parameter, der einen direkten Vergleich ermöglicht: Daher wird die vom Kernspin formula_85 unabhängige Quadrupolekopplungskonstante formula_64 eingeführt.

Wenn am radioaktiven Kern gleichzeitig eine magnetische und elektrische Wechselwirkung vorliegt, ergeben sich kombinierte Wechselwirkungen, wie oben beschrieben. Dies führt zu Aufspaltung der jeweils beobachteten Frequenzen. Die Analyse ist gegebenenfalls nicht trivial aufgrund der hohen Anzahl von Frequenzen, die zugeordnet werden müssen. Diese hängen dann jeweils von der Richtung des elektrischen und magnetischen Feldes zueinander im Kristall ab. PAC ist eine der wenigen Methoden, mit der diese Richtungen bestimmt werden können.

Fluktuiert während der Lebensdauer formula_87 des Zwischenniveaus das Hyperfeinfeld aufgrund von Sprüngen der Sonde in eine andere Gitterposition oder von Sprüngen eines nahen Atoms in eine andere Gitterposition, so geht die Korrelation verloren. Für den einfachen Fall mit einem ungestörten Gitter kubischer Symmetrie gilt bei einer Sprungrate von formula_88 für formula_89 äquivalente Plätze eine exponentielle Dämpfung des statischen formula_90-Terms:

Hier ist formula_93 eine zu bestimmende Konstante, die nicht mit der Zerfallskonstante formula_94 verwechselt werden darf. Bei großen Werten von formula_95, ist nur noch der reine exponentielle Abfall zu beobachten:

Der Grenzfall nach Abragam-Pound ergibt sich für formula_93, wenn formula_98 ist:

Kerne, die vor der formula_3-formula_3-Kaskade transmutieren, führt dies meist zu einer Ladungsänderung in ionischen Kristallen (In zu Cd). In der Folge muss das Gitter auf diese Änderungen reagieren. Dabei können auch Defekte oder Nachbarionen wandern. Ebenso kann durch den hochenergetischen Zerfallsprozeß durch den Auger-Effekt der Kern in höhere Ionisierungszustände gebracht werden. Die Normalisierung des Ladungszustandes hängt dann von der Leitfähigkeit des Materials ab. In Metallen findet der Prozess sehr schnell statt. In Halbleitern und Isolatoren dauert dies erheblich länger. Bei all diesen Prozessen ändert sich das Hyperfeinfeld. Fällt diese Änderung in die formula_3-formula_3-Kaskade, kann sie als Nacheffekt (engl: ) beobachtet werden.

Die Anzahl der Kerne im Zustand (a) im Bild rechts wird sowohl durch den Zerfall nach Zustand (b) als auch nach Zustand (c) depopuliert:

mit: formula_105

Daraus erhält man den exponentiellen Fall:

Für die Gesamtzahl der Kerne im statischen Zustand (c) folgt dann:

Die Anfangsbesetzungswahrscheinlichkeiten formula_108 ergeben sich für statische und dynamische Umgebung zu:

In der allgemeinen Theorie ist für einen Übergang formula_111 gegeben:

mit:




Supernova

Eine Supernova (von ; Plural "Supernovae") ist das kurzzeitige, helle Aufleuchten eines massereichen Sterns am Ende seiner Lebenszeit durch eine Explosion, bei der der ursprüngliche Stern selbst vernichtet wird. Die Leuchtkraft des Sterns nimmt dabei millionen- bis milliardenfach zu, er wird für kurze Zeit so hell wie eine ganze Galaxie.

Dabei wird innerhalb von Sekunden etwa ein Foe beobachtbare Energie freigesetzt. Dies entspricht einem Wert von ≈ 3 × 10 TWh (Terawattstunden). Zum Vergleich: hätte die Sonne während ihrer gesamten Lebensdauer ihre derzeitige Leuchtkraft, würde sie 3,827 · 10 W × 3,1536 · 10 s/Jahr × 10 Jahre ≈ 1,2 foe an Energie freisetzen.

Man kennt zwei grundsätzliche Mechanismen, nach denen Sterne zur Supernova werden können:

Bekannte Supernovae sind die Supernova 1987A in der Großen Magellanschen Wolke und Keplers Supernova (1604). Speziell letztere und Tycho Brahes Supernova (1572) haben die Astronomie beflügelt, da dadurch die klassische Auffassung von der Unveränderlichkeit der Fixsternsphäre endgültig widerlegt wurde. Der bekannteste Supernovaüberrest ist der Krebsnebel (Supernova 1054) im Sternbild Stier.

Die Bezeichnung der Nova geht zurück auf den von Tycho Brahe geprägten Namen einer Beobachtung eines Sterns im Jahr 1572. Er bezieht sich auf das plötzliche Auftauchen eines vorher nicht sichtbaren sternähnlichen Objektes am Firmament. Unter einer Nova verstand man bis zur Mitte des 20. Jahrhunderts jede Art von Helligkeitsausbruch eines Sterns mit einem Anstieg zum Maximum in einem Zeitraum von Tagen bis Jahren und einer Rückkehr zur früheren Helligkeit innerhalb von Wochen bis Jahrzehnten (siehe Lichtkurve). Als die astrophysikalische Ursache der Eruptionen erkannt wurde, wandelte sich der Begriff zu der heutigen Definition, bei der eine Supernova nicht mehr zu den Novae in ihrer ursprünglichen Bedeutung zählt.

Noch zu Beginn des 20. Jahrhunderts hatte man keine Erklärung für das Auftreten "neuer" oder "temporärer Sterne", wie man Supernovae damals nannte. Es gab mehrere Hypothesen, darunter eine von Hugo von Seeliger, wonach das Eintreten eines festen Körpers in eine kosmische Wolke aus fein verteilter Materie (mit der man sich den Weltraum angefüllt vorstellte) zu einer starken Erhitzung der Oberfläche dieses Körpers und damit zu einem Aufleuchten führt. Die beobachteten Verschiebungen des Spektrums der neuen Sterne interpretierte man als Hinweis darauf, dass die Bildung ihrer dichten Hülle in wenigen Tagen vor sich gegangen sein müsse.

Supernovae werden mit dem Vorsatz „SN“, ihrem Entdeckungsjahr und einem alphabetischen Zusatz benannt. Ursprünglich bestand dieser Zusatz aus einem Großbuchstaben, der alphabetisch in der Reihenfolge der Entdeckung vergeben wurde. So war SN 1987A die erste im Jahr 1987 entdeckte Supernova. 1954 wurden (in fernen Galaxien) erstmals mehr als 26 Supernovae in einem Jahr entdeckt. Seither werden ab der 27. Supernova eines Jahres kleine Doppelbuchstaben (von „aa“ bis „zz“) vergeben. Mit modernen Großteleskopen und speziellen Suchprogrammen wurden in den 2000er Jahren pro Jahr mehrere Hundert Supernovae entdeckt: 2005 waren es 367 (bis SN 2005nc), 2006 waren es 551 (bis SN 2006ue), und 2007 sogar 572 (bis SN2007uz). Heute sind es pro Jahr weit über Tausend.

Wie oft Supernovae in einer Galaxie auftreten, hängt davon ab, wie viele Sterne in ihr neu entstehen. Denn sehr massereiche Sterne, die in Supernovae enden, haben eine nach astronomischen Zeitmaßstäben nur kurze Lebensdauer von einigen zehn Millionen Jahren. Für die Milchstraße werden etwa 20 ± 8 Supernovae pro Jahrtausend geschätzt, wovon im letzten Jahrtausend sechs beobachtet wurden. Etwa zwei Drittel der galaktischen Supernovae blieben durch die Extinktion der galaktischen Scheibe verborgen; die übrigen beobachteten Supernovae fanden sich in anderen Galaxien.

In unserer Galaxie wurden die letzten, sogar freiäugig sichtbaren Supernovae 1572 von Brahe und 1604 von Kepler beobachtet. Eine sehr weit entfernte folgte noch 1680, war aber nur teleskopisch sichtbar. Für die moderne Astrophysik bedeutsam wurde hingegen die SN 1885A in der Andromedagalaxie und vor allem jene von 1987 in der relativ nahen Großen Magellanschen Wolke.

Man unterscheidet historisch grob zwei Typen von Supernovae. Die Einteilung erfolgt nach dem Kriterium, ob im Frühstadium der Supernova Spektrallinien des Wasserstoffs in deren Licht sichtbar sind oder nicht. Es gibt einerseits den Typ I, bei dem keine Wasserstofflinien sichtbar sind, mit den Untergruppen Ia, Ib und Ic; und andererseits den Typ II mit Wasserstofflinien (siehe Tabelle). Die groben Typenbezeichnungen wurden 1939 von Rudolph Minkowski eingeführt, seither wurden sie verfeinert.

Diese Einteilung in Typ I und Typ II deckt sich allerdings nicht mit den zwei in der Einleitung erwähnten physikalischen Mechanismen, die zu einer Supernova führen können. Vielmehr sind nur Supernovae vom Subtyp Ia "thermonuklear."

Eine Supernova vom Typ Ia entsteht nach dem derzeit bevorzugten Modell in kataklysmischen Doppelsternsystemen, die aus einem Weißen Zwerg und einem Begleiter bestehen. Der Weiße Zwerg akkretiert im Laufe der Zeit Gas aus der ausgedehnten Hülle seines Begleiters, wobei es zu mehreren Nova-Ausbrüchen kommen kann. Bei diesen Ausbrüchen fusioniert der Wasserstoff des akkretierten Gases, die Fusionsprodukte bleiben zurück. Das setzt sich so lange fort, bis die Masse des Weißen Zwergs dessen Chandrasekhar-Grenze überschreitet und er durch seine Eigengravitation zu kollabieren beginnt. Im Gegensatz zum nicht reaktiven Eisenkern eines Typ-II-Vorläufersterns enthält der Weiße Zwerg jedoch große Mengen an fusionsfähigem Kohlenstoff, sodass beim Kollaps eine plötzliche Kohlenstoff-Kernfusion einsetzt und der Stern explodiert. Daher wird dieses Phänomen auch als "thermonukleare" Supernova bezeichnet.

Eine zweite Route zur Überschreitung der Chandrasekhar-Grenze können die Superweichen Röntgenquellen sein. Hier ist die Massentransferrate zum Weißen Zwerg hoch genug, um zu einem permanenten Wasserstoffbrennen zu führen.

Dieses Standardmodell geriet aber durch Beobachtungen des Röntgenteleskops Chandra in Bedrängnis. Messungen an sechs ausgewählten Galaxien zeigten, dass die weiche Röntgenstrahlung um den Faktor 50 geringer ist als der zu erwartende Wert, wenn Novae und Super Soft X-ray Sources die dominierenden Quellen für Supernova-Ia-Explosionen wären. Seither wird auch über andere Vorläufersterne spekuliert:

Die beiden ersten Entwürfe sind aber mit der gegenwärtig akzeptierten Theorie der Sternentwicklung nicht verträglich.

Der dritte wird auch als das „zweifach entartete Szenario“ bezeichnet. Dabei beginnt ein enges Doppelsternsystem aus Weißen Zwergen, Materie auszutauschen (sogenannte AM-Canum-Venaticorum-Sterne). Entweder überschreitet einer der Sterne die Chandrasekhar-Grenze (wie bei den kataklysmischen Doppelsternen), oder die Supernovaexplosion entsteht durch eine Verschmelzung der beiden Weißen Zwerge.

Unterschiedlichen theoretischen Modellen zufolge kann die Kernfusion sowohl als Detonation als auch als Deflagration ablaufen. Neueren Arbeiten zufolge, die unter Experten heftig diskutiert werden, ist das wahrscheinlichste Szenario eine anfängliche Deflagration, die in eine Detonation übergeht. Andere Theorien sprechen von Magnetfeldern, denen die Explosionsenergie entnommen wird.

Die freigesetzte Energie einer solchen Supernova-Explosion liegt innerhalb definierter Grenzen, da die Bandbreite der kritischen Masse sowie die Zusammensetzung Weißer Zwerge bekannt ist. Wegen dieser Eigenschaft wird sie als Standardkerze bezeichnet und eignet sich zur Entfernungsbestimmung "(siehe unten)."

Bei einer Supernova-Explosion vom Typ Ia bleibt kein kompaktes Objekt übrig – die gesamte Materie des Weißen Zwergs wird als Supernovaüberrest in den Weltraum geschleudert. Der Begleitstern wird zu einem sogenannten „Runaway“-Stern (engl. für einen „Ausreißer“), da er mit der – normalerweise hohen – Orbitalgeschwindigkeit, mit der er seinen Partnerstern bislang umkreist hat, davonfliegt.

Nach der heute allgemein anerkannten Theorie vom Gravitationskollaps, die zuerst 1938 von Fritz Zwicky aufgestellt wurde, tritt eine Supernova dieses Typs am Ende des „Lebens“ eines massereichen Sterns auf, wenn er seinen Kernbrennstoff komplett verbraucht hat. Sterne mit Anfangsmassen von etwa 8 bis 10 bis etwa 30 Sonnenmassen beenden ihre Existenz als Stern in einer Typ-II-Explosion, massereichere Sterne explodieren als Typ Ib/c. Supernovae vom Typ Ib oder Ic durchlaufen vor der Explosion eine Wolf-Rayet-Sternphase, in der sie ihre äußeren, noch wasserstoffreichen Schichten in Form eines Sternwindes abstoßen.

Bei ansatzweise kugelsymmetrischem Sternaufbau ergibt sich folgender Ablauf: Sobald der Wasserstoff im Kern des Sternes zu Helium fusioniert ist (Wasserstoffbrennen), sinkt der durch die Fusionsenergie erzeugte Innendruck des Sterns und der Stern fällt daraufhin unter dem Einfluss seiner Gravitation zusammen. Dabei erhöhen sich Temperatur und Dichte, und es setzt eine weitere Fusionsstufe ein, der Drei-Alpha-Prozess, in dem Helium über das Zwischenprodukt Beryllium zu Kohlenstoff fusioniert (Heliumbrennen). Der Vorgang (Erschöpfung des Kernbrennstoffs, Kontraktion, nächste Fusionsstufe) wiederholt sich, und durch Kohlenstoffbrennen entsteht Sauerstoff. Weitere Fusionsstufen (Neonbrennen und Siliciumbrennen) lassen den schrumpfenden Stern immer neue Elemente fusionieren. Allerdings setzt jede Fusionsstufe weniger Energie als ihr Vorgänger frei und läuft schneller ab. Während ein massereicher Stern von etwa acht Sonnenmassen einige zehn Millionen Jahre im Stadium des Wasserstoffbrennens verbringt, benötigt das folgende Heliumbrennen „nur“ noch wenige Millionen Jahre. Die letzte Fusionsstufe des Siliciumbrennens lässt sich in Stunden bis Tagen messen.

All diese Sterne durchlaufen während ihrer langen Lebenszeit in ihrem Kern die verschiedenen energiefreisetzenden Fusionsketten bis hin zur Synthetisierung von Eisen, dem Element mit der Ordnungszahl 26. Dort endet die Fusionskette, da Eisenatomkerne die höchste Bindungsenergie pro Nukleon aller Atomkerne haben. Fusionen zu schwereren Elementen benötigen Energie von außen und setzen keine mehr frei.

Die Geschwindigkeit, mit der ein Stern den Brennstoff in seinem Inneren umsetzt, hängt von der Temperatur und der Dichte und damit indirekt vom Gravitationsdruck ab, der auf seinem Kern lastet. Eine wichtige Konsequenz dieses Zusammenhangs ist, dass ein Stern aus Schichten besteht, in denen nach außen hin die Umsetzgeschwindigkeit abnimmt. Auch wenn im Kern schon das Heliumbrennen eingesetzt hat, erfolgt in den Schichten darüber noch Wasserstoffbrennen. Die absolute Fusionsgeschwindigkeit im Kern steigt mit zunehmender Sternenmasse stark an. Während ein Stern mit einer Sonnenmasse etwa 10 Milliarden Jahre benötigt, um die Fusionskette in seinem Kern bis zum Erliegen zu durchlaufen, liegt die Lebensdauer extrem schwerer Sterne mit etwa 100 Sonnenmassen nur noch in der Größenordnung von wenigen Millionen Jahren. Siehe Spätstadien der Sternentwicklung für einen genaueren Überblick.

Das Eisen, die „Asche“ des nuklearen Brennens, bleibt im Kern des Sterns zurück. Sobald keine Fusionen mehr stattfinden, endet auch sämtliche Strahlung, die mit ihrem nach außen gerichteten Druck der Gravitation entgegengewirkte und den Stern aufblähte. Zwei weitere Prozesse verstärken diesen Effekt: Erstens werden durch Photonen hochenergetischer Gammastrahlung Eisenatomkerne mittels Photodesintegration zerstört. Dabei entstehen α-Teilchen und Neutronen; die α-Teilchen können ihrerseits durch solche Photonen in ihre Kernbausteine, Protonen und Neutronen, zerlegt werden. Aufgrund der hohen Stabilität von Eisenkernen muss für diesen Prozess Energie aufgewendet werden. Zweitens werden im sogenannten inversen β-Zerfall (Elektroneneinfang) freie Elektronen durch Protonen eingefangen. Dabei entstehen weitere Neutronen, und Neutrinos werden freigesetzt (Jerry Cooperstein und Edward A. Baron, 1990). Sowohl der Energieverlust durch die Photodesintegration als auch der Verlust freier Elektronen bewirken eine weitere Reduktion des der Gravitation entgegenwirkenden Drucks nach außen.

Nun kann sich die Gravitation voll auswirken. Schließlich überschreitet der Kern die Chandrasekhar-Grenze und kollabiert.

Der Kollaps des Zentralgebiets geschieht so schnell – innerhalb von Millisekunden –, dass die Einfallgeschwindigkeit bereits in 20 bis 50 km Abstand zum Zentrum die lokale Schallgeschwindigkeit des Mediums übersteigt. Die inneren Schichten können nur aufgrund ihrer großen Dichte die Druckinformation schnell genug transportieren. Die äußeren Schichten fallen als Stoßwelle in das Zentrum. Sobald der innere Teil des Kerns Dichten auf nuklearem Niveau erreicht, besteht er bereits fast vollständig aus Neutronen, denn die Elektronen werden in die Protonen gepresst (Umkehrung des Beta-Zerfalls). Neutronenansammlungen besitzen ebenfalls eine obere Grenzmasse (Tolman-Oppenheimer-Volkoff-Grenze, je nach Modell ungefähr 2,7 bis 3 Sonnenmassen), oberhalb derer ein Schwarzes Loch entsteht. Hier sei nun die Masse geringer, um den anderen Fall zu betrachten. Der Kern wird aufgrund quantenmechanischer Regeln (Entartungsdruck) inkompressibel, und der Kollaps wird fast schlagartig gestoppt. Dies bewirkt eine gigantische Druck- und Dichteerhöhung im Zentrum, sodass selbst die Neutrinos nicht mehr ungehindert entweichen können. Diese Druckinformation wird am Neutronenkern reflektiert und läuft nun wiederum nach außen. Die Druckwelle erreicht rasch Gebiete mit zu kleiner Schallgeschwindigkeit, die sich noch im Einfall befinden. Es entsteht eine weitere Stoßwelle, die sich jedoch nun nach außen fortbewegt. Das von der Stoßfront durchlaufene Material wird sehr stark zusammengepresst, wodurch es sehr hohe Temperaturen erlangt (Bethe, 1990). Ein großer Teil der Energie wird beim Durchlaufen des äußeren Eisenkerns durch weitere Photodesintegration verbraucht. Da die nukleare Bindungsenergie des gesamten Eisens etwa gleich der Energie der Stoßwelle ist, würde diese ohne eine Erneuerung nicht aus dem Stern ausbrechen und keine Explosion erzeugen. Als Korrektur werden noch die Neutrinos als zusätzliche Energie- und Impulsquelle betrachtet. Normalerweise wechselwirken Neutrinos mit Materie so gut wie nicht. Jedoch bestehen in der Stoßfront so hohe Neutrinodichten, dass die Wechselwirkung der Neutrinos mit der dortigen Materie nicht mehr vernachlässigt werden kann. Da von der gesamten Energie der Supernova der allergrößte Teil in die Neutrinos geht, genügt eine relativ geringe Absorption, um den Stoß wiederaufleben und aus dem kollabierenden Eisenkern ausbrechen zu lassen. Nach Verlassen des Eisenkerns, wenn die Temperatur genug abgesunken ist, gewinnt die Druckwelle zusätzliche Energie durch erneut einsetzende Fusionsreaktionen.

Die extrem stark erhitzten Gasschichten, die neutronenreiches Material aus den äußeren Bereichen des Zentralgebiets mit sich reißen, erbrüten dabei im sogenannten r-Prozess ("r" von engl. "rapid," „schnell“) schwere Elemente jenseits des Eisens, wie zum Beispiel Kupfer, Germanium, Silber, Gold oder Uran. Etwa die Hälfte der auf Planeten vorhandenen Elemente jenseits des Eisens stammt aus solchen Supernovaexplosionen, während die andere Hälfte im s-Prozess von masseärmeren Sternen erbrütet und in deren Riesenphase ins Weltall abgegeben wurde.

Hinter der Stoßfront dehnen sich die erhitzten Gasmassen schnell aus. Das Gas gewinnt nach außen gerichtete Geschwindigkeit. Einige Stunden nach dem Kollaps des Zentralbereichs wird die Oberfläche des Sterns erreicht, und die Gasmassen werden in der nun sichtbaren Supernovaexplosion abgesprengt. Die Hülle der Supernova erreicht dabei Geschwindigkeiten von Millionen Kilometern pro Stunde. Neben der als Strahlung abgegebenen Energie wird der Großteil von 99 % der beim Kollaps freigesetzten Energie in Form von Neutrinos abgegeben. Sie verlassen den Stern, unmittelbar nachdem die Dichte der anfänglich undurchdringlichen Stoßfront genügend klein geworden ist. Da sie sich fast mit Lichtgeschwindigkeit bewegen, können sie von irdischen Detektoren einige Stunden vor der optischen Supernova gemessen werden, wie etwa bei der Supernova 1987A.

Ein anderes „Frühwarnsignal“ für das Aufleuchten einer Kernkollaps-Supernova ist ein sogenannter Röntgenausbruch. Dieser tritt auf, wenn die Wellen der Stoßfront die Sternoberfläche erreichen und in das interstellare Medium ausbrechen – Tage bevor der Helligkeitsausbruch im sichtbaren Licht beobachtet wird. Erstmals wurde ein solches Röntgensignal im Januar 2008 mit dem NASA-Satelliten Swift bei der Supernova 2008D beobachtet.

Supernovae mit Ausnahme des Typs Ia werden, da sie durch den Kollaps des Zentralgebietes bewirkt werden, auch als "hydrodynamische" Supernovae bezeichnet. Das dargelegte Szenario stützt sich auf den weitgehenden Konsens in der Wissenschaft, dass Supernovaexplosionen von massereichen Sternen prinzipiell so ablaufen. Es gibt jedoch noch kein geschlossenes und funktionierendes physikalisches Modell einer Supernovaexplosion, dem alle sich damit beschäftigenden Wissenschaftler zustimmen.

Supernovae vom Typ II werden nach dem Kriterium unterschieden, ob die Helligkeit der Supernova mit der Zeit eher linear abnimmt (Typ SN II-L) oder während des Abklingens eine Plateauphase durchläuft (Typ SN II-P). Die Spitzenwerte der absoluten Helligkeit zeigen bei SN II-P eine breite Streuung, während die meisten SN II-L fast gleiche Maximalhelligkeit besitzen. Die Helligkeit im blauen Spektralbereich von SN II-P erreicht im Mittel −17,0 mag mit einer Standardabweichung von 1,1 mag, während SN II-L meist bei −17,6 ± 0,4 mag liegen.
Die Existenz von Plateauphasen wird dadurch erklärt, dass die ausgestoßene Masse und damit die Geschwindigkeit der Hülle der Supernova sehr groß ist. Der Rückgang der Helligkeit aufgrund der Abkühlung wird durch die rasche Ausdehnung der Hülle wegen der dadurch vergrößerten Oberfläche kompensiert und die Lichtkurve wird durch ein Plateau beschrieben. Die maximale Helligkeit hängt dabei vom Radius des Vorgängersterns ab, wodurch die große Streuung in den Maximalhelligkeiten der SN II-P erklärt wird. Supernovae vom Typ II-L haben eine geringere Expansionsgeschwindigkeit, sodass ihre Helligkeit bereits in frühen Stadien von radioaktiven Prozessen bestimmt wird. Dadurch tritt eine geringere Streuung der Maximalhelligkeiten auf (Young, Branch, 1989). Die Supernova SN 1979C ist ein Beispiel für den Typ II-L. Hier nahm allerdings nur die Helligkeit im sichtbaren Licht ab; im Röntgenbereich strahlt die Supernova noch heute genauso hell wie bei ihrer Entdeckung 1979. Welcher Mechanismus diese andauernde Helligkeit verursacht, ist bis jetzt noch nicht vollkommen erforscht.

Bei Supernovae vom Typ Ib ist vor der Explosion die Wasserstoffhülle abgestoßen worden, sodass bei der Explosion keine Spektrallinien des Wasserstoffs beobachtet werden können. Der Explosionstyp Ic tritt auf, wenn zusätzlich noch die Heliumhülle des Sterns abgestoßen wurde, sodass auch keine Spektrallinien des Heliums auftreten. Auch diese Explosionen werden durch einen Kernkollaps hervorgerufen und es bleibt ein kompaktes Objekt zurück.

Zu einem ähnlichen spektralen Verlauf wie bei Typ Ib – aber weniger hell – kommt es bei einer Supernova vom Typ Calcium-Rich Gap Transient.

Das bei der Supernova ausgeworfene Material bildet einen Emissionsnebel, den sogenannten „Supernovaüberrest“, im Gegensatz zum eventuell entstehenden Überrest des Kernkollapses, der in der Astrophysik als „kompaktes Objekt“ bezeichnet wird. Der wohl bekannteste Supernovaüberrest ist der Krebsnebel, der bei der Explosion der SN1054 ausgestoßen wurde. Diese Supernova ließ auch ein kompaktes Objekt (einen Pulsar) zurück.

Die Form des Überrestes, der von dem Stern zurückbleibt, hängt von dessen Masse ab. Nicht die gesamten äußeren Schichten werden bei der Supernovaexplosion fortgeschleudert. Das zurückbleibende Gas akkretiert auf den kollabierten Kern im Zentrum, der nahezu vollständig aus Neutronen besteht. Das nachfallende Gas wird durch die oben beschriebenen Prozesse ebenfalls in Neutronen zerlegt, sodass ein Neutronenstern entsteht. Wird der Stern durch das nachfallende Material noch schwerer (mehr als etwa 3 Sonnenmassen), so kann die Gravitationskraft auch den durch das Pauli-Prinzip bedingten Gegendruck überwinden, der in einem Neutronenstern die Neutronen gegeneinander abgrenzt und ihn dadurch stabilisiert (siehe Entartete Materie). Der Sternenrest stürzt endgültig zusammen und bildet ein Schwarzes Loch, aus dessen Schwerkraftfeld keine Signale mehr entweichen können. Neuere Beobachtungen legen die Vermutung nahe, dass es eine weitere Zwischenform gibt, die sogenannten Quarksterne, deren Materie aus reinen Quarks aufgebaut ist.

Neutronensterne rotieren aufgrund des Pirouetteneffekts oft mit sehr hoher Geschwindigkeit von bis zu 1000 Umdrehungen pro Sekunde; dies folgt bereits aus der Drehimpulserhaltung beim Kollaps.

Die hohe Drehgeschwindigkeit erzeugt ein Magnetfeld, das mit den Teilchen des abgestoßenen Gasnebels in Wechselwirkung tritt und deshalb Signale erzeugt, die auch von der Erde aus registrierbar sind. Im Falle von Neutronensternen spricht man dabei von Pulsaren.

Eine Variante des Kernkollapsszenarios besteht in der "Paarinstabilitätssupernova" "(pair instability supernova, PISN)," bei der der Stern nicht zu einem kompakten Objekt kollabiert, sondern vollständig zerrissen wird. Die Vorläufersterne sind besonders arm an Elementen, die schwerer sind als Helium. Der Druck im Kern ist nicht hoch genug, um schwere Elemente wie Eisen bilden zu können, was die Voraussetzung für einen Kern-Kollaps ist. In dieser Phase gelangt der Stern nach dem Ende des Heliumbrennens in Temperatur- und Dichtebereiche, in denen die Photonenenergien zur Erzeugung von Elektron-Positron-Paaren führen. Dies führt zu einer Verringerung des Strahlungsdrucks und damit zu einer weiteren schnellen Erhöhung der Dichte – und damit der Temperatur – des Kerns, bis es zu einem explosionsartigen Einsetzen des Sauerstoff- und Siliciumbrennens kommt, das einen erneuten Gegendruck gegen den Gravitationsdruck aufbaut. Abhängig von der Größe des Gravitationsdrucks – und damit der Masse des Kerns – kann diese Kernexplosion den weiteren Kollaps verlangsamen oder sogar verhindern. Bei einer PISN entsteht "kein" kompakter Überrest, sondern der Stern wird vollständig zerrissen. Die dabei freiwerdenden Energien liegen mit bis zu 100 foe (10 J) um etwa einen Faktor 100 über denen einer „gewöhnlichen“ Kernkollapssupernova.

Modellrechnungen für verschwindende Metallizität und ohne Berücksichtigung einer möglichen Rotation oder von Magnetfeldern liefern für das Einsetzen der Paarinstabilität eine kritische Masse des Heliumkerns von 64 Sonnenmassen. Wird die Masse des Heliumkerns größer als 133 Sonnenmassen, so kann die Kernexplosion den weiteren Kollaps nicht verhindern, der sich somit weiter zu einem Schwarzen Loch entwickelt. Rechnet man diese Helium-Kernmassen auf die notwendige Gesamtmasse eines Hauptreihensterns (unter Vernachlässigung von Massenverlusten) hoch, so ergibt sich für die PISN ein Massenbereich von etwa 140 bis 260 Sonnenmassen. Aus diesem Grund wird dieses Szenario im heutigen Universum als äußerst selten angesehen. In Betracht gezogen wird es vorwiegend bei der ersten Sterngeneration der sog. Population III. Dort könnte dieser Mechanismus jedoch eine bedeutende Rolle bei der Anreicherung des intergalaktischen Mediums mit schwereren Elementen gespielt haben.

Einen Sonderfall stellt die Supernova SN 2006gy in der Galaxie NGC 1260 dar, die am 18. September 2006 im Rahmen des "Texas Supernova Search" entdeckt wurde: Die absolute Helligkeit von SN 2006gy lag um mehr als eine Magnitude über der anderer Supernovae. Die Entdecker interpretieren diese etwa 240 Millionen Lichtjahre entfernte Supernova deshalb als ersten Kandidaten, für den der Paarinstabilitätsmechanismus als Erklärung möglich ist – allerdings sind weder das bisherige Datenmaterial noch die theoretischen Modelle ausreichend, um hier eine eindeutige Entscheidung treffen zu können.

Der erste wohl sichere Vertreter einer PISN ist die Supernova SN 2007bi, die sich am 6. April 2007 in einer Zwerggalaxie im Sternbild Jungfrau ereignete. Eine Gruppe von Astronomen vom Weizmann-Institut für Wissenschaften nutzte unter anderem die beiden Keck-Teleskope, um die Spektren und den Helligkeitsverlauf über mehr als ein Jahr lang zu beobachten. Die Untersuchungen ergaben, dass der Vorläuferstern des 1,7 Milliarden Lichtjahre entfernten Sternenrestes als Hyperriese mit vermutlich 200 Sonnenmassen ungewöhnlich massereich und metallarm war. Bei einem ungewöhnlich langsamen Verlauf wurden außerdem große Mengen an Silizium und radioaktivem Nickel freigesetzt.

Da die Strahlung besonders im späteren Verlauf einer Supernova vom Typ Ia größtenteils durch den radioaktiven Zerfall von Ni zu Co und von diesem zu Fe gespeist wird, wobei die Halbwertszeiten etwa 6 beziehungsweise 77 Tage betragen (diese Theorie stellten zuerst Fred Hoyle und William Alfred Fowler im Jahre 1960 auf), ist die Form der Lichtkurve stets annähernd gleich. Auch die freigesetzte Energiemenge sollte, bedingt durch den Mechanismus, immer ungefähr gleich sein, was wegen des ungefähr gleichen Aufbaus eine immer ungefähr gleiche Leuchtkraft ergibt. Durch diese Eigenschaften einer Standardkerze lassen sich anhand solcher Supernova-Explosionen relativ genaue Entfernungsmessungen im Weltall vornehmen, wobei auch die Zeitskala der Lichtkurve neben den Spektrallinien zur Bestimmung der Rotverschiebung verwendet werden kann, da sich bei einer Rotverschiebung von z. B. 2 auch der zeitliche Ablauf für den Beobachter um diesen Faktor verlängert. Die Idee dazu geht auf Fritz Zwicky zurück. Durch die Entfernungsmessungen von Supernova-Explosionen, die sich vor ca. 7 Milliarden Jahren ereigneten, kann man die beschleunigte Expansion des Universums (siehe z. B. Hubble-Konstante oder Supernova Cosmology Project) belegen. Um Supernovae wirklich als Standardkerzen verwenden zu können, müssen die Explosionsmechanismen jedoch noch besser erforscht und verstanden werden.

Seit Anfang des 21. Jahrhunderts ist es möglich, unter Zuhilfenahme von Supercomputern Supernova-Explosionen in Teilen dreidimensional zu simulieren. Bis dahin bereitete vor allem die Modellierung von thermonuklearen Explosionen Probleme, weil die dafür nötige hohe Geschwindigkeit der Reaktionsfront von einigen tausend Kilometern pro Sekunde nicht erreicht wurde. Eine Lösung des Problems deutet sich an, seitdem man mit Methoden arbeitet ähnlich denen, die sich bei der Berechnung von Flammenturbulenzen im Ottomotor bewährt haben. Weiterhin schwierig ist die Berechnung der zugleich in sehr großen wie in sehr kleinen Maßstäben ablaufenden Vorgänge sowie die Tatsache, dass die Vorgänge möglichst dreidimensional darzustellen sind. Ein Hauptproblem aller Simulationen ist allerdings bis heute (April 2010) der unerreichte Übergang vom Kollaps zur eigentlichen Explosion. Laut der Astrophysikerin Fiona Harrison deutet dies auf unzureichende Kenntnisse der physikalischen Grundprinzipien hin und ist Gegenstand aktueller Forschungen. Eine mögliche Interpretation davon ist, dass viele massereiche Sterne in einer sogenannten Un-Nova enden und somit nicht sichtbar explodieren.

Erste hydrodynamische numerische Rechnungen zu Supernova-Explosionen führten Stirling Colgate und Richard White am Lawrence Livermore National Laboratory 1966 aus und erkannten dabei auch die Bedeutung der Neutrinos für den Explosionsmechanismus. Weitere wichtige Fortschritte erzielte James R. Wilson Anfang der 1980er Jahre. Weitere bekannte Wissenschaftler, die sich mit Supernova-Simulationen beschäftigten, sind W. David Arnett, Stanford E. Woosley und Wolfgang Hillebrandt.

Durch die zunehmend schnelleren Supercomputer wurde es möglich, Supernovaberechnungen ohne unnatürliche Symmetrieannahmen durchzuführen. Damit konnten Simulationen wesentlich realistischer werden, da die relevante Physik in den Modellen berücksichtigt wird, insbesondere was die hochkomplexen Wechselwirkungen der Neutrinos betrifft, bewegen sich solche Simulationen an der absoluten Grenze des aktuell auf den größten verfügbaren Superrechnern gerade noch Machbaren.

Im Jahr 2016 konnten einem Team am Max-Planck-Institut für Astrophysik (MPA) 16.000 Prozessorkerne auf dem SuperMUC am Leibniz-Rechenzentrum (LRZ) in Garching und auf dem MareNostrum am Barcelona Supercomputing Center (BSC) zur Verfügung gestellt werden.
Selbst bei paralleler Nutzung dieser 16.000 Prozessorkerne dauert eine einzige Modellsimulation einer Supernova über eine Entwicklungszeit von etwa 0,5 Sekunden immer noch sechs Monate und verschlingt rund 50 Millionen Stunden Rechenzeit.

Der mögliche Ausbruch einer Supernova in der Nähe unseres Sonnensystems wird als "erdnahe Supernova" bezeichnet. Man geht davon aus, dass bei Entfernungen zur Supernova unter 100 Lichtjahren merkliche Auswirkungen auf die Biosphäre unseres Planeten eintreten würden. Die Gammastrahlung einer solchen Supernova kann chemische Reaktionen in den oberen Atmosphärenschichten auslösen, bei denen Stickstoff in Stickoxide umgewandelt wird. Dadurch könnte die Ozonschicht komplett zerstört werden, was die Erde gefährlicher Strahlung aussetzen würde.

Das Massenaussterben im oberen Ordovizium, bei dem etwa 50 Prozent der ozeanischen Arten ausstarben, wird von einigen Autoren mit einer solchen erdnahen Supernova in Verbindung gebracht.
Einige Forscher vermuten, dass eine vergangene erdnahe Supernova noch durch Spuren bestimmter Metallisotope in Gesteinslagen nachweisbar ist. Anreicherungen des Isotops Fe wurden beispielsweise in Tiefseegestein des Pazifischen Ozeans festgestellt.

Potenziell am gefährlichsten sind vermutlich Supernova vom Typ Ia. Da sie aus einem engen halbgetrennten Doppelsternsystem bestehend aus einem lichtschwachen akkretierenden Weißen Zwerg und einem Masse verlierenden Begleiter hervorgehen, erscheinen kataklysmische Veränderliche eher unauffällig und es ist denkbar, dass Vorläufer einer solchen Supernova auch in relativer Erdnähe unentdeckt bleiben oder nur unzureichend studiert werden. Einige Vorhersagen deuten darauf hin, dass eine solche Supernova noch in Entfernungen bis zu 3000 Lichtjahren die Erde beeinflussen könnte.
Als erdnächster "bekannter" Kandidat für eine künftige Supernova dieses Typs gilt IK Pegasi in etwa 150 Lichtjahren Entfernung.

Supernovae vom Typ II gelten hingegen als weniger gefährlich. Neuere Untersuchungen gehen davon aus, dass eine solche Supernova in einer Entfernung von weniger als 26 Lichtjahren aufleuchten muss, um die biologisch wirksame UV-Strahlung auf der Erde zu verdoppeln.

Im Oktober 2011 sprach das Nobelkomitee den drei amerikanischen Astrophysikern Saul Perlmutter, Brian Schmidt und Adam Riess für ihre Beobachtungen an Supernovae den Nobelpreis für Physik zu. Sie hatten in den 1990er Jahren – entgegen der damals herrschenden Lehrmeinung – herausgefunden, dass Dunkle Energie das Universum mit "wachsender" Geschwindigkeit auseinandertreibt.



Spektrum.de: "Das Geheimnis besonders starker Supernovae" 5. Februar 2019





Schwache Wechselwirkung

Die schwache Wechselwirkung (auch schwache Kernkraft genannt, vereinzelt auch β-Wechselwirkung) ist eine der vier Grundkräfte der Physik. Im Gegensatz zu den aus dem Alltag bekannten Wechselwirkungen der Gravitation und des Elektromagnetismus wirkt sie jedoch nur auf sehr kurze Distanzen. Dabei kann sie wie andere Kräfte für Energie- und Impuls-Austausch sorgen, wirkt aber vor allem bei Zerfällen oder Umwandlungen der beteiligten Teilchen, etwa dem Betazerfall bestimmter radioaktiver Atomkerne. Durch die schwache Wechselwirkung lassen sich keine gebundenen Zustände bilden, was sie von den anderen drei Wechselwirkungen unterscheidet.

Entscheidende Bedeutung für das Leben auf der Erde hat die schwache Wechselwirkung durch ihre Rolle bei der Fusion von Wasserstoff zu Helium in der Sonne (Proton-Proton-Reaktion), da nur durch sie die Umwandlung von Protonen in Neutronen möglich ist. So entsteht aus vier Protonen (den Wasserstoffkernen) über mehrere Zwischenschritte ein stabiler Heliumkern mit zwei Protonen und zwei Neutronen. Durch diesen Prozess setzt die Sonne Energie frei. Aufgrund der geringen Stärke der schwachen Wechselwirkung läuft dieser Prozess so langsam ab, dass die Sonne schon seit 4,5 Milliarden Jahren stabil leuchtet und dies voraussichtlich noch fünf bis sechs Milliarden Jahre tun wird.

Die schwache Wechselwirkung lässt sich in geladene Ströme und ungeladene Ströme unterscheiden. Geladene Ströme wirken zwischen allen (linkshändigen) Quarks und (linkshändigen) Leptonen sowie den (rechtshändigen) Antiquarks und (rechtshändigen) Anti-Leptonen. Ungeladene Ströme wirken zwischen denselben Teilchen, die durch geladene Ströme wechselwirken, aber zusätzlich auch zwischen allen geladenen (Anti-)Quarks und (Anti-)Leptonen unabhängig von ihrer Chiralität.

Die elektromagnetische ist ca. 10 Mal, die starke Wechselwirkung ca. 10 Mal stärker als die schwache Wechselwirkung. Wie die starke und die elektromagnetische Wechselwirkung wird sie durch den Austausch von Eichbosonen beschrieben. Diese Austauschteilchen der schwachen Wechselwirkung sind das neutrale Z-Boson sowie die beiden positiv bzw. negativ geladenen W-Bosonen. Da diese massiv sind, hat die schwache Kraft nur eine extrem kurze Reichweite unterhalb eines Atomkernradiusses.

Die schwache Wechselwirkung lässt sich am einfachsten bei Zerfällen von Quarks oder Leptonen beobachten. In Streuexperimenten hingegen ist diese eher schwer zugänglich, da sie bei geladenen Leptonen oder Hadronen von der starken bzw. elektromagnetischen Wechselwirkung überlagert wird. Teilchen, die weder der starken noch der elektromagnetischen Wechselwirkung unterliegen (keine Farbladung und keine elektrische Ladung tragen), sind die ungeladenen Leptonen, also die Neutrinos, die aber in Streuexperimenten äußerst kleine Wirkungsquerschnitte besitzen.

Die schwache Wechselwirkung verletzt die Paritätserhaltung, wie im Wu-Experiment nachgewiesen wurde. Außerdem verletzt sie die CP-Erhaltung etwa beim Zerfall des ungeladenen K-Mesons (Kaonen).

Eine Quantenfeldtheorie, die die schwache Wechselwirkung zusammen mit der elektromagnetischen Wechselwirkung beschreibt, ist das Glashow-Weinberg-Salam-Modell. Man spricht in dieser Formulierung auch von zwei Aspekten der elektroschwachen Wechselwirkung, die durch den Higgs-Mechanismus vereinheitlicht werden.

Die Austauschteilchen der schwachen Wechselwirkung sind massive Vektorbosonen. Sie haben den Spin 1. Ihr Verhalten kann durch die Proca-Gleichung beschrieben werden.

Folgende Tabelle gibt eine Übersicht der Eigenschaften der Austauschteilchen (Masse und Resonanzbreite nach Particle Data Group, Lebensdauer über die Energie-Zeit-Unschärferelation berechnet):

Die Reichweite formula_1 lässt sich grob abschätzen, indem man annimmt, dass sich die Teilchen während ihrer Lebensdauer formula_2 (im Ruhesystems des Teilchens) mit 71 % der Lichtgeschwindigkeit formula_3 im Laborsystem bewegen (Lorentzfaktor formula_4): formula_5. Dies ergibt für eine Lebensdauer von 3·10 s eine Reichweite von etwa 0,09 Femtometer – der kleinste Atomkern, das Proton, hat einen Durchmesser von etwa 1,7 Femtometer.

In der elektroschwachen Theorie ist das Massenverhältnis von W- und Z-Bosonen mit dem Weinbergwinkel formula_6 (CODATA 2014) verknüpft

Als Konsequenz der Weinbergmischung ergibt sich, dass die Kopplungsstärke der Z-Bosonen nicht mit der der W-Bosonen identisch ist. Die Kopplungsstärke des W-Bosons an ein linkshändiges Fermion ist gegeben durch

die Kopplungsstärke des formula_9 an ein Fermion ist dagegen durch

wobei formula_11 die Ladung des Fermions in Einheiten der Elementarladung formula_12 ist. formula_13 bezeichnet die dritte Komponente des schwachen Isospins. Für linkshändige Neutrinos gilt beispielsweise formula_14.

Die Kopplungsstärken von schwacher und elektromagnetischer Wechselwirkung hängen zusammen über

Zur Beschreibung eines schwachen Prozesses verwendet man üblicherweise die Schreibweise einer Reaktionsgleichung, wie

Die Teilchen a und b werden also in einem Prozess zu den Teilchen c und d umgewandelt. Ist dieser Vorgang möglich, so sind auch alle anderen möglich, die nach der Vertauschungsregel des Kreuzens (engl. ) entstehen. Ein Teilchen kann also auf die andere Seite der Reaktionsgleichung geschrieben werden, indem dort sein entsprechendes Antiteilchen notiert wird:

Außerdem sind die Umkehrprozesse möglich.

Ob diese Prozesse tatsächlich in der Natur beobachtet werden (also ihre Wahrscheinlichkeit, die sich um viele "Größenordnungen" unterscheiden kann), hängt nicht nur von der Stärke der schwachen Wechselwirkung ab, sondern unter anderem auch von Energie, Masse und Impuls der beteiligten Teilchen.

Für jede Reaktion gelten die bekannten Sätze der Energieerhaltung, Impulserhaltung und Drehimpulserhaltung, die nach dem Noether-Theorem mit den Invarianzen gegenüber zeitlicher und räumlicher Translation sowie Drehungen im Raum verbunden sind.

Sind die Summen der Massen der beteiligten Teilchen auf der rechten Seite größer als auf der linken, so handelt es sich um eine endotherme Reaktion, die nur möglich ist, wenn die Teilchen auf der linken Seite ausreichend kinetische Energie tragen. Sollte auf der linken Seite nur ein Teilchen stehen, dann ist die Reaktion in diesem Fall verboten, denn es gibt für ein massives Teilchen immer ein Bezugssystem, in dem dieses Teilchen in Ruhe ist (d. h., dass Masse aus dem Nichts erzeugt werden müsste, was nicht möglich ist). Auf der anderen Seite existiert für ein masseloses Teilchen auf der linken Seite nie ein Ruhesystem, sodass im Schwerpunktssystem der Teilchen auf der rechten Seite in diesem Fall die Impulserhaltung verletzt wäre.

Sind die Massen der eingehenden Teilchen größer als die Massen der erzeugten Teilchen, so ist die Reaktion exotherm, und die Differenz der Massen findet sich als Differenz der kinetischen Energien zwischen Ausgangsteilchen und erzeugten Teilchen wieder.

Man unterscheidet schwache Prozesse sowohl danach, ob Leptonen und/oder Quarks an ihnen beteiligt sind, als auch danach, ob der Prozess durch ein elektrisch geladenes formula_20- oder formula_21-Boson (geladene Ströme bzw. charged currents: CC) oder das neutrale formula_9-Boson (neutrale Ströme bzw. neutral currents: NC) vermittelt wurde. Die Bezeichnungen schwacher Prozesse lauten wie folgt:

Alle Reaktionen, an denen Neutrinos beteiligt sind, verlaufen ausschließlich über die schwache Wechselwirkung (die Gravitation vernachlässigt). Umgekehrt gibt es aber auch schwache Reaktionen ohne Beteiligung von Neutrinos.

Ähnlich wie das Photon und im Gegensatz zu den W-Bosonen vermittelt das Z-Boson eine Wechselwirkung zwischen Teilchen, ohne die Teilchenart (genauer: Flavour) dabei zu verändern. Während das Photon aber nur Kräfte zwischen elektrisch geladenen Teilchen vermittelt, wechselwirkt das Z-Boson auch mit den ungeladenen Neutrinos. Bei neutralen Prozessen bleiben die beteiligten Fermionen unverändert (keine Änderung von Masse oder Ladung). Das Z-Boson wirkt auf alle linkshändigen Fermionen und durch die Weinberg-Mischung auch auf die rechtshändigen Anteile von geladenen Fermionen. Es ist nicht wie die W-Bosonen maximal paritätsverletzend, da es einen Anteil des B-Bosons enthält (siehe: Elektroschwache Wechselwirkung).

Beispiele für neutrale Prozesse sind: Die Streuung zweier Elektronen aneinander (wird für geringe Energien aber durch die stärkere elektromagnetische Wechselwirkung überlagert und erst bei hohen Energien werden die Wechselwirkungen in der Stärke vergleichbar). Die Streuung von Myon-Neutrinos an Elektronen (keine konkurrierenden Prozesse, erster experimenteller Nachweis der neutralen Ströme 1973 am CERN).

Ein elementarer geladener leptonischer Prozess ist ein Zerfallsprozess eines Leptons L in ein Lepton L' unter Beteiligung ihrer entsprechenden Neutrinos bzw. Antineutrinos (formula_23):

Ein Beispiel dazu ist der Zerfall von Myonen:

wie auch die damit verbundenen Streuprozesse

Bei einem elementaren geladenen semileptonischen Prozess sind neben Leptonen auch Quarks bzw. Antiquarks (formula_28) beteiligt:

Ein Beispiel für einen semileptonischen Prozess ist der bereits genannte β-Zerfall des Neutrons, bei welchem sich ein Down-Quark des Neutrons in ein Up-Quark umwandelt:

Dadurch wird ein Neutron "n = udd" zu einem Proton "p = uud":

Ein Down- und ein Up-Quark sind unbeteiligt. Sie werden „Zuschauerquarks“ (engl. spectator quarks) genannt.

Dieser Prozess wird durch ein formula_21-Boson vermittelt, wobei das negativ geladene Down-Quark in ein positiv geladenes Up-Quark umgewandelt wird — die negative Ladung wird durch ein formula_21-Boson „weggetragen“. formula_34 und formula_35 müssen also Quarks sein, deren Ladungsdifferenz gerade formula_36 ist.

Weitere Beispiele von semileptonischen Prozessen sind:

Bei einem elementaren geladenen hadronischen (bzw. nichtleptonischen) Prozess sind nur Quarks bzw. Antiquarks beteiligt:

Der Kaon-Zerfall ist ein gutes Beispiel für einen hadronischen Prozess

Quarkdarstellung: formula_40

Hadronendarstellung: formula_41

Wobei die beteiligten Teilchen folgendermaßen aufgebaut sind: formula_42 und formula_43 sowie formula_44. Bei diesem Prozess ist das Up-Quark des Kaons wieder ein unbeteiligter Zuschauer. Die positive Ladung des Strange-Antiquarks wird durch ein formula_20-Boson weggetragen. Durch diesen Austausch ändert das Quark seinen Flavor zu einem Anti-Up-Quark.

Weitere Beispiele von hadronischen Prozessen sind zwei Zerfallskanäle des Λ-Baryons:

Bei geladenen Strömen der schwachen Wechselwirkung können sich nur Teilchen aus demselben Dublett ineinander umwandeln:

Es handelt sich nur um linkshändige Fermionen. Diese besitzen einen schwachen Isospin formula_49, wobei die dritte Komponente des schwachen Isospins für die oberen Teilchen formula_50 und die unteren formula_51 ist. Die schwache Hyperladung formula_52, also die doppelte Differenz aus elektrischer Ladung und dritter schwacher Isospinkomponente, ist innerhalb eines Dubletts konstant. Sie beträgt für die Leptonendubletts formula_53 und für die Quarkdubletts formula_54.

Rechtshändige Fermionen koppeln nicht an W-Bosonen und tragen deshalb keinen schwachen Isospin. Weiterhin stellt man fest, dass Neutrinos in der Natur nur linkshändig vorkommen (Goldhaber-Experiment). Somit werden rechtshändige Fermionen als Singuletts formula_55 beschrieben. Da die geladenen Ströme ausschließlich an die linkshändigen Dubletts koppeln, tritt bei diesen Vorgängen eine maximale Verletzung der Parität auf. Experimentell wurde dies im Wu-Experiment untersucht und durch die V-A-Theorie erklärt.

Bei den Quarks sind die Dubletts (u,d'), (c,s'), (t,b') Eigenzustände der schwachen Wechselwirkung und nicht (u,d), (c,s), (t,b). Die Zustände der gestrichenen Teilchen sind jeweils eine Linearkombination von drei Zuständen. D.h. die gestrichenen Quarkzustände formula_56 sind gegenüber den Quarkzuständen formula_57 wie folgt rotiert:

Dabei ist formula_59 die sog. CKM-Matrix. Diese ist unitär und hat vier unabhängige Parameter. Die Quadrate der Elemente der angegebenen Matrix formula_60 sind proportional zu den Übergangswahrscheinlichkeiten zwischen den Quarks.

Die Übergänge innerhalb derselben Quarkfamilie (u,d), (c,s), (t,b) finden am häufigsten statt, da die Diagonalelemente die größten Übergangswahrscheinlichkeiten anzeigen. Es besteht mit geringerer Wahrscheinlichkeit auch die Möglichkeit, dass sich die Generation des Teilchens ändert. Dieses Verhalten wird dadurch verursacht, dass die Masseneigenzustände nicht mit den so genannten Wechselwirkungseigenzuständen übereinstimmen.

Der Zerfall von Quarks oder Leptonen durch neutrale Ströme, also z. B. die Übergänge c → u oder s → d oder μ → e wurden bisher nicht beobachtet.

Die Neutrino-Eigenzustände der schwachen Wechselwirkung formula_61, formula_62, formula_63 (Flavour-Zustände sind Eigenzustände des schwach wechselwirkenden Teils des Hamilton-Operators) sind nicht identisch mit den Eigenzuständen des Massenoperators formula_64, formula_65, formula_66 (Eigenzustände des kinematischen Teils des Hamilton-Operators). Analog zur CKM-Matrix lässt sich hier die sog. Pontecorvo-Maki-Nakagawa-Sakata (PMNS)-Matrix einführen

Aktuelle Werte liegen bei:

Die Matrix hat große Werte auch außerhalb der Diagonalen. Dies unterscheidet sie von der CKM-Matrix und führt zu einer starken Mischung der Neutrinofamilien mit der Zeit.

Wurde ein Neutrino ursprünglich mit einem bestimmten dieser drei Flavours erzeugt, so kann eine spätere Quantenmessung einen anderen Flavour ergeben (Erhaltung der Leptonenfamilienzahlen ist verletzt). Da die Wahrscheinlichkeiten für jeden Flavour sich periodisch mit der Ausbreitung des Neutrinos ändern, spricht man von Neutrinooszillationen.

Beim Zerfall eines (linkshändigen) Leptons durch die schwache Wechselwirkung ändert sich während der Wechselwirkung nicht der Flavour (Erhaltung der Leptonenfamilienzahl in jedem Wechselwirkungsvertex), jedoch können sich entstehende Neutrinos in der weiteren Zeitevolution ineinander umwandeln, wodurch sich der Flavour ändert und somit die Leptonenfamilienzahl-Erhaltung verletzt ist. Die Leptonenzahl ist jedoch bei dieser Oszillation stets erhalten.

Hätten die Neutrinos keine Masse, dann wäre jeder Flavorzustand auch ein Eigenzustand des Massenoperators. Folglich könnte man keine Flavor-Oszillationen beobachten.

Im Folgenden werden für die Lagrange-Dichte formula_71 der schwachen Wechselwirkung die Wechselwirkungsanteile zwischen Fermionen und Eichbosonen analysiert.

Um die Beschreibung der schwachen Wechselwirkung besser einordnen zu können, wird zunächst die elektromagnetische Wechselwirkung beschrieben. Alle im Folgenden mit griechischen Indizes versehenen Größen stellen Vierervektoren dar.

In der Quantenelektrodynamik ist die Wechselwirkungsenergie die Kopplung von (Vierer-)Strömen geladener Teilchen formula_72 an Photonen, dargestellt durch das elektromagnetische (Vierer-)Potential formula_73, gegeben durch:
Die Kopplungskonstante ist die Elementarladung formula_12. Die Stromdichte ist gegeben durch

wobei formula_77 die Ladungsquantenzahl (die elektrische Ladung der Teilchen in Einheiten der Elementarladung) ist, formula_78 sind die Dirac-Matrizen. formula_79 ist das Feld des einlaufenden Fermions (bzw. auslaufenden Antifermions) mit (Vierer-)Impuls formula_80 und formula_81 das des auslaufenden Fermions (bzw. des einlaufenden Antifermions) mit Impuls formula_82. In einem Feynman-Diagramm beschreiben die Spinoren formula_79 und formula_81 die äußeren durchgezogenen Linien.

Die Streuung zweier geladener Teilchen wird in der Bornschen Näherung (niedrigste Ordnung Störungstheorie) durch das nebenstehende Feynman-Diagramm beschreiben. Die dazugehörige Streuamplitude ist

An jeden Vertex der Ladung formula_86 muss ein Faktor formula_87 multipliziert werden. Am Vertex gilt wegen der Energie-Impuls-Erhaltung für den Vierervektor des Photons formula_88.

Innere Linien des Feynman-Diagramms sind die sog. Propagatoren, hier der Photonenpropagator formula_89, wobei formula_90 der (Vierer-)Impulsübertrag und formula_91 der metrische Tensor der speziellen Relativitätstheorie ist.

Bei der schwachen Wechselwirkung beschreiben formula_92 (neutral current) und formula_93 (charged current) die Summanden der Lagrange-Dichte, die die Wechselwirkung zwischen Fermionen und den Eichbosonen enthalten.

Die schwachen geladenen Ströme werden durch folgenden Wechselwirkungsanteil beschrieben:
Die formula_95-Bosonen koppeln mit derselben Kopplungskonstante formula_96 an alle linkshändigen Leptonen und Quarks.

Bei der Beschreibung der einzelnen Strömen tritt jeweils der Chiralitätsoperator formula_97 auf (dieser transformiert einen polaren in einen axialen Vektor). Bei massiven Teilchen wandelt dieser Teilchenspinoren positiver Helizität in Antiteilchenspinoren negativer Helizität um und umgekehrt (formula_98). Daraus lässt sich der Linkshändigkeitsoperator konstruieren:

Dieser Operator auf einen Spinor formula_100 angewandt, projiziert auf den linkshändigen Anteil:
Wegen des Auftretens dieses Operators ist die schwache Wechselwirkung eine chirale Theorie. Der linkshändige Strom

ist die (Halbe) Differenz aus Vektorstrom formula_103 und Axialvektorstrom formula_104, deswegen V minus A (siehe: V-A-Theorie).

Schwache geladene linkshändige Quarkströme mit formula_105,
formula_106, formula_107,
formula_108 ist die CKM-Mischungsmatrix:

Schwache geladene linkshändige Leptonenströme mit formula_110,
formula_111:

An einen formula_113 Vertex muss folgender Faktor multipliziert werden:

Der Propagator für massive (Masse formula_115) Spin-1-Teilchen, wie es die W- und Z-Bosonen sind, lautet:

Da für die meisten Fälle formula_117 gilt, kann der Propagator genähert werden. Im Gegensatz zum Photonenpropagator formula_118 ist der Propagator formula_119 für kleine Impulsüberträge konstant.

Bei kleinen formula_120-Werten ist die schwache Wechselwirkung viel schwächer als die elektromagnetische. Dies liegt nicht an der Kopplungskonstante der schwachen Wechselwirkung, denn die Kopplungsstärke formula_121 liegt in derselben Größenordnung wie die elektrische Ladung formula_12. Der Grund für die Schwäche der Wechselwirkung liegt in der Gestalt des Propagators der Austauschteilchen, da die riesige Bosonenmasse im Nenner steht und somit den Wechselwirkungsterm herabsetzt.

Die durch ein W-Boson vermittelte Streuung zweier Leptonen, hat eine Streuamplitude (in niedrigster Ordnung) von:

In der genäherten Form
wird die Streuamplitude durch die Kopplung zweier linkshändiger Ströme mittels einer Kopplungskonstanten beschrieben. Dies wurde von Enrico Fermi durch die Fermi-Wechselwirkung, und zwar als Wechselwirkung von vier beteiligten Teilchen an einem Raumzeitpunkt, beschrieben. Die Fermi-Konstante hat den Wert formula_125.

Die schwachen neutralen Ströme werden durch den folgenden Wechselwirkungsanteil beschrieben:

Die formula_127-Bosonen koppeln mit der Kopplungskonstante formula_128 an den neutralen Strom formula_129. Dieser setzt sich aus einem Isospin-Strom formula_130 und dem elektromagnetischen Strom formula_72 zusammen.

Der Isospin-Strom berechnet sich über

f steht dabei für die Spinor-Wellenfunktion des Fermions. formula_133 ist die dritte Komponente des schwachen Isospins. Sie wird wie folgt berechnet:


Wegen des Linkshändigkeits-Operators formula_140 koppelt das formula_127-Boson über den Isospin-Strom also nur an die linkshändigen Anteile von Fermionen.

Der elektromagnetische Strom berechnet sich gemäß

wobei formula_143 die elektrische Ladung des beteiligten Fermions bezeichnet.

Bei der Berechnung von Streuquerschnitten mit Hilfe von Feynman-Diagrammen muss für jeden formula_127 Vertex der Faktor formula_145 mit einem weiteren Faktor in Abhängigkeit der beteiligten Teilchenart multipliziert werden. Dieser lautet


Bei den letzten drei Faktoren treten Summanden ohne den Linkshändigkeits-Operator auf. Diese Z-Kopplungen wirken damit sowohl auf links- und rechtshändige Anteile der beteiligten Fermionen.

Bei den Neutrinos formula_136 koppeln also nur die linkshändigen Anteile an das Z-Boson. Bei den geladenen Fermionen formula_135, formula_138 und formula_139 koppeln dagegen rechts- als auch linkshändige Anteile an das Z-Boson. Bei der Streuung geladener Fermionen kann somit neben der Wechselwirkung über ein elektromagnetisches Feld auch eine Wechselwirkung über das Feld des ungeladenen Z-Bosons stattfinden. Sind die beteiligten Teilchenenergien klein im Vergleich zur Ruheenergie des Z-Bosons überwiegt bei Streuprozessen allerdings die elektromagnetische Wechselwirkung.

In der elektroschwachen Theorie lassen sich elektromagnetische und schwache neutrale Ströme kombinieren. Statt elektromagnetische Ströme an Photonen und schwache neutrale Ströme an Z-Bosonen

koppeln nun Isospin-Ströme an formula_163- und Hyperladungs-Ströme an formula_164-Bosonen:

Wobei ein Hyperladungsstrom basierend auf der Hyperladung eines Fermions formula_166 eingeführt wurde:

Der Zusammenhang der Eichbosonen ist über den Weinbergwinkel mit formula_168 (wobei formula_169 das Photon ist) und formula_170 sowie formula_171 gegeben und der Zusammenhang der beiden schwachen Kopplungskonstanten mit der Elementarladung über formula_172.

Die schwache Wechselwirkung wurde zuerst beim Betazerfall entdeckt (für dessen Geschichte siehe den Artikel Betazerfall). Die Entdeckung, dass der Betazerfall ein kontinuierliches Spektrum zeigte und scheinbar die Energieerhaltung verletzte, führte Wolfgang Pauli 1930 zur Postulierung des Neutrinos als drittem Zerfallspartner. Darauf aufbauend formulierte, nachdem 1932 auch noch das Neutron entdeckt worden war, Enrico Fermi 1934 eine erste Theorie des Betazerfalls. Sie hatte einen ähnlichen Aufbau wie die Quantenelektrodynamik (QED), aber die Form einer Stromkopplung mit verschwindender Reichweite und mit einer dimensionsbehafteten Kopplungskonstante. Sie war im Gegensatz zur QED nicht renormierbar. Weitere Fortschritte in den 1930er Jahren waren die Auswahlregeln von George Gamow und Edward Teller (Gamow-Teller-Übergänge, 1936) und die Entdeckung der Rolle der schwachen Wechselwirkung bei der Nukleosynthese in Sternen durch Gamow und Hans Bethe (1938) und bei der Bildung von Neutronensternen in Supernovae (Robert Oppenheimer, Lew Landau). Außerdem wurden bis in die 1950er Jahre neue schwache Prozesse entdeckt wie die Zerfälle von Myonen, Pionen, Kaonen und Hyperonen. 

In den 1950er Jahren wurde die Paritätsverletzung der schwachen Wechselwirkung entdeckt (theoretisch vorgeschlagen von Tsung-Dao Lee, Chen Ning Yang 1956, experimentell entdeckt durch Chien-Shiung Wu 1957). Das wurde in der V−A-Theorie der schwachen Wechselwirkung von Richard Feynman und Murray Gell-Mann einerseits und Robert Marshak und George Sudarshan andererseits 1958 eingebaut, ein wichtiger Schritt zur modernen Theorie der schwachen Wechselwirkung im Standardmodell. Dazu trugen Sheldon Lee Glashow, Abdus Salam und Steven Weinberg mit der Vereinigung von elektromagnetischer und schwacher Wechselwirkung Ende der 1960er Jahre bei (mit Einführung massiver Vektorbosonen, deren Austausch die punktförmige Wechselwirkung in der Fermi-Theorie ersetzte), sowie Makoto Kobayashi and Toshihide Maskawa mit dem Einbau der 1964 von James Cronin und Val Fitch entdeckten CP-Verletzung in die Theorie über ihre KM-Matrix bzw. CKM-Matrix (zusätzlich nach Nicola Cabibbo, der zur Beschreibung schwacher Zerfälle seltsamer Teilchen 1963 den Cabibbo-Winkel einführte).





Manfred Eigen

Manfred Eigen (* 9. Mai 1927 in Bochum; † 6. Februar 2019 in Göttingen) war ein deutscher Bio- und Physikochemiker sowie Direktor am Max-Planck-Institut für biophysikalische Chemie in Göttingen. Eigen wurde 1967 in Anerkennung seiner Arbeiten zur Geschwindigkeitsmessung von schnellen chemischen Reaktionen der Nobelpreis für Chemie verliehen.

Eigen stammte aus einer Musikerfamilie. Er besuchte das humanistische Gymnasium in Bochum (bis 2010 Gymnasium am Ostring) und studierte ab 1945 Physik und Chemie an der Universität Göttingen, wo er 1951 bei Arnold Eucken auch promovierte. 1953 holte ihn Karl Friedrich Bonhoeffer an das Max-Planck-Institut für physikalische Chemie in Göttingen, wo er 1958 wissenschaftliches Mitglied, 1962 Leiter der Abteilung für chemische Kinetik und 1964 Direktor des Instituts wurde, das 1971 von ihm erheblich erweitert werden konnte.

Ab 1965 war er Honorarprofessor an der TU Braunschweig.

Manfred Eigen starb im Februar 2019 im Alter von 91 Jahren.

Manfred Eigen entwickelte kinetische Methoden zur Untersuchung extrem schneller Reaktionen. Mittels der Relaxationsmethode konnte er die Kinetik von schnellen biochemischen Reaktionen untersuchen. Eigens Name ist mit der Theorie des Hyperzyklus verknüpft, der zyklischen Verknüpfung von Reaktionszyklen als Erklärung für die Selbstorganisation von präbiotischen Systemen, die er zusammen mit Peter Schuster im Jahre 1979 beschrieb. Der Eigen-Wilkins-Mechanismus wurde nach ihm benannt.

Über die Erforschung von Enzymreaktionen wandte er sich später der Erforschung der Evolution zu. Eigen studierte das Verhalten von Nukleinsäuren, die durch Polymerase vervielfältigt und durch Nukleasen abgebaut wurden. Durch die Wiederholung der Abbau- und Aufbauzyklen kam es zum Aufbau von Nukleinsäuren, die durch Mutation gegen den Abbau durch die Nukleasen resistent waren. Die Experimente dauerten dabei oft nur wenige Stunden.

Diese Versuche führten zur Entwicklung sogenannter "Evolutionsmaschinen." Dabei handelt es sich um Bioreaktoren, in denen sich zum Beispiel Viruskulturen züchten und deren Evolution unter Laborbedingungen beobachten lassen.

Durch die Variation der Reaktorparameter lassen sich die Häufigkeit der Mutationen und die Geschwindigkeit der Evolution beeinflussen. Das Verfahren wird heute in technischem Maßstab genutzt.

Eigen gründete zwei Biotechnologiefirmen, "Evotec" und "Direvo," die auf den Gebieten des Hochdurchsatz-Screenings und der gerichteten Evolution "(directed evolution)" tätig sind.

Von 1983 bis 1993 war Eigen als Präsident der Studienstiftung des deutschen Volkes tätig. In dieser Funktion forderte er die Bildung einer Leistungselite, was ihm von zahlreichen Seiten Kritik eintrug. Er war Schirmherr des alljährlichen XLAB-Science-Festivals in Göttingen.

Seit dem Frühjahr 2015 existiert die "Manfred Eigen-Förderstiftung," die eine „unselbstständige Stiftung innerhalb des privaten Vermögens der Max-Planck-Gesellschaft“ ist. Sie fördert wissenschaftliche Projekte am Max-Planck-Institut für biophysikalische Chemie und ist eine Verbrauchsstiftung.

1962 wurde Eigen mit dem Otto-Hahn-Preis für Chemie und Physik ausgezeichnet. 1964 wurde er in die American Academy of Arts and Sciences und zum Mitglied der Leopoldina gewählt, 1966 in die National Academy of Sciences und 1968 in die American Philosophical Society aufgenommen.

Der Nobelpreis für Chemie wurde 1967 auf zwei Forscherteams aufgeteilt. Der 40-jährige Eigen, der mit dem belgischen Chemiker Leo De Maeyer zusammenarbeitete, wurde gemeinsam mit Ronald George Wreyford Norrish und George Porter für seine Studien über die Kinetik extrem schnell ablaufender chemischer Reaktionen mit Relaxationsmethoden ausgezeichnet.

Ab 1965 war Manfred Eigen ordentliches Mitglied der Akademie der Wissenschaften zu Göttingen, ab 1971 Ehrenmitglied der Österreichischen Akademie der Wissenschaften und ab 1972 korrespondierendes Mitglied der Bayerischen Akademie der Wissenschaften, 1973 wurde er in den Orden Pour le Mérite aufgenommen und ab 1976 war er Mitglied der Sowjetischen (heute: Russischen) Akademie der Wissenschaften. Die Republik Österreich ehrte ihn 1976 mit dem Österreichischen Ehrenzeichen für Wissenschaft und Kunst.

1980 erhielt er den Niedersachsenpreis der Kategorie Wissenschaft. 1992 wurde ihm der Paul-Ehrlich-und-Ludwig-Darmstaedter-Preis zuerkannt. 1994 verlieh ihm die Berlin-Brandenburgische Akademie der Wissenschaften die Helmholtz-Medaille. Im selben Jahr erhielt er gemeinsam mit Rudolf Rigler vom Karolinska-Institut den Max-Planck-Forschungspreis. Ab 2001 war Manfred Eigen Ehrenbürger der Ruhr-Universität Bochum. 2002 wurde er zum Ehrenbürger der Stadt Göttingen ernannt. Er hat mehrere Ehrendoktorwürden empfangen, etwa die der Harvard University. 2005 erhielt er den Lifetime Achievement Award des Institute of Human Virology in Baltimore. 2007 wurde Eigen mit der Goldenen Goethe-Medaille und 2011 mit der Wilhelm-Exner-Medaille ausgezeichnet.








Christian Doppler

Christian Andreas Doppler (* 29. November 1803 in Salzburg; † 17. März 1853 in Venedig) war ein österreichischer Mathematiker und Physiker. Er ist durch den nach ihm benannten "Doppler-Effekt" und die Dopplertemperatur bekannt geworden.

Christian Dopplers Vater war Steinmetz in Salzburg. Wegen seiner schwachen körperlichen Konstitution war Christian Doppler selbst jedoch den Anforderungen dieses Berufs nicht gewachsen und studierte am Polytechnischen Institut Wien Mathematik sowie Physik und Philosophie in Salzburg. Ab 1829 arbeitete er als Assistent am Polytechnischen Institut Wien und veröffentlichte seine ersten Publikationen. Ab 1835 lehrte er an einer Realschule in Prag und später am Prager Polytechnischen Institut (etwa im Rang einer heutigen Fachhochschule) Mathematik und Physik. 1840 wurde er außerordentliches Mitglied der Königlich Böhmischen Gesellschaft der Wissenschaften. Er wurde 1841 zum Ordentlichen Professor für Mathematik und Physik an das Prager Polytechnische Institut berufen.

In seiner Zeit als Professor publizierte er in Prag mehr als 50 wissenschaftliche Arbeiten über Physik, Mathematik und Astronomie. Sein Hauptwerk, "Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels", verlas er am 25. Mai 1842 vor der "Königlich Böhmischen Gesellschaft der Wissenschaften". Seine Hypothese, die Farbigkeit der Sterne beruhe auf der Entfernungsänderung während der Lichtaussendung, war schon nach dem damaligen Kenntnisstand der Astronomen unhaltbar, – dazu ist die Sternbewegung viel zu langsam. Sein Bemühen um Anerkennung führte aber zum baldigen experimentellen Nachweis des akustischen, nach ihm Doppler-Effekt benannten Phänomens – die ersten Dampflokomotiven waren dafür gerade schnell genug –, sodass sein Name mit dem Effekt verbunden blieb.

Im Zuge der Wirren um die Märzrevolution 1848 zog er mit seiner Familie nach Wien und wurde 1850 von Kaiser Franz Joseph I. zum Direktor des Physikalischen Instituts der Universität Wien berufen. Er war der erste Professor für Experimentalphysik in Altösterreich. Bei seiner beruflichen Tätigkeit wurde Doppler stets von seiner hochintelligenten Frau Mathilde unterstützt.

1853 starb Doppler im damals zum Kaisertum Österreich zählenden Venedig; in das milde Klima der Stadt war er 1852 wegen seiner Staublungenerkrankung ausgewichen. Im Friedhof von San Michele befindet sich am Eingang eine Gedenktafel, sein tatsächliches Grab auf diesem Friedhof ist aber nicht mehr bekannt.

Am 11. April 1836 vermählte sich Doppler in der Pfarrkirche Mülln mit Mathilde Sturm (1813–1874), der zehn Jahre jüngeren Tochter eines Salzburger Goldschmieds. Das Ehepaar bekam zwischen 1837 und 1844 fünf Kinder, die in Prag und Wien großgezogen wurden: Mathilde (1837–1913), Ludwig (1838–1906), Adolf (1840–1916), Berta (1843–1904) und Hermann (1844–1881).

An seinem 100. Geburtstag wurde an seinem Salzburger Geburts- und Wohnhaus gegenüber dem Salzburger Landestheater eine Gedenktafel angebracht; eine solche gab es in Wien schon länger. Der "Christian-Doppler-Fonds" soll Dopplers Namen und Wirken bekannter machen und Forschungsarbeiten zur Nutzung des Doppler-Effektes unterstützen.

Seit 1972 vergibt die Salzburger Landesregierung den Christian-Doppler-Preis für wissenschaftliche und technische Leistungen und Erfindungen. Die "Christian-Doppler-Forschungsgesellschaft" (CDG) fungiert als Träger der Christian-Doppler-Labors, die an österreichischen Hochschulinstituten zur Förderung anwendungsorientierter Grundlagenforschung gegründet wurden.

In Salzburg gibt es ein Christian-Doppler-Gymnasium; die ehemalige Landesnervenklinik wurde in "Christian-Doppler-Klinik" umbenannt. Das Haus der Natur in Salzburg zeigt eine umfangreiche ständige Christian-Doppler-Schau. Auf der Rückseite des Mondes ist ein Krater nach Doppler benannt.

Anlässlich des 150-Jahre-Jubiläums der Entdeckung des Doppler-Effektes wurde 1992 von der österreichischen Post die Sonderbriefmarke „150 Jahre Doppler-Prinzip“ herausgegeben.

Der am 28. August 1984 entdeckte Asteroid (3905) Doppler wurde 1996 nach ihm benannt. Das Advisory Committee on Antarctic Names benannte 1987 ihm zu Ehren den Doppler-Nunatak in der Antarktis.

Anlässlich seines 200. Geburtstages wurde 2003 in der Christian-Doppler-Klinik Salzburg eine Büste von ihm aufgestellt.

In Salzburg sind die "Christian-Doppler-Straße" nahe dem Christian-Doppler-Gymnasium und der "Christian-Doppler-Platz" vor dem Flughafen nach ihm benannt.








Standardmodell

Das Standardmodell der Elementarteilchenphysik (SM) fasst die wesentlichen Erkenntnisse der Teilchenphysik nach heutigem Stand (Beginn des 21. Jahrhunderts) zusammen. Es beschreibt alle bekannten Elementarteilchen und die wichtigen Wechselwirkungen zwischen ihnen: die starke Wechselwirkung, die schwache Wechselwirkung und die elektromagnetische Wechselwirkung. Nur die (vergleichsweise sehr schwache) Gravitation wird nicht berücksichtigt.

In theoretischer Hinsicht ist das Standardmodell eine Quantenfeldtheorie. Ihre fundamentalen Objekte sind Felder, die nur in diskreten Paketen verändert werden; die diskreten Pakete entsprechen in einer passenden Darstellung den beobachteten Teilchen. Das Standardmodell ist so gebaut, dass die von ihm beschriebenen Teilchen und Felder die Gesetze der speziellen Relativitätstheorie erfüllen. Gleichzeitig enthält es die Aussagen der Quantenmechanik und der Quantenchromodynamik.

Viele Voraussagen des Standardmodells wurden durch Experimente der Teilchenphysik bestätigt. Insbesondere ist die Existenz auch derjenigen Elementarteilchen des Modells nachgewiesen, die erst von der Theorie vorhergesagt wurden. Die gemessenen quantitativen Eigenschaften der Teilchen stimmen sehr gut mit den Vorhersagen des Standardmodells überein. Ein besonders deutliches Beispiel dafür ist der g-Faktor des Elektrons.

Es gibt dennoch Gründe für die Annahme, dass das Standardmodell nur ein Aspekt einer noch umfassenderen Theorie ist. Dunkle Materie und Dunkle Energie werden vom Standardmodell nicht beschrieben. Seine Aussagen führen bei hohen Energien, wie sie beim Urknall auftraten, zu Widersprüchen mit der allgemeinen Relativitätstheorie. Außerdem müssen 18 Parameter, deren Werte nicht aus der Theorie hervorgehen, anhand von experimentellen Ergebnissen festgelegt werden. Es wird dadurch recht „biegsam“ und kann sich in einem gewissen Rahmen den tatsächlich gemachten Beobachtungen anpassen. Es gibt auch zahlreiche Bemühungen, das Standardmodell zu erweitern oder abzulösen.

Im Standardmodell wird die Wechselwirkung der Materiefelder durch abstrakte (mathematische) Eichsymmetrien beschrieben, wodurch das Standardmodell auch eine Eichtheorie ist. Die Eichgruppen des SMs sind formula_1, formula_2 und formula_3. Die jeweiligen Ladungen dieser Symmetrien sind die (schwache) Hyperladung, der (schwache) Isospin und die Farbladung. Die drei üblicherweise als Wechselwirkungen des SMs aufgezählten Wechselwirkungen (die elektromagnetische Wechselwirkung, die schwache Wechselwirkung und die starke Wechselwirkung) ergeben sich aus diesen Eichgruppen:

Die Fermionen des Standardmodells und nichtelementare Teilchen, die aus ihnen aufgebaut sind, sind per Konvention die Teilchen, die als „Materie“ bezeichnet werden. Fermionen, die der Farbwechselwirkung unterliegen, werden „Quarks“ genannt; die anderen Fermionen sind „Leptonen“ ("leichte" Teilchen). Sowohl Leptonen als auch Quarks werden aus praktischen Gründen in drei „Generationen“ mit je einem Paar Teilchen unterteilt. Die Teilchen eines Paares unterscheiden sich in ihrem Verhalten bezüglich der formula_2-Eichgruppe und damit in ihrer elektroschwachen Wechselwirkung – besonders nennenswert ist dabei ihre unterschiedliche elektrische Ladung. Äquivalente Teilchen verschiedener Generationen haben nahezu identische Eigenschaften, der deutlichste Unterschied ist die mit der Generation zunehmende Masse.

Die bosonischen Elementarteilchen des Standardmodells unterscheiden sich in ihrem Spin; die Vektorbosonen (Photon, W, Z, Gluon) haben die Spinquantenzahl 1, das Higgs-Boson die Spinquantenzahl 0. Die Existenz der Vektorbosonen ist mathematisch eine notwendige Folge der Eichsymmetrien des Standardmodells. Sie vermitteln die Wechselwirkungen zwischen Teilchen, können aber prinzipiell auch als eigenständige Teilchen auftreten (insbesondere das Photon, das als Elementarteilchen eine „Quantengröße“ elektromagnetischer Wellen darstellt).

Die Gluonen sind Eichbosonen und repräsentieren direkt die Freiheitsgrade der Eichgruppe formula_8 der starken Kraft. Die W- und Z-Bosonen und die Photonen hingegen repräsentieren nicht direkt die Freiheitsgrade der übrigen Eichgruppe formula_9, werden aber gelegentlich trotzdem als Eichbosonen bezeichnet. Die Vektorbosonen des Standardmodells werden auch „Botenteilchen“ oder „Austauschteilchen“ genannt.

Das Higgs-Boson ist keine direkte Folge einer Eichsymmetrie, vermittelt daher keine Wechselwirkung im Sinne des Standardmodells und wird daher auch nicht als Austauschteilchen angesehen. Das Higgs-Boson wird jedoch „benötigt“, um die elektroschwache formula_10-Symmetrie zu brechen und so sowohl dem Z- als auch den W-Bosonen Masse zu verleihen. Am 4. Juli 2012 wurde in einem Seminar am CERN bekanntgegeben, dass durch Experimente am Large Hadron Collider ein Boson nachgewiesen wurde, das in allen bisher untersuchten Eigenschaften mit dem Higgs-Boson übereinstimmt, was weitere Messungen bestätigen konnten.

Das Standardmodell der Teilchenphysik kann nahezu alle bisher beobachteten teilchenphysikalischen Beobachtungen erklären. Allerdings ist es unvollständig, da es die gravitative Wechselwirkung gar nicht beschreibt. Außerdem gibt es auch innerhalb der Teilchenphysik einige offene Fragen, die das Standardmodell nicht lösen kann, wie z. B. das Hierarchieproblem und die Vereinigung der drei Grundkräfte. Auch die inzwischen bestätigte, von Null verschiedene Masse der Neutrinos führt über die Theorie des Standardmodells hinaus.

Es existiert eine Vielzahl alternativer Modelle, aufgrund derer das etablierte Standardmodell lediglich um weitere Ansätze erweitert wird, um einige Probleme besser beschreiben zu können, ohne sein Fundament zu verändern. Die bekanntesten Ansätze für neue Modelle sind Versuche zur Vereinigung der drei im Standardmodell vorkommenden Wechselwirkungen in einer Großen vereinheitlichten Theorie (GUT). Solche Modelle beinhalten häufig auch Supersymmetrie, eine Symmetrie zwischen Bosonen und Fermionen. Diese Theorien postulieren zu jedem Teilchen des Standardmodells Partnerteilchen mit vom Originalteilchen unterschiedlichen Spin, von denen bisher jedoch noch keines nachgewiesen werden konnte. Ein anderer Ansatz zur Erweiterung des Standardmodells ergibt Theorien der Quantengravitation. Solche Ansätze beinhalten beispielsweise die Stringtheorien, die auch GUT-Modelle enthalten, sowie die Schleifenquantengravitation.

Zusammenfassend gibt es noch folgende offene Fragen im Standardmodell:






Festkörperlaser

Als Festkörperlaser werden optisch angeregte Laser bezeichnet, deren verstärkendes (aktives) Medium aus einem kristallinen oder glasartigen (amorphen) Festkörper besteht. In diesem sog. Wirtsmaterial oder Wirtskristall sind in bestimmter Konzentration (Dotierung) die laseraktiven Ionen enthalten.

Festkörperlaser werden mit Licht oder Infrarotstrahlung gepumpt.

Der Wirtskristall oder ein Glas ist mit Ionen eines fremden Stoffes dotiert. Diese Fremdionen sind das eigentliche aktive Medium der Festkörperlaser.

Die für das Lasern genutzten Elektronenniveaus dieser Ionen liegen innerhalb des d-Orbitals (Titan, Chrom, Cobalt) bzw. f-Orbitals (Neodym, Erbium, Ytterbium). Diese Orbitale sind nicht an chemischen Bindungen beteiligt. Das Trägermaterial (Wirtskristall, Glas) hat daher nur geringen Einfluss auf die Laser-Eigenschaften der Ionen.

Um im aktiven Medium eine Energieaufnahme zu erreichen, müssen Elektronen auf ein höheres Energieniveau gehoben werden. Dieser Vorgang heißt Pumpen. Festkörperlaser werden immer optisch, d. h. durch Strahlung, gepumpt. Aus der für die Dotierungsionen charakteristischen Energiedifferenz zwischen unterem und oberem Energieniveau ergibt sich die wirksame Pumplichtwellenlänge.

Das durch Pumpen erreichte Energieniveau stimmt nicht mit oberem und unterem Laserniveau überein (man spricht von 3- oder 4-Niveau-Lasern): Leert sich z. B. das untere Laserniveau sehr schnell durch Gitter-Relaxationen in das untere Energieniveau, lässt sich viel leichter eine zum Lasern erforderliche Besetzungsinversion erreichen, da ja das untere Laserniveau kaum gefüllt ist.

Ebenso ist es hilfreich, wenn die Elektronen im oberen Laserniveau eine lange Verweildauer haben – deren Energie kann dann mit einer Güteschaltung schlagartig als Lichtpuls freigesetzt werden.

Die Betriebsart kann kontinuierlich („“, CW) oder gepulst sein. Die Pulsung kann durch das Pumpen (Blitzlampen) oder einen resonatorinternen () optischen Schalter (Güteschalter) erfolgen. Kombiniert man beides (hohe Pump-Spitzenleistung durch Blitzlampe, danach Freisetzung der im oberen Laserniveau gespeicherten Energie durch Öffnen des Güteschalters), sind während einiger Nanosekunden Spitzenleistungen von einigen Megawatt erreichbar.

Durch Nachverstärkung und Impulsweitung und -kontraktion sind während weniger Femtosekunden Leistungen im Petawattbereich erzielbar. Mit Festkörperlasern lassen sich die höchsten Spitzenleistungen und die kürzesten Impulsdauern aller Laserarten erzeugen.




Das Pumpen erfolgt z. B. durch das Beleuchten des Lasermediums (Laserstab) mit intensiven Lichtquellen wie zum Beispiel Gasentladungslampen (Bogenlampen oder Blitzlampen).
Die gepulste Laseremission – bei den mit Blitzlampen gepumpten Festkörperlasern- zeigt oft eine stark unregelmäßige Struktur. Die statistischen Peaks der ausgesendeten Laserintensität werden Spikes genannt und sind auf das Fallen (bei Emission) und Steigen der Elektronendichte im gehobenen Energieniveau zurückzuführen.

Die verwendeten Gasentladungslampen müssen einen möglichst hohen Spektralanteil bei der Pump- Wellenlänge (im Allgemeinen im nahen Infrarot NIR) besitzen. Es sind Krypton- oder Xenon-Bogenlampen mit Wolfram-Elektroden, die einzeln oder zu zweit parallel zum Stab angeordnet sind.

Laserstab und Lampen sind meist wassergekühlt (deionisiertes Wasser umspült Lampen und Stab).

Der Laserstab muss möglichst gleichmäßig ausgeleuchtet sein. Das erreicht man mit Innenreflektoren aus einer Goldschicht oder Halbschalen aus einer diffus reflektierenden weißen Keramik.

Der Laserstab muss vor harter Ultraviolettstrahlung der Lampen geschützt werden – dazu dient ein Schutzglasrohr.

Seitdem Halbleiterlaser ausreichender Leistung verfügbar sind, werden Festkörperlaser häufig mit Laserdioden geeigneter Wellenlänge optisch gepumpt.
Dadurch sind ganz neuartige Festkörperlaser realisierbar geworden (Faserlaser und Scheibenlaser), jedoch ergeben sich auch Vorteile bei konventionellen Festkörperlasern durch das Pumpen mit Laserdioden.

Vorteile

Nachteile

Bei Faser- und Scheibenlasern entfällt das Problem der thermischen Einflüsse auf die optischen Eigenschaften – mit ihnen lassen sich daher hohe Leistungen bei guter Strahlqualität erzeugen. Jedoch muss die Pumpstrahlung auf kleine Flächen konzentriert werden, weshalb das Pumpen nur mit Diodenlasern möglich ist.

Bei Scheibenlasern durchläuft die Pumpstrahlung die Scheibe mehrfach, indem sie mit einem Prismenreflektor mehrfach zurück auf die Scheibe gelenkt wird, um möglichst vollständig absorbiert zu werden.

Beim Faserlaser gelangt die fokussierte Pumpstrahlung durch die Endfläche der Faser in diese hinein (endgepumpt) oder eine Umhüllung () der Faser führt die Pumpstrahlung entlang dem aktiven (dotierten) Faserkern. Auch die umgekehrte Anordnung (Pumpstrahlung im Kern) ist möglich.

Ein Resonator ist (außer beim Faserlaser) erforderlich und besteht wie bei anderen Lasern aus einem 100 %-Spiegel (Endspiegel) und einem teildurchlässigen Spiegel (Auskoppelspiegel). Es sind für die Laserwellenlänge geeignete dielektrische Interferenz-Spiegel, da Metallspiegel die Strahlintensität nicht überstehen bzw. zu große Verluste aufweisen.

Innerhalb der Spiegel befindet sich der an seinen Endflächen entspiegelte Kristallstab sowie ggf. weitere optische Bauteile, z. B. Kristalle zur Frequenzverdopplung/-vervielfachung oder zur Güteschaltung.

Festkörperlaser sind die neben dem Kohlendioxidlaser am häufigsten in der Industrie zur Materialbearbeitung eingesetzten Laser.
Typische Anwendungen sind:

Weitere vielfältige Anwendungen gibt es im wissenschaftlichen Bereich. Die Laser mit den kürzesten Pulslängen und den höchsten Spitzenleistungen sind Festkörperlaser.

Der erste je gebaute Laser, entwickelt von Maiman im Jahre 1960, war ein Festkörperlaser – ein lampengepumpter Rubinlaser.

Lampengepumpte für kontinuierlichen und gepulsten Betrieb sowie Nd:Glas-Laser für sehr hohe Pulsenergien bildeten lange Jahre die wesentlichsten Vertreter von Festkörperlasern in Industrie und Forschung.

Seit etwa 1995 erobern durch die Möglichkeit des Pumpens mit Laserdioden eine Vielzahl neuartiger Festkörperlaser und aktiver Materialien zahlreiche neue Anwendungen in Forschung und Industrie.

Herausragende Ergebnisse sind Kurzpulslaser bis in den Sub-Pico-Sekunden-Bereich, miniaturisierte frequenzverdoppelte Festkörperlaser (z. B. grüne Laserpointer) und die extrem gute Fokussierbarkeit von Scheiben- und Faserlasern, die hohe Arbeitsabstände (z. B. in 1 Meter Abstand Metall schweißen) bzw. gute Schnittleistungen ermöglichen.

Festkörperlaser lösen durch ihre gestiegene Effizienz, Strahlqualität und Leistung vielfach die industriellen CO-Laser mittlerer Leistung ab, da auf diese Weise z. B. die Strahlübertragung mit Lichtwellenleitern möglich ist und die Absorption auf Metallen besser ist.






Dirac-Notation

Die Dirac-Notation, auch Bra-Ket-Notation, ist in der Quantenmechanik eine Notation für quantenmechanische Zustände. Die Notation geht auf Paul Dirac zurück. Die ebenfalls von ihm eingeführte Bezeichnung Bra-Ket-Notation ist ein Wortspiel mit der englischen Bezeichnung für eine Klammer ("bracket"). In der Bra-Ket-Notation wird ein Zustand ausschließlich durch seine Quantenzahlen charakterisiert.

In der Bra-Ket-Notation schreibt man die Vektoren eines Vektorraums formula_1 auch außerhalb eines Skalarprodukts mit einer spitzen Klammer als Ket formula_2. Jedem Ket formula_2 entspricht ein Bra formula_4 der dem Dualraum formula_5 angehört, also eine lineare Abbildung von formula_1 in den zu Grunde liegenden Körper formula_7 repräsentiert, und umgekehrt. Das Ergebnis der Operation eines Bras formula_8 auf einen Ket formula_9 wird formula_10 geschrieben, womit der Zusammenhang mit der konventionellen Notation des Skalarprodukts hergestellt ist.

In der Physik wird die Notation verwendet, gleich ob es sich dabei um Vektoren eines Vektorraumes oder um Funktionen in einem Hilbert-Raum handelt. Die mathematische Rechtfertigung für die Bra-Ket-Notation ergibt sich aus dem Satz von Fréchet-Riesz, den F. Riesz und M. Fréchet 1907 unabhängig voneinander bewiesen. Er besagt unter anderem, dass ein Hilbertraum und sein topologischer Dualraum isometrisch isomorph zueinander sind.

Sei formula_11 ein Vektor eines komplexen formula_12-dimensionalen Vektorraums formula_13. Der Ket-Ausdruck formula_14 kann als vertikaler Vektor mit komplexen Elementen formula_15 (formula_16) dargestellt werden:

Wichtig ist dabei, dass formula_14 und der dazugehörige transponierte Vektorformula_19 "nicht" dasselbe mathematische Objekt sind und somit kein Gleichheitszeichen verwendet werden darf. Dies wird insbesondere daran deutlich, dass die Bra-Ket-Schreibweise von der Wahl einer Basis unabhängig ist, während die Darstellung durch Koordinatenvektoren die Wahl einer Basis voraussetzt. Stattdessen sollte deutlich gemacht werden, dass es sich bei formula_19 um die Darstellung von formula_14 handelt. Dies kann durch die Verwendung von Zeichen wie formula_22, formula_23, formula_24 etc. erfolgen.

Der Bra-Ausdruck formula_25 kann demnach als horizontaler Vektor mit den konjugierten Werten dargestellt werden:

Durch die Notation formula_27 kann ein Elektron im Zustand 1s mit Spin up des Wasserstoffatoms bezeichnet werden.

Der Polarisationszustand eines Photons kann als Überlagerung zweier Basiszustände formula_28 (vertikal polarisiert) und formula_29 (horizontal polarisiert), angegeben werden:
wobei
und

Gegeben sei eine Anzahl von formula_33 Bosonen formula_34 mit jeweils einem bestimmten Impuls formula_35. Der Zustand lässt sich mittels der Dirac-Notation kompakt abbilden:

Das Skalarprodukt eines Bra formula_36 mit einem Ket formula_37 wird in Bra-Ket-Notation geschrieben als:
Dies kann als Anwendung des Bras formula_36 auf den Ket formula_37 aufgefasst werden.

Für komplexe Zahlen formula_41 und formula_42 gilt:

Aufgrund der Dualitätsbeziehung gilt außerdem:

Das Tensorprodukt eines Ket formula_45 mit einem Bra formula_46 wird geschrieben als

Im Fall gewöhnlicher Vektoren entspricht das Tensorprodukt einer Matrix.

Für eine vollständige Orthonormalbasis formula_48 führt die Operation

eine Projektion auf den Basiszustand formula_50 aus.
Dies definiert den Projektionsoperator auf den Unterraum des Zustands formula_50:

Eine besonders wichtige Anwendung der Multiplikation von Ket mit Bra ist der Einheitsoperator formula_53, der sich als Summe über die Projektionsoperatoren ergibt zu

Diese „Darstellung des Einheitsoperators“ ist insbesondere deshalb von so herausragender Bedeutung, da man damit jeden Zustand formula_56 in einer beliebigen Basis entwickeln kann.

Ein Beispiel einer "Basisentwicklung durch Einschieben der Eins":

Dies ist die Darstellung des Zustands-Kets formula_58 in der formula_33-Basis durch das sogenannte "Einschieben der Eins".

Dass dies "immer" funktioniert, ist eine unmittelbare Konsequenz der Vollständigkeit des Hilbertraums, in dem die Zustände, also die "Kets", 'leben'.

Für eine kontinuierliche Basis ist statt der Summe ein Integral zu bilden.
So erhält man beispielsweise für den Ortsraum die Summe über das Ortskontinuum und damit den Einheitsoperator als Integral über den ganzen formula_60:
Natürlich ist auch mit einer solchen kontinuierlichen Basis eine Basisentwicklung möglich, was in der Regel auf ein Fourierintegral führt. Technisch handelt es sich dabei "nicht"  um eine Entwicklung nach Basisvektoren des Hilbertraums, da es in den betrachteten separablen Räumen kein Kontinuum von paarweise orthogonalen Vektoren geben kann: Vektoren der Art formula_62 bilden vielmehr eine mathematisch nicht-triviale Erweiterung des betrachteten Hilbertraums, und man nennt sie daher auch manchmal „uneigentliche Vektoren“, weil sie wie die Deltafunktion oder wie monochromatische ebene Wellen nicht quadratintegrierbar sind. (Auch der Begriff der Orthogonalität muss hierbei verallgemeinert werden, indem man statt der sonst üblichen Kroneckersymbole formula_63 Deltafunktionen benutzt.)

Beachtet man bei Rechnungen diese Details, die im Grunde nur auf die „Rezepte“   formula_64  und formula_65 hinauslaufen, so bleibt die Basisentwicklung eine brauchbare Analogie.

In der Quantenmechanik arbeitet man häufig mit Projektionen von Zustandsvektoren auf eine bestimmte Basis anstatt mit den Zustandsvektoren selbst.

Die Projektion auf eine bestimmte Basis wird Darstellung genannt.
Ein Vorteil davon ist, dass die so erhaltenen Wellenfunktionen komplexe Zahlen sind, für die der Formalismus der Quantenmechanik als partielle Differentialgleichung geschrieben werden kann.



Allgemein gilt, dass Skalarprodukte bei einem beliebigen Basiswechsel invariant sind. Beispiele sind die Übergänge („Darstellungswechsel“) von einem vollständigen Satz von Eigenvektoren und/oder uneigentlichen Eigenvektoren selbstadjungierter Operatoren des Systems zum anderen, z. B. der Übergang von einem Matrixsystem zum anderen oder der Übergang von einer Matrixdarstellung zur Orts- oder Impulsdarstellung.




M-Theorie

Die M-Theorie ist der Versuch einer Erweiterung und Verallgemeinerung der Stringtheorie in der Theoretischen Physik. Diese Theorie ist ein Gebiet intensiver Forschung, da man hofft, mit ihr alle bekannten Naturkräfte einheitlich beschreiben zu können.

Die M-Theorie wurde während der so genannten "zweiten Superstringrevolution" geboren, wobei wesentliche Beiträge von Edward Witten stammen, der darüber 1995 auf einer Konferenz an der University of Southern California einen vielbeachteten Vortrag hielt. Hierbei werden die fünf bekannten Superstringtheorien (Type I, Type IIA und IIB sowie die beiden Heterotischen Stringtheorien, im Bild mit E8 und SO(32) bezeichnet) und die elfdimensionale Supergravitation als Grenzfälle einer fundamentaleren Theorie betrachtet.

Die Verbindungen zwischen den verschiedenen Theorien sind durch Dualitäten gegeben, wie S-Dualität und T-Dualität. Mit ihrer Hilfe kann man zeigen, dass die unterschiedlichen Theorien die gleichen Ergebnisse berechnen, allerdings in unterschiedlichen Bereichen ihres Parameterraumes. Damit ist es möglich, Aussagen über die zugrundeliegende Theorie in verschiedenen Grenzbereichen zu machen, obwohl eine explizite Formulierung nicht bekannt ist.

Die elfdimensionale Supergravitation nimmt in gewisser Weise eine Sonderstellung ein, da sie die maximale Anzahl von Dimensionen für eine Supergravitationstheorie besitzt, im Gegensatz zu den Stringtheorien, welche in zehn Dimensionen formuliert sind. Elfdimensionale Supergravitation ist außerdem eine klassische (d. h. nicht quantisierte) Theorie, wohingegen die Stringtheorien Quantentheorien sind. Die Verbindung der Supergravitation mit der heterotischen E8×E8-Stringtheorie bzw. Type IIA wird durch eine Kompaktifizierung der elften Dimension auf einem Intervall (in der Abbildung mit formula_1 bezeichnet) bzw. auf einem Kreis (formula_2) erreicht. Außerdem betrachtet man auf der Stringseite den Supergravitations-Limes der Theorie.

Nichtperturbative Aussagen zur M-Theorie lassen sich mit Hilfe von D-Branen bzw. M-Branen machen. Allerdings gibt es zurzeit noch keine vollständige nichtperturbative Formulierung der M-Theorie, was auch damit zusammenhängt, dass sich für mehr-als-ein-dimensionale Objekte keine konforme Feldtheorie konstruieren lässt (siehe Polyakov-Wirkung).

Die angegebene Bedeutung für den Buchstaben M in der Bezeichnung der Theorie ist nicht einheitlich. Edward Witten schlug den Begriff 1995 mit Hinblick auf die Membranstruktur der Theorie vor. Zugleich war er selbst skeptisch, ob die noch junge Theorie tatsächlich auf eine solche hinauslaufen würde und bemerkte schon sehr früh eher scherzhaft, das 'M' könne „wahlweise für magisch, rätselhaft (Original: mystery) oder Membran stehen“. Weitere Kandidaten, die genannt wurden, waren Matrix (nach einem Vorschlag für die M-Theorie von Tom Banks und anderen), "Mutter aller Stringtheorien" und es wurde sogar vermutet, es stehe für ein umgedrehtes „W“ als Synonym für „Witten“ oder dass der Buchstabe „M“ als einziger Buchstabe neben dem „W“ fünf Punkte harmonisch verbindet, wobei die fünf Punkte für die fünf zuvor widersprüchlichen Stringtheorien stehen. In einem 2013 veröffentlichten Interview stellte er klar: „Manche Kollegen dachten, es gäbe eine elfdimensionale Theorie, die auf Membranen basiert. Doch ich war nicht davon überzeugt, dass sie vollständig funktioniert. Ich wusste aber auch nicht, ob sie falsch ist, und ich wollte ihr nicht widersprechen. Daher behielt ich das M von „Membran“ und meinte, dass es sich mit der Zeit schon zeigen würde, ob das M für „Magie“, „Mysterium“ oder „Membran“ steht. Später wurden die Membranen dann von Matrizen abgeleitet, und zufällig fängt die Matrix-Theorie auch mit „M“ an.“

Als eine mögliche Darstellung der M-Theorie schlugen Tom Banks, Willy Fischler, Stephen Shenker und Leonard Susskind Ende 1996 das BFSS-Matrix-Modell vor. Sie stellten dabei heraus, dass der formula_3 Limes einer supersymmetrischen Matrix-Theorie äquivalent zur elfdimensionalen, nicht kompaktifizierten M-Theorie ist. Die Membranen der M-Theorie sind dabei Anregungen in der Matrix-Theorie, wobei das Weltvolumen der Membranen eine nichtkommutative Geometrie aufweist, die in eine nichtkommutative Raumzeit eingebettet ist.

Ein einfacher Fall des BFSS Modells ist dabei eine 0+0 dimensionale Theorie, deren Wirkung als dimensionale Reduktion einer 9+1 dimensionalen Super-Yang-Mills-Theorie geschrieben wird

wobei formula_5 hermitesche N×N-Matrizen sind, formula_6 SO(9) Pauli-Matrizen in ihrer Darstellung in 16 Dimensionen und formula_7 ein Spinor mit spin(9,1) mit 16 Komponenten. Grob gesagt beschreibt diese Wirkung eine Quantenmechanik mit Matrix-Freiheitsgraden.






Zustand (Thermodynamik)

Der Zustand eines Systems in der Thermodynamik wird durch die Angabe aller für die Abgrenzung zu anderen Zuständen notwendigen Zustandsgrößen charakterisiert.

Zustandsgrößen sind unter anderem Druck, Volumen, Temperatur, Stoffmenge. Diese Größen werden unterschieden nach extensiv (mengenabhängig) und intensiv (nicht mengenabhängig).

Die einzelnen Zustände werden häufig in ein Phasendiagramm eingetragen. Dabei entspricht (je nach Auftragung) meist ein einzelner
Punkt einem Zustand im Zustandsraum, manchmal gehören jedoch auch alle Punkte einer Phasenlinie oder Fläche zu einem einzigen Zustand.

In der statistischen Mechanik (statistische Thermodynamik) gibt es folgende Unterscheidung:

"KZKKZZKK" sei ein spezieller Mikrozustand.
5x "K" und 3x "Z" ist dann der Makrozustand, zu dem formula_1 Mikrozustände gehören (die Zahl ergibt sich aus der Kombinatorik als Permutation von Objekten zweier Klassen "K" und "Z" unter Beachtung der Reihenfolge).





Jones-Formalismus

Der Jones-Formalismus beschreibt lineare optische Abbildungen unter Berücksichtigung der Polarisation. Er wurde nach R. Clark Jones benannt, der diese Darstellung 1941 einführte. Das Licht wird als ebene elektromagnetische Welle repräsentiert, mit einem komplexwertigen zweidimensionalen Jones-Vektor, der Amplitude der Welle, und kann daher genutzt werden um optische Effekte wie Interferenz zu beschreiben. Damit stellt der Formalismus eine Verbesserung ggü. den Stokes-Parametern dar. Im Gegensatz dazu ist der Jones-Formalismus jedoch auf vollständig polarisiertes, kohärentes Licht begrenzt.
Die Abbildungen werden durch Jones-Matrizen dargestellt. Mit ihnen ermöglicht der Jones-Formalismus die Modellierung und Analyse optischer Systeme, in denen ein Lichtstrahl eine Kaskade von optischen Bauelementen durchläuft.

In komplexer Schreibweise hat die Elongation einer monochromatischen ebenen Welle in einem kartesischen Koordinatensystem die Orts- und Zeitabhängigkeit
wobei als Ausbreitungsrichtung die formula_2-Achse gewählt ist. Die reellen Zahlen formula_3 und formula_4 bezeichnen die Kreiswellenzahl bzw. die Kreisfrequenz der Welle. Die komplexen Zahlen formula_5 bzw. formula_6 beschreiben Phase und Amplitude der formula_7- bzw. formula_8-Komponente des Feldes. Der Jones-Vektor dieser Welle ist dann

das heißt, die explizite Raum- und Zeitabhängigkeit der Amplitude wird bei der Beschreibung der Welle unterdrückt. Des Weiteren werden in der Darstellung eines Jones-Vektors üblicherweise dessen Komponenten auf 1 normalisiert und ein Vorfaktor eingeführt, damit die Intensität unverändert bleibt (siehe Beispiele).

Der Effekt eines optischen Bauelements auf die Lichtwelle lässt sich durch die Wirkung einer komplexwertigen 2×2-Matrix formula_10 auf den Jones-Vektor beschreiben, wenn das Element keine nichtlinearen Eigenschaften hat,
Durchläuft der Lichtstrahl ein System optischer Elemente mit Jones-Matrizen formula_12, so lässt sich der Gesamteffekt des optischen Systems durch eine Jones-Matrix
beschreiben (sofern Mehrfachreflexionen zwischen den einzelnen Komponenten keine Rolle spielen). Die Eigenpolarisationen eines optischen Systems entsprechen den Eigenvektoren seiner Jones-Matrix. Der Jones-Vektor eignet sich nur für die Beschreibung vollständig polarisierten Lichts, und entsprechend können nur optische Komponenten, die keine depolarisierenden Eigenschaften besitzen, durch Jones-Matrizen charakterisiert werden. Sind Depolarisations-Effekte von Bedeutung, muss auf den aufwändigeren Stokes-Formalismus zurückgegriffen werden.

Jones-Matrizen können z. B. lineare Polarisationen oder zirkulare Polarisationen (Rotation der Polarisationsebene) und Verzögerungsplatten beschreiben. Bei der formula_14-Viertel Platte wird z. B. eine Polarisationsrichtung gegenüber der dazu senkrechten um eine Viertel Wellenlänge verzögert. Bei zirkularer Polarisation und Verzögerung ändert sich der Betrag der Gesamtamplitude nicht, und die Matrizen sind unitär, es gilt formula_15 (dabei bedeutet formula_16 komplex konjugiert und T die Transposition der Matrix) und formula_17. Bei linearer Polarisation kann sich der Betrag der Gesamtamplitude ändern, die zugehörigen Matrizen sind nicht unitär.
Gemäß der üblichen Sprechweise in der Optik, bezeichnen „H“ wie "horizontal" und „V“ wie "vertikal" die Orientierung in die "x"- und "y"-Richtung.
Wenn es nicht auf die Interferenz mit anderen Strahlen ankommt, kann ein gemeinsamer (komplexer) Phasen-Vorfaktor ausgeklammert werden, und die Matrizen werden häufig so angegeben, dass die erste Diagonalstelle reell ist.

Wird ein optisches Bauteil gegenüber seiner optischen Achse um den Winkel "θ" gedreht, so ist die Jones-Matrix für das gedrehte Bauteil M("θ"). Diese Matrix erhält man aus der Matrix M für das ungedrehte Bauteil durch folgende Transformation:

Man kann die reine x- und reine y-Polarisation als Orthonormalbasis auffassen und diese in Bra-Ket-Schreibweise darstellen, wie oben in der Tabelle angedeutet. Ein Polarisationsfilter lässt sich dann zum Beispiel als quantenmechanischer Operator auffassen, der auf einen Eigenzustand des Systems (reine x- oder y-Polarisation) projiziert (Kollaps der Wellenfunktion). Der entsprechende Projektor wäre für einen x-Polarisationsfilter: formula_20 Der Eigenwert entspricht dann dem Anteil des einfallenden Lichtes, das die entsprechende Polarisation aufweist. Die Observable ist die Polarisation in x-Richtung. Analog lassen sich die oben angegebenen Filter für zirkular polarisiertes Licht konstruieren.

In der Bra-Ket-Darstellung lässt sich auch ein Basiswechsel leicht ausführen. Die Basiswechselmatrix formula_21, die von der x/y-Basis in die Darstellung durch Superposition von gegensinnig zirkular polarisierten Wellen überführt hat folgende Gestalt.

Solche Überlegungen bieten einen anschaulichen Bezug zu den sonst eher abstrakten Formalismen der Quantenmechanik.






Christian Christiansen (Physiker)

Christian Christiansen (* 9. Oktober 1843 in Loenborg, Dänemark; † 28. November 1917) war ein dänischer Physiker.

Nach seinem Studium wurde Christiansen Lehrer an der örtlichen Polytechnischen Schule. 1886 berief die Universität Kopenhagen ihn auf einen Lehrstuhl für Physik. 

Christiansen arbeitete hauptsächlich auf den Gebieten der optischen Dispersion und der Wärmestrahlung und entwickelte den nach ihm benannten Christiansen-Filter (Christiansen-Effekt). Er fand zahlreiche Absorptionsspektren mit anomaler Dispersion, so unter anderem das Anilin-Rot.





Jacques-Arsène d’Arsonval

Jacques-Arsène d’Arsonval (* 8. Juni 1851 in La Porcherie, Département Haute-Vienne; † 31. Dezember 1940 (nach anderen Angaben: 13. Dezember 1940) ebenda) war ein französischer Physiker.

Jacques-Arsène d’Arsonval studierte an den Universitäten Poitiers, Limoges und Paris. Er studierte auch Medizin. Ob er als Arzt praktizierte, ist unbekannt, er arbeitete jedoch als Präparator bei Claude Bernard und führte mit Charles-Édouard Brown-Séquard Experimente zur Physiologie der Atmung durch. Die Doktorwürden erhielt er 1877. 1894 bekam er in Paris seine Professur verliehen.

Er arbeitete vor allem auf dem Gebiet der Elektrophysiologie und Diathermie, seine Anwendungen von Hochfrequenz führten zum damaligen Begriff einer "d’Arsonvalisation". Er ist der Erfinder des Hitzdrahtamperemeter (d’Arsonval-Galvanometer). Zusammen mit Paul Marie Oudin erfand er die Oudinspule zur Erzeugung von Hochspannung. Er war auch an der Einführung des Telephons in Frankreich beteiligt und arbeitete auf dem Gebiet der Wärmelehre. Die Idee eines Meereswärmekraftwerks geht auf eine Idee d’Arsonvals aus dem Jahre 1881 zurück. Auch die Verflüssigung von Gasen und Flüssigluftsprengstoff war eines seiner Arbeitsgebiete.

Seit 1894 war er Mitglied der Académie des sciences.





Mathematisches Pendel

Das mathematische Pendel oder ebene Pendel ist ein idealisiertes Fadenpendel. Hierbei kann eine als punktförmig gedachte Masse, die mittels einer masselosen Pendelstange an einem Punkt aufgehängt ist, in einer vertikalen Ebene hin und her schwingen, wobei Reibungseffekte, insbesondere der Luftwiderstand vernachlässigt werden. Das ebene Pendel ist ein Spezialfall des Kugelpendels, das sich auch in andere Raumrichtungen bewegen kann. Da die Bewegung des Pendelkörpers auf einem vertikalen Kreis erfolgt, wird es auch als Kreispendel bezeichnet, obwohl damit häufiger das Kegelpendel gemeint ist.

In der Praxis kann man ein mathematisches Pendel dadurch annähern, dass man einen möglichst langen und dünnen Stab oder (falls die Auslenkung kleiner als 90° ist) einen dünnen Faden und einen möglichst kleinen und schweren Pendelkörper verwendet. Dass bei diesem Aufbau die Schwingungsweite (Amplitude) erst nach einer großen Anzahl Schwingungen spürbar zurückgeht, zeigt, dass hierbei die Reibung nur einen geringen Einfluss hat.

Pendel, welche die genannten Eigenschaften des mathematischen Pendels nicht nähererungsweise erfüllen, lassen sich durch das kompliziertere Modell des physikalischen Pendels beschreiben.

Die Schwingungsdauer ist unabhängig von der Masse des schwingenden Körpers. Bei kleinen Schwingungen ist die Schwingungsdauer auch nahezu unabhängig von der Größe der Amplitude. Hier zeigt das Pendel eine nahezu harmonische Schwingung, deren Schwingungsdauer ausschließlich von der Länge des Pendels und der herrschenden Fallbeschleunigung bestimmt wird. Die Schwingungsdauer verlängert sich bis ins Unendliche, je näher die Amplitude an 180° herankommt. Größere Anregungen führen zu „Überschlägen“, sodass der Pendelkörper sich periodisch im Kreis bewegt.

Anhand der Kräfte wird im Folgenden die Bewegungsgleichung der Pendelschwingung aufgestellt.

Aufgrund der Schwerkraft (formula_1, formula_2 = Schwerebeschleunigung) ergibt sich bei Auslenkung eines Fadenpendels der Masse formula_3 eine Kraft formula_4, die tangential zur kreisförmigen Pendelbahn wirkt. Die radiale Komponente spielt für die Bewegung keine Rolle, da sie in Richtung des Fadens wirkt. Da das mathematische Pendel nur einen Freiheitsgrad besitzt, genügt eine skalare Gleichung. Der Betrag der Rückstellkraft steigt mit dem Auslenkungswinkel formula_5 bezüglich der Ruhelage. Hierbei zeigt der Vektor der Rückstellkraft formula_6 immer in Richtung der Ruheposition, daher ergibt sich ein Minus in folgender Gleichung:

Beim Betrachten eines schwingenden Fadenpendels zeigt sich, dass die Geschwindigkeit mit zunehmender Auslenkung abnimmt und nach Erreichen des Scheitelpunkts die Richtung wechselt. Die Geschwindigkeitsänderung bedeutet, dass die Pendelmasse eine Beschleunigung erfährt, genauer gesagt findet eine Tangentialbeschleunigung statt, da eine kreisförmige Bewegungsbahn vorliegt. Die Bewegungsgleichung lautet nach dem 2. Newtonschen Gesetz:

Die Tangentialbeschleunigung lässt sich durch die Winkelbeschleunigung formula_9 ausdrücken.

Bei der ungestörten Schwingung stellt die Rückstellkraft des Pendels die einzige äußere Kraft dar. Nach Umstellen und Kürzen der Masse entsteht eine nichtlineare Differentialgleichung zweiter Ordnung:

die sich mit Hilfe der Winkelgeschwindigkeit formula_13 auch als System von zwei gekoppelten Differentialgleichungen erster Ordnung schreiben lässt:

Für kleine Winkel gilt die Kleinwinkelnäherung:

Durch Substitution ergibt sich somit eine lineare Differentialgleichung zweiter Ordnung der allgemeinen Form formula_16, deren allgemeine Lösung formula_17 zur Schwingungsgleichung führt.

Hierbei bezeichnen formula_20 die Winkelamplitude und formula_21 den Nullphasenwinkel zum Zeitpunkt formula_22. Darüber hinaus sind die Eigenkreisfrequenz formula_23 und die zugehörige Periodendauer formula_24 ersichtlich.

Da Pendel in der Realität immer mehr als infinitesimal ausgelenkt werden, verhalten sie sich nichtlinear, d. h. Schwingungen mit endlicher Amplitude sind anharmonisch. Die allgemeine Differentialgleichung ist elementar nicht lösbar und erfordert Kenntnisse über elliptische Integrale. Damit lässt sich die allgemeine Lösung für die Periode in eine Reihe entwickeln:

Alternativ lässt sich das auftretende elliptische Integral auch über das arithmetisch-geometrische Mittel formula_28 auswerten:

Außerdem ist die Dämpfung durch Reibungsverluste bei einem echten Pendel größer als Null, so dass die Auslenkungen ungefähr exponentiell mit der Zeit abnehmen.

Dass die Periodendauer nicht von formula_3, sondern nur von dem Verhältnis formula_31 abhängt, lässt sich auch aus einer Dimensionsanalyse, z. B. mit dem Buckinghamschen Π-Theorem, herleiten. Nur der numerische Faktor (formula_32 bei kleinen Amplituden, formula_33 in der exakten Lösung) ist so nicht zu ermitteln.

Der Winkel formula_5 als explizite Funktion der Zeit formula_35 mit Startwinkel formula_36 und (positiver) Startgeschwindigkeit formula_37 lautet:

formula_38

mit formula_39 , wobei formula_40 die Jacobi-Amplitude und formula_41 das elliptische Integral 1. Art ist. Bei einer negativen formula_37 kann die Situation einfach gespiegelt werden indem das Vorzeichen des Startwinkels vertauscht wird.

Beim mathematischen Pendel gilt der Energieerhaltungssatz der Mechanik. Auf dem Weg von der maximalen Auslenkung zur Ruhelage nimmt die potentielle Energie ab. Die mit ihr verbundene Gewichtskraft – genauer: deren tangentiale Komponente – verrichtet Beschleunigungsarbeit, wodurch die kinetische Energie zunimmt. Nach Durchschreiten des Minimums wirkt eine Komponente der Gewichtskraft entgegen der Bewegungsrichtung. Es wird Hubarbeit verrichtet.

Auch hieraus lässt sich die Differentialgleichung herleiten:
Die Summe ist zeitlich konstant, also
Diese Gleichung hat zwei Lösungen:

Der Zustand des Systems lässt sich durch ein Tupel formula_49 aus dem Winkel formula_5 und der Winkelgeschwindigkeit formula_13 beschreiben.

Es gibt zwei Positionen formula_52 und formula_53, bei dem sich das System in einem mechanischen Gleichgewicht befindet. In beiden Punkten ist die Winkelgeschwindigkeit und die Summe aller angreifenden Kräfte und Momente Null. Der Gleichgewichtspunkt formula_54 bei einem Winkel von Null ist das stabile Gleichgewicht, wenn das Pendel keine Auslenkung und Geschwindigkeit besitzt. Der zweite Punkt formula_55 ist das instabile Gleichgewicht, wenn das Pendel keine Geschwindigkeit besitzt und „auf dem Kopf“ steht.





Boyle-Temperatur

Die Boyle-Temperatur ist die Temperatur, bei der sich ein unter einem kleinen Druck stehendes, reales Gas wie ein ideales Gas verhält. Bei dieser Temperatur verschwindet der zweite Virialkoeffizient. Die pV-p-Graphen (Isothermen im Druck-Volumen-Diagramm) verlaufen, zumindest bei niederen Drücken, dann nach dem Boyle-Mariotteschen Gasgesetz und damit horizontal. Dieser Temperaturwert wurde nach Robert Boyle benannt. Das Formelzeichen der Boyle-Temperatur ist "T". Sie ergibt sich aus folgender Näherung:

Nutzt man die Virialgleichungen und beendet die Näherung beim zweiten Glied der Reihe, ergibt sich für die Boyle-Temperatur:

also

Hierbei stehen die einzelnen Formelzeichen für folgende Größen:


Bei Temperaturen unterhalb der Boyle-Temperatur ist B(T) negativ.






Lenzsche Regel

Die Lenz’sche Regel (auch: "Lenz’sches Gesetz" oder "Regel von Lenz") ist eine Aussage über die Richtung des elektrischen Stromes bei elektromagnetischer Induktion, benannt nach Emil Lenz. Dieser veröffentlichte seine Überlegungen erstmals 1833 und bezog sich dabei auf die vorausgegangenen Arbeiten Michael Faradays und André-Marie Ampères.

Aus heutiger Sicht formuliert man die Lenz’sche Regel etwas allgemeiner, als es Lenz ursprünglich tat, dabei betont man in erster Linie die magnetische Flussänderung (siehe unten) als Ausgangspunkt der Induktion:

So gesehen ist die Lenz’sche Regel eine Folgerung des allgemeinen Faraday’schen Induktionsgesetzes:

Neben ihrer Bedeutung in der Geschichte der Physik hat die Lenz’sche Regel vor allem in der Schulphysik einen hohen Stellenwert, dort wird sie meist in der Mittelstufe zum ersten Mal behandelt. In der universitären Ausbildung und in der Forschung wird die Regel als Teilaspekt des Induktionsgesetzes und der Maxwell-Gleichungen dargestellt.

Die elektromagnetische Induktion ist eines der grundlegenden Phänomene der Elektrophysik. Das Induktionsgesetz stellt einen Zusammenhang zwischen Magnetfeldern und elektrischen Spannungen her und ist insbesondere zum Verständnis elektrischer Maschinen notwendig.

Die Lenz’sche Regel besagt, dass der induzierte Strom eine Änderung des magnetischen Flusses zu verhindern sucht. Die Änderung des magnetischen Flusses ist dem Induktionsgesetz (einem Teil der Maxwell-Gleichungen) entsprechend die Ursache für die Entstehung des Induktionsstromes.

Die Lenz’sche Regel steht in unmittelbarem Zusammenhang mit dem Energieerhaltungssatz: Die Energie für den Aufbau des elektrischen Feldes stammt aus dem Magnetfeld. Ihre physikalische Aussage entspricht der des Minuszeichens innerhalb des Induktionsgesetzes, das in integraler Form wie folgt lautet:

Auf der linken Seite steht die induzierte Spannung (Integration der elektrischen Feldstärke formula_3 über einen geschlossenen Weg formula_4), auf der rechten die zeitliche Änderung des magnetischen Flusses (Integration des Skalarprodukts von magnetischer Flussdichte formula_5 und dem Flächennormalenvektor über die vom Weg formula_4 umschlossene Fläche A).

Kurzfassung: Die Induktionsspannung wirkt immer ihrer Ursache (Änderung des magnetischen Flusses) entgegen.






Jupiterdurchgang von Saturn, Neptun oder Uranus aus

Ein Jupiterdurchgang vor der Sonne von Saturn, Uranus oder Neptun aus gesehen findet statt, wenn Jupiter direkt die Sichtlinie äußerer Planet - Sonne kreuzt. 

Während des Transits kann Jupiter von den äußeren Planeten aus als kleine schwarze Scheibe gesehen werden, die sich über die Oberfläche der Sonne bewegt. Die galileischen Monde Jupiters können ebenfalls gesehen werden, da jedoch ihr Winkeldurchmesser von Saturn aus nur ungefähr eine Bogensekunde und entsprechend weniger von den entfernteren Planeten aus beträgt, werden sie nur sehr schwer zu erkennen sein. Der interessanteste Fall ist ein Transit Jupiters von Saturn aus gesehen. Jupiter würde dabei einen größeren Teil der Sonne bedecken als bei jedem möglichen anderen Transit, an dem Planeten des Sonnensystems beteiligt wären. 
Nach Berechnungen aus dem Jahre 1886 von Albert Marth hat jedoch ein solcher Transit in den letzten 2000 Jahren nicht stattgefunden und wird auch in den 2000 kommenden Jahren nicht stattfinden.

Jupiters Winkeldurchmesser beträgt ungefähr 40,5 Bogensekunden und der der Sonne über 3,2 Bogenminuten von Saturn aus gesehen, so dass Jupiter 4,4 % der Scheibe der Sonne bedeckt. Die synodische Periode des Systems Jupiter-Saturn beträgt 19,85887 Jahre (7253,45 Tage), die entsprechenden Perioden der Systeme Uranus-Jupiter 13,81195 Jahre (5044,81 Tage) und Neptun-Jupiter 12,78219 Jahre (4668,69 Tage). Die gegenseitige Neigung der Bahnen von Jupiter und Saturn ist 1,25°. Die Neigungen 
der Bahnen von Jupiter und von Uranus bzw. Jupiter und Neptun betragen 0,70° bzw. 0,94°.

Die Endlichkeit der Lichtgeschwindigkeit kann einen beträchtlichen Effekt haben, zum Beispiel sind Lichtlaufzeiten in einer Richtung: 122 Minuten (Jupiter-Uranus) und 165 Minuten (Sonne-Uranus) für den 1997 stattfindenden Transit Jupiters von Uranus aus.




Chad (Einheit)

Chad ist die veraltete Einheit des Neutronenflusses, benannt nach dem britischen Physiker und Entdecker des Neutrons Sir James Chadwick.

1 Chad = 1 Neutron/(cms)





Natürliche Konvektion

Bei natürlicher oder auch freier Konvektion strömt ein Fluid aufgrund von im System vorhandenen Dichteunterschieden. Wenn es dabei zu einem geschlossenen Kreislauf kommt, spricht man von "Schwerkraftzirkulation" (siehe Kamineffekt).

Das Gegenteil der freien Konvektion ist die durch technische Hilfsmittel erzwungene Konvektion.

Eine dauerhafte Zirkulation tritt immer dann auf, wenn eine Wärmequelle "tiefer" liegt als eine Wärmesenke (Kühlung), weil das erwärmte Fluid eine geringere Dichte besitzt und deshalb im Gravitationsfeld einen statischen Auftrieb erfährt. Wird Heizung und Kühlung vertauscht, stoppt die Zirkulation, ebenso bei fehlender Gravitation in Satelliten. 

Bei der Erwärmung von Wasser von 0 °C auf 4 °C tritt ausnahmsweise der umgekehrte Fall ein, seine Dichte erhöht sich und es sinkt ab. Das liegt an der Dichteanomalie von Wasser, die in der Umgebung von 4 °C auftritt und eine spezielle Temperaturschichtung bewirkt.

Die Strömungsgeschwindigkeit hängt vom Temperaturunterschied ab und kann für das erwärmte Fluid immer nur in der Richtung von „warm“ nach „kalt“ erfolgen. Für technische Prozesse werden daher häufig Ventilatoren oder Pumpen verwendet, um die Konvektion zu steuern.

Der Dichteunterschied wird durch Erwärmen auf der einen Seite und Abkühlen auf der anderen Seite des Kreislaufes aufrechterhalten. Der daraus resultierende Differenzdruck wird „treibender Druck“ oder auch „wirksamer Druck“ genannt. Man spricht auch von Schwerkraftwirkung.

Der Differenzdruck formula_1 ist vom Dichteunterschied formula_2 und der wirksamen Höhe formula_3 abhängig nach der Formel:


In der technischen Gebäudeausrüstung ist die "Schwerkraftzirkulation" Prinzip jeder Schwerkraftheizung, die allerdings fast vollständig durch die Pumpenheizung ersetzt wurde. Dieses Prinzip wird auch im so genannten "Badestrang" angewendet, der ohne Pumpe parallel zur Warmwasserleitung verläuft und ganzjährig ein warmes Badezimmer bereitstellt.

In Thermosiphonanlagen wird die Wärme über den Sonnenkollektor zugeführt, in dem unter nachfolgender Abkühlung und/oder Entnahme des Wassers die Zirkulation einsetzt.

Im Kesselbau führen besondere Leitblechkonstruktionen zur Zirkulation des Kesselwassers zwischen der Kesselwand und den Nachschaltheizflächen. Damit erreicht man eine gleichmäßige Temperaturverteilung im Kesselmaterial, infolgedessen sich die Wärmespannungen verringern.

Nicht erwünscht ist die Schwerkraftzirkulation als „Rückzirkulation“. Diese tritt beispielsweise auf, wenn bei stillstehender Pumpe das Wasser des Rücklaufs in entgegengesetzter Richtung auf die Pumpe drückt und diese in Bewegung versetzt. Betroffen sind davon Pumpenheizungsanlagen und Solarkreisläufe. Um die Pumpen zu schützen, baut man so genannte Schwerkraftbremsen. Das sind Rückschlagventile oder so genannte „Diskoscheiben“ die – vor oder hinter die Pumpe eingebaut – den Rückstrom des Wassers unterbinden.

siehe Aufwindkraftwerk

In der Erdatmosphäre finden ebenfalls zahlreiche Konvektionsvorgänge statt. In Küstennähe findet man das Land-See-Windsystem. Da sich die Landmasse schneller aufheizt bzw. abkühlt, kommt es über dem Land zu stärkeren tageszeitlichen Schwankungen als über dem Wasser. Daraus resultiert, dass sich die Luft tagsüber über dem Land stärker erwärmt, dort aufsteigt (A) und kühlere Luft vom Meer her nachfließt. Nachts wiederum kühlt die Luft über dem Land stärker ab, d. h., sie sinkt dort zu Boden (B) und strömt auf das Meer hinaus, wo sie wieder erwärmt wird und aufsteigt.

Auch auf der Erdoberfläche kommt es aufgrund von Geländeformen und verschiedenen Oberflächen zu unterschiedlich starken Aufheizvorgängen. Warme Luftmassen lösen sich ab und steigen in Form von Thermik auf. Unterstützt werden können solche Luftbewegungen durch Kondensationsvorgänge, die die aufsteigende Luft aufheizen, indem sie latente Wärme freisetzen, und damit die Aufwärtsbewegung beschleunigen.





Satz von Sarkovskii

Der Satz von Sarkovskii ist ein Satz der Mathematik, der eine wichtige Aussage über die möglichen Perioden
bei der Iteration einer stetigen Funktion macht. Ein Spezialfall des Satzes ist die Aussage,
dass ein stetiges dynamisches System auf der reellen Geraden mit einem Punkt der Ordnung 3 bereits Punkte zu jeder Ordnung besitzt. Dies wird häufig kurz so formuliert, dass Periode 3 Chaos impliziert.

Sei

eine stetige Funktion. Man sagt, dass formula_2 ein "periodischer Punkt der Ordnung (oder der Periodenlänge) m" ist, wenn formula_3 (wobei formula_4 die formula_5-fache Verknüpfung
von formula_6 mit sich selbst bezeichnet) und formula_7 für alle formula_8.
In der Aussage geht es um die möglichen Ordnungen von periodischen Punkten von formula_6. Zu ihrer Formulierung betrachtet man die sogenannte Sarkovskii-Ordnung der natürlichen Zahlen. Es handelt sich dabei um die totale Ordnung
Diese Reihenfolge startet also mit den ungeraden Zahlen in aufsteigender Folge, gefolgt von den Zweifachen der ungeraden Zahlen,
den vierfachen der ungeraden Zahlen usw., und endet mit den Zweierpotenzen in absteigender Folge.

Der Satz von Sarkovskii besagt nun, dass wenn formula_6 einen periodischen Punkt der Länge formula_5 besitzt und formula_13 in der Sarkovskii-Ordnung gilt, dass es dann auch (mindestens) einen periodischen Punkt der Länge formula_14 gibt.

Der Satz besitzt mehrere Konsequenzen. Zum einen, wenn formula_6 nur endlich viele periodische Punkte besitzt,
so müssen diese alle eine Zweierpotenz als Ordnung haben. Wenn formula_6 irgendeinen periodischen Punkt besitzt, so besitzt formula_6 auch einen Fixpunkt. Ferner: sobald es einen Punkt der Ordnung formula_18 gibt, so gibt es
periodische Punkte zu jeder Ordnung. Diese Aussage wird auch Satz von Li und Yorke genannt.

Der Satz von Sarkovskii ist optimal in dem Sinne, dass man zu jeder natürlichen Zahl formula_5 eine stetige Funktion konstruieren kann derart, dass es zu jeder natürlichen Zahl, die in der Sarkovskii Ordnung nach formula_5 kommt (einschließlich formula_5), periodische Punkte mit dieser Periodenlänge gibt, aber keine periodischen Punkte mit kleinerer Ordnung. Es gibt also beispielsweise
Funktionen, die keine periodischen Punkte der Länge formula_18 haben, wohl aber zu allen anderen Zahlen (Periode formula_23 impliziert nicht Chaos).

Der Satz von Sarkovskii gilt nicht für dynamische Systeme auf anderen topologischen Räumen.
Für die Drehung der Kreislinie um 120 Grad (Dritteldrehung) ist jeder Punkt periodisch mit der Länge formula_18, und keine weiteren Periodenlängen tauchen auf.

Dieser Satz wurde 1964 vom ukrainischen Mathematiker Oleksandr Scharkowskyj bewiesen und blieb längere Zeit unbeachtet. Rund 10 Jahre später bewiesen Li und Yorke ohne Kenntnis des Originalresultats den Spezialfall, dass Periode formula_18
Chaos impliziert.





Musilscher Farbkreisel

Der Musilsche Farb- oder Variationskreisel ist ein einfaches Instrument zur kontinuierlichen, stufenlosen Erzeugung von Mischfarben durch Additive Farbmischung mit Hilfe einer rotierenden, zweifarbigen Kreisfläche. Er wurde 1906 vom österreichischen Schriftsteller und Ingenieur Robert Musil für physikalische und wahrnehmungspsychologische Experimente entwickelt.
Zwei aufeinanderliegende Kreisblätter gleicher Größe und unterschiedlicher Farbe werden radial angeschnitten, über die Schnittlinie ineinander verzahnt und leicht gegeneinander verdreht. In der Aufsicht stellt sich dies als eine aus einem großen Kreissegment (mit der Farbe des oberen Kreisblattes) und einem kleinen Kreissegment (mit der Farbe des unteren Kreisblattes) bestehende Kreisfläche dar. Durch weitere Drehung der Kreisblätter gegeneinander lässt sich das Größenverhältnis der beiden Segmente beliebig verändern. 
Versetzt man die Kreisfläche nun in Rotationsbewegung um den Kreismittelpunkt, so entsteht für das menschliche Auge – eine ausreichend hohe Rotationsgeschwindigkeit vorausgesetzt – der Eindruck, die gesamte Kreisfläche besitze eine einzige, aus den Farben der Segmente zusammengesetzte Mischfarbe.

Beim Musilschen Farbkreisel wird die rotierende Kreisfläche über eine Welle kontinuierlich angetrieben. Das Verhältnis der Größe der beiden Kreissegmente – und somit der Farbton der Mischfarbe – kann während des Betriebes stufenlos eingestellt werden.






Exzessvolumen

Das Exzessvolumen "V" ist die Differenz zwischen dem realen Volumen eines Gemisches chemischer Stoffe und dem idealen Volumen, das der Summe der Volumina der Komponenten vor dem Mischen (Reinstoffvolumina) entspricht:

Ist das Volumen des realen Gemisches größer als das des idealen, ist das Exzessvolumen positiv (Volumendilatation), im umgekehrten Fall negativ (Volumenkontraktion). Als Differenz zwischen realem und idealem Verhalten einer Mischung ist das Exzessvolumen eine Exzessgröße.

Bezogen auf die Stoffmenge "n" des Gemisches spricht man vom molaren Exzessvolumen:

Das molare Volumen des Gemisches ist gleich der Summe der partiellen molaren Volumina der Komponenten:

Das partielle molare Volumen eines Stoffes A ist das Volumen, das dieser Stoff als Komponente zum Gesamtvolumen einer Mischung mehrerer Stoffe A und B beiträgt. Es ist sowohl von dem anderen Stoff B als auch vom Mischungsverhältnis abhängig und "nicht" immer identisch mit dem molaren Volumen, das der Stoff A als Reinstoff einnimmt: 

mit

Der Volumen-Effekt des Mischens reiner Stoffe ist relativ klein. Zumeist beträgt die Differenz nur um die ein bis zwei Prozent.






Astroparticle European Research Area

Das Astroparticle European Research Area (ASPERA) ist ein von der Europäischen Kommission gefördertes Forschungsnetzwerk, das Wissenschaftler und staatliche Institutionen aus europäischen Ländern vereint, die im Bereich der Astroteilchenphysik tätig sind bzw. diesen Bereich finanziell fördern. Ziel des Netzwerks ist, die nationalen Anstrengungen der Mitgliedsländer zu bündeln und so europaweit koordiniert die Astroteilchenphysik voranzubringen. ASPERA wird im Rahmen des sechsten Forschungsrahmenprogramms der Europäischen Union mit insgesamt 2,5 Millionen Euro im Zeitraum von 2006 bis 2009 gefördert.

Der Begriff ASPERA wurde in Anlehnung an das lateinische Sprichwort "per aspera ad astra" gewählt, was wörtlich "durch das Raue zu den Sternen" heißt, sinngemäß also "ohne Fleiß kein Preis".

Zu den Zielen des Forschungsnetzwerk ASPERA gehören u. a.:


ASPERA hat Mitte 2006 begonnen und läuft drei Jahre. Bis Juli 2008 soll die wissenschaftliche Strategie ("Roadmap") formuliert sein, die klare Prioritäten in den einzelnen Bereichen der Astroteilchenphysik benennt.

Bis Januar 2009 will man einen gemeinsamen Aktionsplan vorlegen, wie neue Forschungsinfrastruktur (z. B. große Detektoren und Teleskope) im Bereich der Astroteilchenphysik finanziert werden soll. Bis Juli 2009 sollen die nationalen Förderprogramme entsprechend koordiniert und an den Aktionsplan angeglichen werden. Dann läuft ASPERA aus und die als technisch und finanziell machbar erkannten Programme starten.

Von 01.07.2009 bis 30.06.2012 wurde ASPERA als ASPERA-2 fortgeführt. Im Sommer 2012 wurde die European Astroparticle Physics Strategy (APPEC) gegründet die bis 2026 laufen soll

Zu Beginn von ASPERA verschaffen sich die beteiligten Wissenschaftler und Agenturen einen Überblick über den aktuellen Status im Bereich der Astroteilchenphysik in Europa. Dazu finden Workshops und spezielle Aktionstage in allen beteiligten Ländern statt.

Nach der Erfassung der aktuellen Lage formuliert eine Arbeitsgruppe innerhalb von ASPERA dann eine gemeinsame Strategie für diesen Wissenschaftszweig. Parallel wird ein Verfahren entwickelt, europaweit Forschungsprojekte zu bewerten, zu vergleichen und europäische Objekte gemeinsam zu verwalten und zu finanzieren.

Der junge Forschungszweig der Astroteilchenphysik verbindet die Erforschung des ganze Kleinen mit der des ganze Großen. Es spielen Aspekte der Teilchenphysik, Astronomie und Kosmologie eine Rolle. Da die Geräte zur Messung von z. B. Neutrinos, Gammastrahlung und Kosmischer Strahlung höchster Energie immer größer und teurer werden, ist ein koordiniertes Vorgehen der europäischen Staaten im Rahmen von ASPERA unerlässlich, um die führende Stellung Europas zu erhalten.

In der wissenschaftlichen Strategie ("Roadmap") von ASPERA formulieren die Wissenschaftler sieben Kernbereiche, in denen unterschiedliche Geräte und Methoden zum Einsatz kommen: Hochenergetische Gammastrahlen, Neutrinomasse, hochenergetische Kosmische Strahlung, hochenergetische Neutrinos, direkter Nachweis Dunkler Materie, Nachweis von Gravitationswellen, Neutrinos geringer Energie und der Zerfall des Protons. ASPERA soll bis zum Jahr 2009 für alle diese Bereiche die beiden wichtigsten Projekte identifizieren und Wege zur Umsetzung prüfen.

Die in ASPERA vereinigten Wissenschaftler haben am 29. September 2008 auf einem Workshop in Brüssel die sieben am stärksten zu fördernden Projekte vorgestellt, die sich vor allem den Hauptfragen dieses Forschungszweigs widmen: Was ist Dunkle Materie? Woher kommt die kosmische Strahlung? Welche Rolle spielen energiereiche Prozesse im Kosmos? Lassen sich Gravitationswellen nachweisen? Um diese Fragen zu klären, sollen sieben Instrumente zum Einsatz kommen:


Ob wirklich alle sieben Projekte, die die Lenkungsgruppe des Roadmap-Prozesses identifiziert hat, umgesetzt werden, ist fraglich. Die Planung ist bei CTA und KM3NeT am weitesten fortgeschritten. Bei beiden Projekten könnte 2012 mit dem Bau begonnen werden.

Zu ASPERA gehören derzeit 17 Forschungsagenturen aus 13 Staaten: 


ASPERA ist offen für neue Mitglieder aus dem Bereich der Astroteilchenphysik.

ASPERA ist eine Fortentwicklung von ApPEC ("Astroparticle Physics European Coordination"), das 2001 sechs Forschungsagenturen aus Europa gegründet haben, um die Koordinierung der Astroteilchenphysik in Europa voranzubringen.





Diamantstruktur

Die Diamantstruktur (auch Diamantgitter, A4-Typ oder Diamanttyp) ist eine Kristallstruktur, das heißt das Anordnungsmuster der Atome eines kristallinen Materials. Entdeckt wurde dieser Strukturtyp beim Diamanten, einer Modifikation des Kohlenstoffs, aber auch weitere Materialien mit Atomen aus der 4. Hauptgruppe (vierwertige Elemente) können in dieser Struktur kristallisieren, beispielsweise Silicium, Germanium und Silicium-Germanium-Legierungen sowie α-Zinn. Analog zum kristallinen Diamanten können auch niedermolekulare Verbindungen des Kohlenstoffs die Diamantstruktur aufweisen, sogenannte Diamantoide. Ihr einfachster Vertreter ist das Adamantan.

Die Diamantstruktur besteht aus einem kubisch-flächenzentrierten Gitter und der Basis {(0,0,0), (1/4,1/4,1/4)}. Anschaulich kann man die Diamantstruktur auch als Kombination zweier ineinander gestellter kubisch-flächenzentrierter Gitter beschreiben, die um 1/4 der Raumdiagonale gegeneinander verschoben sind.

Jedes Kohlenstoffatom ist gleichwertig mit vier Nachbaratomen kovalent gebunden. Die Diamantstruktur entspricht damit der Zinkblende-Struktur (ZnS) mit dem Unterschied, dass die beiden kristallographischen Lagen (0,0,0) und (1/4,1/4,1/4) in der Zinkblende-Struktur von zwei verschiedenen Ionen besetzt sind. In beiden Strukturen ist jedes Atom mit 4 Atomen des gleichen Elements (beim Diamant C-Atome) verbunden. Der Grund dafür ist die Hybridisierung der Atomorbitale der äußersten Schale des Grundzustandes (Kohlenstoff: 1s2s2p) zu vier sp3-Hybridorbitalen (1s 2[sp]). Diese vier Orbitale sind aufgrund der elektromagnetischen Abstoßung mit größtmöglichem Abstand bzw. Winkel (109°28´) zueinander symmetrisch im Raum orientiert, sie zeigen in die Ecken eines gedachten Tetraeders.

Vereinfachende zweidimensionale Abbildungen von Gittern mit vierwertigen Elementen zeigen ein gewöhnliches zweidimensionales Gittermuster. Im dreidimensionalen Raum nehmen die vier Valenzelektronen jedoch eine Position ein, die den vier Ecken eines Tetraeders entspricht, wobei der Atomkern im Mittelpunkt des Tetraeders liegt. In 2D-Darstellungen der 3D-Struktur von Diamanten wird das Atom gezeichnet, von dem aus sich die vier Ecken des Tetraeders in vier Bindungen (Valenzen) ausstrecken.

Bei Aufsicht auf diese Tetraeder der Diamantstruktur liegen drei Valenzen an den drei Ecken eines gleichseitigen Dreiecks und berühren drei benachbarte Atome, die in einer gemeinsamen Ebene liegen. Die vierte Valenz liegt in der Mitte des Dreiecks und berührt ein viertes benachbartes Atom in einer anderen Ebene – näher zum Betrachter hin bzw. weiter vom Betrachter weg. In der Diamantstruktur sind die Tetraeder abwechselnd so gedreht, dass die vierte Valenz zum Betrachter zeigt bzw. von ihm weg.

Die Diamantstruktur hat die . Es handelt sich also um eine kubische Kristallstruktur.

Wie erwähnt kristallisieren die typischen vierwertigen Halbleiter wie Silicium und Germanium in der Diamantstruktur. Durch die starken kovalenten Bindungen existieren keine freien Elektronen und die Materialien weisen bei "T" = 0 K (Temperatur am absoluten Nullpunkt) abgesättigte Valenzen, d. h. vollbesetzte Valenzbänder (VB), auf. Das Leitungsband (LB) ist hingegen völlig leer. Reine Halbleiter ohne Kristallbaufehler sind daher bei "T" = 0 K Isolatoren, denn es stehen keine Ladungsträger (Elektronen oder Defektelektronen) für den Stromtransport zur Verfügung.

Die Bandstruktur von Materialien mit Diamantstruktur weist meist eine Energielücke (indirekte Bandlücke) auf. Diese nimmt je nach Element unterschiedliche Werte an (bei 300 K: "E" =  5,33 eV, "E" = 1,14 eV, "E"= 0,67 eV, "E"= 0,08 eV). Bei den geringen Werten für die Energielücke bei Silicium, Germanium und Zinn reicht bereits die Wärmeenergie bei Raumtemperatur aus, um Elektronen aus dem Valenzband in das Leitungsband zu heben. Die Elektronen im LB und die zurückbleibenden Defektelektronen im VB können nun unter dem Einfluss eines von außen angelegten elektrischen Feldes den elektrischen Strom leiten. Dieser Übergang der Elektronen vom Valenzband zum Leitungsband kann auch durch Photonen verursacht werden (photoelektrischer Effekt). Außerdem kann die Energielücke durch gezieltes Verunreinigen (Dotierung) und den damit entstehenden Haftstellen (ortsgebunden) verringert und somit die Leitfähigkeit erhöht werden (Störstellenleitung).

Da beim Diamant nur vier der acht tetraedischen Lücken von Kohlenstoffatomen besetzt sind, ist das Gitter relativ stark aufgeweitet. Die Packungsdichte der Diamantstruktur – nicht nur beim Diamant – ist somit vergleichsweise klein, nur etwa 34 Prozent des verfügbaren Volumens sind besetzt.

Die besondere Härte von Diamant lässt sich mit dem Strukturmodell allein nicht erklären, sie ist eine Folge der besonders festen und gerichteten kovalenten Bindungen, die die tetraedischen sp-Orbitale des Kohlenstoffs eingehen.





Herbert Dingle

Herbert Dingle (* 2. August 1890 in London; † 4. September 1978 in Hull) war ein britischer Astrophysiker und Naturphilosoph. Er war von 1951 bis 1953 Präsident der Royal Astronomical Society.

Dingle wurde in London geboren, verbrachte aber nach dem Tod seines Vaters seine ersten Jahre in Plymouth. Wegen unzureichender finanzieller Mittel musste er dort jedoch die Schule verlassen und 11 Jahre als Angestellter arbeiten. Mit 25 Jahren gewann er ein Stipendium für das Imperial College in London und graduierte im Jahr 1918. Im selben Jahr heiratete er Alice Westacott, mit welcher er einen Sohn hatte. Als Quäker wurde Dingle der Militärdienst im Ersten Weltkrieg erlassen. Er erhielt eine Position als Assistent Im Physikdepartment, und widmete sich (seinem Mentor Alfred Fowler folgend) dem Studium der Spektroskopie und dabei vor allem ihrer Anwendung in der Astronomie. 1922 wurde Dingle Mitglied ("Fellow") der Royal Astronomical Society, deren Präsident er von 1951 bis 1953 war.

Dingle war ein Mitglied der Okkultations-Expeditionen der britischen Regierung von 1927 (Colwyn Bay) und 1932 (Montreal), welche beide aufgrund von bedecktem Himmel scheiterten. 1932 verbrachte er die meiste Zeit am California Institute of Technology als ein Gelehrter der Rockefeller Foundation. Hier traf er den theoretischen Kosomologen Richard C. Tolman und studierte relativistische Kosmologie.

1938 wurde Dingle Professor für Naturphilosophie am Imperial College und war Professor für Geschichte und Philosophie der Wissenschaft am University College London von 1946 bis zu seinem Ruhestand 1955. Danach trug er den Titel „Professor Emeritus“ für diese Institution. Er war ein Mitbegründer der "British Society for the History of Science", deren Dingle Prize ihm zu Ehren benannt ist, und gründete die "British Society for the Philosophy of Science" und damit zusammenhängend die Zeitschrift "British Journal for The Philosophy of Science".

Dingle war der Autor von "Modern Astrophysics" (1924) und "Practical Applications of Spectrum Analysis" (1950). Er schrieb auch einführende Arbeiten zur Relativitätstheorie wie "Relativity for All" (1922) und die Monographie "The Special Theory of Relativity" (1940). Eine Sammlung von Dingles Lesungen über die Geschichte und Philosophie der Wissenschaft wurde 1954 publiziert. Er interessierte sich auch für Englische Literatur und veröffentlichte "Science and Literary Criticism" (1949) und "The Mind of Emily Brontë" (1974).

Bekannt wurde Dingle auch durch seine Beteiligung an diversen öffentlichen und polemischen Disputen. So kritisierte er in den 1930ern das kosmologische Modell von Edward Arthur Milne als zu spekulativ und nicht auf Erfahrung basierend. Ebenso kritisierte er Arthur Stanley Eddington, wobei sich an dieser Debatte fast jeder prominente britische Astrophysiker und Kosmologe beteiligte. Dingle charakterisierte seine Gegner als „Verräter“ an der wissenschaftlichen Methode und nannte sie „die modernen Aristoteliker“, weil er glaubte, dass ihre theoretischen Überlegungen mehr auf Rationalismus als auf Empirismus basierten. Willem de Sitter unterstützte einige der weniger radikalen Aussagen von Dingles Kritik an Milne und Eddington. Jedoch sind die modernen Kosmologen der Auffassung, dass die hypothetisch-deduktive Methode von Milne gültig ist.

Obwohl er in den 1920ern als einer der ersten ein Lehrbuch über die Spezielle Relativitätstheorie (SRT) verfasst hatte und sie ursprünglich akzeptierte, wandte sich Dingle im Ruhestand von ihr ab und verwarf sie. Ursprünglich argumentierte er, dass die SRT beim Zwillingsparadoxon keine unterschiedliche Alterung der Zwillinge voraussage, aber er erkannte jedoch selbst seine Fehler in der Argumentation. Danach behauptete er, dass die Voraussagen der SRT empirisch falsch seien, obwohl die Experimente das Gegenteil zeigten. Danach fokussierte Dingle seine Kritik wieder auf die Annahme, dass die SRT logisch inkonsistent sei. Vor allem bezog sich Dingle auf die Zeitdilatation, wonach jeder Beobachter die Uhren des anderen wechselseitig langsamer laufen sieht. Für ihn war die Reziprozität der Lorentz-Transformation eine offenkundige Absurdität.

Dingle brachte seine Kritik in einer umfangreichen öffentlichen Kampagne an die Öffentlichkeit, wie zum Beispiel in Briefen an die Zeitschrift Nature. Viele Wissenschaftler (darunter Whitrow) antworteten ihm und erklärten, dass die Reziprozität der Lorentz-Transformation einfach demonstriert werden kann und somit die SRT keineswegs inkonsistent ist. Dingle lehnte die Lösungen jedoch ab und veröffentlichte schließlich (1972) das Buch "Science at the Crossroads", in dem er behauptete, dass die Widerlegung der SRT erbracht sei, jedoch von der gesamten wissenschaftlichen Welt ignoriert bzw. unterdrückt worden sei. Der Konsens in der wissenschaftlichen Gemeinschaft ist jedoch, dass Dingles Einwände zur logischen Konsistenz der SRT unbegründet sind.






Interpretationen der Quantenmechanik

Interpretationen der Quantenmechanik beschreiben die physikalische und metaphysische Bedeutung der Postulate und Begriffe, aus welchen die Quantenmechanik aufgebaut ist. Besonderes Gewicht hat dabei die Interpretation derjenigen Konzepte, wie z. B. des Welle-Teilchen-Dualismus, die nicht nur einen Bruch mit etablierten Vorstellungen der klassischen Physik bedeuten, sondern auch der Anschauung oftmals zuwiderzulaufen scheinen.
Neben der Kopenhagener Interpretation wurden seit Entwicklung der Quantenmechanik in den 1920er Jahren eine Vielzahl alternativer Interpretationen entwickelt. Diese Interpretationen unterscheiden sich in ihren Aussagen über den Determinismus, die Kausalität, die Frage der Vollständigkeit der Theorie, die Rolle von Beobachtern und einer Reihe weiterer metaphysischer Aspekte.

Hinsichtlich ihres empirischen Erfolges gilt die Quantenmechanik als eine der am besten gesicherten physikalischen Theorien überhaupt. Seit ihrer Formulierung in den 1920er Jahren konnte die Quantenmechanik bis heute experimentell nicht falsifiziert werden. Die Frage, wie die Quantenmechanik zu interpretieren ist, wird jedoch kontrovers diskutiert: Beschreibt die Theorie nur physikalische Phänomene oder erlaubt sie auch Rückschlüsse auf Elemente einer hinter den Phänomenen verborgenen Realität? Fragen zur Ontologie der Quantenmechanik lassen sich weder mit experimentellen noch mit theoretischen Methoden der Physik beantworten, weshalb sie von manchen Physikern als unwissenschaftlich angesehen werden. Allerdings zeigt sich, dass sich viele fundamentale Begriffe der Theorie, wie beispielsweise „Messung“, „physikalische Eigenschaft“ oder „Wahrscheinlichkeit“, ohne interpretativen Rahmen nicht eindeutig definieren lassen. Andere Physiker und Philosophen sehen daher die Formulierung einer konsistenten Interpretation, das heißt einer semantischen Deutung des mathematischen Formalismus, als sinnvollen, wenn nicht notwendigen Bestandteil der Theorie an.

Neben der ersten und lange Zeit dominierenden Kopenhagener Interpretation entstanden im Laufe der Zeit zahlreiche alternative Interpretationen der Quantenmechanik, die im nächsten Kapitel beschrieben werden. Im Folgenden werden zunächst einige der physikalischen und philosophischen Prinzipien und Konzepte erläutert, in welchen sich die Interpretationen der Quantenmechanik voneinander unterscheiden.

Die Gesetze der klassischen Physik gelten gemeinhin als deterministisch: Kennt man den aktuellen Zustand eines abgeschlossenen Systems vollständig, kann man theoretisch sein Verhalten, also alle zukünftig möglichen Beobachtungen an diesem System für jeden beliebigen Zeitpunkt, exakt vorhersagen. Jegliches anscheinend zufällige Verhalten und jegliche Wahrscheinlichkeiten resultieren im Rahmen der klassischen Physik ausschließlich aus Unkenntnis, beziehungsweise in konkreten Experimenten aus der Unfähigkeit des Experimentators, den Zustand exakt zu präparieren, oder Unzulänglichkeiten des Messgerätes. Dieser prinzipielle Determinismus besteht auch zum Beispiel für die statistische Mechanik und Thermodynamik.

Viele Interpretationen der Quantenmechanik, darunter insbesondere die Kopenhagener Interpretation, gehen hingegen davon aus, dass die Annahme einer deterministischen Dynamik physikalischer Systeme nicht aufrechterhalten werden kann: Die Tatsache, dass es nicht möglich ist, beispielsweise den Zeitpunkt des Zerfalls eines radioaktiven Atoms vorherzusagen, sei demnach nicht darin begründet, dass ein Beobachter nicht genügend Informationen über etwaige innere verborgene Eigenschaften dieses Atoms besitzt. Vielmehr "gäbe" es keinen Grund für den konkreten Zeitpunkt des Zerfalls; der Zeitpunkt sei „objektiv zufällig“.

Eine entgegengesetzte Ansicht vertreten Befürworter von Verborgene-Variablen-Interpretationen, wie der bohmschen Mechanik. Die Quantenmechanik biete demnach keine vollständige Beschreibung der Natur, sie lasse bestimmte Einflussfaktoren außer Betracht. Wüssten wir um diese, ließe sich auch ein einzelnes künftiges Messergebnis exakt und deterministisch berechnen.

Diesem Konzept liegt die idealisierte Annahme zugrunde, dass bei Beobachtungen beziehungsweise Messungen zwischen einem beobachteten „Objektsystem“ und einem „Beobachter“ unterschieden werden kann, wobei zur Beschreibung der Eigenschaften des Objektsystems der Beobachter nicht mit in Betracht gezogen werden muss. Dieses Ideal lässt sich zwar aufgrund der zur Durchführung der Messung zwingend notwendigen Wechselwirkung zwischen Objektsystem und Messvorrichtung weder in der klassischen Physik noch in der Quantenmechanik vollständig erreichen, jedoch kann man in der klassischen Physik den Einfluss der Messapparatur auf das Objektsystem prinzipiell als beliebig minimierbar annehmen. In der klassischen Physik ist die Messung demnach kein grundsätzliches, sondern nur ein praktisches Problem.

In der Quantenmechanik kann die Annahme eines vernachlässigbaren Einflusses der Messvorrichtung hingegen nicht aufrechterhalten werden. Generell ist jede Wechselwirkung des Objektsystems mit der Messvorrichtung mit Dekohärenzprozessen verbunden, deren Auswirkungen nicht als „klein“ betrachtet werden können. In vielen Fällen (beispielsweise beim Nachweis eines Photons durch einen Detektor) wird das untersuchte Objekt bei der Messung sogar vernichtet. Die gegenseitige Beeinflussung zwischen Objektsystem und Umgebung beziehungsweise Messvorrichtung wird daher in allen Interpretationen der Quantenmechanik berücksichtigt, wobei sich die einzelnen Interpretationen in ihrer Beschreibung des Ursprungs und der Auswirkungen dieser Beeinflussung wesentlich unterscheiden.

Die von der Quantenmechanik postulierte Gesetzmäßigkeit der Zeitentwicklung des Systemzustands und das Auftreten eindeutiger Messergebnisse scheinen in direktem Widerspruch zu stehen: Einerseits erfolgt die Zeitentwicklung des Systemzustands strikt deterministisch, andererseits sind die Messergebnisse nur statistisch vorhersagbar. Einerseits sollen den Systemzuständen im Allgemeinen überlagerte Linearkombinationen von Eigenzuständen entsprechen, andererseits wird kein verwaschenes Bild mehrerer Werte gemessen, sondern stets eindeutige Werte.

Die in den meisten Lehrbüchern zugrunde gelegte orthodoxe Interpretation erklärt die Vorgänge bei der Durchführung einer quantenmechanischen Messung mit einem so genannten "Kollaps der Wellenfunktion", also einem instantanen Übergang des Systemzustands in einen Eigenzustand der gemessenen Observablen, wobei dieser Übergang im Gegensatz zu sonstigen physikalischen Prozessen nicht durch die Schrödingergleichung beschrieben wird. Hierbei wird in der orthodoxen Interpretation offen gelassen, welcher Vorgang in der Messkette zu dem Kollaps führt, der Messprozess wird im Rahmen dieser Interpretation nicht genauer spezifiziert. Viele Physiker und Interpreten halten es dagegen für notwendig, in physikalischen Begriffen anzugeben, was genau eine „Messung“ ausmacht.

Die Erklärung dieses scheinbaren Widerspruchs zwischen deterministischer Systementwicklung und indeterministischen Messergebnissen ist eine der hauptsächlichen Herausforderungen bei der Interpretation der Quantenmechanik.

Ein grundsätzlicher Aspekt bei der Interpretation der Quantenmechanik ist die wissenschaftstheoretische Fragestellung, welche Art von Kenntnis über die Welt diese Theorie vermitteln kann. Die Standpunkte der meisten Interpretationen der Quantenmechanik zu dieser Frage können grob in zwei Gruppen aufgeteilt werden, die "instrumentalistische" Position und die "realistische" Position.

Gemäß der instrumentalistischen Position stellen die Quantenmechanik beziehungsweise die auf Basis der Quantenmechanik ausgearbeiteten Modelle keine Abbildungen der „Realität“ dar. Vielmehr handele es sich bei dieser Theorie lediglich um einen nützlichen mathematischen Formalismus, der sich als Werkzeug zur Berechnung von Messergebnissen bewährt hat. Diese pragmatische Sicht dominierte bis in die 1960er Jahre die Diskussion um die Interpretation der Quantenmechanik und prägt bis heute viele gängige Lehrbuchdarstellungen.

Neben der pragmatischen Kopenhagener Interpretation existiert heute eine Vielzahl alternativer Interpretationen, die bis auf wenige Ausnahmen das Ziel einer realistischen Deutung der Quantenmechanik verfolgen. In der Wissenschaftstheorie wird eine Interpretation als wissenschaftlich-realistisch bezeichnet, wenn sie davon ausgeht, dass die Objekte und Strukturen der Theorie treue Abbildungen der Realität darstellen und dass sowohl ihre Aussagen über beobachtbare Phänomene als auch ihre Aussagen über nicht beobachtbare Entitäten als (näherungsweise) wahr angenommen werden können.

In vielen Arbeiten zur Quantenphysik wird Realismus gleichgesetzt mit dem Prinzip der Wert-Definiertheit. Dieses Prinzip basiert auf der Annahme, dass einem physikalischen Objekt physikalische Eigenschaften zugeordnet werden können, die es eindeutig entweder "hat" oder "nicht hat". Beispielsweise spricht man bei der Beschreibung der Schwingung eines Pendels davon, dass das Pendel (zu einem bestimmten Zeitpunkt, und innerhalb einer gegebenen Genauigkeit) eine Auslenkung "x" "hat".

In der orthodoxen Interpretation der Quantenmechanik wird die Annahme der Wert-Definiertheit aufgegeben. Ein Quantenobjekt hat demnach im Allgemeinen keine Eigenschaften, vielmehr entstehen Eigenschaften erst im Moment und im speziellen Kontext der Durchführung einer Messung. Die Schlussfolgerung der orthodoxen Interpretation, dass die Wert-Definiertheit aufgegeben werden muss, ist allerdings weder aus logischer noch aus empirischer Sicht zwingend. So geht beispielsweise die (empirisch von der orthodoxen Interpretation nicht unterscheidbare) De-Broglie-Bohm-Theorie davon aus, dass Quantenobjekte Teilchen sind, die sich entlang wohldefinierter Bahnkurven bewegen.

Gemäß dem Prinzip der lokalen Wirkung hat die Änderung einer Eigenschaft eines Subsystems "A" keinen direkten Einfluss auf ein räumlich davon getrenntes Subsystem "B". Einstein betrachtete dieses Prinzip als notwendige Voraussetzung für die Existenz empirisch überprüfbarer Naturgesetze. In der speziellen Relativitätstheorie gilt das Lokalitätsprinzip in einem absoluten Sinn, wenn der Abstand zwischen den zwei Subsystemen raumartig ist.

In der Quantenmechanik bewirkt die Verschränkung statistische Abhängigkeiten (so genannte Korrelationen) zwischen den Eigenschaften verschränkter, räumlich voneinander getrennter Objekte. Diese legen die Existenz gegenseitiger nicht-lokaler Beeinflussungen zwischen diesen Objekten nahe. Allerdings kann gezeigt werden, dass auch im Rahmen der Quantenmechanik keine überlichtschnelle Übertragung von Information möglich ist (siehe No-signalling-Theorem).

Das mit der quantenmechanischen Verschränkung verbundene Phänomen, dass die Durchführung von Messungen an einem Ort die Messergebnisse an einem (im Prinzip beliebig weit entfernten) anderen Ort zu beeinflussen scheint, war einer der Gründe, weshalb Einstein die Quantenmechanik ablehnte. In dem berühmten, gemeinsam mit Boris Podolsky und Nathan Rosen entwickelten EPR-Gedankenexperiment versuchte er, unter der Prämisse der Lokalität, nachzuweisen, dass die Quantenmechanik keine vollständige Theorie sein kann. Dieses Gedankenexperiment erwies sich in seiner ursprünglichen Formulierung als nicht praktisch durchführbar, jedoch gelang es John Stewart Bell im Jahr 1964, die zentrale EPR-Prämisse des lokalen Realismus, das heißt der Existenz lokaler physikalischer Eigenschaften, in der experimentell überprüfbaren Form der Bellschen Ungleichung zu formulieren. Alle bislang vorliegenden experimentellen Untersuchungen haben die Verletzung der Bellschen Ungleichung und damit die Voraussagen der Quantenmechanik bestätigt.

Allerdings sind sowohl die Bewertung der Aussagekraft der Experimente als auch die Interpretation der genauen Natur der EPR/B-Korrelationen Gegenstand einer bis heute andauernden Kontroverse. Viele Physiker leiten aus den experimentellen Ergebnissen zur Bellschen Ungleichung ab, dass das Lokalitätsprinzip nicht in der von Einstein vertretenen Form gültig sei. Andere Physiker interpretieren hingegen die Quantenmechanik und die Experimente zur Bellschen Ungleichung und zur Leggettschen Ungleichung so, dass die Annahme des Realismus aufgegeben werden müsse, das Lokalitätsprinzip hingegen aufrechterhalten werden könne.

Bei der Wechselwirkung eines Quantensystems mit seiner Umgebung (beispielsweise mit Gasteilchen der Atmosphäre, mit einfallendem Licht oder mit einer Messapparatur) kommt es unweigerlich zu Dekohärenz-Effekten. Das Phänomen der Dekohärenz lässt sich unmittelbar aus dem Formalismus der Quantenmechanik ableiten. Es stellt daher keine Interpretation der Quantenmechanik dar. Dennoch spielt Dekohärenz bei den meisten modernen Interpretationen eine zentrale Rolle, da sie einen unverzichtbaren Bestandteil bei der Erklärung des „klassischen“ Verhaltens makroskopischer Objekte darstellt und damit für jeden Versuch relevant ist, die Diskrepanz zwischen den ontologischen Aussagen der Interpretationen der Quantenmechanik und der Alltagserfahrung zu erklären.

Zu den wesentlichen Auswirkungen der Dekohärenz gehören die folgenden Phänomene:


Der Begriff der „Kopenhagener Interpretation“ wurde erstmals 1955 in einem Essay von Werner Heisenberg als Bezeichnung für eine vereinheitlichte Interpretation der Quantenmechanik verwendet, wobei Heisenberg weder in diesem Artikel noch in späteren Veröffentlichungen eine präzise Definition dieser Interpretation formulierte. Dieses Fehlen einer autoritativen Quelle und der Umstand, dass die Konzepte Heisenbergs, Bohrs und der anderen Gründungsväter der Kopenhagener Interpretation in einigen Aspekten untereinander unverträglich sind, führten dazu, dass heute unter dem Begriff der „Kopenhagener Interpretation“ ein breites Spektrum verschiedener Interpretationsvarianten subsumiert wird.

Ein besonderes Kennzeichen von Bohrs Interpretation ist seine Betonung der Rolle der klassischen Physik bei der Beschreibung von Naturphänomenen. Demnach wird zur Beschreibung von Beobachtungsergebnissen – wie wenig der untersuchte Vorgang auch mit der klassischen Mechanik zu tun haben mag – notwendig die klassische Terminologie benutzt. So spricht man beispielsweise von "Zählraten" beim Nachweis von "Teilchen" an einem "Detektor". Messvorrichtungen und Messergebnisse sind prinzipiell nur in der Sprache der klassischen Physik beschreibbar. Für eine vollständige Beschreibung eines physikalischen Phänomens muss daher die quantenmechanische Beschreibung mikroskopischer Systeme um die Beschreibung der verwendeten Messapparatur ergänzt werden. Hierbei spielt die Messapparatur nicht nur die passive Rolle eines losgelösten Beobachters (siehe oben), vielmehr ist jeder Messvorgang gemäß dem Quantenpostulat unvermeidlich mit einer Wechselwirkung zwischen Quantenobjekt und Messapparatur verbunden. Je nach verwendeter Messvorrichtung weist das Gesamtsystem (Quantenobjekt + Messvorrichtung) daher unterschiedliche "komplementäre" Eigenschaften auf (siehe Komplementaritätsprinzip). Da z. B. die Messung der Position und die Messung des Impulses eines Teilchens unterschiedliche Messvorrichtungen erfordern, stellen Position und Impuls zwei unterschiedliche Phänomene dar, die grundsätzlich nicht in einer einheitlichen Beschreibung zusammengefasst werden können.

Bohr verneinte die Möglichkeit einer realistischen Interpretation der Quantenmechanik. Er betrachtete das Komplementaritätsprinzip als eine prinzipielle epistemologische Grenze und lehnte daher ontologische Aussagen über die „Quantenwelt“ ab. Auch zum Formalismus der Quantenmechanik hatte Bohr eine rein instrumentalistische Einstellung, die Wellenfunktion war für ihn nicht mehr als ein mathematisches Hilfsmittel zur Berechnung der Erwartungswerte von Messgrößen unter wohldefinierten experimentellen Bedingungen.

Im Gegensatz zu Bohr vertrat Heisenberg eine Interpretation der Quantenmechanik mit realistischen und subjektivistischen Elementen. Gemäß Heisenberg repräsentiert die Wellenfunktion zum einen eine objektive Tendenz, die von ihm so bezeichnete „Potentia“, dass ein bestimmtes physikalisches Ereignis eintritt. Zum anderen enthält sie "„Aussagen über unsere Kenntnis des Systems, die natürlich subjektiv sein müssen“". Hierbei kommt dem Messvorgang eine entscheidende Rolle zu:


Die von John von Neumann und P.A.M. Dirac erarbeiteten mathematischen Methoden bilden bis heute das formale Fundament der orthodoxen Interpretation. Charakteristische Merkmale der orthodoxen Interpretation sind die Annahme des so genannten „Eigenwert-Eigenzustand-Link“ und das Kollaps-Postulat.

Gemäß dem Eigenwert-Eigenzustand-Link hat eine Observable dann – und nur dann – einen definierten (d. h. prinzipiell vorhersagbaren) Wert, wenn sich das System in einem Eigenzustand der Observablen befindet. Wenn sich das System hingegen in einem Superpositionszustand verschiedener Eigenzustände befindet, kann der Messgröße in dieser Interpretation kein definierter Wert zugeordnet werden. Der Ausgang eines einzelnen Messvorgangs ist in diesem Fall zufällig, die Entwicklung des Systems ist bei Durchführung einer Messung nicht deterministisch.

Bei der Beschreibung des Messprozesses ging von Neumann im Unterschied zu Bohr davon aus, dass neben dem Objektsystem auch die Messvorrichtung quantenmechanisch dargestellt werden muss. Zur Vermeidung des Messproblems übernahm er Heisenbergs Konzept des „Kollapses der Wellenfunktion“.

Gemäß der Ensemble-Interpretation beschreibt der quantenmechanische Zustand ein Ensemble ähnlich präparierter Systeme (z. B. den Zustand eines einzelnen Atoms). Sie widerspricht der Annahme der Kopenhagener Interpretation, dass die Quantenmechanik eine "vollständige" Beschreibung der Eigenschaften mikroskopischer Objekte darstellt und daher auch die Eigenschaften eines einzelnen Systems vollständig beschreibt. Frühe Befürworter der Ensemble-Interpretation waren unter anderen A. Einstein und K. R. Popper. Heute vertritt diese Interpretation insbesondere der kanadische Physiker Leslie Ballentine.

Bis in die 1970er Jahre gingen die meisten Vertreter der Ensemble-Interpretation davon aus, dass das Auftreten von Wahrscheinlichkeiten in der Quantenmechanik eine Folge ihrer "Unvollständigkeit" ist, dass mikroskopische Objekte in Wirklichkeit exakt determinierte Werte („PIVs“) für alle ihre dynamischen Größen (insbesondere: Position und Impuls) haben und die Quantenmechanik nur nicht in der Lage ist, diesen Sachverhalt vollständig zu beschreiben. Damit sind diese frühen Ensemble-Interpretationen eng verwandt zu den Verborgene-Variablen-Theorien.

Durch die Bellsche Ungleichung ist der Spielraum für PIV-Ensemble-Interpretationen stark eingeschränkt. Die „minimale Ensemble-Interpretation“ verzichtet daher auf die Annahme von PIVs. Zur Determiniertheit physikalischer Größen macht sie keine Aussage.

Die De-Broglie-Bohm-Theorie, häufig auch als „bohmsche Mechanik“ bezeichnet, geht davon aus, dass Quantenobjekte, wie z. B. Elektronen, Teilchen sind, die sich entlang wohldefinierter Bahnkurven bewegen. Die Bahnkurve eines solchen Teilchens lässt sich durch eine Bewegungsgleichung („Führungsgleichung“) berechnen. Eine andere (mathematisch äquivalente) Formulierung dieser Theorie führt ein so genanntes „Quantenpotential“ ein, welches aus der schrödingerschen Wellenfunktion abgeleitet wird und unter dessen Wirkung die Teilchenbewegung erfolgt.

Nach dieser Theorie ist der physikalische Zustand eines Teilchens also nicht nur durch die Wellenfunktion, sondern erst durch die Kombination aus Wellenfunktion und Teilchenposition vollständig definiert. Da diese Definition über die Zustandsdefinition der orthodoxen Interpretation hinausgeht, wird die Teilchenposition in der Terminologie der Quantenphysik als eine "verborgene Variable" bezeichnet, die De-Broglie-Bohm-Theorie zählt damit zur Klasse der Verborgene-Variablen-Theorien.

Die Dynamik der De-Broglie-Bohm-Theorie ist deterministisch. Wäre die Ausgangsposition eines Teilchens und die Wellenfunktion zu einem Zeitpunkt t₀ bekannt, ließe sich seine Position zu einem beliebigen späteren Zeitpunkt berechnen. Der beobachtete indeterministische Charakter von Quantenphänomenen wird in dieser Theorie auf die faktische Unmöglichkeit zurückgeführt, die Anfangswerte zu bestimmen, da im Rahmen dieser Interpretation der Versuch einer Ermittlung dieser Anfangswerte auf den Versuch einer Ermittlung der initialen Gesamtwellenfunktion des Universums hinausläuft.

Eine Eigenschaft der De-Broglie-Bohm-Theorie, in der sie sich wesentlich zur klassischen Physik unterscheidet, ist ihr explizit nichtlokaler Charakter: Bei einem Mehrteilchensystem führt jede Änderung an einem Teilchen zu einer instantanen Änderung der Gesamtwellenfunktion; diese Änderung beeinflusst unmittelbar das Quantenpotential und damit die Bahnkurven aller Teilchen des Mehrteilchensystems, unabhängig vom Abstand zwischen den Teilchen.

Die Viele-Welten-Interpretation entstand ausgehend von einer Veröffentlichung von Hugh Everett aus dem Jahr 1957. In dieser Arbeit hat Everett den Ansatz untersucht, wonach der physikalische Zustand des gesamten Universums mit allen darin enthaltenen Objekten durch eine einzige universale Wellenfunktion beschrieben werden soll, die sich gemäß einer durch eine Schrödingergleichung gegebenen Dynamik entwickeln soll. Diese Arbeit wirft somit die Frage auf, inwieweit der Kollaps der Wellenfunktion durch die Grundprinzipien der Quantenmechanik selbst beschrieben werden muss.

Während nach der bornschen Wahrscheinlichkeitsinterpretation die Wellenfunktion die Wahrscheinlichkeiten für das Auftreten verschiedener möglicher Messergebnisse beschreibt, von welchen dann bei Durchführung einer Messung nur eines realisiert wird, entwickelten in der Folge die Autoren der Viele-Welten-Interpretation die Vorstellung, dass "alle" physikalisch möglichen Ereignisse auch tatsächlich realisiert werden. Um keine Widersprüche zur Realität zu erhalten, wird dabei jedoch davon ausgegangen, dass „Beobachter“ keine vollständige Sicht auf diese parallel stattfindenden Ereignisse haben können. Dabei wird auch angenommen, dass bei einer Messung oder auch allgemeiner bei jeder physikalischen Wechselwirkung an einem überlagerten Quantensystem mehrere überlagerte „Welten“ entstehen, wobei in jeder dieser Welten jeweils nur eines der verschiedenen möglichen Ergebnisse realisiert ist.

Welten, die sich in makroskopischen Größenordnungen voneinander unterscheiden, entwickeln sich aufgrund von Dekohärenzeffekten fast unabhängig voneinander, weshalb ein Beobachter im Normalfall nichts von der Existenz der anderen Welten bemerkt. Das einzige nachweisbare Indiz der Existenz der anderen Welten sind Interferenzeffekte, die sich beobachten lassen, wenn sich die Welten nur auf mikroskopischer Ebene (z. B. in den Bahnkurven einzelner Photonen beim Durchlaufen eines Interferometers) unterscheiden.

Die Viele-Welten-Interpretation wird sehr kontrovers diskutiert. Befürworter, wie der Physiker D. Deutsch oder der Philosoph D. Wallace, betonen, dass sie die einzige realistische Interpretation sei, die das Messproblem ohne Modifikation des Formalismus der Quantenmechanik löse. Auch in der Quantenkosmologie wurde die Viele-Welten-Interpretation als konzeptioneller Rahmen zur Beschreibung der Entwicklung des Universums verwendet.

Kritiker werfen ihr eine extravagante Ontologie vor. Es gibt bislang auch keinen Konsens darüber, wie in einem Multiversum, in dem alle physikalisch möglichen Ereignisse tatsächlich stattfinden, unterschiedliche Wahrscheinlichkeiten für die verschiedenen Ereignisse erklärt werden können.

Die konsistente-Historien-Interpretation besteht im Kern aus einem Satz an Regeln, die festlegen, wie die zeitliche Entwicklung eines physikalischen Systems in Form so genannter „konsistenter Historien“ beschrieben werden kann. Eine Historie ist hier (ähnlich den bewegten Bildern in einem Film) als zeitlich geordnete Sequenz physikalischer Ereignisse definiert. Beispielsweise können bei dem rechts dargestellten einfachen Experiment folgende zwei Historien formuliert werden:
Die zwei Historien (Bewegung in Richtung D1 bzw. in Richtung D2) bilden hierbei eine so genannte „Historienfamilie“. Als konsistent gelten Historienfamilien, wenn ihre Wahrscheinlichkeit identisch zur Summe der Wahrscheinlichkeiten der einzelnen Historien der Familie ist, d. h. wenn die einzelnen Historien nicht interferieren. Inkonsistenten Historienfamilien kann keine Wahrscheinlichkeit zugeordnet werden, sie sind physikalisch nicht sinnvoll.

Die konsistenten Historienfamilien sind durch die Konsistenzbedingungen nicht eindeutig festgelegt, vielmehr lassen die Konsistenzregeln mehrere alternative inkompatible Historienfamilien zu. Beispielsweise lässt sich in dem oben beschriebenen Strahlteiler-Experiment die Bewegung eines Photons alternativ durch drei einander ausschließende konsistente Historienfamilien beschreiben, 1.) durch die oben beschriebene „Teilchen-Historienfamilie“, 2.) durch eine „Interferenz-Historienfamilie“ oder 3.) durch eine Historienfamilie mit einem makroskopischen Überlagerungszustand des Gesamtsystems. Die Interpretation trifft keine Aussage, welche der alternativen Historienfamilien die „richtige“ ist. Dies ist das Messproblem in der Terminologie der konsistente-Historien-Interpretation.

Die konsistente-Historien-Interpretation wurde seit ihrer Einführung im Jahr 1984 durch R. Griffiths mehrfach weiterentwickelt. Ab 1988 arbeitete R. Omnes eine explizit logische Formulierung der Quantenmechanik auf Basis konsistenter Historien aus. 1990 integrierten M. Gell-Mann und J. Hartle die Dekohärenz als Bedingung praktisch interferenzfreier Historienfamilien in die Konsistenzbedingungen. Konsistente Historienfamilien werden seither auch häufig als „dekohärente Historien“ bezeichnet.

Als Vorteil der Interpretation gilt ihr Verzicht auf metaphysisches „Gepäck“, wie z. B. die Annahme der Existenz vieler unbeobachtbarer Welten, oder die Annahme einer speziellen Rolle von Beobachtern, des Bewusstseins oder des Messprozesses. Da sie auf geschlossene Systeme anwendbar ist, wird sie auch in der Quantenkosmologie als konzeptioneller Rahmen zugrunde gelegt.

Allerdings weist auch die konsistente-Historien-Interpretation konzeptionelle Probleme auf. Insbesondere die Tatsache, dass sie keine Lösung des Messproblems anbietet, wird als Nachteil dieser Interpretation angesehen.

Die Grundidee der dynamischer-Kollaps-Theorien ist die Lösung des Messproblems durch die Annahme, dass der Zustand von Quantensystemen zu zufälligen Zeitpunkten spontan in einen räumlich lokalisierten Zustand kollabiert. Zur Beschreibung der Kollaps-Vorgänge wird die Schrödingergleichung um nichtlineare und stochastische Terme erweitert, die so gewählt sind, dass die Lokalisierungsrate bei isolierten mikroskopischen Systemen praktisch vernachlässigbar, bei makroskopischen Systemen hingegen dominant ist. Die Theorie erklärt damit, weshalb die aus der Schrödingergleichung resultierenden Überlagerungszustände nur bei mikroskopischen Systemen auftreten, während makroskopische Systeme immer in lokalisiertem Zustand vorgefunden werden.

Die älteste vollständig ausgearbeitete dynamischer-Kollaps-Theorie ist die so genannte GRW-Theorie (nach ihren Autoren Ghirardi, Rimini und Weber), deren Grundzüge erstmals im Jahr 1984 formuliert wurden. Die ursprüngliche Fassung der GRW-Theorie wies zunächst noch einige schwerwiegende Probleme auf, unter anderem war sie nicht auf Systeme identischer Teilchen anwendbar, und es ergaben sich zunächst Schwierigkeiten beim Versuch einer relativistischen Verallgemeinerung. Im Rahmen verschiedener Weiterentwicklungen konnten diese Schwierigkeiten jedoch gelöst werden. Zu den ausgereiftesten Varianten der dynamischer Kollaps-Theorien zählen heute das CSL-Modell (englische Abkürzung für „Continuous Spontaneous Localization“), sowie R. Tumulkas rGRWf-Theorie (Abkürzung für „relativistische GRW-Theorie mit flash-Ontologie“).

Die GRW-Theorie entstand zunächst als rein phänomenologischer Ansatz zur Lösung des Messproblems. Eine Reihe von Physikern, darunter T. P. Singh, R. Penrose und L. Diósi, vermuten jedoch auch aufgrund theoretischer Überlegungen zur Quantengravitation, dass die Wirkung der (Selbst-)Gravitation bei massebehafteten Quantensystemen mit nichtlinearen Effekten verbunden ist, welche zu dynamischen Kollapsprozessen führen. Im nichtrelativistischen Fall wird dies durch die Schrödinger-Newton-Gleichung beschrieben.

Da sich die Grundgleichungen der dynamischer-Kollaps-Theorien von der Schrödingergleichung unterscheiden, handelt es sich bei diesen Theorien genaugenommen nicht um Interpretationen der Quantenmechanik, sondern um alternative Theorien, deren Abweichungen zur Quantenmechanik im Prinzip experimentell überprüfbar sind. Allerdings erfordert der Nachweis dieser Abweichungen die kontrollierte Erzeugung makroskopischer Quantenzustände in einer Größenordnung, die mit den heute verfügbaren technischen Mitteln nicht realisierbar ist.

Bereits die älteste Interpretation der Quantenmechanik, die Kopenhagener Interpretation, enthält in manchen Varianten subjektivistische Elemente (siehe oben). W. Heisenberg, W. Pauli, R. Peierls und andere vertraten den Standpunkt, dass die Quantenmechanik nicht die Eigenschaften von Quantensystemen beschreibt, sondern "„unsere Kenntnis ihres Verhaltens“". Allerdings wurden mögliche Zusammenhänge zwischen subjektiver Kenntnis, Information und Quantenmechanik bis in die 1980er Jahre weder in der Physik noch in der Informatik systematisch untersucht.

Mit dem Aufkommen der Quanteninformatik in den 1990er Jahren verdichteten sich die Hinweise, dass Quantenphänomene als neuartige (d. h. im Rahmen der klassischen Informatik unbekannte) Mittel zur Übertragung und Verarbeitung von Information verwendet werden können. Untersuchungen der theoretischen Grundlagen von Quantencomputern, der Quantenteleportation, der Quantenkryptografie und anderen neuen Ansätzen der Quanteninformatik zeigten eine Reihe enger Zusammenhänge und Abhängigkeiten zwischen den Konzepten der Informatik und der Quantenmechanik.

Einige Physiker und Philosophen zogen hieraus den Schluss, dass auch eine zufriedenstellende Interpretation der Quantenmechanik nur aus einer informationstheoretischen Perspektive möglich ist. Heute existieren verschiedene Varianten informationsbasierter Interpretationen, die sich unter anderem in ihrer Definition des Begriffs „Information“ unterscheiden: Bei einigen Interpretationsvarianten bezeichnet Information die (subjektive) Einschätzung eines Beobachters (siehe übernächster Abschnitt). Die im Folgenden beschriebenen informationstheoretischen Rekonstruktionen der Quantenmechanik basieren hingegen auf der technischen Definition von Information als der Größe, deren Informationsgehalt durch die von-Neumannsche Entropie bemessen ist.

Verschiedene Philosophen, wie z. B. J. Bub oder A. Grinbaum, sehen eine Ursache für viele Interpretationsprobleme in der Verwendung einer ungeeigneten Methodik: Bei den meisten Interpretationen wird der mathematische Formalismus vorausgesetzt und dann versucht, den formalen Begriffen der Theorie, wie z. B. dem quantenmechanischen Zustand, eine semantische Bedeutung zu geben. Diese Methode habe sich jedoch nicht bewährt. Vielmehr sei analog zu Einsteins Vorgehensweise bei der Herleitung der Relativitätstheorie eine Rekonstruktion der Quantenmechanik erforderlich, d. h. ihre Ableitung aus geeignet gewählten physikalischen Prinzipien. Erst aus der Perspektive dieser Prinzipien lasse sich eine sinnvolle Interpretation der Quantenmechanik formulieren.

Es existiert eine Reihe verschiedener Vorschläge für axiomatische Rekonstruktionen der Quantenmechanik, wobei die meisten entsprechenden Ansätze seit Anfang der 1990er auf informationstheoretischen Prinzipien basieren. Neben Rovellis „Relationaler Interpretation“ und Zeilingers „Grundprinzip der Quantenphysik“ zählt das CBH-Theorem zu den bekanntesten informationsbasierten Rekonstruktionen.

In informationstheoretischen Rekonstruktionen der Quantenmechanik hat Information den Status einer physikalischen Fundamentalgröße. Quantentheorien sind in diesen Interpretationen keine Theorien über die Eigenschaften materieller Objekte, sondern über die Darstellung und Manipulation von Information. Die hier zugrundeliegende Idee, dass Physik auf Information zurückführbar ist, gewann in den letzten Jahren mit dem Aufkommen der Quanteninformatik an Bedeutung. Der bekannteste Vertreter und Vordenker dieser Denkschule, der Physiker J. A. Wheeler, formulierte 1990 seine „it from bit“-These, der zufolge alle physikalischen Objekte, wie z. B. Elementarteilchen, Kraftfelder, selbst die Raumzeit, einen informationstheoretischen Ursprung haben.

Einige Interpreten, wie z. B. G. Jaeger oder A. Duwell, halten diesen Standpunkt für eine unhaltbare Extremposition: Insbesondere sei Information nicht als physikalische Substanz (d. h. als physikalische Materie) zu betrachten, Information sei daher nicht als Grundbegriff zur Beschreibung der Eigenschaften von Materie geeignet. Andererseits bezweifelt der Philosoph C. Timpson, dass mit informationstheoretischen Interpretationen eine immaterialistische Position begründet werden kann: Letztlich liefen auch diese Interpretationen entweder auf eine instrumentalistische Position oder auf Verborgene-Variablen-Theorien hinaus.

Mit dem Quanten-Bayesianismus entstand in den letzten Jahren eine konsequent subjektivistische Interpretation der Quantenmechanik. Die Physiker C. Fuchs, Carlton M. Caves, R. Schack und andere zeigten, dass die Quantenmechanik in konsistenter Weise auf Basis des Bayesschen Wahrscheinlichkeitskonzeptes formuliert werden kann. Demnach bezieht sich die Wellenfunktion nicht auf ein Quantensystem, sondern sie repräsentiert die Einschätzung eines rationalen Agenten über das Ergebnis einer Messung an einem System. Der Kollaps der Wellenfunktion bei der Durchführung einer Messung beschreibt im Rahmen dieser Interpretation keinen realen physikalischen Prozess, sondern die Aktualisierung der Einschätzung des Agenten über den möglichen Ausgang einer weiteren Messung an dem System.

Der Quanten-Bayesianismus enthält sowohl realistische als auch anti-realistische Elemente: Physikalische Objekte, wie z. B. Elektronen, Neutrinos oder Quarks, werden als existierend angenommen. Jedoch besitzen Quantensysteme im Rahmen dieser Interpretation nur dispositionelle Eigenschaften, d. h. die Fähigkeit, im Falle von Wechselwirkungen mit anderen Quantensystemen bestimmte physikalische Ereignisse zu verursachen. Der Verlauf dieser Ereignisse ist durch kein physikalisches Gesetz bestimmt, selbst die Gültigkeit stochastischer Gesetzmäßigkeiten auf mikroskopischer Ebene wird in dieser Interpretation verneint.

Als Vorteil des Quanten-Bayesianismus führen seine Anhänger an, dass viele der gängigen (scheinbaren) Paradoxien der Quantenmechanik, wie z. B. das Wigners Freund-Paradoxon, im Rahmen dieser Interpretation vermieden werden können, da die meisten dieser Paradoxien auf einer objektiven Interpretation des quantenmechanischen Zustandes basieren. Weiterhin vertreten sie in der (unabhängig von der Quantentheorie existierenden) Kontroverse um die Interpretation von Wahrscheinlichkeit den Bayesschen Standpunkt, dass nur eine subjektivistische Wahrscheinlichkeitsinterpretation ohne logische Inkonsistenzen formulierbar sei.

Andere Autoren, wie der Physiker G. Jaeger oder der Philosoph C. Timpson, kritisieren, dass der Quanten-Bayesianismus ein Defizit an Erklärungsvermögen aufweise. Die Zielsetzung von Physik sei die Beschreibung und Erklärung der Eigenschaften physikalischer Systeme, nicht die Beschreibung der Einschätzungen von Agenten.

Neben den in den letzten Abschnitten erwähnten Interpretationen entstanden im Zeitraum seit ca. 1980 eine Reihe weiterer Interpretationen mit einem etwas geringeren Bekanntheitsgrad. Hierzu zählen unter anderem die "Modale Interpretation", die
"Relationale Interpretation", die
"Existential Interpretation", die
"Transactional Interpretation", sowie die "Empiricist Interpretation" (zu den drei letztgenannten Interpretationen sind keine deutschen Übersetzungen etabliert).

Nachdem Mitte 1926 die Ausarbeitung des Formalismus der Quantenmechanik weitgehend abgeschlossen war, verschärfte sich unter den Quantenphysikern jener Zeit die Frage nach einer zufriedenstellenden Interpretation der Quantenmechanik. Innerhalb kurzer Zeit, bis Ende 1927, setzten sich Bohr und Heisenberg gegen die Opposition Einsteins und Schrödingers weitgehend in der wissenschaftlichen Gemeinschaft durch. Die grundsätzlichen Fragen zur Quantenmechanik wurden als geklärt angesehen, und die meisten Physiker wandten sich den vielfältigen Anwendungen der Theorie zu. Selbst spätere Kritiker der Kopenhagener Interpretation, wie z. B. Landé, Louis Victor de Broglie oder David Bohm, traten zunächst für diese Interpretation ein.

In den 1950er Jahren wurde die Beschäftigung mit den konzeptionellen Grundlagen der Quantenmechanik von den meisten Physikern als philosophische und nicht als wissenschaftliche Aktivität betrachtet. Gegen den wissenschaftlichen Mainstream befassten sich einzelne Physiker kritisch mit den Prinzipien der Kopenhagener Interpretation. Der Physiker D. Bohm bewies mit der De-Broglie-Bohm-Theorie, dass die Formulierung empirisch adäquater verborgene-Variablen-Theorien möglich ist. Bei der Analyse der grundlegenden Prämissen dieser Theorien gelang J. Bell mit der Formulierung des Bellschen Theorems ein wissenschaftlicher Durchbruch, der wesentlich dazu beitrug, dass sich Untersuchungen der Grundlagen der Quantenphysik ab Anfang der 1970er Jahre zu einem rasch wachsenden Forschungsgebiet der Physik entwickelten.

Die Schwierigkeiten bei der Interpretation der Quantenmechanik, wie z. B. die vielfach als unzureichend empfundene Behandlung des Messproblems in der orthodoxen Interpretation, waren ein wesentliches Motiv für die Ausarbeitung bzw. Weiterentwicklung der im letzten Kapitel beschriebenen Alternativ-Interpretationen. Fortschritte auf experimenteller Seite, konzeptionelle Weiterentwicklungen der Theorie, wie z. B. die Ausarbeitung des Dekohärenz-Programms, sowie neue Entwicklungen auf dem Gebiet der Quanteninformatik trugen zusätzlich zu einem bis heute anhaltenden Interesse vieler Physiker und Philosophen an der Grundlagenforschung zur Quantenmechanik bei.

Bei der experimentellen Untersuchung verschiedener grundlegender Quantenphänomene wurden in den letzten Jahrzehnten erhebliche experimentelle Fortschritte erzielt. Alle diese Experimente zeigen, dass die Prinzipien der klassischen Physik nicht auf Quantensysteme übertragbar sind, während bislang keine Abweichungen zu den theoretischen Ergebnissen der Quantenmechanik nachgewiesen werden konnten. Allerdings sind die bislang durchgeführten Experimente nicht zur Unterscheidung zwischen den verschiedenen Interpretationen der Quantenmechanik geeignet, weshalb sich der folgende Überblick auf eine kurze Aufzählung der bekanntesten Schlüsselexperimente seit 1970 beschränkt:


1970 stellte der Heidelberger Physiker Dieter Zeh fest, dass viele der (scheinbaren) Paradoxien der Quantenmechanik, wie z. B. das Wigners-Freund-Paradoxon und das Messproblem, unter anderem durch falsche Prämissen bei der Beschreibung der Messvorrichtung bzw. des Beobachters begründet sind. Insbesondere zeigte er, dass makroskopische Quantensysteme aufgrund unvermeidlicher Wechselwirkungen mit der Umgebung nicht als geschlossene Systeme betrachtet werden können, und schlug daher als Lösungsansatz vor, dass die Umgebung der Messvorrichtung (bzw. des Beobachters) in der quantenmechanischen Beschreibung des Messprozesses berücksichtigt wird. Allerdings wurden Zehs Anregungen bis Anfang der 1980er Jahre kaum beachtet.

1981–1982 erfolgte die Ausarbeitung der wesentlichen Konzepte der Dekohärenz durch Wojciech Zurek. 1991 veröffentlichte er einen Artikel in der Zeitschrift „Physics Today“ und machte die Dekohärenz damit einer breiteren Öffentlichkeit bekannt. In den Folgejahren entwickelte sich die Dekohärenz zum Gegenstand zahlreicher experimenteller und theoretischer Untersuchungen. Diese Arbeiten warfen ein neues Licht auf den quantenmechanischen Messprozess und auf den Zusammenhang zwischen Quantenmechanik und klassischer Physik, was in der Folge dazu führte, dass die Konzepte der Dekohärenz in vielen Interpretationen der Quantenmechanik als zentrale Bestandteile integriert wurden.






Zustandsänderung

Mit Zustandsänderung wird allgemein die Veränderung eines Systems von einem Zustand in einen anderen bezeichnet.

In der Biologie finden sich solche Zustandsänderungen etwa in der Metamorphose oder im Sterben.

Die Physik beschäftigt sich wesentlich mit der Beschreibung und der Veränderung von Zuständen.

So beschreibt etwa die physikalische Größe der Beschleunigung die Änderung des Bewegungszustandes eines Körpers.

In der Thermodynamik sind zum einen die Änderungen von Aggregatzuständen und die damit einhergehenden Phasenübergänge von Stoffen zu erwähnen, die stets auch mit einer Entropieänderung verbunden sind. Beispiele hierzu sind das Schmelzen von Eis oder das Verdampfen und Kondensieren von Wasser.

Zum anderen werden Zustandsänderungen von Gasen beschrieben. Dabei wird das thermodynamische System von einem Zustand in einen anderen überführt. Eine einfache Zustandsänderung ist z. B. bei einem Luftballon zu beobachten, der an einem Sommertag bei hoher Temperatur mit einem Gas gefüllt wurde. Kommt dieser in einen klimatisierten Raum, so schrumpft der Ballon.
Man unterscheidet folgende Spezialfälle von Zustandsänderungen:

Die Zustandsänderungen a) bis f) sind Spezialfälle von g). Bei der Zustandsänderung g) (polytrope Zustandsänderung) kann der Exponent theoretisch jeden Wert zwischen formula_1 und +formula_2 annehmen. Zu bemerken ist, dass es sich bei diesen Zustandsänderungen um Idealisierungen handelt, denn bei realen Zustandsänderungen ist auch n nicht konstant.

In den mikrophysikalischen Spezialgebieten der Atom-, Kern- und Quantenphysik werden vor allem die Veränderung von Energiezuständen beschrieben.





Schatz der Optik

Schatz der Optik (, lateinisch "De aspectibus" oder "Perspectiva") ist eine von dem arabischen Gelehrten Alhazen (965–1039/1040) verfasste siebenbändige Schrift, in der unter anderem optische, physikalische und meteorologische Themen abgehandelt werden. Teilweise basiert sie auf älteren Schriften und Auffassungen griechischer Philosophen, enthält aber auch zahlreiche revolutionäre neue Theorien von Alhazen selbst. Das Buch wurde Ende des zwölften Jahrhunderts vom Arabischen in Latein übersetzt und damit der westlichen Welt zugänglich. 1572 wurde es von Friedrich Risner gedruckt. Es hatte einen großen Einfluss auf die mittelalterliche Wissenschaft und gilt als ein Startpunkt der Entwicklung der neuzeitlichen Optik.

gemäß der englischen Übersetzung

1. Über das Sehen herrscht Verwirrung.
2. Das Sehen muss durch Mathematik und Naturwissenschaft beschrieben werden.
3. Naturwissenschaftler dachten bisher, irgendeine Form geht vom Objekt zum Auge. Mathematiker dachten, Strahlen gehen vom Auge zum Objekt.
4. Die Mathematiker haben sich verschiedene Versionen der Extramission ausgedacht.
5. Die Wahrheit muss Extra- oder Intromission sein, oder irgendetwas anderes. Einigkeit ist erreichbar durch gründliche Forschung.
6. Mein Ausgangspunkt ist die Vergewisserung über die Gegebenheiten, das Unveränderliche, die Prämissen und Prinzipien, die durch Induktion zu finden sind. Zur Vertiefung soll Kritik an den Prämissen und Schlüssen geübt werden. Entscheidend ist gerechtes Urteil statt Vorurteil, Wahrheitssuche statt Meinungsbeeinflussung. So kann die herzerfüllende Wahrheit erkannt werden; so können wir Gewissheit erlangen. Gott unterstützt uns beim Vertreiben der Trübung des menschlichen Geistes.
7. Ich habe 7 Bände zur Optik geschrieben.
8. Mein Vorgängerbuch zum Schatz der Optik möge man bitte wegwerfen.

1. Zum Sehen ist ein gewisser Abstand zwischen Auge und Objekt nötig.
2. Das Objekt muss dem Auge gegenüber sein. Gedachte Verbindungslinien zwischen Auge und Objekt dürfen nicht auf ein opakes Hindernis stoßen.
3. Ansonsten ist das Objekt unsichtbar, so lange es sich in derselben Atmosphäre befindet und nicht gespiegelt wird.
4. Nur derjenige Teil ist unsichtbar, dessen Verbindungslinie zum Auge unterbrochen wird vom Hindernis.
5. Sichtbarkeit herrscht, wenn eine ununterbrochene gerade Linie zwischen dem gesehenen Punkt und dem Auge verläuft.
6. Dies lässt sich experimentell mit einer Konstruktion aus Lineal und Röhre prüfen.
7. Betrachte das Objekt durch eine gerade Röhre, mit und ohne Abschirmung der Röhre.
8. Durch die Unsichtbarmachung ist die Geradlinigkeit der Lichtausbreitung gezeigt.
9. Zweifelsfrei schlussfolgern wir: Sichtbares liegt auf einer geraden Linie zwischen Auge und Gesehenem.
10. Ein Objekt ist sichtbar, wenn es selbst Licht enthält oder von einem anderen Objekt empfängt, unabhängig davon, ob das Auge in lichterfüllter Luft ist.
11. Ein Objekt wird unsichtbar, wenn es klein genug ist; abhängig von der Sehkraft.
12. Ein Objekt ist unsichtbar, wenn es transparent ist.
13. Ein Objekt wird ab einem gewissen Abstand unsichtbar, abhängig von der Objektgröße.
14. und von der Helligkeit des Objekts (z. B. Feuer).
15. Wir schlussfolgern: Der Abstand der Sichtbarkeit hängt ab vom Licht im Objekt
16. und von dessen Farbe (z. B. beim Schiff).
17. Weiß wird vor grellen Farben sichtbar, diese wiederum vor trüben Farben.
18. Wir schlussfolgern: Der Abstand der Sichtbarkeit ist abhängig von der Farbe des Objekts
19. und von der Sehkraft.
20. Sichtbarkeit hängt also ab vom Zustand des Objekts und der Augen.
21. Das Auge sieht kein Objekt, es sei denn, das Objekt erfüllt die genannten Bedingungen und das Auge ist unbeschadet.
22. Details werden schon bei kleineren Abständen unsichtbar.
23. Ein Objekt wird als kleiner werdend empfunden, wenn es sich wegbewegt.
24. Nah am Auge verschwimmt und verschwindet das Objekt scheinbar.
25. Wir halten fest: Für das Sehen ist ein gemäßigter Abstand nötig.
26. Als nächstes werden wir Licht und Strahlung untersuchen, dann das Auge, um dann den Sehvorgang zu diskutieren.

1. Selbstleuchtende Körper strahlen in alle Richtungen und erhellen alle zugewandten Flächen, so lange diese nicht abgeschirmt oder zu weit weg sind.
2. Die Strahlung ist geradlinig, wenn die Luft oder der transparente Körper zwischen leuchtendem und beleuchtetem Körper durchgängig gleichartig ist.
3. Diese Geradlinigkeit ist eine unveränderliche Gegebenheit. Wir sehen sie zum Beispiel, wenn Licht durch Löcher, Spalte oder Türen in ein staubiges Zimmer eintritt. In einem staubfreien Zimmer können wir die Geradlinigkeit an einem opaken Gegenstand mit Hilfe einer geraden Stange nachvollziehen. Wir finden kein Licht auf krummen Bahnen.
4. Ebenso bei Mond- und Sternenlicht, das durch ein Loch in die Dunkelkammer tritt. Blickt das Auge vom Lichtfleck zum Loch, sieht es den Stern.
5. Licht kann ebenso mit einem opaken Körper bei geradlinigem Abstand abgefangen werden.
6. Das funktioniert auch mit Feuer, und lässt sich mit einer durchs Loch geschobenen geraden Stange überprüfen.
7. Die Geradlinigkeit ist auch an Schatten nachvollziehbar, denn hier ist die geradlinige Verbindung zum luminösen Körper unterbrochen.
8. Wir schlussfolgern: Strahlung von selbstleuchtenden Körpern verläuft geradlinig.
9. Die Gesamtheit des selbstleuchtenden Körper besitzt mehr Licht als dessen Teile, wie wir zum Beispiel beim Sonnenauf- und Untergang merken.
10. Licht strahlt von jenem Teil der Sonne auf ein Objekt, der dem Objekt zugewandt ist.
11. Dasselbe gilt bei Sonnenfinsternis; jeder Teil der Sonne strahlt.
12. Licht kommt geradlinig von jedem Teil der Sonne, siehe Sonnenfinsternis.
13. Das wird auch daran deutlich, dass die Strahlen vom Loch in der Dunkelkammer auseinanderlaufen.
14. Die Strahlung geht geradlinig von jedem Teil der Sonne zu jeder gegenüberliegenden Fläche.
15. Dies gilt ebenso für den zunehmenden und abnehmenden, aufgehenden und untergehenden, sowie für den verfinsterten Mond.
16. Ebenso für Feuer, wie wir mit einem Rohr in einer Kupferplatte experimentell prüfen können.
17. Verschiedene Varianten unseres Experiments bestätigen, dass Licht vom gesamten Feuer stärker ist als von einem Teil davon.
18. Wenn wir das Rohr unterschiedlich neigen, sehen wir, dass von jedem Teil der Flamme in alle Richtungen Licht strahlt.
19. Wir schlussfolgern: Licht geht entlang jeder geraden Linie von jedem Teil jedes selbstleuchtenden Körpers aus.
20. Ausgedehnte selbstleuchtende Körper sind kontinuierlich, insofern die kleinen Teile die Form des Feuers bewahren (z. B. in der Sonne, im Mond, und in den anderen Himmelskörpern), jeder noch so kleine Teil strahlt.
21. Wir definieren das Licht von selbstleuchtenden Körpern als Primärlicht.
22. Wenn das Tageslicht von der Sonne kommt, wieso sind dann selbst Schatten in Innenhöfen erhellt?
23. Selbst vor und nach Sonnenaufgang sind die Atmosphäre und die Erde lichterfüllt.
24. Beleuchtete Flächen erhellen gegenüberliegende Flächen.
25. Dies können wir experimentell prüfen anhand einer Tür vor sonnenbeschienener Wand.
26. Den Versuch können wir steigern mit einer Kammer in einer Kammer mit weißen Wänden. Das ist ebenso machbar bei Mond- und Tageslicht.
27. Die Experimente zeigen, dass von dem Licht, welches auf einen Körper scheint, Licht in jede entgegengesetzte Richtung strahlt. So beleuchtet die Sonne die Atmosphäre, die Atmosphäre wiederum die Erde.
28. Dies erklärt die hellen Schatten.
29. Diese zufälligen Lichter können anhand von Gewissheit bringenden Experimenten untersucht werden; z. B. in einer Kammer in einer Kammer mit Löchern von Ost nach West.
30. Wir spannen Fäden durch die Löcher und markieren die Schnittpunkte mit den Wänden.
31. Bei Nacht ohne helle Sterne wird die Wand an den markierten Stellen dunkel sein.
32. Bei erhellter Atmosphäre ergeben sich an den markierten Stellen Lichtflecken, die einzeln abgeschattet werden können.
33. Wir können das Licht mit einem opaken Körper auffangen und auch zusätzliche Löcher in die Wände bohren.
34. Wir schlussfolgern: Licht strahlt geradlinig von der Atmosphäre ab.
35. Auf krummen Bahnen zwischen den Löchern und Wänden ist kein Licht.
36. Ein Hindernis unterbricht nicht die Kontinuität der Luft.
37. Licht strahlt geradlinig von jedem Teil der erleuchteten Luft in alle entgegengesetzten Richtungen ab.
38. Tageslicht kommt geradlinig von der erhellten Atmosphäre.
39. Man könnte einwenden, dass abends kein Licht durch die Atmosphäre kommt, obwohl stets die halbe Atmosphäre hell ist.
40. Darauf antworte ich: Licht wird mit der Entfernung schwächer, also je mehr es in den Schattenkegel der Erde eindringt.
41. Bei Sonnenaufgang grenzt die beleuchtete Atmosphäre direkt an den Schattenkegel an, ist also dem Betrachter nah und erscheint deshalb hell.
42. Die Helligkeit ist abstandsabhängig; so entsteht eine Lichtsäule bei Dämmerung, Dunkelheit bei Nacht – dies ist geometrisch begründbar.
43. Das Licht ist nicht der Luft innewohnend, sondern wird wie von einer Wand zurückgeworfen.
44. Die Lichtmenge ist volumenabhängig.
45. Morgen- und Abendlicht rühren vom großen Luftvolumen her, das durchblickt wird.
46. Entlang der geraden Linie zum Auge multiplizieren sich die schwachen Lichter der verschiedenen Luftabschnitte, daher wird das Licht sichtbar.
47. So hätten wir dieses Problem geklärt: Morgen- und Abendlicht kommen von der sonnenerhellten Luft.
48. Wir können zufälliges Licht experimentell anhand von Wänden mit Türen und einem speziell markierten Holzblock prüfen.
49. Wir markieren Winkel und Abstände auf dem Würfel.
50. Wir bohren ein horizontales sowie ein schräg durchgehendes Loch ein.
51. Wir setzen den Würfel in eine Wand ein, die einer weißen, beleuchteten Wand gegenübersteht.
52. Wir schieben einen Stab mit Spitze durch das horizontale Loch im Würfel und markieren den Auftreffpunkt auf der weißen Wand.
53. Wir schauen durch das Loch auf den markierten Punkt und kreisen den Kopf so weit, bis der Punkt unsichtbar wird.
54. Den entsprechend anvisierten Kreis markieren wir auf der weißen Wand.
55. Der Kreis muss gegebenenfalls korrigiert werden.
56. Auch durch das schräge Loch wird der Kreis zu sehen sein.
57. Das liegt an den vorher festgestellten Proportionen.
58. Daher schneiden sich die Achsen der Löcher im Kreis auf der Wand.
59. Der Kreismittelpunkt liegt auf der Achse des horizontalen Lochs
60. Ebenso wie auf der Achse des schrägen Lochs.
61. Die Längen der Löcher verhalten sich zueinander wie die Strecken von den Löchern bis zur weißen Wand.
62. Dementsprechend sind die Kreise proportioniert.
63. Dementsprechend verhalten sich die Kreise zu den horizontalen Strecken.
64. Dementsprechend verhalten sich die schrägen Strecken zu den horizontalen Strecken.
65. Dementsprechend verhalten sich die Kreise zu den schrägen Strecken.
66. Daher kann nur der markierte Kreis durch das schräge Loch gesehen werden.
67. Wenn mehr zu sehen ist, muss der Würfel korrigiert werden.
68. An Stelle des Kreises auf der Wand bohren wir nun ein konisches Loch in die weiße Wand und bedecken das Loch mit weißem Tuch.
69. Bei bedecktem schrägen Loch lassen wir in der Dunkelkammer durch das horizontale Loch einen weißen Gegenstand beleuchten, indem Licht von der weißen Wand kommt, welches wiederum von der Atmosphäre kommt.
70. Wenn das weiße Tuch entfernt wird, verschwindet der Lichtfleck in der Dunkelkammer. Falls Restlicht vom Innern des Rohres kommt,
71. sollte man das Rohr schwarz ausmalen.
72. Licht erscheint, sobald das weiße Tuch das Loch in der weißen Wand bedeckt.
73. Licht kommt also nur vom weißen Tuch herein,
74. Licht auf krummen Bahnen nicht.
75. Ein weißer Gegenstand zwischen dem Loch in der weißen Wand und dem Loch im Würfel wird die Kammer erhellen; ein weiterer Gegenstand zwischen dem Würfel-Loch und dem innen befindlichen Gegenstand wird das Licht wegnehmen.
76. Wie [73] & [74].
77. Zufälliges Licht geht nur geradlinig
78. Denn auf krummen Linien kommt nichts an.
79. Das Licht wird mit zunehmendem Abstand schwächer.
80. Dieselben Beobachtungen sind beim schrägen Loch möglich.
81. Wir schlussfolgern: Licht kommt nur vom weißen Tuch und nur geradlinig.
82. Das Licht wird schwächer mit zunehmendem Abstand des Gegenstands vom schrägen Loch.
83. Nun öffnen wir beide Löcher gleichzeitig.
84. Nun verwenden wir beliebig viele Löcher. Wir schlussfolgern: Ein von Tageslicht beleuchteter Körper strahlt geradlinig in alle Richtungen.
85. Bei Sonnenschein funktioniert das gleich, nur besser.
86. Ebenso funktioniert es mit Mond- und Feuerlicht.
87. Wir schlussfolgern: Zufälliges Licht in opaken Körpern strahlt geradlinig in alle Richtungen und wird mit zunehmendem Abstand schwächer.
88. Wir definieren Licht von zufällig beleuchteten Körpern als Sekundärlicht, jedoch geht es auf dieselbe Weise vom Körper aus wie Primärlicht.
89. Dies können wir im Experiment prüfen: Ein Lichtfleck von einem Loch in der Dunkelkammer strahlt ähnlich einem Primärlicht und kann durch eine Tasse abgeschirmt werden.
90. Mit einem Spiegel wird die Dunkelkammer heller auf Grund des hellen Lichtflecks.
91. Ohne Spiegel ist das Licht in der Dunkelkammer nicht basierend auf Spiegelung.
92. Der Spiegel kann ein Objekt beleuchten.
93. Wir tauschen ein weißes gegen ein schwarzes Objekt.
94. Wir schlussfolgern: Das überall in der Dunkelkammer gesehene Licht ist Sekundärlicht, es entsteht nicht auf Grund von Spiegelung.
95. Ebenso verhält sich Licht vom Mond.
96. Dasselbe gilt für Licht von Gegenständen, die von einem Feuer beleuchtet sind.
97. Spiegel hingegen reflektieren; zudem senden sie Licht wie jeder andere Körper in alle Richtungen aus.
98. Jeder Teil eines Objekts strahlt in alle Richtungen, auch wenn dies wegen geringer Lichtmenge manchmal nicht wahrnehmbar ist.
99. Reflektiertes Licht geht geradlinig.
100. Dies können wir im Experiment prüfen, indem wir das reflektierte Licht mit einem opaken Objekt auffangen.
101. Mit einer Nadel können wir einen Teil des reflektierten Lichts auffangen; mit Lineal prüfen wir anhand des Schattens die geradlinige Lichtausbreitung
102. Wir schlussfolgern: Licht wird stets geradlinig, nie krummlinig reflektiert.
103. Die Reflexion erfolgt nur in einer bestimmten Richtung.
104. Auch nach Übergang in einen transparenten Körper von anderer Transparenz ist die Ausbreitung geradlinig.
105. Dies können wir experimentell prüfen: Wir halten eine Glaskugel im Sonnenlicht vor eine Wand, ihr Schatten empfängt Licht, das durch einen opaken Körper aufgefangen werden kann. Die Geradlinigkeit der Lichtausbreitung überprüfen wir mit einer Nadel als Schattengeber.
106. Licht geht nur in bestimmten geraden Linien durch den transparenten Körper.
107. Licht im transparenten Körper hat nicht dieselbe Richtung wie das Licht davor oder dahinter, außer bei senkrechtem Eintritt.
108. Der Austrittspunkt strahlt Sekundärlicht ab.
109. Dies können wir experimentell prüfen: Ein Loch in der Dunkelkammer beleuchtet eine Glaskugel so, dass sie Gegenstände drum herum erhellt.
110. Licht geht also von allen leuchtenden Körpern geradlinig in alle Richtungen.
111. Sekundärlichter sind schwächer als Primärlichter; beide werden mit zunehmendem Abstand schwächer.
112. Reflexion und Brechung geschehen geradlinig und nur in bestimmten Richtungen.
113. Farbe geht einher mit Licht.
114. Sekundärlichter sind stark farbig wenn der strahlende Körper stark farbig und der gegenüberliegende Körper schwach farbig ist.
115. Grüne Pflanzen im Sonnenlicht färben eine nahe stehende, im Schatten befindliche Hauswand grün.
116. Das Blattgrün erscheint im Schatten auf der Wand, im Boden und auf einer weißen Robe.
117. Eine experimentelle Prüfung ist jederzeit möglich.
118. Wir können folgendes Experiment durchführen: Ein vom Loch in der Dunkelkammer beleuchteter purpurner Körper färbt alle Innenwände purpur.
119. Weißes Tuch in der Nähe des purpurnen Körpers ist umso stärker gefärbt, je näher es ist; rundum platzierte weiße Körper werden gefärbt.
120. Dies funktioniert mit allen hellen Farben. Weiß erhellt alles, Schwarz verdunkelt alles.
121. Farbe geht also stets einher mit Licht. Sie breitet sich ebenso geradlinig aus und nimmt mit vergrößertem Abstand ab.
122. Die von anderen Objekten empfangene Farbe gelangt nicht durch Reflexion ins Auge, sondern wie von farbigen Köpern.
123. Wir halten [122] nochmals fest.
124. Wenn wir eine farbige Flüssigkeit in einem Glas vor das Loch einer Dunkelkammer halten, erscheint ein hingehaltenes weißes Tuch in klarer Farbe, die mit zunehmendem Abstand vom Glas schwächer wird.
125. Dies funktioniert ebenso mit gefärbtem Wasser, die Farbe ist also an das Licht gebunden.
126. Dies funktioniert ebenso mit einem flüssigkeitsgefüllten Glas, das von Feuer beleuchtet wird.
127. Wir schlussfolgern: Zusammen mit dem Licht geht auch die Farbe aus transparenten Körpern hervor.
128. Wir schlussfolgern weiterhin: Farbe allgemein geht stets mit dem Licht einher; wenn sie nicht sichtbar ist, ist der Sehsinn zu schwach dafür.
129. Transparente Körper können ihre Farbe so wie Licht von selbst oder von anderen Körpern erhalten; sie strahlt wie Licht geradlinig in alle Richtungen.
130. Farbe kann nicht von Körpern ausgehen, außer sie werden mit dem Licht abgestrahlt.
131. Es ist nicht anzweifelbar oder ungewiss, dass Farbe und Licht gemeinsam von Körpern abstrahlen.
132. Einige betrachten Farbe als etwas, was zwischen Auge und Licht entsteht, aber die schillernden Farben der Pfauenfedern beruhen auf Reflexion.
133. Die Farbe von opaken Körpern hingegen ist rundum sichtbar (wenn auch unterschiedlich stark), beruht also nicht auf Reflexion.
134. Farbe kommt von farbigen Körpern, z. B. von einem schamerfüllten Gesicht (während das äußere Licht und der Beobachterstandpunkt gleichbleiben).
135. Angst färbt das Gesicht gelb.
136. Wir schlussfolgern, dass Farbe eine Form des farbigen Körpers ist; es ist nicht ungewiss, dass sie nicht von externen Faktoren herrührt.
137. Selbst da Farbe mit den Lichtbedingungen schwankt, kann ihre eigene Realität nicht bezweifelt werden.
138. Da Licht natürlich in alle Richtungen abstrahlt, und da Farbe ans Licht gebunden ist, ist die Farbausbreitung um den Körper unabhängig vom Auge.
139. Farbe beruht nicht auf Reflexion und wird nicht vom Sehsinn erzeugt.
140. Wenn dem so ist, dann ist die Farbe des vom farbigen Körper bestrahlten Objekts eine Form auf dem bestrahlten Objekt.
141. Da dem so ist, strahlt der beleuchtete Körper Farbe und Licht in alle Richtungen, unabhängig vom Auge.
142. Farbe breitet sich mit dem Licht in alle Richtungen geradlinig aus.
143. Dies geschieht, so lange der Körper beleuchtet ist und die angrenzenden Körper kontinuierlich transparent sind.
144. Warum aber erscheint die Farbe nur auf abgeschatteten und vorrangig auf weißen Körpern, und nur wenn der gegenüberliegende Körper farbenkräftig ist? Dies ist durch den Sehsinn bedingt.

1. In helles Licht zu schauen – zum Beispiel in direktes oder gespiegeltes Sonnenlicht – schmerzt.
2. Nach langem Betrachten eines hell beleuchteten weißen Körpers legt sich scheinbar ein dunkler Schleier zwischen Betrachter und Betrachtetem; ob bei Sonnen – oder Feuerbeleuchtung.
3. Nach langem Betrachten eines weißen Objekts bei Tageslicht erscheint etwas Gleichartiges wieder, sobald man auf eine dunkle Stelle schaut, sowie bei geschlossenen Augen.
4. Gleiches geschieht nach langem Betrachten eines feuerbeleuchteten Körpers oder eines Lochs in der Dunkelkammer.
5. Offenbar erzeugt Licht eine gewisse Wirkung auf den Sehsinn.
6. Nach langem Betrachten einer hell beleuchteten Wiese oder eines purpurnen Körpers erscheint ein dunkler Ort grün beziehungsweise purpur.
7. Wir schlussfolgern: Beleuchtete Farben wirken sich auf den Sehsinn aus.
8. Die nachts sichtbaren Sterne sehen wir tagsüber nicht, da dann die Atmosphäre lichterfüllt ist.
9. Von Feuer beleuchtete Details eines Objekts sind nicht mehr sichtbar, wenn der Betrachter auch das Feuer im Blick hat.
10. Offenbar behindert helles Licht, das auf die Augen scheint, das Sehen von schwach beleuchteten Objekten.
11. Details auf einem spiegelnden Objekt verschwinden, wenn es den hellen Himmel oder eine helle Wand ins Auge spiegelt.
12. Dasselbe gilt für feine Schrift auf glattem Papier.
13. Ein angezündeter Körper im Sonnenschein ist unsichtbar, aber das Feuer ist sichtbar.
14. Ebenso wirkt ein weißes Objekt, das von einem beleuchteten farbigen Körper bestrahlt wird, im Schatten farbig, nicht aber im Sonnenschein.
15. Ebenso wirkt ein Objekt, das von einem mit farbiger Flüssigkeit gefüllten Glas beleuchtet wird, nicht mehr farbig, wenn es mit einem weiteren Feuer beleuchtet wird.
16. Manche Meerestiere und Glühwürmchen scheinen im Dunkeln wie Feuer zu leuchten, aber nicht bei Tageslicht oder Feuer.
17. All dies zeigt, dass die starken Lichter sichtbarer Objekte gewisse Eigenschaften verschwinden lassen, die bei schwacher Beleuchtung sichtbar sind.
18. An dunklen Orten verschwinden Details, die im Hellen sichtbar sind, z. B. Gravuren oder Schrift.
19. Starkes Licht kann also gewisse Details hervorbringen, die in starkem Licht verschwinden.
20. Dunkle Farben wirken im Hellen klar und deutlich, wirken in schwachem Licht jedoch schwarz.
21. Starke Beleuchtung macht weiße Körper heller und mattfarbige Körper farbiger.
22. Transparente, stark gefärbte Flüssigkeit wirkt in schwachem Licht schwarz, in hellem Licht farbig.
23. Dasselbe gilt für transparente Steine.
24. Der Schatten eines transparenten, farbigen Körpers wirkt farbig bei starkem und farblos bei schwachem Licht.
25. Pfauenfedern und der Stoff abu qalamun ändern je nach Beleuchtung scheinbar ihre Farben.
26. Wir schlussfolgern: Der Sehsinn nimmt Farben je nach Beleuchtung unterschiedlich wahr.
27. Weiterhin hängt die Farbe ab vom Licht im Gegenstand, auf dem Auge, und auf der Luft.
28. Warum starkes Licht die Sicht auf gewisse Objekte behindert, werde ich im Zusammenhang mit dem Sehvorgang erklären.

1. Das Auge besteht aus diversen Schichten, Membranen und Körpern; es entspringt aus dem Vorderhirn.
2. Zwei hohle, zweischichtige Nerven kreuzen sich vor dem Gehirn und führen in die Augenhöhlen.
3. Die Nerven gehen durch Öffnungen in den Augenhöhlen und führen trichterartig zum Auge.
4. Die Augäpfel bestehen aus mehreren Schichten.
5. Die erste Schicht besteht aus weißem Fett und heißt Conjunctiva.
6. Die zweite Schicht ist schwärzlich, samtartig ausgekleidet und heißt wegen ihrer Traubenartigkeit Uvea. Sie ist von der Conjunctiva bis auf vorn bedeckt.
7. Gegenüber vom Nerv ist ein Loch in der Uvea.
8. Das Loch und die offenliegende Uvea sind bedeckt von der hornartigen Cornea.
9. Gegenüber vom Loch liegt das linsenförmige, feuchte, eisartig transparente Krystallin.
10. Diese Körperflüssigkeit ist zweiteilig. Der hintere Teil ist ähnlich transparent wie zerbröseltes Glas, daher heißt er Glaskörper. Beide Teile sind umschlossen von der spinnennetzartigen Aranea.
11. Die Cornea geht aus der äußeren Schicht des hohlen Nervs hervor, die Uvea aus der inneren.
12. Eine eiweißartige Flüssigkeit, die albuginöse Flüssigkeit, erfüllt das Augeninnere.
13. Zwischen all den transparenten Körpern bestehen gerade Linien.
14. Man sagt, der Sehgeist fülle die Nerven und verleihe dem Krystallin die Sehkraft.
15. Der Nerv erstreckt sich von der Öffnung in der Augenhöhle bis zum Krystallin. Dabei weitet er sich.
16. Die Conjunctiva umschließt den divergierenden Nerv. Das Auge bewegt sich stets als Ganzes.
17. Der Nerv wird nur an der Öffnung zur Augenhöhle gebogen.
18. Da die Hornhaut übergeht in die Augenoberfläche, ist ihr Radius größer als der der Uvea.
19. Laut Kugelgeometrie liegt das Zentrum der Hornhautinnenseite weiter innen als das der Uvea.
20. Das Zentrum der äußeren Oberfläche ist ebenso weiter innen gelegen.
21. Eine gerade Linie geht durch die beiden Zentren, und durch die Zentren der Öffnung und der Hornhaut.
22. Die Zentren von albuginöser Flüssigkeit und Cornea fallen zusammen.
23. Die gerade Linie durch Uvea, Cornea und Loch davor geht durch das Zentrum des Nervenhohlraums.
24. Der Schnittkreis zwischen der Vorder- und Rückseite vom Krystallin ist der verbindende Kreis, oder parallel dazu.
25. Die gerade Linie durch die Zentren von Uvea und Krystallin geht senkrecht durch den verbindenden Kreis
26. und senkrecht durch den Rest vom Krystallin.
27. So geht die Linie durch den Nervenhohlraum.
28. Diese Linie geht senkrecht durch alle Teile des Auges.
29. Höchstwahrscheinlich fallen das Zentrum der Kugel der hinteren Oberfläche vom Krystallin und das Zentrum der Kugel der Cornea zusammen.
30. Alle Schichten, die dem Loch der Uvea gegenüberliegen, haben ihr Zentrum mitten im Augapfel.
31. Bei Augenbewegung ändert sich die Lage dieses Zentrums nicht.
32. Die Nervbiegung findet hinter diesem Zentrum statt.
33. Der Nerv liegt symmetrisch in Bezug auf den verbindenden Kreis.
34. Die Oberfläche des Nervenhohlraums ändert ihre Position bezüglich des Augapfels nicht.
35. Die Linie durch die Zentren der Augenschichten geht gerade durch die Mitte des Nervenhohlraums.
36. Dies sind also die Positionen und Proportionen.
37. Die Geometrie ist für beide Augen dieselbe.
38. Die Conjunctiva ist mit je zwei Muskeln befestigt; Lidern und Wimpern bedecken die Augen.
39. Alles Gezeigte steht in den Anatomiebüchern.




Pseudorapidität

Die Pseudorapidität formula_1 (eta) ist eine räumliche Koordinate, die in der experimentellen Teilchenphysik verwendet wird, um den Winkel eines Vektors relativ zur Strahlachse anzugeben. Sie wird gegenüber der Angabe des Polarwinkels formula_2 bevorzugt, weil bei Hadron-Hadron-Kollisionen der Fluss der erzeugten Teilchen pro Pseudorapiditäts-Intervall etwa konstant ist.

Die Pseudorapidität ist definiert als

Für ein Teilchen mit Impuls formula_4 (und formula_5) lässt sich dies umschreiben in:

worin

In der Hochenergienäherung, d. h. für ein Teilchen mit der Energie formula_9, dessen Masse formula_10 gegenüber seinem Impuls formula_11 vernachlässigbar ist, formula_12, ist die Pseudorapidität numerisch in etwa gleich der Rapidität
die in der experimentellen Teilchenphysik definiert wird als

Zum Vergleich: die originale Rapidität gemäß der speziellen Relativitätstheorie ist
worin formula_16 das Verhältnis der Teilchengeschwindigkeit formula_17 zur Lichtgeschwindigkeit formula_18 ist.

Die Form des differentiellen Wirkungsquerschnitts formula_19 ist invariant unter einem Lorentz-Boost. Das Gleiche gilt in guter Näherung auch für die Pseudorapidität, nur ist diese leichter zu messen: Nicht die Masse des Teilchens muss ermittelt werden, sondern lediglich seine Flugrichtung durch den Detektor.




Strahlerzeile

Unter einer Strahlerzeile versteht man in der Akustik eine äquidistante Anordnung von N punktförmigen Einzelschallquellen (Monopole). Aufgrund der Überlagerung einzelner Wellen, deren Ursprung in den einzelnen Quellen liegt, entsteht eine Richtcharakteristik. Dies wird in der Praxis zur gezielten Beschallung, beziehungsweise bei Mikrofonen zur gerichteten Aufnahme genutzt. Als Beispiele seien hier die Beschallung von Kirchen oder Bahnhöfen zu nennen.

Bei gleichem Schallfluss für alle Monopole können für die Richtcharakteristik der Strahlerzeile drei Fälle unterschieden werden:


Mathematisch lässt sich die Richtcharakteristik wie folgt bestimmen:

Ferner lässt sich die Richtcharakteristik durch Verzögerungen der einzelnen Schallquellen zueinander schwenken. Durch eine Abnahme des Schallflusses nach außen hin können Nebenkeulen, bei geringerer Richtwirkung der Hauptkeule, unterdrückt werden.





Henry Barschall

Henry Herman „Heinz“ Barschall (* als Heinrich Hermann Barschall 29. April 1915 in Berlin; † 4. Februar 1997 in Madison, Wisconsin) war ein US-amerikanischer Experimentalphysiker deutscher Abstammung, der sich mit Kernphysik beschäftigte.

Barschall war der Sohn eines Patentanwalts, war protestantisch getauft, wie sein Vater, hatte aber jüdische Großeltern. 1934 begann Barschall sein Studium dennoch in Berlin, konnte dort aber 1936 keinen Betreuer für eine Abschlussarbeit, d. h. Promotion, mehr finden. Auf Anraten von Max Planck ging er nach Marburg zu Grüneisen, der ihn am dortigen Physikinstitut ohne Umschweife aufnahm und eine Abschlussarbeit übertrug. Dennoch entschloss er sich 1937 in die USA auszuwandern. 1940 promovierte er bei Rudolf Ladenburg an der Princeton University über die Wechselwirkung schneller Neutronen mit Helium. Kurz nachdem Niels Bohr in Vorträgen in den USA von der Entdeckung der Kernspaltung berichtet hatte, führte er entsprechende Experimente in den USA aus (wie viele andere Gruppen auch). In einer Arbeit in Physical Review (Bd. 58, 1940, S. 682) mit John Archibald Wheeler berichtete er über die Entdeckung von Spin-Bahn-Kopplung bei der Neutronenstreuung an Heliumkernen. 1940 war er Instructor in Princeton und 1941 bis 1943 an der University of Kansas. Im Zweiten Weltkrieg arbeitete er von 1943 bis 1946 am Manhattan Project. Ab 1946 war er an der University of Wisconsin–Madison tätig, wo er Professor für Physik wurde. Er bestimmte dort unter anderem Wirkungsquerschnitte für schnelle Neutronen.

Nachdem sein Labor 1970 bei einem terroristischen Anschlag zerstört wurde, wobei es auch einen Toten und mehrere Verletzte gab (darunter einen seiner Studenten), wandte er sich der Anwendung von Neutronen in der Materialforschung und in der Medizin zu (und war danach in der Fakultät für Medizinische Physik und Kerntechnik), insbesondere in der Krebstherapie. Vorübergehend arbeitete er 1971 bis 1973 am Lawrence Livermore National Laboratory. 1986 ging er an der University of Wisconsin in den Ruhestand.

Er war seit 1972 Mitglied der National Academy of Sciences, deren Physik-Abteilung er 1980 bis 1983 vorstand. 1983 bis 1988 war er im Leitungsrat des American Institute of Physics und 1983 bis 1986 im Rat der American Physical Society. 1987 wurde er Mitglied der American Academy of Arts and Sciences.

Ein Aufsatz von Barschall in Physics Today (Juli 1988) über die (hohen) Kosten von Physik-Zeitschriften aus kommerziellen Verlagen führte zu einer Klage des negativ dabei erwähnten Verlags Gordon and Breach, aus dem Barschall aber schließlich als Sieger hervorging.

1972 bis 1987 war er Herausgeber von Physical Review C. 1965 erhielt er den ersten Tom-W.-Bonner-Preis für Kernphysik.

Er hatte 41 Doktoranden, darunter Robert K. Adair.






Protonenemission

Die Protonenemission (, auch "Protonenumwandlung, Protonenaktivität, Protonenradioaktivität" oder "Protonenzerfall" genannt) ist ein sehr seltenes Phänomen aus der Kernphysik, bei dem sich ein Atomkern unter Emission eines einzelnen Protons in das Element mit der nächst niederen Ordnungszahl umwandelt. Die Protonenemission darf nicht mit dem Protonenzerfall, nämlich dem hypothetischen Zerfall des Protons, verwechselt werden.

Eine Protonenemission wurde erstmals 1970 von K. Jackson an dem Nuklid Co-53m beobachtet. Dieses zerfiel unter Abspaltung eines Protons direkt zu Fe-52 statt, wie zu erwarten, durch β-Zerfall zu Fe-53. Es unterliegt somit einem dualen Kernzerfall mit den Wahrscheinlichkeiten 1,5 % (Protonenemission) und 98,5 % (β-Zerfall) für die jeweiligen Zerfälle.

Im Jahre 1981 wurde von S. Hofmann eine weitere Protonenemission an dem Nuklid Lu-151, das im UNILAC in Darmstadt hergestellt wurde, beobachtet.

In den darauf folgenden Jahren schlossen sich weitere Beobachtungen an: Die Nuklide Tm-147, Tm-147m und Lu-150, sowie Cs-113 und I-109 wurden als Protonen-Emitter identifiziert, wobei ihre Hauptzerfallsart der β-Zerfall bleibt. Mittlerweile wurden 95 solcher Protonen-Emitter entdeckt.

Eine Protonenemission tritt nur bei Nukliden auf, die ein sehr hohes Protonen-Neutronen-Verhältnis haben. Aufgrund dieses großen Verhältnisses können die Protonen nur noch sehr schlecht gebunden werden, die Bindungsenergie der Protonen sinkt sogar so stark ab, dass diese den Kern verlassen können. Solche „protonenreichen“ Kerne entledigen sich ihres „positiven Überschusses“ zwar meist durch einen β-Zerfall, also der Aussendung eines Positrons, doch in seltenen Fällen kann auch stattdessen ein Proton abgespalten werden.

In dem folgenden Reaktionsschema wird der Zerfall des bereits oben genannten Nuklids Co-53m veranschaulicht:
In fast allen Fällen wird sich das Nuklid durch den β-Zerfall umwandeln, doch mit einer geringen Wahrscheinlichkeit wird eine Protonenemission stattfinden.

In diesem Fall handelt es sich bei dem Ausgangsnuklid um ein Nuklid im metastabilen Zustand, ein Kernisomer. Normalerweise zerfallen solche Kerne über einen Isomerieübergang; bei Co-53m ist dieser aber durch eine hohe Drehimpulsbarriere (Kernspin 19/2) unterdrückt und wurde noch nicht beobachtet.

Bei den meisten Nukliden jedoch tritt die Protonenemission aus dem Grundzustand heraus auf, wie bei dem bereits oben genannten Nuklid Lu-151.

Noch viel seltener ist das Auftreten eines Zwei-Protonen-Zerfalls, das erstmals von Witali Iossifowitsch Goldanski 1960 und später von B. Alex Brown in Erwägung gezogen wurde. Hierbei werden nicht nur ein, sondern gleich zwei Protonen gleichzeitig ausgestoßen. Das geschieht nur bei Nukliden, bei denen das Protonen-zu-Neutronen-Verhältnis sogar noch größer ist als bei Ein-Proton-Emittern. Beobachtet wurde ein solcher Zerfall erstmals 2002, wiederum im UNILAC, am Nuklid Fe-45.

und gleichzeitig am GANIL (Bertram Blank und andere).

Bis heute sind 13 Zwei-Protonen-Emitter bekannt.





Mohammed El Naschie

Mohammed Saladin El Naschie (* 1943 in Kairo) ist ein ägyptischer Mathematiker, Physiker und Ingenieur. Er war Chefredakteur der Zeitschrift "Chaos, Solitons & Fractals" und hatte Positionen an verschiedenen europäischen und ägyptischen Universitäten inne. Kritik erfolgte aufgrund des angeblichen pseudowissenschaftlichen Charakters seiner Arbeiten, in denen unter dem Deckmantel der Mathematik Numerologie betrieben werde.

El Naschies Arbeiten konzentrieren sich auf seine "E-infinity"-Theorie, eine "fraktal-kosmologische" Weltformel. Er behauptet, dass seine Theorie "eine harmonische Produktion von Quarks und Elementarpartikeln durch eine auf einen goldenen Schnitt zentrierte fraktale Raumzeit modelliert". El Naschies Theorien werden als wissenschaftlich wertlos betrachtet, Mathematiker sind der Ansicht, dass El Naschies Theorien und Aussage keinen Sinn ergeben. Die E-Infinity Theorie ist zum Gegenstand zahlreicher Diskussionen im Internet geworden, Kritiker sind der Ansicht dass die E-Infinity-Theorie durch eine große Zahl an Sockenpuppen beworben werde.

El Naschie besuchte laut eigenen Angaben Ausbildungsstätten in Hamburg und Hannover. Sein Doktorat der Mathematik erhielt er von der University of London. Er gibt an, „“ der Johann-Wolfgang-Goethe-Universität zu sein, was laut einem Bericht der Zeitschrift Nature von dieser dementiert wird. Er gibt ferner an, führender Berater des saudi-arabischen Ministeriums für Wissenschaft und Technologie zu sein, eine unabhängige Bestätigung existiert nicht.

El Naschie ist Autor zahlreicher Artikel in wissenschaftlichen Zeitschriften. Bislang veröffentlichte er über 300 Arbeiten, die sich hauptsächlich mit der E-Infinity-Theorie befassen, einzelne Arbeiten befassten sich mit der Quantenmechanik und nichtlinearer Dynamik.

Charakteristisch für El Naschies Arbeiten ist die Numerologie und das Anführen von ungefähren numerischen Ähnlichkeiten.

Seinen Thesen wird weitgehend keine Bedeutung zugemessen. 
Auf "MathSciNet", einer von der American Mathematical Society veröffentlichten Sammlung von Rezensionen mathematischer Arbeiten, bemerkte ein Rezensent über "Anmerkungen über die Quantengravitation und die Cantorianische Raumzeit": "„Diese Arbeit erweckt beim Rezensenten den Eindruck, dass sie keine Mathematik enthält“".

El Naschie übernahm die Führung von "Chaos, Solitons & Fractals", nachdem die Zeitschrift aufgrund der sinkenden Zahl vorgelegter Beiträge von der Einstellung bedroht war. Obwohl die Zeitschrift von Elsevier verlegt wurde, fand keine nähere Kontrolle von El Naschies Blattführung statt.

El Naschie nutzte die Zeitschrift größtenteils zur Veröffentlichung seiner eigenen Artikel. Allein Band 38, Nr. 5 von "Chaos, Solitons and Fractals" enthielt fünf Artikel El Naschies, die keinem Peer Review unterzogen wurde. Da sich El Naschie häufig selbst zitierte, hatte die Zeitschrift bald einen höheren Impact Factor als jede qualifizierte mathematische Zeitschrift. Seine redaktionellen Praktiken wurden unter anderem von Douglas Arnold, dem Präsidenten der "Society for Industrial and Applied Mathematics" kritisiert.

Der Elsevier-Verlag „bündelt“ seine Zeitschriften und verkauft sie als Gesamtpaket zu einem günstigeren Preis an seine Abnehmer. Da die Kosten für die verlegten Fachzeitschriften mit z. B. 4.500 US-Dollar für ein ungebündeltes Jahresabo von Chaos, Solitons & Fractals hoch liegen, war dieses Vertriebssystem für die Abnehmer attraktiv. Die Nichtabnahme einzelner Zeitschriften war nicht möglich, weshalb Chaos, Solitons and Fractals ungeachtet seines unwissenschaftlichen Inhalts in den Bestand zahlreicher Universitätsbibliotheken kam.

Im Laufe des Jahres 2008 mehrte sich ausgehend von einem Artikel des Mathematikers John Baez die Kritik an El Naschies Arbeitsweise und dem Elsevier-Verlag, der trotz seiner hohen Verkaufspreise keine Qualitätskontrolle durchführte und durch seine Verkaufspraxis die Vertrieb von Chaos, Solitons and Fractals gefördert hatte. Die Redaktion der ebenfalls von Elsevier herausgegebenen Zeitschrift "Topology" trat aus Protest zurück.

Die Kritik wurde von der Zeitschrift Nature und mehreren nicht-wissenschaftsspezifischen Medien, darunter die New York Times Slashdot und Die Zeit aufgegriffen. Als Reaktion auf die Kritik enthob Elsevier El Naschie von seinem Posten. Eine Klage El Naschies gegen Nature und den Autor Quirin Schiermeier wegen übler Nachrede wurde am 6. Juli 2012 abgewiesen.

Als Konsequenz aus den Vorgängen um El Naschie wurden Änderungen am Impact-Factor und dem Prinzip der Bewertung von Wissenschaftlern, Universitäten und Zeitschriften anhand der Zahl an Zitierungen gefordert. Zusätzlich zu seinem hohen Impact-Factor hatte El Naschie durch seine Selbstzitierungen die Universität Alexandria unter anderem durch den international vierten Platz bei Zitaten in die Top-200-Liste des weltweiten Universitätsrankings der Times Higher Education gehoben.





Σ-Baryon

Die drei Σ-Baryonen, auch Sigma-Baryonen, sind Baryonen, die aus einem Strange-Quark sowie allen Paarkombinationen von Up- und Down-Quark bestehen. Sie besitzen Spin 1/2, Isospin 1 und werden durch ihre elektrische Ladung gekennzeichnet: Σ, Σ und Σ.

Die Σ-Baryonen gehören zu den Hyperonen.

Die Σ-Baryonen gehören dem SU(3)-Oktett an.

Die geladenen Σ-Baryonen zerfallen zu beinahe 100 % in ein Nukleon und ein Pion. Dabei kann das Σ aufgrund seiner negativen Ladung nur in ein Neutron und ein π zerfallen, wogegen das Σ mit gleicher Wahrscheinlichkeit in ein Neutron und ein π wie in ein Proton und ein π zerfällt.

Das ungeladene Σ zerfällt mit nahezu 100 % Wahrscheinlichkeit in ein Λ-Baryon und ein Photon. Dieser Zerfall erfolgt mittels der elektromagnetischen Wechselwirkung, so dass dieses Teilchen deutlich kurzlebiger als die geladenen Σ-Baryonen ist.
Die Σ-Baryonen besitzen angeregte Zustände, welche als "Σ-Baryonen" bezeichnet werden. Sie haben denselben Quarkinhalt wie die Σ-Baryonen, jedoch Spin 3/2, und sind Teil des Baryon-Dekupletts. Sie sind etwas schwerer als die Σ-Baryonen (Σ≈ 1387 MeV/c, Σ≈ 1384 MeV/c, Σ≈ 1383 MeV/c).

Die Σ-Baryonen zerfallen zum größten Teil unter der starken Kernkraft in ein Pion und ein Σ-Baryon oder in ein Pion und ein Λ-Baryon. Zu einem geringen Anteil (≈ 1,3 %) zerfällt das Σ-Baryon in ein Λ-Baryon und ein Photon.




Gregori-Aminoff-Preis

Der "Gregori-Aminoff-Preis für Kristallografie" () ist ein von der Königlich Schwedischen Akademie der Wissenschaften vergebener Wissenschaftspreis auf dem Gebiet der Kristallographie und benachbarter Disziplinen.

Der Preis ist nach dem schwedischen Künstler und Mineralogen Gregori Aminoff benannt und wurde erstmals 1979 vergeben. Er ist mit 100.000 schwedischen Kronen dotiert.





Philip Nelson (Physiker)

Philip Charles Nelson (* 22. November 1957 in Kansas City) ist ein US-amerikanischer theoretischer Physiker.

Nelson studierte Physik an der Princeton University mit dem Bachelor-Abschluss 1980, erhielt 1981 einen Mathematikabschluss an der Universität Cambridge und 1983 den Master-Abschluss an der Harvard University, an der er 1984 bei Sidney Coleman promoviert wurde ("Global conflicts"). 1984 bis 1987 war er Junior Fellow in Harvard. 1987 wurde er Assistant Professor an der Boston University und 1988 Assistant Professor, 1991 Associate Professor und 1998 Professor an der University of Pennsylvania. Dort ist er seit 1998 auch Mitglied des "Institute of Medicine and Engineering".

Er war 2000 Gastwissenschaftler am Weizmann-Institut und 1984 und 1994 am Institute for Theoretical Physics in Santa Barbara.

Nachdem er sich zunächst mit Stringtheorie und verwandten Theorien (Supergravitation, Supersymmetrie, Konforme Feldtheorien, Topologie von Anomalien in der Quantenfeldtheorie) und dabei verwendeten geometrischen Methoden befasste, wandte er sich später Themen der Biophysik zu, wie der Topologie und Elastizität von DNA, dynamischer Musterbildung, molekularen Motoren oder der statistischen Mechanik von biologischen Membranen.

Von 1988 bis 1991 war er Sloan Research Fellow. 2003 wurde er Fellow der American Physical Society. 2009 erhielt er den "Emily Gray Prize" der Biophysical Society.






Henry James Brooke

Henry James Brooke (* 25. Mai 1771 in Exeter; † 26. Juni 1857 in Clapham Rise, London) war ein britischer Kristallograph und Mineraloge.

Er war der Sohn eines Walkstoff-Herstellers und wurde nach abgebrochenem Jura-Studium erfolgreicher Geschäftsmann (Wollhandel mit den Spaniern, Bergwerke in Südamerika, Lebensversicherungen in London).

Sein Hobby war das Sammeln von Mineralien, Pflanzen und Muscheln und er vermachte seine Sammlung später der University of Cambridge. 1815 wurde er Fellow der Geological Society of London, 1818 der Linnean Society of London, 1819 der Royal Society und 1825 der American Academy of Arts and Sciences. Seit 1853 war er korrespondierendes Mitglied der Bayerischen Akademie der Wissenschaften. Nach seinem Artikel im Dictionary of National Biography fand er 13 neue Mineralarten wie unter anderem Annabergit, Autunit und Whewellit (zusammen mit William Hallowes Miller), Arfvedsonit, Caledonit, Childrenit, Linarit, Nitronatrit, Susannit und Thomsonit.

Sein Buch über Kristallographie war zu seiner Zeit sehr einflussreich. In ihm schlug er eine neue Nomenklatur vor.

Sein Sohn Charles Brooke (1804–1879) war ein bekannter Chirurg und Erfinder, ebenfalls Fellow der Royal Society.

1825 wurde das Titandioxid-Mineral "Brookit" nach ihm benannt.






Max Thomas Edelmann

Max Thomas Edelmann (* 18. Oktober 1845 in Ingolstadt; † 28. April 1913 in München) war ein deutscher Ingenieur, Physiker, Hochschullehrer und Fabrikbesitzer.

Max Thomas Edelmann war der Sohn des Gymnasialprofessors und Zeichenlehrers Leonhard Edelmann. Nach dem Besuch der Gewerbeschule studierte er zunächst an der Polytechnischen Schule Augsburg und der Ludwig-Maximilians-Universität München. 1864 wurde er Mitglied der Germania München. Am Deutschen Krieg nahm er als Artillerist teil. 1868 wurde Assistent von Wilhelm von Beetz am Physikalischen Labor der Technischen Hochschule München. 1873 wurde er Privatdozent. Im Jahr 1880 wurde er zum Mitglied der Leopoldina gewählt. 1881 wurde er mit einer Arbeit über die Bestimmung der magnetischen Inklination zum Dr. phil. promoviert. 1893 erfolgte seine Ernennung zum Professor. Er las unter anderem über magnetische und galvanische Messungen, Meteorologie und Astronomie.

1870 gründete Edelmann in München sein "Physikalisch-Mechanisches Institut zur Herstellung physikalischer Präzisionsapparate". Seit den frühen 1870er Jahren bereits für die Hoftheaterverwaltung tätig, ließ er sich zunächst für ein Jahr von der Hochschule beurlauben, um die Berufung zum Leiter des gesamten Beleuchtungs- und Maschinenwesens der beiden Königlichen Hofbühnen übernehmen zu können. Auf Schloss Linderhof realisierte er die technische Ausstattung der Venusgrotte. Hierzu gehörten eine Wellenmaschine und die gesamte Lichttechnik, die mit dampfmaschinenbetriebenen Innenpoldynamomaschinen aus der eigenen Fabrikation gespeist wurde.

In seinem Institut fertigte er für den industriellem Einsatz komplette Kabelmesswagen für die Ausrüstung von Elektrizitätswerken. Für experimentalphysikalische Fragestellungen konstruierte er unter anderem Projektionsinstrumente, fotografische Apparate zum Registrieren elektrischer Erscheinungen und neuartige Messinstrumente zur Erforschung des Erdmagnetismus. Aus seiner Zusammenarbeit mit Heinrich von Wild entstandene Messapparate fanden weltweit Einsatz in geomagnetischen Beobachtungsstationen. 

Für Friedrich Bezold fertigte er zur Erforschung des Resthörvermögens von Gehörlosen präzise Stimmgabeln und eine verstellbare Galtonpfeife für sehr hohe Frequenzen. Seine hochempfindlichen Galvanometer zur Messung geringster Spannungsunterschiede fanden auch Einzug in die medizinische Forschung ebenso seine elektromagnetischen Apparate in der Augenheilkunde.

Edelmann war Mitglied im Polytechnischen Verein für das Königreich Bayern. 1889 wurde er Direktor der elektrotechnischen Versuchsstation. Er war Mitglied des Museumsbeirats des Deutschen Museums und half mit seiner Expertise und Instrumenten aus seiner Fertigung insbesondere bei der Errichtung der physikalischen Abteilung. 

Edelmann war Philister des Corps Germania, gehörte den ersten Automobilclubs und dem Isartalverein an und war außerordentliches Mitglied der Künstlergesellschaft Allotria.

Max Thomas Edelmann war in erster Ehe mit Anna Maria, geborene Geiß, verheiratet, die 1872 in München starb. In zweiter Ehe heiratete er Sofie Erhardt aus Schwabing (* 1. November 1852). Die gemeinsamen Söhne dieser Ehe waren Otto Edelmann (1870–1928) - er wurde Professor und Abteilungsvorstand der Landesgewerbeanstalt Bayern in Nürnberg - und Max Edelmann (1874–1940), der ebenfalls Professor wurde und das väterliche Institut übernahm.








Elastizitätstensor

Der Elastizitätstensor bildet in der linearen Elastizität die Verzerrungen auf die Spannungen ab. Anisotropes Materialverhalten kann durch ihn abgebildet werden. Das Teilgebiet der Physik, das sich mit elastischen Verformungen befasst, wird Elastizitätstheorie genannt. Sie ist Teil der Kontinuumsmechanik und dadurch gekennzeichnet, dass elastische Deformationen reversibel sind: nach Wegfallen der äußeren Kraft kehrt das Material in seine Ausgangsform zurück. Das ist nicht mehr der Fall, wenn es zu Brüchen kommt oder zu plastischem Fließen – letzterer Fall wird durch die Plastizitätstheorie behandelt.

Mechanische Spannungen werden zur Berechnung als Kraftansatz an einer (Schnitt-)Fläche eines Körpers betrachtet. Eine Kraft wird dabei zur Berechnung in die Komponente Normalspannungen (formula_1), senkrecht auf der gewählten Ebene, und Schubspannungen (formula_2), in der Ebene, geteilt. Diese verschiedenen Spannungen werden im Spannungstensor zusammengefasst:

Entsprechend werden die Deformationen im Verzerrungstensor zusammengefasst:

An einem einfachen Radiergummi kann man erkennen, dass ein Ziehen entlang der x-Achse nicht nur eine Deformation in x-Richtung verursacht, sondern den Radiergummi auch seitlich dünner werden lässt (Querkontraktion), d. h., formula_5 hängt auch linear mit den seitlichen Verschiebungen formula_6 und formula_7 zusammen.

Im Folgenden seien "kleine Auslenkungen" angenommen. Dadurch kann das verallgemeinerte Hookesche Gesetz herangezogen werden, das einen linearen Zusammenhang zwischen Spannung und Verzerrung herstellt

Hierbei ist der Elastizitätstensor formula_9 ein Tensor vierter Stufe, mit 3×3×3×3 = 81 Komponenten. Komponentenweise lautet der Zusammenhang
bzw. mit der Einsteinschen Summenkonvention

Zur Vereinfachung der Darstellung lassen sich unter gewissen Voraussetzungen Symmetrien der beteiligten Tensoren ausnutzen:
Diese beiden Voraussetzungen ergeben die beiden "Nebensymmetrien"
des Elastizitätstensors. Außerdem ergibt sich daraus, dass die Matrizen formula_18 und formula_19 jeweils nur 6 unabhängige Komponenten haben. Die Anzahl der unabhängigen Komponenten im Elastizitätstensor hat sich dadurch auf 6×6=36 reduziert.

Die "Hauptsymmetrie"
folgt aus der Hyperelastizität, welche das hier angenommene Hookesche Gesetz als Spezialfall enthält, unter Berücksichtigung des Satzes von Schwarz. Dadurch reduziert sich die Anzahl der unabhängigen Elastizitätskomponenten auf 21.

Mithilfe der Voigtschen Notation werden die Komponenten der Spannungs- und Verzerrungsmatrizen jeweils in einem Spaltenvektor zusammengefasst. Dadurch lässt sich das Hookesche Gesetz in der Kelvin-Voigtschen Notation zu

bzw. in der Kelvin-Mandelscher Notation

darstellen, wobei in dieser Notation aufgrund der Hauptsymmetrie formula_23 gilt. Je nach Material und dessen Symmetrieeigenschaften können weitere Komponenten eliminiert werden, wie unten deutlich wird.

Im Falle eines quadratisch nichtlinearen Materials ergibt sich der Zusammenhang
zwischen Spannungstensor und Verzerrungstensor. Auch hier lassen sich obengenannte Symmetrien ausnutzen und die Matrix-Vektor-Notation einführen.

Die vollständige (trikline) Anisotropie ist die allgemeinste Form eines Elastizitätsgesetzes. Sie zeichnet sich durch die folgenden Eigenschaften aus:
Viele Faser-Kunststoff-Verbundwerkstoffe sind anisotrop. Ingenieure versuchen die aus vollständiger Anisotropie resultierenden Effekte zu nutzen.

Die monokline Anisotropie hat für Konstruktionswerkstoffe wenig Bedeutung. Folgende Eigenschaften zeichnen die monokline Anisotropie aus:

Viele Konstruktionswerkstoffe sind orthotrop, z. B. technisches Holz, Gewebe, viele Faser-Kunststoff-Verbunde, Walzbleche mit Textur usw. Die Orthotropie darf nicht mit der Anisotropie verwechselt werden. Der bloße richtungsabhängige Elastizitätsmodul ist noch kein Hinweis auf die Anisotropie. Die Orthotropie ist ein Sonderfall eines vollständig anisotropen Elastizitätsgesetzes.
Die Orthotropie zeichnet sich durch die folgenden Eigenschaften aus:
Orthotrope Werkstoffe machen also keine Schubverzerrung, wenn sie gedehnt werden. Dies macht sie für den Konstrukteur leicht handhabbar. Daher wird in der Faserverbundtechnik gezielt mit orthotropen Schichten wie dem ausgeglichenen Winkelverbund gearbeitet. Schichtholz wird so aufgebaut, dass es orthotrope Eigenschaften besitzt.

Die transversale Isotropie zeichnet sich dadurch aus, dass das Elastizitätsgesetz um eine Achse gedreht werden kann, ohne dass es sich ändert. Es ist also gegenüber der Drehung invariant. Ein Beispiel für ein transversal isotropes Material ist ein Rundholz oder eine unidirektionale Schicht. Die elastischen Eigenschaften des Rundholzes ändern sich nicht, wenn man es um seine Längsachse dreht. Dennoch besitzt das Holz unterschiedliche Moduln längs und quer zur Faser.
Die transversale Isotropie wird durch die folgenden Eigenschaften charakterisiert:
Die transversale Isotropie ist ein Sonderfall der allgemeinen Orthotropie.

Das isotrope Gesetz ist das bekannteste und wichtigste Elastizitätsgesetz. Mit ihm können nahezu alle Metalle und unverstärkte Kunststoffe beschrieben werden. Auch kurzfaserverstärkte Kunststoffe können isotrop sein, wenn man die Verstärkungsfasern statistisch verteilt (siehe: Faser-Matrix-Halbzeuge). Das isotrope Elastizitätsgesetz zeichnet sich für den Konstrukteur hauptsächlich durch die Invarianz gegenüber der Drehung aus. In einer Konstruktion ist es also unerheblich, wie der isotrope Werkstoff orientiert wird. Gewalzte metallische Bleche können eine schwache Anisotropie aufweisen.



Siehe auch: Lamé-Konstanten.

Die unterschiedlichen Elastizitätsgesetze zeichnen sich durch ihre Kopplungen aus. Eine Kopplung bezeichnet den Effekt, dass das Material mit einer Verformung außerhalb der Wirkrichtung der Belastung reagiert.

Dies ist die bekannteste Kopplung. Sie wird auch als Querkontraktionskopplung bezeichnet. Die Kopplung bewirkt, dass sich der Werkstoff bei Zug einschnürt, bzw. bei Druck verbreitert. Ingenieure haben gelernt mit der Dehnungskopplung umzugehen und wenden sie gezielt an, z. B. beim Nieten. Praktisch alle Konstruktionswerkstoffe besitzen diese Kopplung.

Besonders bei anisotropen Werkstoffen tritt diese Kopplung auf. Orthotrope Werkstoffe besitzen sie nicht. Die Dehnungs-Schiebungs-Kopplung erzeugt eine Schiebung bei einer Dehnung des Materials. Umgangssprachlich wird dies auch als Verzug bezeichnet. Mit Hilfe der klassischen Laminattheorie kann untersucht werden, ob ein Werkstoff eine Dehnungs-Schiebungs-Kopplung besitzt.

Die Schiebungs-Schiebungs-Kopplung tritt nur bei anisotropen Werkstoffen auf. Eine Schiebung in der Ebene erzeugt hier auch eine Schiebung aus der Ebene heraus.




British Association Unit

Die British Association Unit (Einheitenzeichen: B.A.U.), auch unter dem Namen Ohmad (Einheitenzeichen: ohmad) bekannt, war eine 1865 in Großbritannien eingeführte Maßeinheit des Elektrischen Widerstandes.

1 B.A.U. = 1,988 Ohm




Harald Pfeiffer (Physiker)

Harald Pfeiffer (* 1974) ist ein deutscher Astrophysiker.

Er besuchte das Steigerwald-Landschulheim Wiesentheid und machte dort mit 19 Jahren sein Abitur. 1993 holte er bei der Internationalen Physikolympiade eine Goldmedaille. Von 1994 bis 1997 studierte er Physik an der Universität Bayreuth und von 1997 bis 1998 an der University of Cambridge. Danach studierte er von 1998 bis 2003 an der Cornell University bis er einen Abschluss als Ph.D. machte. 2003 bis 2009 war er am California Institute of Technology tätig. Seitdem übt er Lehrtätigkeiten am Canadian Institute for Theoretical Astrophysics der University of Toronto aus. 2016 übernahm er Forschungstätigkeiten im Gebiet Astrophysik am Max-Planck-Institut für Gravitationsphysik. Am 18. März 2016 wurde ihm der Friedrich Wilhelm Bessel-Forschungspreis verliehen.





Westcott-Faktor

Der Westcott-Faktor ist eine physikalische Kennzahl, durch die ein Zusammenhang zwischen den Wirkungsquerschnitten σ für einen Neutroneneinfang eines bestimmten Nuklids bei unterschiedlichen Temperaturen hergestellt werden kann.
Üblicherweise wird der Wirkungsquerschnitt für eine Temperatur von 20 °C angegeben, was einer Neutronengeschwindigkeit von 2200 m/s entspricht. Der Grund für diesen Wert erklärt sich aus der Tatsache, dass die Bestrahlungsrohre üblicherweise thermisch vom Reaktor entkoppelt sind und ihre Temperatur daher bei Umgebungstemperatur liegt.
Da in Reaktoren unterschiedlichste, meist höhere Temperaturen herrschen, genügt der Wirkungsquerschnitt für thermische Neutronen oft nicht für reaktorphysikalische Berechnungen.

Daher entwickelte C.H. Westcott eine Methode, um den Wirkungsquerschnitt bei Raumtemperatur σ₀ in den Wirkungsquerschnitt bei der gewünschten Temperatur umzurechnen.
Für die Berechnung wird das Neutronenspektrum als eine Überlagerung zwei verschiedener Neutronenspektren beschrieben. Die erste Komponente wird durch die temperaturabhängige maxwellsche Geschwindigkeitsverteilung der Neutronen gebildet, während die zweite Komponente aus epithermischen Neutronen besteht, deren Häufigkeitsverteilung mit steigender Neutronengeschwindigkeit eine Abnahme zeigt.

Bei Vernachlässigung des epithermischen Anteils im Neutronenspektrum ist der Westcott-Faktor (g) der Quotient aus der durchschnittlichen Wirkungsquerschnitt bei maxwellscher Geschwindigkeitsverteilung und dem Wirkungsquerschnitt bei einer Neutronengeschwindigkeit von 2200 m/s.

Wenn der Wirkungsquerschnitt reziprok mit der Geschwindigkeit abnimmt, so ist der Westcott-Faktor genau 1,0.




Giovanni Bignami

Giovanni „Nanni“ Fabrizio Bignami (* 10. April 1944 in Desio; † 24. Mai 2017 in Madrid) war ein italienischer Astrophysiker.

Bignami studierte Physik an der Universität Mailand mit dem Laurea Abschluss 1968 bei Giuseppe Occhialini. Danach wandte er sich der Weltraumforschung mit Satelliten zu. Er war leitender Wissenschaftler (Principal Investigator) beim Röntgensatelliten XMM-Newton der ESA. Außerdem war er Professor für Astronomie und Astrophysik am Istituto Universitario di Studi Superiori (IUSS). 

2004 bis 2007 war er Präsident des Space Science Advisory Committee (SSAC) der ESA und 2007 bis 2008 Vorstand der italienischen Raumfahrtagentur. 2011 bis 2015 war er als erster Italiener Präsident des Committee on Space Research (COSPAR). und 2011 bis 2015 Präsident des italienischen nationalen Instituts für Astrophysik (INAF, Istituto Nazionale di Astrofisica). Er war auch Sprecher des Square Kilometre Array, wesentlich am Cherenkov Telescope Array beteiligt und er war Direktor des Centre d’Etude Spatiale des Rayonnements in Toulouse.

Er befasste sich vor allem mit Röntgen- und Gammastrahlenastronomie. Er war beteiligt an den Missionen COS-B, SAS-2, XMM-Newton, Fermi Gamma-ray Space Telescope, INTEGRAL und AGILE. Von ihm stammen auch populärwissenschaftliche Bücher.

1993 erhielt er den Bruno-Rossi-Preis mit Jules Halpern. Er erhielt die Blaise-Pascal-Medaille, den Von Karman Award der International Academy of Astronautics, war Offizier der Ehrenlegion und des Ordre national du Mérite und Mitglied der Académie des sciences und der Accademia dei Lincei.

Er war mit der Astrophysikerin Patrizia A. Caravero (* 1954) verheiratet. Sie war wie Bignami an der Identifikation der Gammastrahlen- und Röntgenquelle Geminga (ein naher Neutronenstern und Pulsar ohne beobachtete Radioemission) beteiligt.






Leon Van Speybroeck

Leon Van Speybroeck (* 27. August 1935 in Wichita (Kansas); † 25. Dezember 2002 in Newton (Massachusetts)) war ein US-amerikanischer Physiker, Astrophysiker und Ingenieur. Er galt als führender Konstrukteur von Spiegeln für Röntgenteleskope und war Teleskop-Wissenschaftler des Chandra-X-Ray-Observatory.

Van Speybroeck studierte Physik am Massachusetts Institute of Technology mit dem Bachelor-Abschluss 1957 und der Promotion 1965 bei Henry Kendall und Jerome Friedman. Die Dissertation war über elastische Elektron-Deuteron-Streuung bei hohem Impulsübertrag. Danach forschte er noch zwei Jahre am MIT bevor er 1967 zu American Science and Engineering (AS&E) in Cambridge (Massachusetts) ging, wo er in der Röntgenastronomiegruppe vno Riccardo Giacconi war, wo er Spiegel für Röntgenteleskope entwickelte, beginnend mit dem Teleskop im Skylab, das dort 1973/74 aktiv war und bedeutende neue Erkenntnisse über die Sonnenkorona und magnetische Dynamo-Prozesse darin lieferte. 1973 ging er mit Giacconi an das neu gegründete Harvard-Smithsonian Center for Astrophysics (CfA) in die Hochenergie-Astrophysik-Gruppe. Van Speybroeck entwickelte die Spiegel für das Einstein-Observatorium (aktiv 1978 bis 1981), das eine Pionierrolle in der Röntgenastronomie hatte. Dafür erhielt er 1985 den George W. Goddard Award von SPIE. Danach leitete er die Entwicklung eines Röntgenteleskop-Spiegels von noch viel größerer Präzision, die Advancedx-ray Astrophysics Facility (AXAF), das spätere Chandra Teleskop (aktiv ab 1999). Dafür wurden viele neue Technologien, auch in der Metrologie, entwickelt und eine Auflösung von bis zu einer halben Bogensekunde erreicht (zehnmal besser als vorher). Er arbeitete dabei mit Optical Coating Laboratory Inc. in Santa Rosa, Hughes Danbury Optical Systems Inc. in Danbury (Connecticut) und Eastman Kodak Co. in Rochester zusammen.

Van Speybroeck leitete bei seinem Tod an einem metastasierenden Melanom ein Team am Chandra-Teleskop, das durch Beobachtungen an Galaxienclustern die Hubble-Konstante überprüfen wollte. Die Ergebnisse des von ihnen weitergeführten Projekts wurden von seinen Kollegen im August 2006 in Astrophysical Journal veröffentlicht. 

Als einer der führenden Teleskop-Spezialisten war er auch am COSTAR-Projekt der Korrektur des Hubble-Teleskop-Fehlers beteiligt.

2002 erhielt er den Bruno-Rossi-Preis. Er hatte noch eine Rede über Röntgenastronomie dafür vorbereitet, die dann sein Kollege Harvey Tananbaum hielt.





Hermann Lorberg

Albrecht Ludolf Hermann Lorberg (* 2. März 1831 in Biebrich; † 6. März 1906 in Bonn) war ein deutscher Lehrer und Physiker.

Hermann Lorberg studierte nach dem Schulbesuch in Bückeburg und Weilburg ab 1850 Naturwissenschaften an den Universitäten Göttingen, Heidelberg und Berlin.

Im Jahr 1857 wurde er bei Otto Hesse an der Ruprecht-Karls-Universität Heidelberg zum Dr. phil. promoviert und wandte sich anschließend dem Lehramt zu. Nach Stationen als Lehrer in Brandenburg, Barmen, Ruhrort und Saargemünd wurde Lorberg Gymnasiallehrer in Straßburg, wo ihm 1888 der Professorentitel verliehen wurde. 

Im Jahr 1888 habilitierte er sich für Theoretische Physik an der Universität Straßburg und erhielt im Sommer 1890 einen Ruf als Professor für Theoretische Physik an die Rheinische Friedrich-Wilhelm-Universität Bonn.

Am 18. Januar 1892 wurde Hermann Lorberg als Mitglied (Matrikel-Nr. 2942) in die Leopoldina aufgenommen.

Seine wissenschaftlichen Schwerpunkte lagen vorwiegend auf mathematischem Gebiet im Themenkreis der Elektrizität.

Die letzten Jahre seines Lebens waren durch eine zunehmende Geisteskrankheit geprägt, die zu einer einjährigen Einlieferung von Lorberg in eine Anstalt führte. Dass dieser Umstand die Universität Bonn veranlasste, die Professur für Theoretische Physik anderweitig zu besetzen, verbitterte ihn und veranlasste ihn darüber hinaus, sich völlig aus dem Leben zurückzuziehen.

Hermann Lorberg starb völlig vereinsamt bei einem „Unglücksfall“, der „auf seine Krankheit zurückzuführen ist“.







Kommensurabilität (Quantenmechanik)

In der Quantenmechanik heißen zwei Observablen kommensurabel, wenn sie gleichzeitig beliebig genau gemessen werden können. Observablen, die nicht gleichzeitig beliebig genau gemessen werden können, heißen inkommensurabel. Zwei Observablen sind genau dann kommensurabel, wenn der Kommutator der zugehörigen Operatoren verschwindet. 

Inkommensurable Observablen, deren Kommutator den Wert formula_1 annimmt, heißen zueinander komplementäre Observablen.

Nach der (verallgemeinerten) Heisenbergschen Unschärferelation gilt für zwei Operatoren formula_2 und einen Zustand formula_3 für deren Messunsicherheiten formula_4 beziehungsweise formula_5 im Zustand formula_6:
Aus formula_8 für jeden beliebigen Zustand folgt formula_9. 

Andererseits folgt aus formula_9, dass ein Satz gemeinsamer Eigenzustände zu den Operatoren formula_11 und formula_12 existiert. Durch Messung einer der beiden Größen kollabiert der Zustand auf den entsprechenden Eigenzustand und befindet sich bereits in einem Eigenzustand des zweiten Operators, sodass eine Messung der anderen Größe das System nicht erneut verändert.





Mori-Zwanzig-Formalismus

Der Mori-Zwanzig-Formalismus, benannt nach den Physikern Hajime Mori und Robert Zwanzig, ist eine Methode der statistischen Physik. Er ermöglicht, die Dynamik eines Systems mithilfe von Projektionsoperatoren in einen relevanten und einen irrelevanten Teil zu zerlegen, wodurch sich geschlossene Bewegungsgleichungen für den relevanten Teil finden lassen.

Makroskopische Systeme mit einer großen Zahl mikroskopischer Freiheitsgrade werden typischerweise sehr gut durch eine kleine Zahl relevanter Variablen beschrieben, etwa der Gesamtmagnetisierung bei Spinsystemen. Der Mori-Zwanzig-Formalismus ermöglicht, basierend auf bekannten mikroskopischen Bewegungsgleichungen eines Systems – typischerweise basierend auf der Hamilton-Funktion der klassischen Mechanik bzw. dem quantenmechanischen Hamiltonoperator – makroskopische Gleichungen zu erhalten, welche nur von den relevanten Variablen abhängen. Der irrelevante Teil tritt in diesen Gleichungen als Rauschen auf. Der Formalismus ermöglicht keine Aussagen darüber, was die relevanten Variablen sind, diese ergeben sich typischerweise aus den Eigenschaften des untersuchten Systems.

Die Observablen, welche das System beschreiben, bilden einen Hilbertraum. Der Projektionsoperator entspricht dann einer Abbildung auf den Unterraum, welcher nur von den relevanten Variablen aufgespannt wird. Der irrelevante Teil der Dynamik hängt von zu den relevanten Variablen orthogonalen Observablen ab. Als Skalarprodukt im Raum der dynamischen Variablen dient dabei eine Korrelationsfunktion. Infolgedessen kann der Mori-Zwanzig-Formalismus auch zur Behandlung von Korrelationsfunktionen verwendet werden.

Eine nicht explizit zeitabhängige Observable formula_1 gehorcht im Heisenbergbild der hamiltonischen Bewegungsgleichung
wobei der Liouville-Operator formula_3 im quantenmechanischen Fall durch den Kommutator formula_4 und im klassischen Fall durch die Poissonklammer formula_5 definiert ist (es wird hier angenommen, dass der Hamiltonoperator formula_6 nicht von der Zeit abhängt, Verallgemeinerungen auf den zeitabhängigen Fall existieren auch). Diese Gleichung wird formal gelöst durch
Der Projektionsoperator, der auf eine Observable formula_8 wirkt, ist definiert durch
wobei formula_1 die relevante Observable ist (dies kann auch ein Vektor aus mehreren relevanten Größen sein). Als Skalarprodukt – hier mit runden Klammern notiert – wird typischerweise das Mori-Produkt, eine Verallgemeinerung der üblichen Korrelationsfunktion, verwendet. Dieses ist für Observablen formula_11 definiert durch
Hierbei ist formula_13 die inverse Temperatur, formula_14 ist die Spur (im klassischen Fall das Phasenraumintegral) und formula_15 der Hamiltonoperator bzw. die Hamiltonfunktion. formula_16 ist die relevante Wahrscheinlichkeitsdichte (bzw. der Dichteoperator in der Quantenmechanik). Diese wird so gewählt, dass sie lediglich als Funktion der relevanten Observablen geschrieben werden kann, gleichzeitig aber die (i. d. R. unbekannte) tatsächliche Wahrscheinlichkeitsdichte möglichst gut approximiert, d. h. insbesondere die gleichen Erwartungswerte für die relevanten Variablen liefert wie diese.

Nun wendet man die Operatoridentität
auf die Größe
an. Unter Ausnutzung der obigen Definition des Projektionsoperators und der Definitionen
(Frequenzmatrix),
(stochastische Kraft) und
(Gedächtnisfunktion) lässt sich dies schreiben als

Dies ist eine Bewegungsgleichung für die relevante Observable formula_23, welche von dem Wert der Observable zum Zeitpunkt formula_24, dem Wert zu früheren Zeitpunkten (Gedächtnisterm) und der stochastischen Kraft (Rauschen, entspricht dem Einfluss der zu formula_23 orthogonalen Dynamik) abhängt.

Aufgrund des Faltungsterms ist die obigen Gleichung meist nur schwer zu lösbar. Typischerweise ist man an langsamen (makroskopischen) Variablen interessiert, deren Änderung auf größeren Zeitskalen stattfindet als das (mikroskopische) Rauschen. Entwickelt man die Gleichung bis zur zweiten Ordnung in formula_26, so erhält man
wobei
ist.

Für den Fall beliebiger Abweichungen vom thermodynamischen Gleichgewicht wird die allgemeinere Form des Mori-Zwanzig-Formalismus verwendet, aus welcher sich obige Gleichungen als Linearisierung ergeben. In diesem Fall hängt der Projektionsoperator explizit von der Zeit ab. In diesem Fall kann die Gleichung für eine relevante Observable
wobei
und
Hierbei wurden das zeitgeordnete Exponential
sowie der zeitabhängige Projektionsoperator
verwendet. Auch diese Gleichungen können mittels einer Verallgemeinerung des Mori-Produkts durch Korrelationsfunktionen dargestellt werden. Weitere Varianten des Mori-Zwanzig-Formalismus dienen der Beschreibung von Systemen mit zeitabhängigen Hamiltonoperatoren bzw. von allgemeinen dynamischen Systemen.






Katharine Way

Way studierte Physik von 1929 bis 1934 an der Columbia University, wo sie mit Edward Kasner ihre erste Veröffentlichung schrieb und 1932 mit einem Bachelor in Physik abschloss. Danach wechselte sie an die University of North Carolina und wurde die erste Doktorandin in Kernphysik von John Archibald Wheeler. Nach einem Lehrjahr am Bryn Mawr College war sie von 1939 bis 1942 Dozentin und Assistenzprofessorin an der University of Tennessee. Von 1942 bis 1945 arbeitete sie am Manhattan Project an der University of Chicago im Metallurgical Laboratory am Reaktordesign, an der Bewertung von Reaktorkonstanten und an der Organisation von Radioaktivitätsdaten für Spaltprodukte. Way untersuchte Neutronenflussdaten aus Enrico Fermis Experimenten. Ihre Berechnungen trugen zum Bau von Chicago Pile-1 bei. Way untersuchte auch Reaktorkonstanten und wie Spaltprodukte Kernreaktoren „vergiften“ könnten (wie im Fall des plötzlichen Abschaltens von B Reactor in Hanford im Jahr 1944). Die theoretische Arbeit mit Eugene Wigner führte zu der sogenannten Way-Wigner-Formel, die Beta-Zerfallsraten von Spaltprodukten berechnet. 1945 unterschrieb sie die Szilard-Petition und zog nach Oak Ridge, Tennessee, wo sie in den Clinton Laboratories ihre Arbeit an Spaltprodukten fortsetzte und Interesse an der Zusammenstellung von Arbeiten zu nuklearen Daten entwickelte. Besorgt über die Atombombe und ihren Einsatz gegen Japan, war Way Mitherausgeber des Buches One World or None. 1947 wechselte Way zum National Institute of Standards and Technology in Washington, DC, wo sie sich mit Datenauswertung beschäftigte und der angewandten Forschung widmete. Sie war Mitautorin einer Reihe von Veröffentlichungen, aus denen die Nuclear Data Sheets hervorgingen, und gründete 1953 das Nuclear Data Project. 1964 vereinbarte sie mit Academic Press die Einrichtung eines neuen Journals, Nuclear Data Sheets, um die umfangreichen Daten zu veröffentlichen, die sie und ihre Kollegen erstellt hatten, und 1965 war sie maßgeblich an der Einrichtung eines zweiten Journals mit dem Titel Atomic Data and Nuclear Data beteiligt. 1968 wurde sie außerordentlicher Professor an der Duke University in North Carolina.








Aggregatzustand

Als Aggregatzustände werden die unterschiedlichen Zustände eines Stoffes bezeichnet, die sich durch bloße Änderungen von Temperatur oder Druck ineinander umwandeln können. Es gibt die drei klassischen Aggregatzustände "fest", "flüssig" und "gasförmig" sowie in der Physik weitere nicht klassische Zustände wie z. B. das Plasma.

Die in der Thermodynamik verwendete Phase ist enger gefasst, sie unterteilt insbesondere den festen Zustand nach seiner inneren Struktur.
Welcher Aggregatzustand bzw. welche Phase abhängig von Druck und Temperatur stabil ist, wird in einem Phasendiagramm dargestellt.

Es gibt drei klassische Aggregatzustände:

Für feste Stoffe und flüssige Stoffe gibt es den zusammenfassenden Begriff kondensierte Materie.

Bei Feststoffen unterscheidet man auch nach anderen Merkmalen:

Die klassischen Aggregatzustände lassen sich mit einem Teilchenmodell erklären, das die kleinsten Teilchen eines Stoffes (Atome, Moleküle, Ionen) auf kleine runde Kugeln reduziert.

Die mittlere kinetische Energie aller Teilchen ist in allen Zuständen ein Maß für die Temperatur. Die Art der Bewegung ist in den drei Aggregatzuständen jedoch völlig unterschiedlich. Im Gas bewegen sich die Teilchen geradlinig wie Billardkugeln, bis sie mit einem anderen oder mit der Gefäßwand zusammenstoßen. In der Flüssigkeit müssen sich die Teilchen durch Lücken zwischen ihren Nachbarn hindurchzwängen (Diffusion, Brownsche Molekularbewegung). Im Festkörper schwingen die Teilchen nur um ihre Ruhelage.

Die kleinsten Teilchen sind bei einem Feststoff nur wenig in Bewegung. Sie schwingen um eine feste Position, ihren Gitterplatz, und rotieren meist um ihre Achsen. Je höher die Temperatur wird, desto heftiger schwingen bzw. rotieren sie, und der Abstand zwischen den Teilchen nimmt (meist) zu. Ausnahme: Dichteanomalie.

Hinweis: Betrachtet man die Teilchen mit quantenmechanischen Grundsätzen, so dürfen aufgrund der Heisenbergschen Unschärferelation eigentlich Teilchen nie ruhig stehen. Sie haben kleine Schwingungen, die man auch als Nullpunktsfluktuationen bezeichnet. Das entspricht dem Grundzustand des harmonischen Oszillators.

Zwischen den kleinsten Teilchen wirken verschiedene Kräfte, nämlich die Van-der-Waals-Kräfte, die elektrostatische Kraft zwischen Ionen, Wasserstoffbrückenbindungen oder kovalente Bindungen. Die Art der Kraft ist durch den atomaren Aufbau der Teilchen (Ionen, Moleküle, Dipole …) bestimmt. Bei Stoffen, die auch bei hohen Temperaturen fest sind, ist die Anziehung besonders stark.

Durch die schwache Bewegung und den festen Zusammenhalt sind die Teilchen regelmäßig angeordnet.

Durch die starke Anziehung sind die Teilchen eng beieinander (hohe Packungsdichte)

Die Teilchen sind nicht wie beim Feststoff ortsfest, sondern können sich gegenseitig verschieben. Bei Erhöhung der Temperatur werden die Teilchenbewegungen immer schneller.

Durch die Erwärmung ist die Bewegung der Teilchen so stark, dass die Wechselwirkungskräfte nicht mehr ausreichend sind, um die Teilchen an ihrem Platz zu halten. Die Teilchen können sich nun frei bewegen.

Obwohl der Abstand der Teilchen durch die schnellere Bewegung ein wenig größer wird (die meisten festen Stoffe nehmen beim Schmelzen einen größeren Raum ein), hängen die Teilchen weiter aneinander. Für die Verringerung des Volumens einer Flüssigkeit durch Kompression gilt ähnliches wie bei einem Festkörper, wobei der entsprechende Kompressionsmodul der Flüssigkeit zum Tragen kommt. Bei einer Temperaturverringerung wird das Volumen ebenfalls kleiner, bei Wasser jedoch nur bis zu einer Temperatur von 4 °C (Anomalie des Wassers), während darunter bis 0 °C das Volumen wieder ansteigt.

Obwohl die Teilchen sich ständig neu anordnen und Zitter-/Rotationsbewegungen durchführen, kann eine Anordnung festgestellt werden. Diese Nahordnung ist ähnlich wie im amorphen Festkörper, die Viskosität ist jedoch sehr viel niedriger, d. h. die Teilchen sind beweglicher.

Bei Stoffen im gasförmigen Zustand sind die Teilchen schnell in Bewegung. Ein Gas oder gasförmiger Stoff verteilt sich schnell in einem Raum. In einem geschlossenen Raum führt das Stoßen der kleinsten Teilchen gegen die Wände zum Druck des Gases.

Beim gasförmigen Zustand ist die Bewegungsenergie der kleinsten Teilchen so hoch, dass sie nicht mehr zusammenhalten. Die kleinsten Teilchen des gasförmigen Stoffes verteilen sich gleichmäßig im gesamten zur Verfügung stehenden Raum.

Durch die schnelle Bewegung der Teilchen in einem Gas sind sie weit voneinander entfernt. Sie stoßen nur hin und wieder einander an, bleiben aber im Vergleich zur flüssigen Phase auf großer Distanz. Ein gasförmiger Stoff lässt sich komprimieren, d. h. das Volumen lässt sich verringern.

Wegen der Bewegung sind die Teilchen ungeordnet.

In der physikalischen Chemie unterscheidet man zwischen Dampf und Gas. Beide sind physikalisch gesehen nichts anderes als der gasförmige Aggregatzustand; die Begriffe haben auch nicht direkt mit realem Gas und idealem Gas zu tun. Was "umgangssprachlich" als „Dampf“ bezeichnet wird, ist physikalisch gesehen eine Mischung aus flüssigen und gasförmigen Bestandteilen, welche man im Falle des Wassers als Nassdampf bezeichnet.

Bei einem Dampf im engeren Sinn handelt es sich um einen Gleichgewichtszustand zwischen flüssiger und gasförmiger Phase. Er kann, ohne Arbeit verrichten zu müssen, verflüssigt werden, das heißt beim Verflüssigen erfolgt kein Druckanstieg. Ein solcher Dampf wird in der Technik als Nassdampf bezeichnet im Gegensatz zum sogenannten Heißdampf oder überhitzten Dampf, der im eigentlichen Sinn ein reales Gas aus Wassermolekülen darstellt und dessen Temperatur oberhalb der Kondensationstemperatur der flüssigen Phase beim jeweiligen Druck liegt.

Reinstoffe werden entsprechend ihrem Aggregatzustand bei einer Temperatur von 20 °C (siehe Raumtemperatur) und einem Druck von 1013,25 hPa (Normaldruck) als Feststoff, Flüssigkeit oder Gas bezeichnet. Beispiel: Brom ist bei Raumtemperatur und Normaldruck flüssig (siehe Tabelle), also gilt Brom als Flüssigkeit.

Diese Bezeichnungen (Feststoff, Flüssigkeit, Gas) werden zwar auch gebraucht, wenn Stoffe unter veränderten Bedingungen einen anderen Aggregatzustand annehmen. Im engeren Sinne bezieht sich die Einteilung jedoch auf die oben genannten Standardbedingungen; jeder Stoff gehört dann zu einer der Kategorien.

Bei der Vermischung von Stoffen ergeben sich abhängig vom Aggregatzustand der Bestandteile und ihrem mengenmäßigen Anteil charakteristische Gemische, zum Beispiel Nebel oder Schaum.

Die Übergänge zwischen den verschiedenen Aggregatzuständen haben spezielle Namen (eoc, omc, eon) und spezielle Übergangsbedingungen, die bei Reinstoffen aus Druck und Temperatur bestehen. Diese Übergangsbedingungen entsprechen dabei Punkten auf den Phasengrenzlinien von Phasendiagrammen. Hierbei ist für jeden Phasenübergang eine bestimmte Wärmemenge notwendig bzw. wird dabei freigesetzt.

Die Sublimation und das Verdampfen kommen auch unterhalb der Sublimations- beziehungsweise Siedepunktes vor. Man spricht hier von einer Verdunstung.

Alle Übergänge können am Beispiel Wasser im Alltag beobachtet werden (siehe Abbildung):

Schnee oder Eis fängt im Frühjahr an flüssig zu werden, sobald Temperaturen oberhalb der Schmelztemperatur herrschen.

Kühlt das Wasser in Seen unter den Gefrierpunkt ab, bilden sich Eiskristalle, die mit der Zeit immer größer werden, bis die Oberfläche mit einer Eisschicht überzogen ist.

Wird Wasser im Kochtopf über seine Siedetemperatur erhitzt, so wird das Wasser gasförmig. Das „Blubbern“ im Kochtopf kommt zustande, weil das Wasser am heißen Topfboden zuerst die Siedetemperatur erreicht - Die aufsteigenden Blasen sind der Wasserdampf, der (wie die meisten gasförmigen Stoffe) unsichtbar ist. "Verdunstung", der Übergang von flüssig in gasförmig ohne Erreichen der Siedetemperatur, ist bei Schweiß auf der Haut gut zu beobachten.

Der deutlich sichtbare Nebel oberhalb kochenden Wassers, der meist umgangssprachlich als „Dampf“ bezeichnet wird, ist zu winzigen Wassertröpfchen kondensierter Wasserdampf. Tau und Wolken entstehen ebenfalls durch kondensierenden Wasserdampf.

Gefrorene Pfützen können im Winter, auch bei Temperaturen weit unterhalb des Gefrierpunktes, durch Sublimation nach und nach „austrocknen“, bis das Eis vollständig sublimiert und die Pfütze verschwunden ist.

Raureif oder Eisblumen, die sich im Winter bilden, entstehen durch den aus der Umgebungsluft resublimierenden Wasserdampf.

Durch Erhöhen der Temperatur (Zufuhr von thermischer Energie) bewegen sich die kleinsten Teilchen immer heftiger, und ihr Abstand voneinander wird (normalerweise) immer größer. Die Van-der-Waals-Kräfte halten sie aber noch in ihrer Position, ihrem Gitterplatz. Erst ab der sogenannten Schmelztemperatur wird die Schwingungsamplitude der Teilchen so groß, dass die Gitterstruktur teilweise zusammenbricht. Es entstehen Gruppen von Teilchen, die sich frei bewegen können. In ihnen herrscht eine Nahordnung, im Gegensatz zur Fernordnung von Teilchen innerhalb des Kristallgitters fester Stoffe.

Mit Sinken der Temperatur nimmt die Bewegung der Teilchen ab, und ihr Abstand zueinander wird immer geringer. Auch die Rotationsenergie nimmt ab. Bei der sogenannten Erstarrungstemperatur wird der Abstand so klein, dass sich die Teilchen gegenseitig blockieren und miteinander verstärkt anziehend wechselwirken – sie nehmen eine feste Position in einem dreidimensionalen Gitter ein.

Es gibt Flüssigkeiten, die sich bei sinkender Temperatur ausdehnen, beispielsweise Wasser. Dieses Verhalten wird als Dichteanomalie bezeichnet.

Die Geschwindigkeit der kleinsten Teilchen ist nicht gleich. Ein Teil ist schneller, ein Teil ist langsamer als der Durchschnitt. Dabei ändern die Teilchen durch Kollisionen ständig ihre aktuelle Geschwindigkeit.

An der Grenze eines Festkörpers oder einer Flüssigkeit, dem Übergang einer Phase in eine gasförmige, kann es mitunter vorkommen, dass ein Teilchen von seinen Nachbarn zufällig einen so starken Impuls bekommt, dass es aus dem Einflussbereich der Kohäsionskraft entweicht. Dieses Teilchen tritt dann in den gasförmigen Zustand über und nimmt etwas Wärmeenergie in Form der Bewegungsenergie mit, das heißt die feste oder flüssige Phase kühlt ein wenig ab.

Wird thermische Energie einem System zugeführt und erreicht die Temperatur die Sublimations- oder Siedetemperatur, geschieht dieser Vorgang kontinuierlich, bis alle kleinsten Teilchen in die gasförmige Phase übergetreten sind. In diesem Fall bleibt die Temperatur in der verdampfenden Phase in der Regel unverändert, bis alle Teilchen mit einer höheren Temperatur aus dem System verschwunden sind. Die Wärmezufuhr wird somit in eine Erhöhung der Entropie umgesetzt.

Wenn die Kohäsionskräfte sehr stark sind, beziehungsweise es sich eigentlich um eine viel stärkere Metall- oder Ionenbindung handelt, dann kommt es nicht zur Verdampfung.

Die durch Verdampfen starke Volumenzunahme eines Stoffes kann, wenn sehr viel Hitze schlagartig zugeführt wird, zu einer Physikalischen Explosion führen.

Der umgekehrte Vorgang ist die Kondensation beziehungsweise Resublimation. Ein kleinstes Teilchen trifft zufällig auf einen festen oder flüssigen Stoff, überträgt seinen Impuls und wird von den Kohäsionskräften festgehalten. Dadurch erwärmt sich der Körper um die Energie, die das kleinste Teilchen mehr trug als der Durchschnitt der kleinsten Teilchen in der festen beziehungsweise flüssigen Phase.

Stammt das Teilchen allerdings von einem Stoff, der bei dieser Temperatur gasförmig ist, sind die Kohäsionskräfte zu schwach, es festzuhalten. Selbst wenn es zufällig so viel Energie verloren hat, dass es gebunden wird, schleudert es die nächste Kollision mit benachbarten kleinsten Teilchen wieder in die Gasphase. Durch Absenken der Temperatur kann man den kleinsten Teilchen ihre Energie entziehen. Dadurch ballen sie sich beim Unterschreiten der Sublimations- oder Erstarrungstemperatur durch die Wechselwirkungskräfte mit anderen Teilchen zusammen und bilden wieder einen Feststoff oder eine Flüssigkeit.

Das p-T-Phasendiagramm eines Stoffes beschreibt in Abhängigkeit von Druck und Temperatur, in wie vielen Phasen ein Stoff vorliegt und in welchem Aggregatzustand sich diese befinden. Anhand der Linien kann man also erkennen, bei welchem Druck und welcher Temperatur die Stoffe ihren Aggregatzustand verändern. Gewissermaßen findet auf den Linien der Übergang zwischen den Aggregatzuständen statt, weshalb man diese auch als Phasengrenzlinien bezeichnet. Auf ihnen selbst liegen die jeweiligen Aggregatzustände in Form eines dynamischen Gleichgewichts nebeneinander in verschiedenen Phasen vor.






Neben den drei klassischen Aggregatzuständen gibt es weitere Materiezustände, die zum Teil nur unter extremen Bedingungen auftreten (nach Temperatur, tendenziell von hoher zu niedriger, sortiert).







Kernreaktor

Ein Kernreaktor, auch Atomreaktor oder Atommeiler ist eine Anlage, in der eine Kernspaltungsreaktion kontinuierlich als Kettenreaktion im makroskopischen, technischen Maßstab abläuft.

Weltweit verbreitet sind "Leistungsreaktoren", Kernreaktoranlagen, die durch die Spaltung () von Uran oder Plutonium zunächst Wärme und daraus meist elektrische Energie (siehe Kernkraftwerk) gewinnen. Dagegen dienen "Forschungsreaktoren" zur Erzeugung von freien Neutronen, etwa für Zwecke der Materialforschung oder zur Herstellung von bestimmten radioaktiven Nukliden für medizinische oder ähnliche Zwecke.

Im Erdaltertum kam es wiederholt zur Bildung natürlicher Kernreaktoren.

Ein Kernkraftwerk hat oft mehrere Reaktoren. Die beiden Begriffe werden oft ungenau verwendet. Zum Beispiel ist mit der Aussage „in Deutschland liefen bis zum Atomausstieg 17 Kernkraftwerke“ gemeint, dass 17 Kernreaktoren an deutlich weniger Standorten liefen. So etwa bestand das Kernkraftwerk Gundremmingen ursprünglich aus drei Reaktorblöcken; jeder Block besteht aus einem Reaktor mit Dampferzeuger und einem Turbosatz.

Die meisten Kernreaktoren sind ortsfeste Anlagen. In der Atom-Euphorie der späten 1950er und frühen 1960er Jahre kam der Gedanke an atomgetriebene Straßenfahrzeuge, Flugzeuge oder Raumschiffe auf. Inzwischen gibt es einige Kernreaktoren in U-Booten, Überwasserschiffen und Raumflugkörpern.

Zwischen den Protonen und den Neutronen eines Atomkerns wirken sehr starke anziehende Kräfte, die jedoch eine nur sehr begrenzte Reichweite haben. Daher wirkt diese Kernkraft im Wesentlichen auf die nächsten Nachbarn – weiter entfernte Nukleonen tragen zu der anziehenden Kraft nur in geringem Maße bei. Solange die Kernkraft größer ist als die abstoßende Coulombkraft zwischen den positiv geladenen Protonen, hält der Kern zusammen. Kleine Atomkerne sind stabil, wenn sie je Proton ein Neutron enthalten: Ca ist das schwerste stabile Nuklid mit gleicher Protonen- und Neutronenzahl. Mit zunehmender Protonenzahl wird ein immer höherer Neutronenüberschuss zur Stabilität erforderlich; die abstoßende Coulombkraft der Protonen untereinander wird durch die anziehende Kernkraft der zusätzlichen Neutronen kompensiert.

Fängt ein sehr schwerer Kern, etwa des Uranisotops U oder des Plutoniumisotops Pu, ein Neutron ein, so wird er durch die gewonnene Bindungsenergie zu einem hoch angeregten, instabilen U- beziehungsweise Pu-Kern. Solche hochangeregten schweren Kerne regen sich mit extrem kurzen Halbwertszeiten durch Kernspaltung ab. Anschaulich gesagt gerät der Kern durch die Neutronenabsorption wie ein angestoßener Wassertropfen in Schwingungen und zerreißt in (meist) zwei Bruchstücke (mit einem Massenverhältnis von etwa 2 zu 3), die mit hoher Bewegungsenergie auseinanderfliegen; außerdem werden etwa zwei bis drei "schnelle" Neutronen frei. Diese Neutronen stehen für weitere Kernspaltungen zur Verfügung; das ist die Grundlage der nuklearen Kettenreaktion.

Wenn Neutronen auf Kernbrennstoff treffen, finden neben der Kernspaltung unvermeidlich auch andere Kernreaktionen statt. Von besonderem Interesse sind Reaktionen, in denen Bestandteile des Kernbrennstoffs, die selbst nicht spaltbar sind, in spaltbare umgewandelt werden. Solche Reaktionen heißen Brutreaktionen, der Vorgang "Brüten" oder auch "Konversion". Von einem Brutreaktor spricht man allerdings erst dann, wenn mehr neues spaltbares Material erzeugt wird, als der Reaktor selbst in der gleichen Zeit verbraucht, die Konversionsrate also über 1,0 beträgt.

Der Brennstoff fast aller Kernreaktoren enthält hauptsächlich Uran. Daher ist die Brutreaktion an dem nicht spaltbaren Uranisotop U besonders wichtig. Das U wandelt sich durch Neutroneneinfang in U um. Dieses geht durch zwei aufeinander folgende Betazerfälle in das spaltbare Plutoniumisotop Pu über:

Das Pu wird teilweise noch im Reaktor wieder gespalten, teilweise kann es aber durch Aufarbeitung des gebrauchten Brennstoffes abgetrennt und zu anderen Zwecken verwendet werden.

Falls das abgetrennte Plutonium zu Kernwaffenzwecken dienen soll ("Waffenplutonium"), muss es isotopisch möglichst rein sein, d. h., es darf nicht zu viel Pu enthalten. Dieses nächstschwerere Plutoniumisotop entsteht, wenn der Pu-Atomkern ein weiteres Neutron einfängt. Daher erhält man waffenfähiges Plutonium nur aus solchen Brennelementen, die schon nach relativ kurzer Betriebszeit dem Reaktor entnommen werden.

In entsprechender Weise wie Pu-239 aus U-238 kann auch das spaltbare U-233 aus Thorium Th-232 „erbrütet“ werden.

Die neu entstandenen Kerne mittlerer Masse, die so genannten Spaltprodukte, haben eine größere Bindungsenergie pro Nukleon als der ursprüngliche schwere Kern. Die Differenz der Bindungsenergien tritt größtenteils als kinetische Energie der Spaltfragmente auf (Berechnung). Diese geben die Energie durch Stöße an das umgebende Material als Wärme ab. Die Wärme wird durch ein Kühlmittel abgeführt und kann beispielsweise zur Stromerzeugung, Heizung oder als Prozesswärme etwa zur Meerwasserentsalzung genutzt werden.

Etwa 6 % der gesamten in einem Kernreaktor frei werdenden Energie wird in Form von Elektron-Antineutrinos frei, die praktisch ungehindert aus der Spaltzone des Reaktors entweichen und das gesamte Material der Umgebung durchdringen. Diese Teilchen üben keine merklichen Wirkungen aus, da sie mit Materie kaum reagieren. Ihre Energie kann daher auch nicht technisch genutzt werden. Die verbleibende, nutzbare Energie aus der Spaltung von 1 Gramm U-235 beträgt etwa 0,91 MWd (Megawatt-Tage) oder 21500 Kilowattstunden. Dies entspricht etwa 9,5 Tonnen Braunkohle oder 1,8 Tonnen Heizöl.

Zusammengenommen erzeugen die rund 440 Kernreaktoren der derzeit 210 Kernkraftwerke, die es weltweit in 30 Ländern gibt, eine elektrische Leistung von etwa 370 Gigawatt. Dies ist ein Anteil von 15 % der gesamten elektrischen Energie weltweit (Stand: 2009).

Die Kettenreaktion besteht darin, dass Neutronen Atomkerne des Kernbrennstoffs spalten, wobei außer den energiereichen Spaltfragmenten auch jeweils einige neue Neutronen frei werden; diese können weitere Kerne spalten usw. Der Wirkungsquerschnitt der Kerne für Spaltung nimmt bei den meistgenutzten Brennstoffen mit abnehmender Energie, also abnehmender Geschwindigkeit des Neutrons zu: Je langsamer das Neutron ist, desto wahrscheinlicher ist es, dass es von einem spaltbaren Kern absorbiert wird und dieser sich anschließend spaltet. Daher bremst man in den meisten Reaktoren die schnellen Neutronen aus der Kernspaltung mittels eines Moderators ab. Dies ist ein Material wie etwa Graphit, schweres oder normales Wasser, das leichte Atomkerne (kleinere Massenzahl) enthält und einen sehr niedrigen Absorptionsquerschnitt für Neutronen hat. In diesem Material werden die Neutronen durch Stöße mit dessen Atomkernen stark abgebremst, aber nur selten absorbiert. Sie stehen also der Kettenreaktion weiter zur Verfügung. Die Neutronen können bis herunter auf die Geschwindigkeiten der Kerne des Moderators abgebremst werden; deren durchschnittliche Geschwindigkeit ist nach der Theorie der Brownschen Bewegung durch die Temperatur des Moderators gegeben. Es findet also eine "Thermalisierung" statt. Man spricht daher statt von abgebremsten meist von thermischen Neutronen, denn die Neutronen besitzen anschließend eine ähnliche thermische Energieverteilung wie die Moleküle des Moderators. Ein Reaktor, der zur Kernspaltung thermische Neutronen verwendet, wird als "Thermischer Reaktor" bezeichnet. Im Gegensatz dazu nutzt ein "schneller" Reaktor die nicht abgebremsten, schnellen Neutronen zur Spaltung (daher die Bezeichnung "Schneller Brüter").

Im abgeschalteten Zustand, d. h. bei eingefahrenen Steuerstäben, ist der Reaktor unterkritisch. Einige freie Neutronen sind zwar stets im Reaktor vorhanden – beispielsweise freigesetzt durch Spontanspaltung von Atomkernen des Kernbrennstoffs – und lösen zum Teil Spaltungen aus, aber das Anwachsen einer Kettenreaktion wird dadurch unterbunden, dass die meisten Neutronen von dem in den Steuerstäben enthaltenen Material (z. B. Bor) absorbiert werden, so dass der Multiplikationsfaktor "k" unter 1 liegt.

Zum Anfahren des Reaktors werden die Steuerstäbe unter ständiger Messung des Neutronenflusses mehr oder weniger weit aus dem Reaktorkern herausgezogen, bis leichte Überkritikalität durch verzögerte Neutronen, also eine selbsterhaltende Kettenreaktion mit allmählich zunehmender Kernreaktionsrate erreicht ist. Neutronenfluss und Wärmeleistung des Reaktors sind proportional zur Reaktionsrate und steigen daher mit ihr an. Mittels der Steuerstäbe – bei Druckwasserreaktoren auch über die Konzentration von Borsäure im Wasser – wird der Neutronenfluss auf das jeweils gewünschte Fluss- und damit Leistungsniveau im gerade kritischen Zustand eingeregelt und konstant gehalten; "k" ist dann gleich 1,0. Etwaige Änderungen von "k" durch Temperaturanstieg oder andere Einflüsse werden durch Verstellen der Steuerstäbe ausgeglichen. Dies geschieht bei praktisch allen Reaktoren durch eine automatische Steuerung, die auf den gemessenen Neutronenfluss reagiert.

Der Multiplikationsfaktor 1,0 bedeutet, dass durchschnittlich gerade eines der pro Kernspaltung freiwerdenden Neutronen eine weitere Kernspaltung auslöst. Alle übrigen Neutronen werden entweder absorbiert – teils unvermeidlich im Strukturmaterial (Stahl usw.) und in nicht spaltbaren Brennstoffbestandteilen, teils im Absorbermaterial der Steuerstäbe, meist Bor oder Cadmium – oder entweichen aus dem Reaktor nach außen (Leckage).

Zum Verringern der Leistung und zum Abschalten des Reaktors werden die Steuerstäbe eingefahren, wodurch er wieder unterkritisch wird. Der Multiplikationsfaktor sinkt auf Werte unter 1, die Reaktionsrate nimmt ab, und die Kettenreaktion endet.

Ein verzögert überkritischer Reaktor steigert seine Leistung langsam genug, dass die Regeleinrichtungen dem Vorgang folgen können. Falls die aktive Regelung bei wassermoderierten Reaktoren versagt, also die Kritikalität nicht auf 1 zurückgeregelt wird, steigert sich die Leistung über den Nennwert hinaus. Dabei erwärmt sich der Moderator und dehnt sich in der Folge aus oder verdampft. Da moderierendes Wasser jedoch notwendig ist, um die Kettenreaktion aufrechtzuerhalten, kehrt der Reaktor – sofern "nur" das Wasser verdampft, aber die räumliche Anordnung des Brennstoffs noch erhalten geblieben ist – in den unterkritischen Bereich zurück. Dieses Verhalten heißt eigenstabil.

Dieses Verhalten gilt beispielsweise nicht für graphitmoderierte Reaktortypen, da Graphit auch bei zunehmender Temperatur seine moderierenden Eigenschaften behält. Gerät ein solcher Reaktor durch Versagen der Regelungssysteme in den verzögert überkritischen Bereich, so kommt die Kettenreaktion nicht zum Erliegen, und dies kann zur Überhitzung und ggf. Zerstörung des Reaktors führen. Ein solcher Reaktor ist also nicht eigenstabil. Die Reaktoren aus Tschernobyl gehörten zu dieser Bauweise, die nur noch in Russland vorhanden ist.

Im Gegensatz zum "verzögert" überkritischen Reaktor ist ein "prompt" überkritischer Reaktor nicht mehr regelbar, und es kann zu schweren Unfällen kommen. Der Neutronenfluss und damit die Wärmeleistung des Reaktors steigt exponentiell mit einer Verdoppelungszeit im Bereich von 10 Sekunden an. Die erreichte Leistung kann die Nennleistung während einiger Millisekunden um mehr als das Tausendfache übersteigen, bis sie durch die Dopplerverbreiterung im so erhitzten Brennstoff wieder gesenkt wird. Die Brennstäbe können durch diese Leistungsexkursion schnell auf Temperaturen über 1000 °C erhitzt werden. Je nach Bauart und den genauen Umständen des Unfalls kann dies zu schweren Schäden am Reaktor führen, vor allem durch schlagartig verdampfendes (Kühl-)Wasser. Beispiele für prompt überkritische Leichtwasserreaktoren und die Folgen zeigen die BORAX-Experimente oder der Unfall im US-Forschungsreaktor SL-1. Der bisher größte Unfall durch einen zumindest in Teilbereichen prompt überkritischen Reaktor war die Nuklearkatastrophe von Tschernobyl, bei der unmittelbar nach der Leistungsexkursion schlagartig verdampfende Flüssigkeiten, Metalle und der anschließende Graphitbrand zu einer weiträumigen Verteilung des radioaktiven Inventars geführt haben.

Die automatische Unterbrechung der Kettenreaktion bei einer Leistungsexkursion eines wassermoderierten Reaktors ist, anders als gelegentlich behauptet, kein Garant dafür, dass es nicht zu einer Kernschmelze kommt, denn bei zusätzlichem Versagen aller aktiven Kühleinrichtungen reicht die Nachzerfallswärme aus, um diese herbeizuführen. Aus diesem Grunde sind die Kühlsysteme redundant und diversitär ausgelegt. Eine Kernschmelze wird als Auslegungsstörfall seit dem Unfall in Three Mile Island bei der Planung von Kernkraftwerken berücksichtigt und ist prinzipiell beherrschbar. Wegen der durch die Leistungsexkursion eventuell veränderten geometrischen Anordnung des Reaktorkerns ist erneute Kritikalität allerdings nicht grundsätzlich auszuschließen.

Eine Kettenreaktion mit gleichbleibender Reaktionsrate kann auch in einem unterkritischen Reaktor erreicht werden, indem man freie Neutronen aus einer unabhängigen Neutronenquelle einspeist. Ein solches System wird manchmal als "getriebener" Reaktor bezeichnet. Wenn die Neutronenquelle auf einem Teilchenbeschleuniger beruht, also jederzeit abschaltbar ist, bietet das Prinzip verbesserte Sicherheit gegen Reaktivitätsstörfälle. Die Nachzerfallswärme (siehe unten) tritt hier jedoch ebenso wie beim kritisch arbeitenden Reaktor auf; Vorkehrungen zur Beherrschung von Kühlungsverlust-Störfällen sind hier also ebenso nötig wie bei den üblichen Reaktoren.

Getriebene Reaktoren sind gelegentlich zu Versuchszwecken gebaut und betrieben worden. Sie werden auch als Großanlagen zur Energiegewinnung und gleichzeitigen Transmutation von Reaktorabfall (siehe Accelerator Driven System) entworfen und in diesem Fall manchmal als "Hybridreaktoren" bezeichnet. In ihnen könnten auch die in Reaktoren entstehenden schwereren Actinoide, deren Generationenfaktor für eine kritische Kettenreaktion zu klein ist, als Kernbrennstoffe genutzt werden.

Durch einen Fortluft-Kamin und das Abwasser werden auch im Normalbetrieb ständig entstehende, radioaktive Verunreinigungen (Tritium, radioaktives Jod etc. pp.) in die Umgebung geleitet.
Diesbezüglich wird vermutet, dass angebliche Häufungen von Krebs-Fallzahlen ursächlich mit diesen Emissionen zusammenhängen.

Wird ein Reaktor abgeschaltet, so wird durch den radioaktiven Zerfall der Spaltprodukte weiterhin Wärme produziert. Die Leistung dieser so genannten "Nachzerfallswärme" entspricht anfänglich etwa 5–10 % der thermischen Leistung des Reaktors im Normalbetrieb und klingt in einem Zeitraum von einigen Tagen größtenteils ab. Häufig wird dafür der Begriff "Restwärme" verwendet, welcher aber irreführend ist, denn es handelt sich nicht um die verbleibende aktuelle Hitze des Reaktorkerns, sondern um zusätzliche Wärmeproduktion, die durch die weiterlaufenden Zerfallsreaktionen hervorgerufen wird.

Um die Nachzerfallswärme in Notfällen (bei ausgefallenem Hauptkühlsystem) sicher abführen zu können, besitzen alle Kernkraftwerke ein aufwändiges "Not- und Nachkühlsystem". Sollten jedoch auch diese Systeme versagen, kann es durch die steigenden Temperaturen zu einer Kernschmelze kommen, bei der Strukturteile des Reaktorkerns und unter Umständen Teile des Kernbrennstoffs schmelzen. Dies war der Fall bei den Kernschmelzen in Fukushima, da dort bedingt durch einen kompletten Ausfall der Stromversorgung sämtliche aktiven Kühlsysteme zum Erliegen kamen.

Wenn Brennstäbe niederschmelzen und dadurch eine Zusammenballung von Brennstoff entsteht, nimmt der Multiplikationsfaktor zu, und es kann zu einer schnellen unkontrollierten Aufheizung kommen. Um diesen Prozess zu verhindern oder wenigstens zu verzögern, werden in einigen Reaktoren die im Reaktorkern verarbeiteten Materialien so gewählt, dass ihr Neutronen-Absorptionsvermögen mit steigender Temperatur anwächst, die Reaktivität also abnimmt. Bei Leichtwasserreaktoren, die fast 90 % des gesamten Atomstroms liefern, ist eine Kernschmelze im Betrieb nicht möglich, da die Kernspaltungskettenreaktion nur in Anwesenheit von Wasser stattfindet. Eine Kernschmelze ist jedoch bei mangelnder Kühlung im ausgeschalteten Reaktor aufgrund der Nachzerfallswärme möglich, wenn auch über längere Zeiträume. Der Fall der Kernschmelze wird als "größter anzunehmender Unfall" (GAU) betrachtet, also als der schwerste Unfall, der bei der Planung der Anlage in Betracht zu ziehen ist und dem sie ohne Schäden für die Umgebung standhalten muss. Solch ein Unfall ereignete sich beispielsweise im Kernkraftwerk Three Mile Island.

Den schlimmsten Fall, dass zum Beispiel das Reaktorgebäude nicht standhält und eine größere, die zulässigen Grenzwerte weit überschreitende Menge radioaktiver Stoffe austritt, bezeichnet man als Super-GAU. Dies geschah zum Beispiel 1986 bei der Katastrophe von Tschernobyl und 2011 bei der Katastrophe von Fukushima.

Als inhärent sicher gegen Kernschmelzen gelten beim derzeitigen Stand der Technik nur bestimmte Hochtemperaturreaktoren geringerer Leistung und Leistungsdichte; ganz allgemein inhärent sicher ist aber auch dieser Reaktortyp nicht, da Unfälle wie Graphitbrand oder Wassereinbruch katastrophale Folgen haben könnten.

Die Leistungsdichte in MW/m³ (Megawatt thermischer Leistung pro Kubikmeter Reaktorkern) bestimmt, welche technischen Vorsorgen getroffen werden müssen, um nach einer Schnellabschaltung die anfallende Nachzerfallswärme abzuführen. Typische Leistungsdichten sind für gasgekühlte Hochtemperaturreaktoren 6 MW/m³, für Siedewasserreaktoren 50 MW/m³ und für Druckwasserreaktoren 100 MW/m³.

Der Europäische Druckwasserreaktor (EPR) hat unterhalb des Druckbehälters zur Sicherheit für den Fall einer Kernschmelze ein besonders geformtes Keramikbecken, den "Core-Catcher". In diesem soll das geschmolzene Material des Reaktorkerns aufgefangen, aber an einer Zusammenballung gehindert und durch eine spezielle Kühlung abgekühlt werden.

Die ersten Versuchsreaktoren waren simple Aufschichtungen von spaltbarem Material. Ein Beispiel dafür ist der Reaktor Chicago Pile, in dem die erste kontrollierte Kernspaltung stattfand. Moderne Reaktoren werden nach der Art der Kühlung, der Moderation, des verwendeten Brennstoffs und der Bauweise unterteilt.

Mit normalem leichten Wasser moderierte Reaktionen finden im Leichtwasserreaktor (LWR) statt, der als "Siedewasserreaktor" (SWR) oder "Druckwasserreaktor" (DWR) ausgelegt sein kann. Leichtwasserreaktoren erzeugen fast 90 % der Kernenergie weltweit (68 % DWR, 20 % SWR) und 100 % in Deutschland. Eine Weiterentwicklung des Vor-Konvoi, Konvoi (die deutschen DWR) und des N4 ist der Europäische Druckwasserreaktor (EPR). Ein russischer Druckwasserreaktor ist der "WWER". Leichtwasserreaktoren benötigen angereichertes Uran, Plutonium oder Mischoxide (MOX) als Brennstoff. Ein Leichtwasserreaktor war auch der Naturreaktor Oklo.

Wesentliches Merkmal des Leichtwasserreaktors ist der negative Dampfblasenkoeffizient: Da Wasser gleichzeitig Kühlmittel und Moderator ist, ist ohne Wasser keine Kettenreaktion möglich, also auch keine Kernschmelze beim Reaktorbetrieb.

Die Brennelemente des LWR sind empfindlich gegenüber thermodynamischen und mechanischen Belastungen. Um diese zu vermeiden, sind ausgeklügelte, technische und betriebliche Schutzmaßnahmen erforderlich, welche die Auslegung des Kernkraftwerkes in Gänze prägen. Gleiches gilt für den Reaktordruckbehälter mit seinem Risiko des Berstens. Die verbleibenden Restrisiken der Kernschmelze der Brennelemente aufgrund der Nachzerfallswärme und des Berstens des Reaktordruckbehälters wurden in der Kernenergiewirtschaft wegen der Unwahrscheinlichkeit ihres Eintretens lange Zeit als irrelevant erklärt, zum Beispiel von Heinrich Mandel.

Mit schwerem Wasser moderierte "Schwerwasserreaktoren" erfordern eine große Menge des teuren schweren Wassers, können aber mit natürlichem, nicht angereichertem Uran betrieben werden. Der bekannteste Vertreter dieses Typs ist der in Kanada entwickelte CANDU-Reaktor.

Gasgekühlte graphitmoderierte Reaktoren wurden bereits in den 1950er-Jahren entwickelt, zunächst primär für militärische Zwecke (Plutoniumproduktion). Sie sind die ältesten kommerziell genutzten Kernreaktoren; das Kühlmittel ist in diesem Fall Kohlenstoffdioxid. In Großbritannien ist (2011) noch eine Reihe dieser Anlagen in Betrieb.
Wegen der aus einer Magnesiumlegierung hergestellten Brennstabhülle heißt dieser Reaktortyp "Magnox-Reaktor". Ähnliche Anlagen wurden auch in Frankreich betrieben, sind aber inzwischen alle abgeschaltet.

Am 17. Oktober 1969 schmolzen kurz nach Inbetriebnahme des Reaktors 50 kg Brennstoff im gasgekühlten Graphitreaktor des französischen Kernkraftwerks Saint-Laurent A1 (450 MW). Der Reaktor wurde daraufhin 1969 stillgelegt (die heutigen Reaktoren des Kernkraftwerks sind Druckwasserreaktoren).

Ein Nachfolger der Magnox-Reaktoren ist der in Großbritannien entwickelte "Advanced Gas-cooled Reactor" (AGR). Im Unterschied zu den Magnox-Reaktoren verwendet er leicht angereichertes Urandioxid statt Uranmetall als Brennstoff. Dies ermöglicht höhere Leistungsdichten und Kühlmittelaustrittstemperaturen und damit einen besseren thermischen Wirkungsgrad. AGR haben mit 42 % den höchsten Wirkungsgrad aller bisherigen Kernkraftwerke erzielt.

"Hochtemperaturreaktoren" (HTR) nutzen ebenfalls Graphit als Moderator; als Kühlmittel wird Helium-Gas verwendet. Eine mögliche Bauform des Hochtemperaturreaktors ist der "Kugelhaufenreaktor" nach Farrington Daniels und Rudolf Schulten, bei dem der Brennstoff vollständig in Graphit eingeschlossen ist. Dieser Reaktortyp galt lange als einer der sichersten, da hier bei einem Versagen der Not- und Nachkühlsysteme eine Kernschmelze aufgrund des hohen Schmelzpunktes des Graphits unmöglich ist. Allerdings gibt es eine Reihe anderer schwerwiegender Unfalltypen wie Wassereinbruch oder Lufteinbruch mit Graphitbrand, welche die behaupteten Sicherheitsvorteile in Frage stellen, wie Rainer Moormann herausstellte, der dafür den Whistleblowerpreis 2011 erhielt. Auch eine Reihe ungelöster praktischer Probleme hat die kommerzielle Umsetzung des Konzepts verhindert. Hinzu kommt, dass die Anlagekosten des HTR höher als die des Leichtwasserreaktors sind. In Deutschland forschte man am Versuchskernkraftwerk AVR (Jülich) und baute das Prototypkraftwerk THTR-300 in Schmehausen, letzteres mit einem Reaktordruckbehälter aus Spannbeton. Beide wurden 1989 stillgelegt.

Die sowjetischen Reaktoren vom Typ "RBMK" nutzen ebenfalls Graphit als Moderator, jedoch leichtes Wasser als Kühlmittel. Hier liegt der Graphit in Blöcken vor, durch die zahlreiche Kanäle gebohrt sind, in denen sich Druckröhren mit den Brennelementen und der Wasserkühlung befinden. Dieser Reaktortyp ist träge (man braucht viel Zeit zum Regeln) und unsicherer als andere Typen, da der Dampfblasenkoeffizient positiv ist: Anders als bei Leichtwasserreaktoren bedeutet ein Kühlmittelverlust hier nicht Moderatorverlust, verringert aber die Neutronenabsorption durch das Kühlmittel; er erhöht also die Reaktivität, statt sie zu verringern. Die dadurch erhöhte Wärmeleistung ohne genügende Kühlung kann schnell zur Kernschmelze führen. Der havarierte Reaktor in Tschernobyl war von diesem Typ. Reaktoren dieser Art sind heutzutage nur noch in Russland zu finden.

Weiterhin gibt es Brutreaktoren ("Schnelle Brüter"), in denen zusätzlich zur Energiefreisetzung U so in Pu umgewandelt wird, so dass mehr neues Spaltmaterial entsteht als zugleich verbraucht wird. Diese Technologie ist auch sicherheitstechnisch anspruchsvoller als die der anderen Typen. Ihr Vorteil ist, dass mit ihr die Uranvorräte der Erde bis zu 50–100 mal besser ausgenutzt werden können als wenn nur das U „verbrannt“ wird. Brutreaktoren arbeiten mit schnellen Neutronen und verwenden flüssiges Metall wie Natrium als Kühlmittel.

Kleinere nicht brütende Reaktoren mit Flüssigmetallkühlung (Blei-Bismut-Legierung) wurden in sowjetischen U-Booten eingesetzt.

In einem Flüssigsalzreaktor () wird eine Salzschmelze, die den Kernbrennstoff (beispielsweise Thorium und Uran) enthält, in einem Kreislauf umgewälzt. Die Schmelze ist gleichzeitig Brennstoff und Kühlmittel. Dieser Reaktortyp ist jedoch nicht über das Experimentierstadium hinausgekommen.

Zugunsten von Flüssigsalzreaktoren sind verschiedene Sicherheits- und Nachhaltigkeitsargumente vorgebracht worden: Die verwendeten Fluoridsalze sind nicht wasserlöslich, was eine Kontamination der Umgebung bei Unfällen erschwert. Als Brutreaktoren können die Flüssigsalzreaktoren den Brennstoff sehr effizient verwenden, sowie mit einem breiten Spektrum an Brennstoffen betrieben werden. Diese Reaktoren wurden in den 60er Jahren in den USA für den Antrieb für Flugzeuge erforscht. Die Entwicklung wurde etwa 1975 aufgegeben, vor allem wegen Korrosionsproblemen. Erst in den 2000er Jahren wurde das Konzept wieder aufgegriffen, u. a. auch in den "Generation-IV"-Konzepten.

Es gibt weiterhin einige Sondertypen für spezielle Anwendungen. So wurden kleine Reaktoren mit hochangereichertem Brennstoff für die Stromversorgung von Raumflugkörpern konstruiert, die ohne flüssiges Kühlmittel auskommen. Diese Reaktoren sind nicht mit den Isotopenbatterien zu verwechseln. Auch luftgekühlte Reaktoren, die stets hochangereicherten Brennstoff erfordern, wurden gebaut, zum Beispiel für physikalische Versuche im BREN-Tower in Nevada.
Es wurden Reaktoren für den Antrieb von Raumfahrzeugen konstruiert, bei denen flüssiger Wasserstoff zur Kühlung des Brennstoffes dient. Allerdings kamen diese Arbeiten über Bodentests nicht hinaus (Projekt NERVA, Projekt Timberwind). Ebenfalls nicht über das Versuchsstadium hinaus kamen Reaktoren, bei denen der Brennstoff in gasförmiger Form vorliegt (Gaskernreaktor).

Derzeit wird weltweit aktiv an neuen Reaktorkonzepten gearbeitet, den "Generation-IV"-Konzepten, insbesondere mit Blick auf den erwarteten wachsenden Energiebedarf. Diese sollen besondere Kriterien von Nachhaltigkeit, Sicherheit und Wirtschaftlichkeit erfüllen. Insbesondere wird durch Brutreaktoren eine deutlich höhere Effizienz in der Ausnutzung vom Brennstoff erzielt und eine geringere Menge an radioaktivem Abfall. Das Risiko der Kernschmelze durch die Nachzerfallswärme wird mit einer ausreichend starken passiven Kühlung auf Null reduziert. Die ersten Gen-IV-Reaktoren sollen ab 2030 zum Einsatz kommen.

Ein weiterer, zurzeit noch im Experimentalstadium befindlicher Reaktortyp ist der Laufwellen-Reaktor. Dieses Konzept verspricht, sofern die Umsetzung gelingen sollte, eine vielfach effizientere Nutzung des Kernbrennstoffs sowie die massive Reduzierung der Problematik des radioaktiven Abfalls, da ein Laufwellen-Reaktor mit radioaktivem Abfall betrieben werden könnte und diesen dabei systematisch aufbrauchen würde.

Eine Kernspaltungs-Kettenreaktion erfordert nicht notwendigerweise komplexe technische Systeme. Sie kann sich unter bestimmten – wenn auch seltenen – Umständen auch in der Natur entwickeln. 1972 entdeckten französische Forscher in der Region Oklo des westafrikanischen Landes Gabun die Überreste des natürlichen Kernreaktors Oklo, der vor etwa zwei Milliarden Jahren, im Proterozoikum, durch Naturvorgänge entstanden war. Insgesamt wurden bisher in Oklo und einer benachbarten Uranlagerstätte Beweise für frühere Spaltungsreaktionen an 17 Stellen gefunden.

Eine Voraussetzung für das Zustandekommen der natürlich abgelaufenen Spaltungs-Kettenreaktionen war der im Erdaltertum viel höhere natürliche Anteil an spaltbarem U im Uran. Er betrug damals ca. 3 %. Auf Grund der kürzeren Halbwertszeit von U gegenüber U beträgt der natürliche Gehalt von U im Uran derzeit nur noch etwa 0,7 %. Bei diesem geringen Gehalt an spaltbarem Material können neue kritische Spaltungs-Kettenreaktionen auf der Erde nicht mehr natürlich vorkommen.

Ausgangspunkt für die Entdeckung des Oklo-Reaktors war die Beobachtung, dass das Uranerz aus der Oklo-Mine einen geringfügig kleineren Gehalt des Isotops Uran-235 als erwartet aufwies. Die Wissenschaftler bestimmten daraufhin die Mengen verschiedener Edelgasisotope, die in einer Materialprobe der Oklo-Mine eingeschlossenen waren, mit einem Massenspektrometer. Aus der Verteilung der verschiedenen bei der Uranspaltung entstehenden Xenonisotope in der Probe ergab sich, dass die Reaktion in Pulsen abgelaufen ist. Der ursprüngliche Urangehalt des Gesteins führte mit der Moderatorwirkung des in den Spalten des Urangesteins vorhandenen Wassers zur Kritikalität. Die dadurch freigesetzte Wärme im Urangestein erhitzte das Wasser in den Spalten, bis es schließlich verdampfte und nach Art eines Geysirs entwich. Infolgedessen konnte das Wasser nicht mehr als Moderator wirken, so dass die Kernreaktion zum Erliegen kam (Ruhephase). Daraufhin sank die Temperatur wieder ab, so dass frisches Wasser einsickern und die Spalten wieder auffüllen konnte. Dies schuf die Voraussetzung für erneute Kritikalität, und der Zyklus konnte von vorne beginnen. Berechnungen zeigen, dass auf die etwa 30 Minuten dauernde aktive Phase (Leistungserzeugung) eine Ruhephase folgte, die mehr als zwei Stunden anhielt. Auf diese Weise wurde die natürliche Kernspaltung für etwa 500.000 Jahre in Gang gehalten, wobei über fünf Tonnen Uran-235 verbraucht wurden. Die Leistung des Reaktors lag (im Vergleich zu den heutigen Megawatt-Reaktoren) bei geringen 100 Kilowatt.

Bedeutsam ist der Naturreaktor von Oklo für die Beurteilung der Sicherheit von Endlagerungen für Radionuklide (Atommüll). Die dort beobachtete geringe Migration einiger Spaltprodukte und des erbrüteten Plutoniums über Milliarden Jahre hinweg wurden von Kernenergiebefürwortern so interpretiert, dass atomare Endlager existieren können, die über lange Zeiträume hinreichend sicher sind.

Die meisten Kernreaktoren dienen der Erzeugung von elektrischer (selten: nur thermischer) Energie in Kernkraftwerken. Daneben werden Kernreaktoren auch zur Erzeugung von Radionukliden zum Beispiel für die Nutzung in Radioisotopengeneratoren oder in der Nuklearmedizin verwendet. Dabei werden die gesuchten Nuklide
Theoretisch könnte man in einem Reaktor auch Gold herstellen, was allerdings sehr unwirtschaftlich wäre.

Die wichtigste im Reaktor stattfindende Stoffumwandlungs-Reaktion (neben der Erzeugung von Spaltprodukten) ist die Erbrütung (siehe oben) von Plutonium-239 aus Uran-238, dem häufigsten Uranisotop. Sie erfolgt unvermeidlich in jedem mit Uran betriebenen Reaktor. Es gibt aber speziell dafür optimierte militärische Reaktoren, die insbesondere auf die Entnahme des Brennstoffs nach nur kurzem Betrieb eingerichtet sind, so dass Pu mit nur geringem Gehalt an Pu verfügbar wird.

Kernreaktoren dienen auch als intensive regulierbare Neutronenquellen für physikalische Untersuchungen aller Art. Weitere Anwendungen sind der Antrieb von Fahrzeugen (Kernenergieantrieb) und die Energieversorgung mancher Raumflugkörpern.

Das von Kernreaktoren ausgehende Gefahrenpotenzial sowie die bislang ungelöste Frage der Lagerung der anfallenden radioaktiven Abfälle haben nach Jahren der Euphorie seit den 1970er-Jahren in vielen Ländern zu Protesten von Atomkraftgegnern und zu einer Neubewertung der Kernenergie geführt. Während in den 1990er-Jahren vor allem in Deutschland der Ausstieg aus der Kernenergie propagiert wurde, fand etwa 2000 bis 2010 vor dem Hintergrund der verblassenden Erinnerungen an die Risiken (die Katastrophe von Tschernobyl lag 20 Jahre zurück) ein Versuch statt, die Atomkraft wieder gesellschaftsfähig zu machen. Anlass ist die durch internationale Verträge geforderte Reduktion des CO-Ausstoßes bei der Verbrennung fossiler Energieträger. Dem steht ein wachsender Energiebedarf aufstrebender Volkswirtschaften wie China gegenüber.

Aus diesen Gründen entschlossen sich einige europäische Staaten, in neue Kernkraftwerke zu investieren. So bauen der deutsche Konzern Siemens und die französische Gruppe Areva einen Druckwasserreaktor vom Typ EPR im finnischen Olkiluoto, der 2018 ans Netz gehen soll. Russland will seine alten und teilweise maroden Kernkraftwerke erneuern und mindestens zehn Jahre lang pro Jahr einen neuen Reaktorbau beginnen. In Frankreich wird ebenfalls über den Neubau eines Reaktors verhandelt. Schweden stoppte seine Pläne zum Atomausstieg. Daneben gibt es kleinere und größere Neubauprojekte im Iran, der Volksrepublik China, Indien, Nordkorea, Türkei und anderen Staaten. (Hauptartikel: Kernenergie nach Ländern). Außerdem sind viele Länder im Forschungsverbund Generation IV International Forum bei der Entwicklung von sechs neue Reaktortypen, die höhere Nachhaltigkeit, Sicherheit und Wirtschaftlichkeit garantieren sollen.

Die atomaren Unfälle in dem japanischen Kraftwerk Fukushima-Daiichi in der Folge des Magnitude-9-Erdbebens und darauffolgenden Tsunami vom 11. März 2011 brachten hierzu fast überall neue Überlegungen in Gang. Anders als beim Unfall in Tschernobyl, bei dem eine völlig veraltete und bekanntlich gefährliche Reaktorart verwendet wurde, zeigten die Unfälle in Fukushima eine Schwäche von Leichtwasserreaktoren, der häufigsten Bauart.

Die Lebensdauer von Kernreaktoren ist nicht unbegrenzt. Besonders der Reaktordruckbehälter ist ständiger Neutronenstrahlung ausgesetzt, die zur Versprödung des Materials führt. Wie schnell das geschieht, hängt unter anderem davon ab, wie die Brennelemente im Reaktor angeordnet sind und welchen Abstand sie zum Reaktordruckbehälter haben. Die Kernkraftwerke Stade und Obrigheim wurden auch deshalb als erste vom Netz genommen, weil hier dieser Abstand geringer war als bei anderen, neueren Kernreaktoren. Zurzeit versuchen die Betreiber von Kernkraftwerken, durch eine geschickte Beladung mit Brennelementen und zusätzliche Moderatorstäbe die Neutronenbelastung des Reaktordruckbehälters zu reduzieren. Unter anderem das Helmholtz-Zentrum Dresden-Rossendorf erforscht diese Problematik.








Korrespondenzprinzip

Mit Korrespondenzprinzip wurde ursprünglich eine Beziehung zwischen Termini der klassischen Physik und der Quantenmechanik bezeichnet. Der Ausdruck wurde 1920 von Niels Bohr im Kontext der älteren Quantentheorie geprägt. Es wird in diesem Zusammenhang auch als Bohrsches Korrespondenzprinzip bezeichnet.

Bohr ging in seinem Atommodell von 1913 noch von einem klassischen Modell aus, war aber gezwungen, sehr einschränkende Bedingungen für die vorkommenden Elektronenbahnen zu formulieren, um die beobachteten diskreten optischen Spektren zu erklären. Trotzdem konnte die so formulierte „ältere Quantentheorie“ keine vollständige Theorie der Spektren liefern. Für große Quantenzahlen ergaben sich jedoch asymptotische Formeln, die denen der klassischen Physik entsprachen und diese Erklärungslücken teilweise füllen konnten. Das Korrespondenzprinzip diente in diesem Sinn in der älteren Quantentheorie als heuristisches Prinzip, um den Übergang zur klassischen Physik (in diesem Fall Elektrodynamik) für große Quantenzahlen zu beschreiben.

Auch in der ab 1925 entstandenen Quantenmechanik diente das Korrespondenzprinzip zur Beschreibung einer heuristischen Methode, quantenmechanische Operatoren und ihre Vertauschungsrelationen mit denen der klassischen Mechanik in Verbindung zu bringen.

In der Wissenschaftstheorie wird (angeregt durch das Beispiel der Quantentheorie) unter Korrespondenzprinzip die Beziehung verschiedener Theorien, in der Regel einer älteren und einer neueren, zum selben Phänomenbereich verstanden. Es geht damit um das grundlegende Konzept einer Theorienhierarchie und -entwicklung in den Naturwissenschaften. Auch in weiteren Wissenschaften wie der Kristallographie wird in diesem Sinn von Korrespondenzprinzipien gesprochen.

Das Korrespondenzprinzip beschreibt ein bestimmtes Verhältnis zwischen einer älteren naturwissenschaftlichen Theorie und einer neueren mit größerem Gültigkeitsbereich. Es liegt vor, wenn die neuere Theorie auf dem Gültigkeitsbereich der älteren zu denselben Ergebnissen kommt wie diese. Diese Art der Theorienentwicklung ist in den Naturwissenschaften typisch und erstrebenswert. In den Sozialwissenschaften war dies Gegenstand umfangreicher epistemologischer Positionskämpfe, die wissenschaftstheoretisch von Imre Lakatos und Thomas S. Kuhn begründet wurden.

Die neuere Theorie enthält in diesem Fall die ältere als Grenzfall und erklärt so ihren früheren Erfolg. Ferner gerät die neue Theorie nicht in Konflikt mit den älteren experimentellen Befunden. Dabei kann sich die neuere Theorie strukturell und begrifflich komplett von der älteren unterscheiden. Die ältere Theorie ist damit zwar im Prinzip widerlegt, sie bleibt jedoch in ihrem begrenzten Gültigkeitsbereich weiterhin nützlich.

Im Folgenden werden einige bedeutende wissenschaftsgeschichtliche Beispiele für Erfüllungen dieses Korrespondenzprinzips erläutert.

Obwohl die Relativitätstheorie völlig neue Vorstellungen von Raum und Zeit einführt, gehen ihre Vorhersagen in die der newtonschen Physik über, wenn man sie auf unseren Alltagsbereich anwendet.

In der speziellen Relativitätstheorie hängen räumliche und zeitliche Distanzen vom Bewegungszustand des Beobachters ab. Sind die entsprechenden Geschwindigkeiten hinreichend klein gegen die Lichtgeschwindigkeit, so geraten die Differenzen dieser Distanzen unter die experimentelle Nachweisgrenze, so dass die an sich überholten Konzepte von Raum und Zeit der newtonschen Physik angewendet werden können. Ebenso ist die Krümmung des Raumes durch die Anwesenheit von Massen und die Abhängigkeit des Ganges von Uhren von ihrer Position im Gravitationsfeld, wie sie in der allgemeinen Relativitätstheorie vorhergesagt werden, für hinreichend kleine Raumgebiete wie beispielsweise innerhalb unseres alltäglichen Aktionsradius experimentell kaum feststellbar. Auch das Verhältnis zwischen spezieller und allgemeiner Relativitätstheorie entspricht dem Korrespondenzprinzip.

Die Gesetze der newtonschen Physik lassen sich als Grenzfall aus denen der Quantenphysik herleiten, obwohl letztere auf völlig andersartigen und nicht mehr anschaulich zugänglichen Konzepten von Materie und Bewegung beruhen und obwohl es in der Quantenmechanik Größen gibt (z. B. den Spin), die in der klassischen Mechanik nicht vorkommen.

Die Quantenphysik erlaubt in der Regel lediglich Wahrscheinlichkeitsprognosen für den Wert einer Messgröße wie beispielsweise den Ort, an dem sich ein Objekt befinden wird. Sie ist daher nicht mehr bezüglich jeder Fragestellung deterministisch. Berechnet man den so genannten Erwartungswert, das heißt den Mittelwert dieser Messgröße im Grenzfall unendlich häufiger Wiederholung des Experiments, so stellt sich bei Existenz der Größe in der klassischen Physik heraus, dass dieser den bekannten Gleichungen der newtonschen Physik gehorcht (Ehrenfest-Theorem). Wendet man die Regeln der Quantenphysik auf makroskopische mechanische Systeme an, so wird die statistische Streuung der Messergebnisse nahezu unmessbar klein. Dabei entsprechen solche Systeme i. a. einem statistischen Ensemble aus einer großen Zahl von sogenannten reinen Quantenzuständen mit großen Quantenzahlen. Damit folgt der deterministische Charakter der klassischen Physik für den makroskopischen Grenzfall aus der Quantenphysik, obwohl letztere selbst nicht deterministisch ist.

Eines der großen Probleme des Theoriengebäudes der Physik besteht derzeit darin, dass seine beiden Säulen, die allgemeine Relativitätstheorie und die Quantenphysik, in ihrer Beziehung zueinander das Korrespondenzprinzip nicht erfüllen. Beide Theorien haben daher nur einen begrenzten Gültigkeitsbereich, so dass die heutige Physik keine abgeschlossene Beschreibung der Natur liefern kann. Es wird daher nach einer Theorie der so genannten Quantengravitation gesucht, die die Relativitätstheorie und die Quantenphysik vereinigt, indem sie beide als Grenzfall im Sinne des Korrespondenzprinzips enthält.

Die ältere Quantentheorie kombiniert die klassische Mechanik quasiperiodischer Systeme mit zusätzlichen Annahmen, deren wichtigste die Einschränkung der zulässigen Bahnen im Phasenraum ist auf solche, für die die Quantisierung des Bahndrehimpulses gilt:

mit
bzw.

mit

Das Korrespondenzprinzip fordert nun einen Zusammenhang zwischen den Koeffizienten formula_9 der Fourierentwicklung der Ortskoordinaten nach der Zeit

und den quantentheoretisch möglichen Strahlungsübergängen, sowie der Intensität und Polarisation des dabei ausgesandten Lichts. So lassen sich u. a. die spektroskopischen Auswahlregeln ableiten, indem vom Verschwinden der "n"-ten Fourierkomponente auf die Unmöglichkeit des korrespondierenden Quantensprungs um "n" Einheiten geschlossen wird.

Eine Bedingung, die Bohr an diese Korrespondenz stellt, ist die der näherungsweisen Übereinstimmung mit der klassischen Elektrodynamik für große Quantenzahlen. Dies stellt somit eines der oben beschriebenen wissenschaftstheoretischen Korrespondenzprinzipien dar.

Im Anschluss an Heisenberg wird die Zuordnung klassischer Observablen zu ihren Entsprechungen in der mathematischen Formulierung der Quantenmechanik, den Operatoren auf Hilbert-Räumen, als Korrespondenz bezeichnet. Damit dient die klassische Theorie in der Anwendung des Korrespondenzprinzips in dieser Bedeutung dazu, die physikalisch sinnvollen Gleichungen der Quantenmechanik zu finden – eben durch Übernahme der algebraischen Form der Gleichungen, wobei bestimmte klassische Observable durch die ihnen korrespondierenden quantenmechanischen Operatoren ersetzt werden. Beispielsweise entsteht durch die Ersetzung der Impulsvariable durch die entsprechenden Impulsoperatoren (und entsprechend für die Ortsvariable) aus der klassischen Energiegleichung die Schrödingergleichung. Diese Zuordnung wurde früher gelegentlich auch als "Jordan’sche Regel" bezeichnet.

Paul Niggli formulierte die Korrespondenz zwischen Kristallstruktur und Morphologie. Zum einen ist die Symmetrie der äußeren Kristallflächen (s. "Punktgruppe") höher oder gleich der Symmetrie der Kristallstruktur (s. "Raumgruppe"). Zum anderen verläuft jede äußere Kristallfläche parallel zu einer Schar von Netzebenen. Ebenso verläuft eine Kristallkante parallel zu einer Schar von Gittergeraden.

Diese morphologisch-strukturelle Korrespondenz gilt auch für alle anderen Eigenschaften des Kristalls und ist als Neumann’sches Prinzip von Woldemar Voigt 1910 formuliert worden. Die Symmetrie einer Eigenschaft ist höher oder gleich der Symmetrie der Kristallstruktur.





Potentielle Energie

Die potentielle Energie (auch potenzielle oder Lageenergie genannt) beschreibt die Energie eines physikalischen Systems, die durch seine Lage in einem Kraftfeld oder durch seine aktuelle System-Konfiguration bestimmt wird.

Beispielsweise ist in einem Schwerefeld die „potentielle Energie“ jene Energie, die ein Körper durch seine Höhenlage hat: Wenn ein Stein aus 20 Meter Höhe herabfällt, hat er doppelt so viel Arbeitsfähigkeit wie bei 10 Metern Fallhöhe. Während des Falls wird die potentielle Energie in kinetische Energie oder andere Energieformen umgewandelt und verringert sich. In Wasserkraftwerken kann man potentielle Energie des Wassers eines Stausees in elektrische Energie umwandeln.

Ebenso wie andere Formen der Energie ist die potentielle Energie eine Zustandsgröße eines physikalischen Systems. In einem abgeschlossenen System kann die potentielle Energie bei Zustandsänderungen zwar zu- oder abnehmen, etwa bei Verschiebung des Körpers, bei seiner Höhenänderung oder bei Anregung eines Atoms durch Strahlung. Dann nimmt aber stets eine andere Energieform (z. B. kinetische Energie, elektrische Feldenergie) im selben Maß ab oder zu. Diese Erfahrungstatsache wird durch den Energieerhaltungssatz ausgedrückt.

Die SI-Einheit der potentiellen Energie ist das Joule (Einheitenzeichen J). Als Formelzeichen für die potentielle Energie wird "E" oder "U" verwendet, in der theoretischen Physik ist "V" verbreitet. Oft wird auch ungenau vom "Potential" gesprochen, wenn die potentielle Energie gemeint ist.

Zur Einführung betrachten wir einen Radfahrer, der eine ebene Strecke befährt, dann einen Berg hoch und als letztes hinunter fährt. Die Betrachtung soll zunächst ohne Reibungskräfte erfolgen.

Auf ebener Strecke fährt der Radfahrer mit einer bestimmten Geschwindigkeit, was einer bestimmten kinetischen Energie entspricht. Fährt er den Berg hinauf, so muss er mehr Energie aufwenden, um die Geschwindigkeit (und damit kinetische Energie) aufrechtzuerhalten. Aufgrund der Energieerhaltung kann aber keine Energie verloren gehen und die Energie, die der Radfahrer beim Anstieg mehr aufwendet, muss irgendwo hin fließen: Die mehr aufgewendete Energie wird in "potentielle Energie" umgewandelt. Je höher er steigt, desto mehr potentielle Energie besitzt der Radfahrer. Beim Abstieg muss der Radfahrer dagegen bremsen, um seine Geschwindigkeit zu halten und damit seine kinetische Energie konstant zu halten. Bremst er nicht, so wird er schneller und besitzt immer mehr kinetische Energie. Der Zuwachs seiner kinetischen Energie kann aber aufgrund der Energieerhaltung nicht ohne Verlust einer anderen Energieform einhergehen. Der Zuwachs der kinetischen Energie ist gleich dem Verlust an potentieller Energie.

Der Radfahrer erreicht auf ebener Strecke ohne viel Mühe 20 km/h, da er nur gegen den Luftwiderstand und die Rollreibung antreten muss. Kommt er nun an einen ansteigenden Streckenabschnitt, muss er sich bei gleicher Geschwindigkeit stärker anstrengen als zuvor. Nach Erreichen der Kuppe geht es bergab und der Radfahrer rollt ohne Tretbewegungen weiter, muss sogar bremsen, damit er nicht zu schnell wird.

Auf den Fahrer samt Rad wirken zwei Kräfte: die Reibungskraft und die Gewichtskraft. Im ersten Streckenabschnitt zeigt die Gewichtskraft senkrecht zur Straße und weist somit nach Anwendung der Kräftezerlegung keine Kraftkomponente in Bewegungsrichtung auf. Kommt nun ein Anstieg, ergibt die Zerlegung der Gewichtskraft eine Kraftkomponente entgegen der Bewegungsrichtung. Nach Überschreiten der Kuppe hat die Schwerkraft eine Komponente in Bewegungsrichtung und entgegen der Reibungskraft.

Für eine Bewegung entgegen der Gewichtskraft muss am Körper Arbeit aufgewendet werden, die nun als potentielle Energie in ihm gespeichert ist. Bei einer Bewegung, die eine Komponente in Richtung der Gewichtskraft enthält, leistet der Körper Arbeit und seine potentielle Energie nimmt ab. Die Wegkomponente in Richtung Gewichtskraft heißt Höhe und zusammen mit der Kraft ergibt sich:

Im Allgemeinen ist die Gravitationsfeldstärke und damit die Gewichtskraft ortsabhängig. Damit gilt:

Das negative Vorzeichen ergibt sich dadurch, dass man etwas entgegen der wirkenden Kraft bewegen muss, um die potentielle Energie zu erhöhen. formula_7 beschreibt dabei den Bezugspunkt, an dem die potentielle Energie eines Teilchens verschwinden soll. Er ersetzt die ansonsten bei der Integration auftretende Integrationskonstante. Meist wählt man hierfür die Erdoberfläche (siehe erstes Beispiel) oder das Unendliche (siehe zweites Beispiel). Die obere Integralgrenze formula_8 entspricht der Position des betrachteten Teilchens, nicht zu verwechseln mit der Integrationsvariablen formula_9.

Setzt man formula_10 konstant (was auf der Erdoberfläche für kleine Höhenunterschiede näherungsweise gilt), so ergibt sich wieder die im vorigen Abschnitt beschriebene Gleichung:

Betrachtet man ein System aus einem Planeten und einem Probeteilchen fernab der Planetenoberfläche, reicht obige Näherung nicht mehr aus; die lokale Gravitationsfeldstärke variiert mit dem Abstand vom Massenmittelpunkt des Planeten. Eine genauere Beschreibung ist mithilfe des Newtonschen Gravitationsgesetzes möglich,
Bei dieser Art der Betrachtung wählt man häufig den Bezugspunkt im unendlich fernen, d. h. formula_13. Durch diese Wahl kann die potentielle Energie nur negative Werte annehmen. Die potentielle Energie des Teilchens auf der Planetenoberfläche entspricht dann der Arbeit, die verrichtet werden muss, um dieses Teilchen ins Unendliche zu transportieren, es also aus dem Gravitationsfeld zu entfernen. Auf der Planetenoberfläche ist die potentielle Energie des Teilchens minimal, im Unendlichen maximal. Mit der Vereinbarung, dass der Ursprung im Planetenmittelpunkt liegt, der Planet einen Radius formula_14 hat und formula_15, erhält man
als potentielle Energie des Probeteilchens auf der Planetenoberfläche. Im letzten Schritt wurde die neue planetenabhängige Konstante
definiert. Hierbei ist formula_18 die Masse des Planeten, formula_2 die Masse des Probeteilchens und formula_20 die Gravitationskonstante.

Aus der Federkraft

ergibt sich für die potentielle Energie

Hierbei ist formula_23 die Federkonstante und formula_24 die Auslenkung der Feder aus der Ruhelage.

In einem abgeschlossenen System ohne Energieaustausch mit der Umgebung und unter Vernachlässigung jedweder Reibung gilt zu jedem Zeitpunkt der Energieerhaltungssatz der klassischen Mechanik:


In Worten: Die Summe aus potentieller und kinetischer Energie, einschließlich der Rotationsenergie, ist konstant und entspricht der Gesamtenergie des mechanischen Systems.

Im Hamilton-Formalismus wird diese Gleichung als

geschrieben, wobei formula_30 die Hamiltonfunktion und formula_31 die Lagrangefunktion ist.

Da ein konservatives Kraftfeld die Kraft formula_32 auf einen Probekörper an einem beliebigen Ort definiert und mathematisch ein Gradientenfeld ist, existiert ein zum Kraftfeld äquivalentes skalares Feld formula_33. Dies ist die potentielle Energie für den jeweiligen Ort. Aus der Umkehrung des Arbeitsintegrals folgt, dass ein Energieanstieg entlang eines Weges eine Kraftkomponente in entgegengesetzter Richtung des Weges voraussetzt. Durch Zerlegung des Kraftfeldes in kartesische Komponenten ergeben sich in Abhängigkeit vom Ort folgende partielle Ableitungen:

Allgemein lässt sich dies durch den Nabla-Operator formula_35 ausdrücken.

Die Umkehrung der Ableitung führt zum Integral und ermittelt die Änderung der potentiellen Energie im Kraftfeld als Arbeitsintegrals mit negativem Vorzeichen. Hieran zeigt sich auch nachvollziehbar die Übertragbarkeit auf verschiedene Kraftfelder.

Um die potentielle Energie eines Körpers zu vergrößern, muss Feldarbeit gegen die Kräfte eines konservativen Kraftfeldes verrichtet werden. So besitzt jeder massebehaftete Körper in einem Gravitationsfeld potentielle Energie. Diese kann jedoch nur erhöht oder vermindert werden, wenn der Körper gegen oder in Richtung der Gravitationskraft verschoben wird. Bei einer Verschiebung senkrecht zu den Feldlinien behält der Körper seine potentielle Energie bei. Ein solcher Bereich nennt sich Äquipotentialfläche oder -linie und entspricht einer Höhenlinie auf der Landkarte. Die Feldlinie dagegen beschreibt die Richtung der Steigung.
Sofern keine Reibungsverluste oder sonstige Wechselwirkungen mit der Umgebung auftreten gilt für eine Verschiebung in konservativen Kraftfeldern das Prinzip der Wegunabhängigkeit. Das bedeutet unabhängig vom eingeschlagenen Weg muss gleich viel Feldarbeit verrichtet werden damit ein Körper vom Ausgangspunkt zum Zielpunkt gelangt. Hierin spielt sich der Energieerhaltungssatz wieder, da die Arbeit der Energieänderung entspricht.

Die Wahl des Bezugsniveaus kann beliebig erfolgen, jedoch reduzieren pragmatischen Gründe die Auswahl. Im Zweifelsfall immer als Nullniveau geeignet ist der Ausgangspunkt des untersuchten Körpers. Beim Gravitationsfeld bildet häufig die Erdoberfläche den Bezugspunkt oder allgemein der niedrigste Punkt der Umgebung. Darüber hinaus kann der Bezugspunkt an einen unendlich weit entfernten Ort verlegt werden (formula_38). Die Umkehrung davon bildet die maximale potentielle Energie, bei der ein Körper von seinem Ausgangspunkt aus dem Kraftfeld heraus bewegt wird, wobei ein zentrales Kraftfeld angenommen sei.

Bei elektrischen Ladungen gleichen Vorzeichens führt dies zur minimalen potentiellen Energie.

Die Kraft auf eine Ladung in einem gegebenen elektrischen Feld errechnet sich aus:

Durch Einsetzen in das Arbeitsintegral zeigt sich die Beziehung zwischen der potentiellen Energie einer Ladung und dem Coulombpotential, das ebenfalls ein Skalarfeld darstellt. Beide Felder unterscheidet nur der Proportionalitätsfaktor Ladung:

formula_42 ist dabei das sogenannte Coulombpotential.

Der Begriff der potentiellen Energie hängt eng mit dem Begriff des Potentials zusammen, welches eine äquivalente Darstellung eines konservativen Kraftfeldes darstellt. Die potentielle Energie formula_43 eines physikalischen Systems ist das Produkt aus Kopplungskonstante formula_23 des Teilchens bezüglich des Kraftfeldes formula_45, dem es ausgesetzt ist (z. B. Masse formula_2 im Falle des Gravitationsfeld, Ladung formula_47 im Falle des Elektrischen Felds), und dem Potential formula_48 des Kraftfeldes:

Das Potential hängt über die Definition formula_50 mit dem Kraftfeld zusammen.
Aufgrund dieser Definition ist die potentielle Energie nur für Teilchen in konservativen Kraftfeldern definiert und der Nullpunkt der Energieskala beliebig festlegbar.

Die Kraft auf einen Probekörper der Masse formula_2 in einem gegebenen Gravitationsfeld errechnet sich aus:

Durch Einsetzen in das Arbeitsintegral zeigt sich nun die Beziehung zwischen der potentiellen Energie einer Masse und dem Gravitationspotential formula_53, das ebenfalls ein Skalarfeld darstellt.

Anschaulich beschreibt der Faktor formula_2 die Abhängigkeit von dem Probekörper und das Potential formula_56 die Feldeigenschaft.





Spinpolarisation

In einer Ansammlung gleichartiger Teilchen wie Elektronen, Atome oder Ionen besteht Spinpolarisation, wenn die Spinvektoren der Teilchen mehr oder weniger ausgerichtet sind, die Richtungen also nicht zufällig verteilt sind. In der Fachsprache z. B. der Kernphysik wird dann meist einfach von "Polarisation" gesprochen.

Der Axialvektor eines durch die Quantenzahl formula_1 beschriebenen Spins kann gegenüber einer gewählten Quantisierungsachse formula_2 verschiedene Richtungen einnehmen (siehe Richtungsquantelung, Multiplizität). Diese werden durch eine "spinmagnetische" Quantenzahl formula_3 bezeichnet:

Im einfachsten Fall formula_5 ergeben sich die beiden Werte formula_6 und formula_7 (Multiplizität 2).

Zustände, die sich nur im Wert von formula_3 unterscheiden, sind zwar quantenmechanisch verschieden. Sie haben aber normalerweise gleiche Energie, sind also „entartet“. In einem Ensemble gleichartiger Teilchen sind diese Zustände daher im Allgemeinen bis auf zufällige statistische Schwankungen gleich stark besetzt (eine Ausnahme bilden die Elektronen und Positronen der Betastrahlung, siehe unten).

Eine Polarisation, also Abweichung von der Gleichverteilung, lässt sich bei Spin-1/2-Teilchen beschreiben durch den "Polarisationsgrad formula_9":

Dabei sind formula_11 und formula_12 die Anzahlen der Teilchen mit den beiden Spinausrichtungen („up“ und „down“) zur gewählten Achse. Auch der Polarisationsgrad wird oft kurz als „die Polarisation“ bezeichnet. formula_9 beträgt für ein unpolarisiertes Ensemble 0, für ein maximal polarisiertes ±1, häufig als ±100 % ausgedrückt. Auch die Beschreibung durch einen "Polarisationsvektor" ist möglich; dieser ist die Vektorsumme aller Spins im Ensemble geteilt durch die Teilchenanzahl und wird meist ebenfalls auf den Betrag 1 für maximale Polarisation normiert. Bei Teilchen mit höherem Spin als 1/2, also drei oder mehr möglichen Ausrichtungen, ist die Beschreibung der Polarisation komplizierter und erfordert im Allgemeinen einen Tensor entsprechender Stufe.

Spinpolarisation ist also keine Eigenschaft eines einzelnen Teilchens, sondern des Ensembles. Quantenmechanisch lässt sie sich mit dem Dichtematrix-Formalismus beschreiben.

Der Spin von Teilchen ist mit einem magnetischen Moment verbunden. Bringt man das Teilchenensemble in ein Magnetfeld, ändert sich daher die Energie des einzelnen Zustands je nach Stellung zur Feldrichtung, die Entartung wird aufgehoben. Daher rührt die Bezeichnung "magnetische" Quantenzahl. Die entsprechende beobachtbare Aufspaltung optischer Spektrallinien heißt Zeeman-Effekt. 

Da sich die Teilchen bevorzugt in Zuständen kleinerer Energie sammeln, führt das Magnetfeld schon ohne weitere Maßnahmen zu einer gewissen Spinpolarisation. Diese ist allerdings bei Umgebungstemperatur meist gering, weil die magnetischen Energieunterschiede klein sind gegenüber der thermischen Energie der Teilchen (dies gilt insbesondere für Atomkerne mit ihren kleinen magnetischen Momenten). Mit speziellen Verfahren lassen sich weitaus höhere Polarisationen erreichen. Dies wird in manchen, aber nicht allen Fällen als Hyperpolarisation bezeichnet.

Wird ein zunächst auf gerader Bahn fliegendes Teilchen mit Spin aus seiner Flugrichtung abgelenkt, beeinflusst die Wechselwirkung zwischen Spin und Bahndrehimpuls die Bewegung, ähnlich wie in Atomen und Atomkernen (siehe Spin-Bahn-Kopplung). Zeigt beispielsweise der Spinvektor in die formula_14-Richtung, während das Teilchen in formula_15-Richtung fliegt, stehen die Vektoren von Spin und Bahndrehimpuls bei Ablenkung (Streuung) in die formula_16-Richtung antiparallel, in die -formula_16-Richtung parallel zueinander (siehe Skizze). Der differentielle Wirkungsquerschnitt ist dadurch bei gleichem Streuwinkel verschieden, je nachdem die Streuung zur +formula_16-Seite oder zur hin erfolgt. Allgemeiner gesagt: er hängt außer vom Streuwinkel auch vom Azimutwinkel (siehe Kugelkoordinaten), dem Winkel zwischen der Bahnebene und der "xz"-Ebene, ab. Für einen polarisierten Teilchenstrahl stellt der Streuprozess auf diese Weise einen Analysator dar, denn zwei symmetrisch zueinander links und rechts der formula_19-Ebene aufgestellte Detektoren registrieren verschieden viele Teilchen. Andererseits sind bei unpolarisiertem Strahl die Teilchen, die nach einer bestimmten Seite gestreut werden, ein mehr oder weniger stark polarisiertes Ensemble; der Streuprozess wirkt also auch als Polarisator. 

Wegen der Drehimpulserhaltung zeigt sich auch bei Kernreaktionen entsprechendes Verhalten wie bei Streuung. Streu- und Reaktionsexperimente mit Beobachtung der Polarisation der emittierten Teilchen oder mit polarisiertem Strahl oder Target sind daher in der Kernphysik ein wichtiges Mittel zur näheren Bestimmung der Spin-Bahn-Wechselwirkung. Bevor man polarisierte Teilchenstrahlen oder polarisierte Targets herstellen konnte, lieferten "Doppelstreuexperimente", bei denen dieselben Teilchen zwei Streuungen nacheinander durchliefen, Informationen dazu. Bei ihnen stellte die erste Streuung den Polarisator, die zweite den Analysator dar.

Mit einem polarisierten 1,16-GeV-Elektronenstrahl ist in einem Streuexperiment die schwache Ladung des Protons genau gemessen worden. Dabei wurde ausgenutzt, dass nur in der schwachen Wechselwirkung die Nichterhaltung der Parität gilt.

In Feststoffen, Flüssigkeiten oder Gasen wird Polarisation der Atomkerne mittels eines Magnetfelds erzeugt, oft mit Hilfe tiefer Temperatur, um die thermische Energie der Teilchen klein zu halten (siehe Boltzmann-Verteilung). Mit dieser Technik wurde z. B. im Wu-Experiment bei 10 Millikelvin ein Polarisationsgrad der Cobalt-60-Kerne von ca. 60 % erreicht. 

Statt eines starken äußeren Feldes kann zur Polarisation der Kerne in manchen Fällen das in einem paramagnetischen Ion vom Elektronenspin verursachte Feld ausgenutzt werden, so dass ein relatives schwaches äußeres Feld genügt, das die Ionen ausrichtet.

Eine weitere Methode besteht darin, Atome durch optisches Pumpen mit zirkular polarisiertem Licht auszurichten und die Kopplung der Elektronen mit dem Kernmoment (siehe Hyperfeinstruktur) auszunutzen.

Polarisierte Ionenstrahlen zur Verwendung in Teilchenbeschleunigern lassen sich nach dem weiterentwickelten Konzept des Stern-Gerlach-Experiments herstellen: aus einem Atomstrahl, z. B. Wasserstoff oder Deuterium, wird im inhomogenen Magnetfeld ein polarisierter Teilstrahl gewonnen und dieser dann – im einfachsten Fall – in einem schwachen Magnetfeld unter Ausnützung der Hyperfeinaufspaltung ionisiert.

Ein anderer Typ „polarisierter Ionenquellen“ nutzt die Aufspaltung der Energieniveaus durch die Lamb-Verschiebung aus.

Elektronen in Speicherringen werden durch Emission von Synchrotronstrahlung longitudinal polarisiert.

Polarisierte langsame Neutronen für die Neutronenstreuung werden durch Reflexion an den ausgerichteten Atomen eines ferromagnetischen Spiegels (siehe Neutronensuperspiegel) gewonnen. 


Die beim Betazerfall emittierten Teilchen sind entlang ihrer Emissionsrichtung spinpolarisiert. Anschaulich gesagt rotieren z. B. die Elektronen aus Beta-Minus-Zerfällen, in ihrer Flugrichtung gesehen, vorzugsweise gegen den Uhrzeigersinn ("linkshändige Elektronen"). Erklärt wird dies damit, dass die für den Betazerfall verantwortliche schwache Wechselwirkung nur chiral linkshändige Teilchen und chiral rechtshändige Antiteilchen erzeugt, insofern also die Spiegelsymmetrie der Naturgesetze maximal verletzt (siehe Paritätsverletzung). Dies wirkt sich als longitudinale Spinpolarisation der emittierten Teilchen aus. Theorie und Messungen ergeben, dass der Polarisationsgrad formula_20 beträgt (formula_21 Teilchengeschwindigkeit, formula_22 Lichtgeschwindigkeit), für relativistische Beta-Elektronen und für Neutrinos aus dem Betazerfall also praktisch 100 %.





Edward Mills Purcell

Edward Mills Purcell (* 30. August 1912 in Taylorville, Illinois; † 7. März 1997 in Cambridge, Massachusetts) war ein amerikanischer Physiker und Nobelpreisträger.

Purcell studierte an der Purdue University (Bachelor-Abschluss in Elektrotechnik 1933) und der Harvard University, wo er 1935 seinen Master-Abschluss machte und 1938 promovierte. Danach war er von 1941 bis 1946 am Radiation Laboratory des Massachusetts Institute of Technology und ab 1938 zunächst Instructor in Harvard und von 1949 ab Professor für Physik (und 1950 bis 1971 Senior Fellow der Universität). Ab 1980 war er dort Professor Emeritus.

Purcell entdeckte 1945 die Kernspinresonanz (NMR), und unabhängig von ihm etwa gleichzeitig Felix Bloch. Diese Entdeckung bildete die Grundlage für die Entwicklung der Kernspinresonanzspektroskopie (NMR-Spektroskopie), mit deren Hilfe z. B. die Struktur von Molekülen untersucht werden kann und die weit reichende Anwendung in den Naturwissenschaften und der Medizin (z. B. Kernspintomographie) findet. Bekannt ist Purcell in diesem Zusammenhang für die von ihm zusammen mit Herman Y. Carr (1924–2008) entwickelte sogenannte Carr-Purcell-Pulssequenz. 1950 wurde Purcell in die American Academy of Arts and Sciences gewählt, 1951 in die National Academy of Sciences und 1954 in die American Philosophical Society.

1951 entdeckte er zusammen mit Harold Irving Ewen und Gart Westerhout die Emission der Frequenz von 1,42 Gigahertz (der 21-Zentimeter-Linie) des atomaren Wasserstoffs der Milchstraße, die durch die Änderung der Orientierung des Elektronenspins relativ zum Kernspin hervorgerufen wird. Sie war 1944 von Hendrik Christoffel van de Hulst theoretisch vorhergesagt worden.

Die 21-Zentimeter-Linie ist im Labor praktisch nicht nachzuweisen, wird aber im Weltraum durch hoch verdünntes interstellares Gas in genügender Stärke ausgesendet und ist mit empfindlichen Antennen auf der Erde nachweisbar (→Radioastronomie). Purcell und Norman Ramsey waren die ersten, die die (mittlerweile widerlegte) CP-Symmetrie in Frage stellten.

1951 schlug er mit Robert Pound ein Experiment zur Erzeugung negativer Temperaturen (über eine Besetzungsinversion) vor. Dieser Vorschlag beeinflusste die Ideen von Charles Townes zur Entwicklung des Masers.

Er erhielt 1952 zusammen mit Felix Bloch für seine Arbeiten zur NMR den Nobelpreis für Physik. 1967 erhielt er die Oersted Medal. 1984 erhielt er den Max Delbruck Prize in Biophysik.






Leon Max Lederman

Leon Max Lederman (* 15. Juli 1922 in New York City; † 3. Oktober 2018 in Rexburg, Idaho) war ein US-amerikanischer Physiker und Nobelpreisträger für Physik.

Leon M. Lederman studierte Chemie am City College of New York (Bachelor 1943) und an der Columbia University, wo er nach dem Wehrdienst 1943 bis 1946 als Offizier im US Army Signal Corps 1948 seinen Masterabschluss machte und 1951 promovierte. Ab 1952 war er dort "Assistant Professor" und ab 1958 Professor für Physik. 1972 bis 1979 war er "Eugene Higgins Professor of Physics" an der Columbia University. 1961 bis 1979 war er dort Direktor der Nevis Laboratories. 1979 wurde er zum Direktor des "Fermilab" ernannt und leitete es bis 1989. Danach war er "Frank L. Sulzberger Professor of Physics" an der University of Chicago und ab 1992 "Pritzker Professor of Physics" am Illinois Institute of Technology.

Lederman erhielt 1988 zusammen mit Melvin Schwartz und Jack Steinberger den Nobelpreis für Physik „für die Neutrinostrahlmethode und den Nachweis der Doublettstruktur der Leptonen durch die Entdeckung des Myon-Neutrinos“. Innerhalb dieser experimentellen Untersuchungen gelang es ihnen zu zeigen, dass es unterschiedliche Arten von Neutrinos gibt. Neben dem bereits bekannten Elektron-Neutrino entdeckten sie 1962 im amerikanischen Brookhaven National Laboratory das Myon-Neutrino und bestätigten damit ein grundlegendes Postulat der Leptonen-Theorie.

Mit der Entdeckung des Bottom-Quark 1977 am Fermi National Accelerator Laboratory in Batavia, Illinois setzte Lederman einen weiteren Meilenstein in der Physik der Elementarteilchen.

1957 war er mit Richard Garwin und Marcel Weinrich an einem der grundlegenden Experimente zur Entdeckung der Paritätsverletzung in der schwachen Wechselwirkung beteiligt.

1958 absolvierte Lederman sein erstes Sabbatical am CERN (der Europäischen Organisation für Kernforschung), wo er ein Team zur Durchführung des g-2-Experiments zusammenstellte. Dieses CERN-Programm sollte etwa 19 Jahre dauern und viele CERN-Physiker (Picasso, Farley, Charpak, Johannes Sens, Zichichi usw.) einbeziehen. Es leitete auch einige Teamarbeiten in der CERN-Forschung ein, die bis in die Mitte der 1970er Jahre andauerten.

1992 war er Präsident der American Association for the Advancement of Science. Seit 1965 war er Mitglied der National Academy of Sciences, seit 1970 der American Academy of Arts and Sciences und seit 1989 der American Philosophical Society. 2003 wurde er als auswärtiges Mitglied in die Russische Akademie der Wissenschaften aufgenommen.

Mit seinem Buch "The God particle. If the universe is the answer, what is the question?" prägte er den Begriff des „Gottesteilchens“ für das Higgs-Boson. Ursprünglich verwendete er dafür den Ausdruck "goddamn particle," also „gottverdammtes Teilchen“, weil sich dieses bisher nicht nachweisen ließ und den Physikern Kopfzerbrechen bereitete. Erst in der Redaktion des Buches wurde der Begriff auf "God particle" verkürzt und führte so zu der missverständlichen Annahme, dass diese Bezeichnung darauf anspiele, dass ein Nachweis des Higgs-Bosons einem Beweis für die Existenz des Higgs-Feldes gleichkommt, das eine theoretische Erklärung für die Massen der Elementarteilchen liefern kann.

Ab 2011 setzte bei ihm Demenz ein und er zog mit seiner Frau Ellen nach Driggs, Idaho. 2015 versteigerte er seine Nobelpreis-Medaille für 765.000 US-Dollar, um Behandlungs- und Pflegekosten bezahlen zu können. Am 3. Oktober 2018 starb Lederman in einem Pflegeheim in Rexburg, Idaho.







Gustav Robert Kirchhoff

Gustav Robert Kirchhoff (* 12. März 1824 in Königsberg (Preußen); † 17. Oktober 1887 in Berlin) war ein deutscher Physiker, der sich insbesondere um die Erforschung der Elektrizität verdient gemacht hat.

Seine Eltern waren der Justizrat und Landrichter in Königsberg "Carl Friedrich Kirchhoff" und dessen Ehefrau "Johanne Henriette Wittke". Sein Bruder Carl († 1893) war Reichsgerichtsrat.

Gustav Robert Kirchhoff studierte von 1842 bis 1847 Mathematik und Physik an der Universität Königsberg unter anderem bei Franz Neumann und Friedrich Julius Richelot. Von 1850 bis 1854 war er an der Universität Breslau tätig, wechselte dann an die Universität Heidelberg (wo er 1865/66 Prorektor war) und kam 1875 als Professor für theoretische Physik an die Universität Berlin. Diese Stelle hatte er bis 1886 inne. 1864 wurde er in die American Philosophical Society und 1870 in die American Academy of Arts and Sciences gewählt. Ab 1861 war er Mitglied der Preußischen Akademie der Wissenschaften. 1862 wurde er als auswärtiges Mitglied in die Göttinger Akademie der Wissenschaften und als korrespondierendes Mitglied in die Russische Akademie der Wissenschaften in Sankt Petersburg aufgenommen. Im Jahr 1876 erhielt er die Cothenius-Medaille der Leopoldina, 1883 wurde er Mitglied der National Academy of Sciences.

1857 heiratete er Clara Richelot († 1869), eine Tochter des Königsberger Mathematikers Friedrich Julius Richelot. Mit ihr hatte Kirchhoff zwei Söhne und zwei Töchter. Die aus dieser Ehe stammende Tochter Paula war mit dem Geologen Wilhelm von Branca verheiratet. 

Nach dem Tod seiner ersten Frau heiratete er 1872 Luise Brömmel, die an der Heidelberger Augenklinik beschäftigt war. Kirchhoffs Grab befindet sich auf dem Alten St.-Matthäus-Kirchhof in Berlin-Schöneberg. Es ist seit 1956 als Ehrengrab der Stadt Berlin gewidmet.

Kirchhoff ist bekannt für seine Regeln der elektrischen Stromkreise zur Beschreibung der Abhängigkeit von elektrischer Spannung, elektrischem Strom und elektrischem Widerstand, die er 1845 fand. Diese sogenannten Kirchhoffsche Regeln sind fundamental für Aufbau und Analyse elektrischer Schaltungen sowie die Elektrotechnik allgemein.

Kirchhoff entdeckte 1861 zusammen mit Robert Wilhelm Bunsen bei der Spektralanalyse des Mineralwassers der neu erschlossenen Maxquelle in Dürkheim die Elemente Caesium und Rubidium. Durch ihre Studien wurde es zudem möglich, die Fraunhoferlinie zu erklären und somit eine der wesentlichen Grundlagen der modernen Astronomie zu schaffen.

Das Kirchhoffsche Strahlungsgesetz besagt: Materie gleich welcher Art sendet bei Erhitzung eine kontinuierliche Strahlung aus, die je nach der Temperatur unsichtbar oder sichtbar ist. Diese Strahlung nennt man Temperatur- oder Wärmestrahlung. An eine ausgedehnte experimentelle Untersuchung dieses Gesetzes war zunächst nicht zu denken, da die Mittel für die Messung hoher Temperaturen und kleiner Strahlungsenergie fehlten. Die weitreichende Bedeutung wurde jedoch sofort erkannt. Das daraus entwickelte Konzept des Schwarzen Körpers führte schließlich zur Quantenphysik.

Kirchhoff beschäftigte sich auch mit der Plattentheorie; der Piola-Kirchhoff-Spannungstensor, die Kirchhoff-Love-Hypothese und die sogenannten Kirchhoff-Platten erinnern daran.

Kirchhoff bildete keine Schule, aber er brachte durch sein Vorbild manchen Physiker auf den Weg, so den ungarischen Physiker und Geophysiker Loránd Eötvös.

Der Mondkrater Kirchhoff und der Asteroid (10358) Kirchhoff sind nach ihm benannt, ebenso das „Kirchhoff-Institut für Physik“ (KIP) der Universität Heidelberg. Am 15. Februar 1974 gab die Deutsche Bundespost Berlin anlässlich seines 150. Geburtstages eine Sonderbriefmarke (MiNr. 465) heraus. In Berlin-Adlershof, Bad Dürkheim und in Heidelberg trägt eine Straße seinen Namen.







Wendelstein 7-AS

Wendelstein 7-AS war ein Experiment zum Einschluss eines heißen Plasmas mit dem Ziel der Entwicklung eines Kernfusionsreaktors zur Energiegewinnung. Die Anlage basiert auf dem Prinzip des Stellarators, in dem Plasmen von Magnetfeldern eingeschlossen sind, die ausschließlich von stromdurchflossenen Spulen außerhalb erzeugt werden. Im Gegensatz dazu wird beim Tokamak-Prinzip die benötigte Verdrillung der magnetischen Feldlinien durch einen Strom erzeugt, der im Plasma selbst fließt.

Wendelstein 7-AS (für „Advanced Stellarator“) war weltweit der erste einer neuen Klasse von fortgeschrittenen Stellaratoren mit modularen Spulen und wurde von 1988 bis 2002 in Garching vom Max-Planck-Institut für Plasmaphysik betrieben.

Das seit 2002 in Greifswald im Aufbau befindliche Nachfolgeexperiment Wendelstein 7-X wurde 2014 fertiggestellt und nahm im Dezember 2015 den Betrieb auf. Hierbei soll mit Plasmen im Langzeitbetrieb die Reaktortauglichkeit der gewählten Anordnung untersucht werden.

Stellaratoren erzeugen das zum Einschluss eines heißen Wasserstoffplasmas notwendige Magnetfeld ausschließlich über stromdurchflossene Spulen außerhalb des Plasmas. Da dort der Strom kontinuierlich fließen kann, sind Stellaratoren Kandidaten für einen im Dauerbetrieb arbeitenden späteren Fusionsreaktor. Beim dazu alternativen Tokamak-Konzept wird dagegen ein Teil des benötigten Magnetfeldes durch einen im Plasma selbst fließenden Strom erzeugt. Dabei gelingt es derzeit noch nicht, diesen Strom mit vertretbarem Aufwand kontinuierlich aufrechtzuerhalten.

Wendelstein 7-AS war der erste Stellarator mit modularen – d. h. entlang des Plasmarings aufgestellten getrennten – Spulen, die die benötigten Magnetfelder durch ihre verwundene Formung erreichen. Die große Zahl dadurch möglich gewordener Freiheitsgrade bei der Gestaltung des Magnetfeldes wurden genutzt, um das erzeugte Magnetfeld an das theoretische Optimum anzunähern. Um möglichst schnell die grundlegende Richtigkeit des Konzepts zu testen und auch wegen der bis 1980 nur begrenzt zur Verfügung stehenden Computerkapazität wurde bei Wendelstein 7-AS zunächst nur eine Teil-Optimierung durchgeführt. Erst im nächsten Schritt, dem voll optimierten Stellarator Wendelstein 7-X, welcher 2015 in Greifswald in Betrieb gegangen ist, soll die Reaktortauglichkeit dieses Konzept überprüft werden.

Die experimentellen Ergebnisse des Wendelstein 7-AS bestätigten die Vorhersagen hinsichtlich der teilweisen Optimierung und führten damit zum Bau des Wendelstein 7-X als dem nächsten Entwicklungsschritt:






Lock-in-Effekt (Physik)

Unter Lock-in-Effekt versteht man die gegenseitige Beeinflussung schwach gekoppelter Oszillatoren.

Der Lock-in-Effekt wurde wahrscheinlich als erstes von Christiaan Huygens beobachtet. Er berichtet in seinem 1673 erschienenen Buch "Horologium Oscillatorium" von einem merkwürdigen Phänomen, das er bei Pendeluhren beobachtete.

Auf Schiffen verwendete man zur Navigation kardanisch aufgehängte Pendeluhren. Für den Fall, dass eine Uhr ausfallen sollte, führte man meist zwei gleiche Uhren mit sich. Huygens beobachtete nun, dass zwei Uhren auf einem solchen Schiff nicht nur im Rahmen ihrer Präzision ungefähr gleich gingen, sondern exakt gleich. Sie tickten immer im gleichen Augenblick. Selbst wenn man die Übereinstimmung absichtlich störte, synchronisierten sich die Pendel in kurzer Zeit wieder, in diesem Falle bewegten sich die Pendel im Gegentakt (gegenphasig, spiegelsymmetrisch). 

Dieses Phänomen trat auch nur dann auf, wenn die Uhren beide präzise eingestellt waren. Lief eine Uhr deutlich schneller als die andere, kamen sie nicht zur Übereinstimmung.

Huygens' Deutung war, dass über den Balken, an dem beide Uhren hingen, eine Schwingung übertragen wurde. Die Bewegung des Balkens war jedoch so gering, dass sie nicht sichtbar war.

Dieser Effekt wurde auch in Sternwarten an deren hochpräzisen Pendeluhren beobachtet, wenn diese an der gleichen Wand aufgehängt waren. Nach Verdrehung der senkrechten Achse einer Uhr um 90 Grad trat diese Spontansynchronisation nicht mehr auf.

Der Lock-in-Effekt führt bei einer Frequenzdifferenz ungekoppelter Oszillatoren unterhalb der "Lock-in-Schwelle" dazu, dass bei einer Koppelung beide mit der gleichen Frequenz schwingen. Die Lock-in-Schwelle hängt hierbei von der Stärke der Kopplung ab. Ist die Frequenzdifferenz größer als die Lock-in-Schwelle, führt die Koppelung der Oszillatoren zu einer Reduzierung der Differenzfrequenz.

Dieser Effekt tritt nicht nur bei mechanischen, sondern auch bei allen anderen Schwingungen auf. Auch elektrische Schwingkreise und Laserresonatoren zeigen dieses Phänomen. Insbesondere beim Laserkreisel wird die Genauigkeit im Wesentlichen durch den Lock-in-Effekt beschränkt.

Die Frequenzdifferenz schwach gekoppelter Oszillatoren beträgt

wobei formula_2 für die Frequenzdifferenz ohne Kopplung und formula_3 für die Lock-in-Schwelle steht.





Neutronenstrahlung

Neutronenstrahlung ist eine ionisierende Strahlung, die aus freien Neutronen (mit u. U. verschiedenen kinetischen Energien) besteht.

Da Neutronen elektrisch neutral sind, haben die Ladungen von Atomkernen und Elektronen auf ihre Bewegung keinen Einfluss. Neutronenstrahlung durchdringt Materie deshalb relativ leicht. Der ionisierende Effekt entsteht indirekt, meist durch Anstoßen leichter Atomkerne bzw. deren Bestandteile (z. B. Protonen), die dann ihrerseits ionisierend wirken. Durch derartige Stöße werden die Neutronen energieärmer (langsamer).

Die Hauptwirkung von langsamen, vor allem thermischen Neutronen beruht auf ihrer Fähigkeit, sich an Atomkerne anzulagern (Neutroneneinfang). Dabei bildet sich ein Isotop des einfangenden Atoms mit einer um 1 erhöhten Massenzahl. Viele dieser so entstandenen Isotope sind radioaktiv, so dass noch sehr lange nach einer Neutronenbestrahlung (je nach Halbwertszeit des Isotops) durch den Zerfall ionisierende Strahlung auftreten kann. 

Der freie Zustand des Neutrons endet nach kürzester Zeit immer mit einem Neutroneneinfang oder einer anderen Kernreaktion. Nur im Hochvakuum hat ein freies Neutron eine „Chance“, seinen radioaktiven Zerfall zu „erleben“.

Die kosmische Strahlung setzt in der Atmosphäre oder am Boden durch Wechselwirkung mit Molekülen natürliche Neutronenstrahlung frei. Durch natürlichen Zerfall von Atomkernen entsteht Neutronenstrahlung selten; man stellt sie künstlich mit Hilfe von Neutronenquellen her. Im Kernreaktor werden bei der Kernspaltung Neutronen freigesetzt, ebenso bei der Kernfusion.

Eine weitere starke Quelle sind Neutronenbomben. Sie kann mit Hilfe von Neutronenstrahlung Personen im Zielgebiet töten, aber Gebäude und Infrastruktur relativ unbeschädigt lassen.

In der Materialforschung werden Neutronenstrahlen eingesetzt, um die atomare oder molekulare Struktur von Festkörpern zu bestimmen (Neutronenstreuung). Zur Überwachung der Unterkritikalität eines Kernreaktors kann die Neutronenstrahlung z. B. einer Radium-Beryllium-Neutronenquelle verwendet werden. Bei der Strahlentherapie wurde versucht, Krebszellen mit Neutronenstrahlen abzutöten; wegen der Nebenwirkungen im gesunden Gewebe wird dies nur noch selten angewandt.

Die wichtigste Schadwirkung "schneller" Neutronen in lebendem Gewebe ist die elastische Streuung an Wasserstoff. Sie erzeugt Rückstoßprotonen, die ihrerseits stark ionisierend und damit im Gewebe schädlich wirken. Eine indirekte Schädigung durch "thermische" Neutronenstrahlung kommt durch die Gammastrahlung zustande, die beim Einfang des Neutrons an Wasserstoff entsteht: H + n → H + 2,2 MeV.

Die Schädlichkeit von Neutronenstrahlung wird durch die hohen Strahlungswichtungsfaktoren formula_1 der deutschen Strahlenschutzverordnung mit Werten von 5 bis 20 berücksichtigt.

Schnelle wie auch thermische Neutronenstrahlung kann stabile Atomkerne durch Kernreaktionen in radioaktive Atomkerne umwandeln – dies ist die sogenannte Aktivierung.

Neutronenstrahlung hat im Allgemeinen einen negativen Einfluss auf strukturelle Materialien wie Stahl (siehe Strahlenschaden). Neutronen erzeugen durch Streuung an Atomkernen Defekte im Kristallgitter, die meist zur Versprödung des Materials führen. Auch die Aktivierung und die damit verbundene Umwandlung von Legierungsbestandteilen können sich (meist negativ) auf die Materialeigenschaften auswirken. Diese Prozesse treten besonders an Orten mit sehr hoher Neutronenfluenz auf wie etwa Reaktordruckbehältern, deren Einbauten und den Brennstabhüllen. In Fusionsreaktoren treten ähnlich große Neutronenfluenzen auf, wobei hier auch noch die Energie der Neutronen besonders hoch ist. Daher ist die Werkstoffentwicklung für künftige Fusionskraftwerke eine große Herausforderung.

Eine Abschirmung gegen Neutronenstrahlung nutzt meist eine Kombination physikalischer Effekte und ist aus mehreren Materialien in Schichten aufgebaut. Ein Moderator, zum Beispiel Wasser, Paraffin, Graphit oder Kunststoff, bremst schnelle freie Neutronen ab. Thermische Neutronen werden beispielsweise von Cadmium oder Bor absorbiert. Die begleitende Gammastrahlung wird insbesondere durch entsprechend starke Beton-, Stahl- und Bleischichten reduziert.





Gilbert (Einheit)


Das Gilbert (Einheitenzeichen Gb, nach dem englischen Arzt und Physiker William Gilbert) ist eine veraltete Einheit der magnetischen Spannung "U" (= magnetische Feldstärke formula_3 × Weg formula_4) in einem cgs-System für elektromagnetische Einheiten.

Das Gilbert ist definiert durch die Arbeit, die das magnetische Potenzial eines positiven Einheitspols um 1 erg erhöht:

mit der CGS-Einheit Oe (Oersted) für die magnetische Feldstärke.

Umrechnung in SI-Einheiten:

mit der SI-Einheit A (Ampere) für die magnetische Spannung bzw. die elektrische Stromstärke.

Weitere Konversion:

mit formula_8 für die cgs-Einheit Biot.




Mischungslücke

Eine Mischungslücke bezeichnet bei einem Stoffgemisch einen potenziellen Zustandsraum, der dadurch gekennzeichnet ist, dass das Stoffgemisch in ihm keinen stabilen Zustand besitzt, also die möglichen Zustände nicht realisiert werden. Ein Gemisch zerfällt (entmischt sich) dabei in mindestens zwei verschiedene Phasen mit sehr unterschiedlichen Zusammensetzungen. Die Phasen stehen im thermodynamischen Gleichgewicht miteinander. 

Das Gleichgewicht zweier oder mehrerer flüssiger Phasen wird zumeist als "LLE" für engl. Liquid-Liquid Equilibrium bezeichnet.

Beispielsweise zerfällt ein Benzol/​Wasser-Gemisch in eine benzolreiche Phase, in der nur etwa 0,3 mol-% Wasser enthalten ist, und eine wasserreiche Phase, in der etwa 0,04 mol-% Benzol enthalten ist; eine 1:1-Mischung ist nicht möglich. Einige Gemische sind hingegen bei einigen Temperaturen vollständig ineinander löslich, während bei anderen Temperaturen eine Entmischung stattfindet (Beispiel: Methylvinylketon/​Wasser).

Meist werden Mischungslücken durch T-x-Phasendiagramme veranschaulicht, wobei diese im Falle von drei verschiedenen Stoffen auch eine Dreiecksform annehmen können. Die Stoffmischungen selbst bezeichnet man als Mischphasen.

→ "Hauptartikel: Binodale"

Eine Mischungslücke wird durch zwei Binodalen, die temperaturabhängigen Zusammensetzungen der beiden Phasen im thermodynamischen Gleichgewicht, begrenzt. Binodalen können sich bei niedriger oder hoher Temperatur treffen. Dieser Treffpunkt nennt sich (obere und untere) kritische Lösungstemperatur ("engl. upper and lower critical solution temperature", abgekürzt UCST und LCST). Oberhalb der UCST und unterhalb der LCST sind die Komponenten in allen Verhältnissen mischbar.

→ "Hauptartikel: Spinodale"

Spinodalen sind die Grenzen innerhalb einer Mischungslücke, die einen metastabilen Bereich begrenzen. Die Spinodale verläuft zumeist in der Nähe der Binodale und berührt diese im kritischen Punkt. Zusammensetzungen innerhalb der Spinodalen sind kinetisch labil und zerfallen, während im Grenzbereich zwischen Binodale und Spinodale das Gemisch metastabil ist.

Eine Konode verbindet die im Gleichgewicht befindlichen Zusammensetzungen der beiden Phasen. In Diagrammen binärer Systeme sind dies stets waagerechte Linien und werden deswegen zumeist nicht eingezeichnet. Konoden dienen zur Bestimmung der beiden Phasen, in die ein Stoffgemisch, dessen Zusammensetzung in der Mischungslücke liegen würde, zerfällt.

Siehe

Siehe Legierung.





Nicolaus Steno

Nicolaus Steno, auch Nicolas Stenon (Latinisierung von Niels Stensen bzw. "Niels Steensen"; * in Kopenhagen, Königreich Dänemark; † in Schwerin, Herzogtum Mecklenburg), war ein dänischer Mediziner, Anatom und Naturforscher, später katholischer Priester und Bischof. Er wird in der römisch-katholischen Kirche als Seliger verehrt. Wilhelm von Humboldt bezeichnete ihn als „Vater der Geologie“.

Der Universalgelehrte beherrschte acht Sprachen. Von Freunden und Bekannten seiner Eltern erlernte er die deutsche Sprache. Die lateinische, griechische, hebräische und arabische Sprache lernte er in der Schule. Später eignete er sich die niederländische, französische, italienische und englische Sprache an.

Niels Stensen, 1638 als Sohn eines Goldschmieds in Kopenhagen geboren und in der St.-Nikolai-Kirche lutherisch getauft, besuchte von 1648 bis 1656 in seiner Heimatstadt die Lateinschule bei der Liebfrauenkirche, die damals angesehenste Schule des Landes. Es folgte 1656 im Alter von 18 Jahren ein dreijähriges Medizinstudium an der Kopenhagener Universität. Studien- und Vortragsreisen führten ihn 1660–1665 u. a. nach Rostock, Amsterdam (Begegnung mit Baruch Spinoza und dessen Philosophie), Leiden, Paris, Montpellier und Pisa. Dort kam er mit den führenden Medizinern seiner Zeit in Kontakt. Durch eigenes Forschen entdeckte er schon 1660 den Ausführungsgang der Ohrspeicheldrüse bei der Sektion eines Schafskopfes. Seine Vorlesungen und anatomischen Demonstrationen machten ihn in ganz Europa berühmt. 1661 verfasste und verteidigte er seine Dissertationsschrift über den von ihm entdeckten Ausführungsgang der Ohrspeicheldrüse. Nachdem er 1663 vom Tod seines Stiefvaters erfuhr, kehrte er nach Kopenhagen zurück, wo er in lateinischer Schrift das Buch „Beobachtungen über Muskeln und Drüsen“ veröffentlichte, in dem er nachwies, dass das Herz ein Muskel ist. Als bald darauf seine Mutter starb, verließ er Kopenhagen und reiste nach Paris. Ihm wurde während dieser Zeit von der Universität Leiden in Abwesenheit der Titel eines Doktors der Medizin verliehen.

Im Jahre 1666 reiste Steno über Pisa und Rom nach Florenz. Ferdinand II. von Medici machte ihn zu seinem Leibarzt und unterstützte großzügig seine Forschungstätigkeit. In dieser Zeit dehnte sich sein Interesse auf geologische und paläontologische Themen aus. 1668 wurde er in die Florentiner Accademia della Crusca aufgenommen.

Gleichzeitig hatten Eindrücke in den Niederlanden (Zersplitterung der reformierten Kirchen) und Italien (1666 Fronleichnamsprozession in Livorno) ein intensives Studium theologischer Fragen ausgelöst. Im November 1667 konvertierte Steno zur katholischen Kirche. Seitdem nahm er regelmäßig an der kirchlichen Liturgie teil und vertiefte sein persönliches Gebetsleben. Ab 1668 unternahm Steno eine dreijährige geologische Forschungsreise durch Südeuropa und kehrte schließlich nach Florenz zurück. Dort schrieb er seine ersten theologischen Schriften. 1672 folgte er einem Ruf des dänischen Königs und ging als königlicher Anatom und Universitätslehrer wieder nach Kopenhagen. Die konfessionelle Differenz war jedoch trotz guten Willens aller Beteiligten nicht zu überbrücken, und gleichzeitig wuchs in Steno der Wunsch, sich in den kirchlichen Dienst zu stellen. 1674 kehrte er als Erzieher des Erbprinzen nach Florenz zurück. Im folgenden Jahr bat er um die Priesterweihe. Ostern 1675 feierte Stensen in Florenz seine erste hl. Messe und wirkte seitdem am Hof der Medici auch als Seelsorger und Beichtvater. Er tat das laut Zeitzeugen, hier wie an allen weiteren Wirkungsstätten, mit Liebenswürdigkeit und Bescheidenheit, aber auch mit klaren Forderungen an die Lebensführung.
In Hannover residierte seit 1665 Herzog Johann Friedrich, der ebenfalls in Italien zum Katholizismus übergetreten war. Dieser bat nach dem Tod Valerio Maccionis Papst Innozenz XI. 1677 um die Entsendung Stenos nach Hannover. Am 19. September 1677 empfing Niels Stensen in Rom durch Kardinal Gregorio Barbarigo die Bischofsweihe zum Titularbischof von "Titiopolis" und wurde als Apostolischer Vikar für die versprengten Reste katholischer Gemeinden in Norddeutschland und Skandinavien (Apostolisches Vikariat des Nordens) mit Sitz in Hannover ausgesandt. Hier begegnete er u. a. Gottfried Wilhelm Leibniz, der ihn als Naturwissenschaftler bewunderte, seine religiöse Haltung jedoch als starr empfand. Als Herzog Johann Friedrich im Dezember 1679 starb und sein Bruder, somit wieder ein Lutheraner, die Herrschaft in Hannover übernahm, verlor das nordische Vikariat in der Stadt den Rückhalt. Das Fürstbistum Paderborn leitete zu dieser Zeit Ferdinand von Fürstenberg, der zugleich Fürstbischof von Münster war. Er bat Rom um Entsendung Stensens nach Münster als Weihbischof und Leiter der Seelsorge.

1680–1683 versuchte Stensen in Münster das geistliche Leben von Klerus und Laien zu ordnen und Disziplinlosigkeit und Ämterkauf zu überwinden. Das Amt des Stiftsdechanten an St. Ludgeri, dessen Einkünfte seinen Lebensunterhalt sichern sollten, gab er schon nach einem Jahr zurück, weil er ihm nicht gerecht werden zu können glaubte. Persönlich wurde Stensen jetzt noch asketischer. Was er von geistlichen Amtsträgern forderte, zeigte er beispielhaft durch die eigene Lebensführung. Damit geriet er in Widerspruch zum Lebensstil der oft aus dem Adel stammenden höheren Geistlichkeit und wurde zu einem lebenden Vorwurf.

Als nach dem Tod Ferdinands von Fürstenberg statt eines Seelsorger-Bischofs für Münster Maximilian Heinrich von Bayern seinen fünften Bischofssitz einnahm, nachdem ihm 60.000 Reichstaler Bestechungsgeld an das Domkapitel diese Wahl gesichert hatten, protestierte Stensen öffentlich und verließ Münster am 1. September 1683. Er ging nach Hamburg, wo er im Hause des niederländischen Anatomen Theodor Kerckring Aufnahme fand, um der dortigen katholischen Gemeinde zu dienen. Auch dort traf er auf starke Spannungen und heftigen Widerstand gegen den fremden Mahner und Schlichter.

Im Jahre 1685 schließlich wurde er nach Schwerin gerufen. Als einfacher Priester ohne bischöfliche Insignien kümmerte er sich um die kleine Gemeinde. Auch hier gab es Enttäuschungen.
Nach einer fünfwöchigen, mit schweren Koliken verbundenen Gallenkrankheit starb Niels Stensen 48-jährig in Schwerin. Als sein letztes Wort ist das Gebet überliefert: "Jesus, sis mihi Jesus" – „Jesus, sei mir Retter“. Sein Hamburger Freund Kerckring ließ im Auftrag des toskanischen Großherzogs seinen Leichnam einbalsamieren und per Schiff nach Livorno überführen. Er wurde in einer Kapelle der Basilika San Lorenzo in Florenz beigesetzt.

Dreihundert Jahre später, am 23. Oktober 1988, wurde Nicolaus Steno auf maßgebliches Betreiben von Bischof Heinrich Theissing, dem Apostolischen Administrator von Schwerin, durch Papst Johannes Paul II. seliggesprochen. Sein kirchlicher Gedenktag ist der 25. November.

Stensens bis heute andauernde Verehrung gründet in seiner wissenschaftlichen Vorurteilslosigkeit und Beobachtungsschärfe sowie in der großen Geduld und Ausdauer, mit der er seine religiöse Mission unter inner- und außerkirchlichen Schwierigkeiten und in zunehmender Vereinsamung erfüllte.

Steno wurde bekannt für sein eigenständiges Studium der Natur und die Abkehr von der Berufung auf überkommene Autoritäten. Die zeitgemäß in lateinischer Sprache veröffentlichten wissenschaftlichen Werke trugen stets seinen latinisierten Namen "Nicolaus Stenonis" (Gen. "Nicolai"). Den Irrtum, dass in späterer Zeit auch der Nachname "Stenonis" als Genitiv von "Steno" angesehen wurde, erläuterte bereits 1916 John G. Winter in einer Einführung zur englischen Ausgabe von Stenos Hauptwerk "De solido intra solidum naturaliter contento dissertationis prodromus" (1669). Stattdessen tritt der Genitiv bereits durch die Latinisierung des dänischen Namens Stensen auf; denn dieser besagt soviel wie „Sohn des Sten“, auf Latein "filius Stenonis" oder verkürzt "Stenonis". In den überlieferten Handschriften unterschrieb Stensen mit "Nicolaus Stenonis". Die Verkürzung des Namens zu „Steno“ ist erst auf spätere Werkausgaben in verschiedenen Sprachen zurückzuführen, inzwischen aber allgemein gebräuchlich.

Steno untersuchte und beschrieb als erster die Tränen- und Speicheldrüsen des menschlichen Körpers und unterschied Drüsen von Lymphknoten. Er beschrieb das Ausführungsgangsystem der Ohrspeicheldrüse, den Ductus parotideus. Dieser wird unter Klinikern auch als „Ductus stenonianus“ („Stensen-Gang“, „Stenon-Gang“) bezeichnet.

Bei der Untersuchung von Quarz entdeckte Steno das Gesetz von der Winkelkonstanz, also die Tatsache, dass die Oberflächen der Kristalle immer im selben Winkel zueinander stehen, und zwar unabhängig von ihrer Größe oder Form. Er schlug daraufhin vor, dass dies eine Eigenschaft aller Mineralkristalle ist, und legte damit bahnbrechend das Fundament für die moderne Kristallographie.

Auf Steno geht die Einsicht über die biologische Herkunft der Fossilien als Überreste von Lebewesen zurück, die bis dahin als natürliche Gesteinsauswüchse (Lusus naturae) betrachtet worden waren. Mit seiner 1667 erschienenen Schrift "Canis carchariae dissectum caput" belegte er, dass es sich bei den so genannten „Zungensteinen“ in Wirklichkeit um fossile Haizähne handelt.

Steno leistete mit dem „Stratigraphischen Grundgesetz“ (auch „Lagerungsgesetz“) einen zentralen Beitrag zur Entstehung der Geologie. In seinem bedeutendsten Werk "De solido intra solidum naturaliter contento dissertationis prodromus" (Vorläufer einer Abhandlung über Festes, das in der Natur in Festem eingeschlossen ist) entwickelte er als erster eine auf wissenschaftlicher Grundlage stehende Theorie zur Entstehung von Sedimentgesteinen. Nach Steno bildeten sich die Gesteine als horizontal gelagerte Schichten aus im Wasser abgelagertem Material. Die Schichten lagern sich übereinander ab (Superpositionsprinzip). Steno erkannte damit, dass das Alter einer Sedimentschicht nach oben hin abnimmt, da sich stets jüngere Schichten auf älteren ablagern. Die Existenz von Sedimentgesteinen mit bis zu senkrecht verlaufender Schichtung und großen Verwerfungen erklärte Steno korrekt durch Deformationen, die "nach" der Bildung des Gesteins stattgefunden haben mussten.

In der norddeutschen katholischen Diaspora ist Niels Stensen nach langer Vergessenheit im 20. Jahrhundert wiederentdeckt worden. In Worphausen bei Bremen wurde in den 1960er Jahren das „Niels-Stensen-Kloster“ gebaut. Es wurde aber nie als solches genutzt, sondern diente Jahrzehnte unter dem Namen „Niels-Stensen-Haus“ als katholische Bildungsstätte. Als das Bistum Hildesheim 2007 die Bildungsstätte aufgab, übernahm die anthroposophisch geprägte Stiftung „Leben und Arbeiten“ heilpädagogischer und sozialtherapeutischer Arbeit den größten Teil der Anlage. Auch andere Bildungs- und Erholungseinrichtungen tragen seinen Namen. Die Katholische Jugend Hamburg betreibt in Wentorf bei Hamburg das "Niels-Stensen-Haus". Die erste Kirchengemeinde, die nach Stensen benannt wurde, ist die Pfarrgemeinde Niels Stensen in Grevesmühlen (Mecklenburg). In den 1980er Jahren wurde in Schwerin – damals DDR-Bezirkshauptstadt – eine Straße nach Niels Stensen benannt.

Im Bistum Münster erinnert seit 1988 eine Stele an der Alt-St.-Clemens-Kirche in Münster-Hiltrup an Steno.
Seit 2006 heißt eine Kirchengemeinde im Bistum nach ihm. Sie entstand aus der Fusion der Pfarreien von Lengerich, Ladbergen, Lienen und Tecklenburg und ist mit 8700 Christen und 300 Quadratkilometern Ausdehnung die größte Gemeinde im Bistum Münster. Zudem wurde im Westen Münsters die Niels-Stensen-Straße in der Nähe des Universitätsklinikums nach Steno benannt.

Seit 2008 heißt ein in der Region Osnabrück angesiedelter Verbund kirchlicher Krankenhäuser und angeschlossener Einrichtungen Niels-Stensen-Kliniken. Weiterhin ist er Namenspatron für das Niels Stensen Pflegezentrum in Ankum.

Dem Leben und Werk des Universalgelehrten und „Vaters der Geologie“ widmet sich ein eigenes Museum in Aarhus. Es wurde 1993 von Olaf Pedersen gegründet, dem Professor für Wissenschaftsgeschichte der Universität Aarhus.







Kristallit

Kristallite sind Kristalle, die die eigentliche Kristallform nicht oder nur teilweise abzeichnen. In der Metallkunde und Petrographie werden Kristallite auch als Korn bezeichnet. 

Kristallite entstehen, wenn Kristalle in einer Schmelze erstarren, in der sie von umliegenden Kristallen am freien Wachstum gehindert werden. Sie erstarren zu einem polykristallinen Gefüge mit einer meist mikroskopisch kleinen Korngröße. Die benachbarten Kristallite eines Polykristalls unterscheiden sich in der Orientierung ihrer Kristallstruktur.

Auf Gefügeschliffbildern von polierten und mit Säure behandelten Metall- oder Gesteinsproben sind die als Korngrenzen bezeichneten Übergänge von einem Kristallit zum nächsten als dunkle Linien oder Farbwechsel zu erkennen.

Die technische Bedeutung von Kristalliten ist hoch, so haben sie einen entscheidenden Einfluss auf das Kriechen von Werkstoffen insbesondere bei Hochtemperaturanwendungen und behindern die Ausbreitung von Versetzungen, was in der Feinkornhärtung ausgenutzt wird.





David Brewster

Sir David Brewster (* 11. Dezember 1781 bei Jedburgh, Schottland; † 10. Februar 1868 in Allerly bei Melrose) war ein schottischer Physiker, Wiederentdecker des Kaleidoskops sowie Erfinder des dioptrischen Stereoskops.

Brewsters Vater James Brewster war Rektor der Grammar School von Jedburgh. Brewster interessierte sich schon früh für Naturwissenschaften und konstruierte mit 10 Jahren ein Teleskop. Er studierte zunächst in Edinburgh Theologie (wo er auch seinen Abschluss machte und eine Lizenz als Pfarrer der Kirche von Schottland hatte, die er aber nie nutzte) und dann Naturwissenschaften und Jura. Er war danach Anwalt und später Professor für Physik an der Universität St. Andrews. 1859 wurde er Prinzipal der Universität Edinburgh.

Seine ersten Untersuchungen betrafen die Polarisation von Licht (Brewster-Winkel, 1814 bei der Untersuchung von polarisiertem Licht in Kristallen mit zwei optischen Achsen) und die doppelte Strahlenbrechung. Ergebnisse erschienen in den "Transactions of the Royal Society of Edinburgh", deren Mitglied er 1808 wurde. Später erhielt er auch das Amt des Vizepräsidenten dieser Gesellschaft.

Im Jahr 1816 gelang ihm die Erfindung des Kaleidoskops. Er erhielt am 10. Juli 1817 auf diese Neuerung ein englisches Patent und schrieb darüber 1819 ein Buch. Dieses wissenschaftliche Instrument wurde schnell ein beliebtes Spielzeug und fand so weite Verbreitung.

Eine weitere Erfindung von ihm, das Stereoskop, wurde ebenfalls um 1850 ein beliebtes Spielzeug. Es entstand aus seiner Verbindung zu Experimenten der frühen Photographie.

Bekannt ist er auch für seine große Biographie von Isaac Newton. Darin setzte er sich kritisch mit der Methodologie von Francis Bacon auseinander:
Solche Vorbehalte wurden später von Paul Feyerabend umfassender entfaltet.

Brewster schrieb zahlreiche populärwissenschaftliche Aufsätze und Bücher. Er war mit vielen prominenten schottischen Persönlichkeiten bekannt wie Walter Scott und vielen Malern wie Alexander Nasmyth und William Turner.

1808 übernahm Brewster auch die Redaktion der "Edinburgh Encyclopedia", die bis zum Jahr 1830 in 30 Bänden herauskam. Zusammen mit Robert Jameson gründete Brewster 1819 das "Edinburgh Philosophical Journal", das er von 1824 bis 1832 allein herausgab. Er verfasste auch viele Artikel für die 7. und 8. Auflage der "Encyclopedia Britannica". David Brewster starb am 10. Februar 1868 in Allerly bei Melrose an einer Lungenentzündung.

Brewster gilt als Erstbeschreiber des Minerals Gmelinit (heute "Gmelinit-Na").

1815 wurde er als Mitglied („Fellow“) in die Royal Society gewählt, die ihm im gleichen Jahr die Copley-Medaille, 1818 die Rumford-Medaille und 1830 die Royal Medal verlieh. 1822 wurde er in die American Academy of Arts and Sciences, 1825 in die Académie des sciences, 1826 in die Göttinger Akademie der Wissenschaften und 1827 in die Königlich-Preußische Akademie der Wissenschaften gewählt. Seit 1850 war er auswärtiges Mitglied der Bayerischen Akademie der Wissenschaften, seit 1864 der National Academy of Sciences. 1831 wurde er als "Ritter" des Guelphen-Ordens sowie 1832 als Knight Bachelor geadelt.

Der Mondkrater Brewster, der Asteroid (5845) Davidbrewster sowie die Minerale Brewsterit-Ba und Brewsterit-Sr wurden nach ihm benannt. Ebenso ist er Namensgeber für den Mount Brewster, einen Berg im ostantarktischen Viktorialand, und die Insel Brewster Island vor der Westküste des westantarktischen Grahamlands.

Auch die Einheit der Spannungsoptischen Empfindlichkeit, das Brewster trägt seinen Namen.






Halochromie

Halochromie („Salzfarbigkeit“) ist die Chemie der Farbveränderung einer Substanz in Abhängigkeit vom Ladungszustand seiner Moleküle.

Halochrome zeigen Farbverschiebungen bei Änderung des Ladungszustands (zum Beispiel bei der Salzbildung). Diese Erscheinung in der Chemie der Farbstoffe lässt sich an verschiedenen Säure-Base-Indikatoren (zum Beispiel Phenolphthalein oder Lackmus) beobachten. Als schwache organische Säure ist Phenolphthalein im schwach sauren bis neutralen Milieu ungeladen und farblos. In alkalischen Lösungen zeigt das Indikatormolekül auf Grund negativer Ladung eine rote Farbe. Lackmus wechselt je nach pH-Wert einer wässrigen Lösung seine Farbe von Rot (sauer) nach Blau (basisch).





Trägerstaueffekt

Der (Ladungs-)Trägerstaueffekt (TSE) tritt unter anderem in elektronischen Bauelementen wie beispielsweise Thyristor und Triac auf und stellt eine Gefährdung für diese dar.

Beim Nulldurchgang der anliegenden Ventilspannung befinden sich noch Ladungsträger in den Sperrschichten und aus diesem Grund fließt auch dann ein Strom, wenn die Spannung schon in Sperrrichtung gepolt ist. Sind alle Ladungsträger abtransportiert, reißt dieser Strom ab. An Induktivitäten, die evtl. im Stromkreis liegen, wird aufgrund dieser steilen Stromänderung (formula_1) eine Spannungsspitze induziert. Diese Spannungsspitzen treten periodisch auf (siehe Wechselstrom) und überlagern sich mit der Betriebsspannung, wodurch die Halbleiterschichten gefährdet werden. Die Spannung liegt als Sperrspannung an und kann durch Lawinendurchbruch das Bauelement zerstören.

Vor Überspannung, ausgelöst z. B. durch den Trägerstaueffekt, werden Halbleiterventile durch entsprechend dimensionierte RC-Schaltungen (Reihenschaltung von ohmschem Widerstand und Kondensator) geschützt. Diese RC-Schaltung liegt parallel zum Thyristor oder Triac und wird auch als Snubber (engl.) bezeichnet.

Der Trägerstaueffekt wird bei Bauteilen wie Ladungsspeicherdiode, Speicherschaltdiode oder PIN-Diode gezielt ausgenutzt.




Krümmungseffekte

An stark gekrümmten Oberflächen können aufgrund der speziellen Bedingungen an solchen Flächen besondere physikalische Effekte auftreten. Insbesondere haben kleinste Flüssigkeitströpfchen oder Nanoteilchen im Verhältnis zu ihrem Volumen eine relativ große Oberfläche, so dass sich die Oberflächenenergie bzw. die Oberflächenspannung auf die Eigenschaften auswirken kann. Wichtige Beispiele für solche Effekte, die Krümmungseffekte genannt werden, sind für die Oberfläche von Flüssigkeiten:

Solche Effekte treten nicht nur bei Flüssigkeiten auf, sondern auch bei Festkörpern. Die Zusammenhänge sind bei ihnen allerdings komplizierter, nicht nur weil unterschiedlich orientierte Flächen eines Kristalls verschiedene Oberflächenenergien haben. Außerdem muss zwischen der Oberflächenenergie bzw. Oberflächenerzeugungsarbeit und der elastischen Grenzflächenspannung (englisch: "surface stress") unterschieden werden. Dennoch gibt es entsprechende Effekte auch bei Festkörpern:

Die genannten Effekte führen auch zur Ostwaldreifung, das heißt, dass bei unterschiedlich großen Teilchen die kleinsten kleiner werden und verschwinden, weil sie verdampfen oder sich auflösen, während die größeren wachsen.





DRIFTS

Der Begriff DRIFTS (, dt. »diffuse Reflexions-Fouriertransformationsinfrarotspektroskopie«) bezeichnet eine Methode der Infrarotspektroskopie, bei der die diffuse Reflexion einer Probe gemessen wird.

Infrarotes Licht einer thermischen Quelle (siehe z. B. Globar) wird zur spektralen Auflösung durch ein
FTIR-Spektrometer geführt und dann auf eine Probe fokussiert. Dort treten zwei Effekte auf, die sich bei rauer Probenoberfläche experimentell nicht einfach trennen lassen:

Neben der hohen Signalintensität der Fourier-spektroskopischen Methode bietet die diffuse Reflexion Vorteile gegenüber einer Transmissionsmessung. So erlaubt sie etwa die Untersuchung stark streuender oder absorbierende Proben, wie Textilien, Lackschichten, Schäume, Papier. Zudem ist keine Probenpräparation erforderlich, weshalb Pulver direkt gemessen werden können – an Katalysator-Pellets (beispielsweise aus Zeolithen) würde eine Probenpräparation die Reaktionsbedingungen verfälschen.

Auch wenn die Reproduzierbarkeit bei sorgfältiger Probenpräparation gegeben ist, ist eine exakte quantitative Auswertung nicht möglich. Allerdings wurden verschiedene semiquantitative Modelle entwickelt. Am häufigsten wird die Kubelka-Munk-Theorie verwendet, in der der spektrale Reflexionsgrad der Probe auf zwei wellenlängenabhängige Materialkonstanten, den Absorptions- und den Streukoeffizienten, zurückgeführt wird. Der Theorie liegt ein eindimensionales System zu Grunde, sodass die gemessenen Koeffizienten nur unter weiteren Annahmen, die in der Praxis selten erfüllt sind, auf mikroskopische Größen schließen lassen.




Forbush-Ereignis

Ein Forbush-Ereignis (auch: Forbush-Effekt) ist ein beobachteter plötzlicher Abfall in der hochenergetischen kosmischen Strahlung nach einem solaren Ausbruch. Dies geschieht, weil der Sonnenwind diese hochenergetischen Teilchen von der Erde fernhält. Das Ereignis kann mit einem Teilchendetektor leicht nachgewiesen werden.

Namensgeber für den Begriff "Forbush-Ereignis" war der Geophysiker Scott E. Forbush, der sich in den 1930er- und 1940er-Jahren mit der Erforschung der kosmischen Strahlung beschäftigte und diesen Effekt entdeckte.





Metaphysische Anfangsgründe der Naturwissenschaft

Metaphysische Anfangsgründe der Naturwissenschaft (Abk. MAN) ist der Titel eines Buchs des Philosophen Immanuel Kant. Es erschien 1786, ein Jahr vor Herausgabe der zweiten Auflage der Kritik der reinen Vernunft (KrV).

Das Buch ist die Anwendung der in der KrV erarbeiteten Prinzipien über die menschliche Erkenntnis auf den Bereich der Physik. Bereits in der KrV hatte Kant gesagt, dass es sich bei dieser nicht um ein philosophisches System handele, sondern um einen "Traktat von der Methode". Die MAN sind eine Anwendung dieser Methode. Sie zeigen, wie die Grundsätze der Erkenntnis a priori als Bedingungen der Möglichkeit der Erkenntnis der Natur gültig sind. Kant ging dabei davon aus, dass die von Isaac Newton formulierten Gesetze eine tatsächliche Beschreibung der Natur darstellen. Entsprechend der Unterteilung der Kategorien suchte Kant nach den Prinzipien, die der Physik a priori zugrunde liegen. Die der MAN zugrunde liegende Prämisse besagt, Bewegung sei die Grundbestimmung von sinnlich wahrnehmbaren Gegenständen. Daher müsse der Begriff der Materie in Hinblick auf die darin enthaltenen vier Kategorienbereiche untersucht werden. Kant entwickelte entsprechend vier Untersuchungsbereiche.





Kant betrachtete Physik als „strenge Wissenschaft“. Hiermit verband er die Auffassung, dass die Prinzipien der Physik vollständig und unbezweifelbar in einer mathematischen Formulierung darstellbar sind. Im Opus postumum Kants finden sich Aufzeichnungen, die zeigen, dass er die in den MAN aufgestellten Prinzipien nicht als endgültig betrachtete. In der Praxis haben die Grundsätze der MAN nur wenig Beachtung gefunden. Insbesondere seit der Aufstellung der Relativitätstheorie und der Quantenphysik gelten Kants Überlegungen zu den Grundprinzipien der Physik als überholt.





Umwandlungstemperatur

Als Umwandlungstemperatur bezeichnet man die Temperatur, bei der ein Stoff eine andere Phase annimmt.

Es kann die Temperatur sein, bei der ein anderer Aggregatzustand angenommen wird oder bei der sich innerhalb eines Materials z. B. eine zweite Phase bildet, weil sich die Löslichkeit eines anderen Stoffes ändert.

Umwandlungstemperaturen können direkt gemessen oder durch thermodynamische Modelle berechnet werden. Ihre Abhängigkeiten von anderen Größen wie Druck, Stoffkonzentration u. a. werden in Phasendiagrammen dargestellt.

Beispiele für Umwandlungstemperaturen sind Gefrierpunkt und Schmelzpunkt.




Lambda-CDM-Modell

Das ΛCDM-Modell bzw. Lambda-CDM-Modell ist ein kosmologisches Modell, das mit wenigen – in der Grundform sechs – Parametern die Entwicklung des Universums seit dem Urknall beschreibt. Da es das einfachste Modell ist, das in guter Übereinstimmung mit kosmologischen Messungen ist, wird es auch als Standardmodell der Kosmologie bezeichnet.

Der griechische Buchstabe Lambda (Λ) steht dabei für die kosmologische Konstante, "CDM" für "cold dark matter" (kalte dunkle Materie).

Das Lambda-CDM-Modell ist in guter Übereinstimmung mit den drei wichtigsten Klassen von Beobachtungen, die uns Aufschluss über das frühe Universum geben:

Das Universum wird dabei als global flach (ungekrümmt) angenommen, die Energieanteile relativ zur kritischen Dichte sind dann auch relativ zur tatsächlichen Gesamtenergiedichte und der relative Anteil der dunklen Energie ergibt sich zu (69,1 ± 0,6) %. Die heutige Gesamtenergiedichte ist 8,62 · 10 kg/m, die Rotverschiebung "z", die dem Zeitalter der Reionisierung entspricht, beträgt 11,37. Das Alter des Universums wird zu 13,8242 Mrd. Jahren bestimmt.






TOP-Falle

Eine TOP-Falle (englisch: „time-averaged, orbiting potential trap“ oder „Time-Orbiting-Potential“-Falle) ist eine Weiterentwicklung der Quadrupolfalle. Sie nutzt das magnetische Moment von neutralen Atomen, um diese einzufangen und zu speichern. 

Die TOP-Falle besteht aus einem magnetischen Quadrupolfeld, das um die Symmetrieachse rotiert. Die Rotation ist notwendig, weil stets ein Ort innerhalb der Falle existiert, an dem das Magnetfeld verschwindet. An diesem Ort können die Atome die Falle durch Ändern ihres inneren Drehimpulses oder Spins verlassen (Spin-Flip).

Es gibt dabei drei entscheidende Frequenzen:

Damit eine TOP-Falle funktioniert, muss die Rotationsfrequenz größer sein als die Fallenfrequenz, jedoch kleiner als die Larmorfrequenz. Dies kann man wie folgt verstehen: Die Rotationsfrequenz muss viel größer sein als die Fallenfrequenz, damit sich der Ort, an dem die Atome die Falle verlassen könnten, schneller bewegt, als die Teilchen reagieren können. Durch die schnelle Rotation des Potentials „spürt“ jedes Atom ein zeitlich gemitteltes (effektives) Potential. Andererseits muss die Rotationsfrequenz kleiner sein als die Larmorfrequenz, weil ansonsten die Elektronen dem Magnetfeld nicht mehr folgen können. Die Elektronen würden überhaupt kein Feld spüren.

Die Quic-Falle benutzt zusätzliche Joffe-Spulen.




EPICS

Das Experimental Physics and Industrial Control System (EPICS) ist eine Reihe von Softwarewerkzeugen und Anwendungen zur Entwicklung und Implementierung eines Prozessleitsystem zum Betreiben von Geräten wie Teilchenbeschleuniger, Teleskope und anderen großen Experimente. Die Tools wurden entwickelt, um die Entwicklung von Systemen zu unterstützen, die oft mit einer großen Anzahl von Computernetzwerken ausgestattet sind, die Kontrolle und Feedback liefern.

EPICS wurde 1988 von Bob Dalesio, Jeff Hill et al. als Ground Test Accelerator Controls System (GTACS) am Los Alamos National Laboratory (LANL) entwickelt.
1989 kam Marty Kraimer vom Argonne National Laboratory (ANL) für 6 Monate an die Seite des GTA-Steuerungsteams und brachte seine Erfahrungen aus seiner Arbeit am Advanced Photon Source (APS) Control System in das Projekt ein. Die daraus resultierende Software wurde in EPICS umbenannt und 1991 auf der International Conference on Accelerator and Large Experimental Physics Control Systems (ICALEPCS) vorgestellt.
EPICS war ursprünglich unter einer kommerziellen Lizenz erhältlich, wobei erweiterte Versionen von Tate & Kinetic Systems verkauft wurden. Die Lizenzen für Mitarbeiter waren kostenlos, erforderten aber eine rechtliche Vereinbarung mit LANL und APS. Eine EPICS-Community wurde gegründet und die Entwicklung wuchs, da sich weitere Einrichtungen der Zusammenarbeit anschlossen. Im Februar 2004 wurde EPICS nach seiner Veröffentlichung unter der EPICS Open License frei verteilbar.
Es wird heute von über 50 großen Wissenschaftseinrichtungen weltweit sowie von mehreren kommerziellen Unternehmen genutzt und entwickelt.

EPICS verwendet Client-Server-Modell und Publish/Subscribe-Techniken zur Kommunikation zwischen Computern. Die " Input/Output-Controller" (IOCs) sammeln Experiment- und Kontrolldaten in Echtzeit mit den daran angeschlossenen Messgeräten. Diese Informationen werden dann den Clients über den hochbandbreiten Channel Access (CA) oder das kürzlich hinzugefügte pvAccess-Netzwerkprotokoll zur Verfügung gestellt, die für Computer-Echtzeitanwendungen wie wissenschaftliche Experimente ausgelegt sind.

IOCs halten und führen eine Datenbank mit "Aufzeichnungen", die entweder Geräte oder Aspekte der zu steuernden Geräte darstellen. Sie können entweder von Standardservern oder PCs, oder von VME oder MicroTCA-Standard-Eingebettetes System-Prozessoren gehostet werden. Für 'harte Echtzeit'-Anwendungen werden in der Regel die Betriebssysteme RTEMS oder VxWorks verwendet, während 'weiche Echtzeit'-Anwendungen unter Linux oder Microsoft Windows laufen.

Daten, die in der Datenbank der Protokolle gespeichert sind, werden durch eindeutige Identifikatoren dargestellt, die als Prozessvariablen (PVs) bezeichnet werden. Diese PVs sind über die Kanäle zugänglich, die vom Channel Access Netzwerkprotokoll bereitgestellt werden.

Datenbankaufzeichnungen sind für verschiedene Arten von Ein- und Ausgängen (z. B. analog oder binär) oder zur Bereitstellung eines unterschiedlichen Funktionsverhaltens, wie z. B. eines Berechnungsprotokolls, verfügbar. Es ist auch möglich, benutzerdefinierte Satzarten zu erstellen. Jeder Datensatz besteht aus einer Reihe von Feldern, die seine Daten enthalten und sein Verhalten festlegen. Die meisten Satzarten sind im EPICS-Referenzhandbuch aufgeführt.

Es stehen Grafische Benutzeroberfläche-pakete zur Verfügung, mit denen Benutzer PV-Daten über typische Anzeige-Widgets wie Zifferblätter und Textfelder anzeigen und mit ihnen interagieren können. Beispiele sind EDM (Extensible Display Manager), MEDM (Motif/EDM) und CS:Studio/Phoebus.

Jede Software, die das CA/pvAccess-Protokoll implementiert, kann PV-Werte lesen und schreiben. Erweiterungspakete sind verfügbar, um Unterstützung für MATLAB, LabVIEW, Perl, Python, Tcl, ActiveX etc. zu bieten. Diese können verwendet werden, um Skripte zu schreiben, die mit EPICS-gesteuerten Geräten interagieren.






Markolf Niemz

Markolf H. Niemz (* 1964 in Hofheim am Taunus) ist ein deutscher Biophysiker.

Markolf Niemz studierte Physik an der Ruprecht-Karls-Universität Heidelberg (Diplom) und anschließend Bioengineering an der University of California, San Diego (UCSD) (Master of Science). Er wurde 1992 mit einer Arbeit über den Aufbau eines pulskomprimierten Nd:YLF-Lasers zur Untersuchung der plasmainduzierten Ablation von Gewebe zum "Dr. rer. nat." promoviert und habilitierte sich anschließend in Physik in Heidelberg.

Niemz war bis 1999 Abteilungsleiter für Optische Spektroskopie am Fraunhofer-Institut für Physikalische Messtechnik (IPM) in Freiburg. Im Jahr 2000 erhielt er einen Ruf auf den Lehrstuhl für Medizintechnik/Biomedical Engineering an der Ruprecht-Karls-Universität Heidelberg, angesiedelt als Ordinarius an der Medizinischen Fakultät Mannheim. Niemz ist seither Direktor der Mannheim Biomedical Engineering Laboratories (MABEL), einem "joint venture" der Universität Heidelberg und der Hochschule Mannheim. Er forscht auf den Gebieten der Laser-Gewebe-Wechselwirkungen, der Lasermedizin, der elektrischen Biosignale und des Patientenmonitorings. Er war der erste Wissenschaftler weltweit, der ultrakurze Laserpulse in der Zahnheilkunde angewandt hat, um Karies schmerzfrei zu behandeln.

Niemz setzt sich außerdem mit einem neuen Zweig der Sterbeforschung, der sogenannten Nahtodforschung, auseinander. Mit seinem Wissenschaftsroman "Lucy mit c" wurde er 2005 einem breiten Publikum bekannt, indem er Nahtoderfahrungen mit relativistischen Effekten aus Albert Einsteins Relativitätstheorie verglich. Niemz lehrt, dass die Ewigkeit im Licht ist, in dem alle Distanzen den Wert null haben. Er bietet auch eine neue, physikalische Erklärung für Nahtoderfahrungen an: Wenn etwas von uns beim Sterben ins Licht eintaucht, lässt uns der sogenannte "Searchlight-Effekt" einen dunklen Tunnel mit einem hellen Licht an dessen Ende wahrnehmen. Das Licht selbst sei ein Weltgedächtnis und stelle die "Lebensrückschau" zur Verfügung, von der die Sterbenden oft berichten.

Niemz' Roman schaffte es als erstes deutsches Sachbuch im Eigenverlag auf die Gong-Bestsellerliste. Aus dem Autorenhonorar seiner Lucy-Trilogie hat Niemz die gemeinnützige und mildtätige "Stiftung Lucys Kinder" gegründet. Diese Stiftung setzt sich dafür ein, dass auch Kinder aus den ärmsten Ländern dieser Welt Zugang zu Liebe und Wissen erhalten.







David J. Thouless

David James Thouless (* 21. September 1934 in Bearsden, Schottland; † 6. April 2019 in Cambridge) war ein britischer theoretischer Physiker und Gewinner des Wolf-Preises. 2016 erhielt er (zusammen mit F. Duncan M. Haldane und J. Michael Kosterlitz) für Forschungen zur Theorie topologischer Phasen der Materie den Nobelpreis für Physik.

Thouless machte 1955 seinen Bachelor-Abschluss an der Universität Cambridge, promovierte 1958 an der Cornell University unter Hans Bethe und war danach 1958/59 als Postdoc in Berkeley (Lawrence Berkeley National Laboratory) und 1959 bis 1961 an der Birmingham University bei Rudolf Peierls. 1961 bis 1965 war er Lecturer an der Universität Cambridge. 1965 bis 1978 war er Professor für mathematische Physik in Birmingham, war 1978 Professor an der Queen’s University (Kingston), bevor er 1979/1980 Professor of Applied Science an der Yale University und 1980 Professor für Physik an der University of Washington in Seattle wurde. 1988 bis 1998 war er dort "Uehling Distinguished Scholar". Seit 2003 ist er dort Professor Emeritus. 1983 bis 1985 war er Royal Society Research Fellow und Professor an der Universität Cambridge.

Thouless leistete viele wichtige Beiträge zur Theorie der kondensierten Materie und Vielteilchentheorie. Während seiner Zeit in Birmingham untersuchte er mit mathematischen Mitteln die Natur der Kosterlitz-Thouless-Übergänge im XY-Modell, die Beispiele für topologische Phasenübergänge sind (Übergänge zwischen zwei Unordnungsphasen, von denen die eine durch Bildung topologischer Vortex-Anregungen erzeugt wird). Außerdem beschäftigte er sich mit der Anderson-Lokalisierung von Elektronen in ungeordneten Festkörpern und mit Spin-Gläsern. Ab den 1980er Jahren beschäftigte er sich u. a. mit quantisierten Vortex-Anregungen in Supraflüssigkeiten und Supraleitern und dem Quanten-Hall-Effekt. Anfang der 1960er Jahre beschäftigte er sich auch mit kollektiven Anregungen in Atomkernen und mit der Theorie der Kernmaterie.

Eine Arbeit von Thouless, Mahito Kohmoto, M. P. Nightingale und M. den Nijs von 1982 identifizierte eine topologische Invariante im ganzzahligen Quanten-Hall-Effekt (die TKNN-Invariante) (in einem vereinfachten Modell ohne Wechselwirkung der Elektronen untereinander), die eine Erklärung der Stabilität des Effekts liefert. Die TKNN-Zahl ist eine ganze Zahl formula_1, die über formula_2 die quantisierte Hall-Leitfähigkeit für jedes Band ergibt und über ein Integral der Bloch-Wellenfunktionen über die magnetische Brillouin-Zone ausgedrückt werden kann. Sie entspricht topologisch der ersten Chernklasse eines U(1)-Faserbündels. Topologische Phasen in Festkörpern wurden 2005 aktuell mit der Einführung Topologischer Isolatoren durch Charles L. Kane und Shoucheng Zhang, die auch experimentell beobachtet wurden.

Thouless war Fellow der Royal Society, Fellow der American Physical Society, Fellow der American Academy of Arts and Sciences und seit 1995 Mitglied der US-amerikanischen National Academy of Sciences. Er erhielt 1973 die Maxwell-Medaille, 1990 den Wolf-Preis für Physik, 1993 die Dirac-Medaille, den Holweck-Preis, den Fritz London Memorial Prize (1984), 2000 den Lars-Onsager-Preis sowie 2016 den Nobelpreis für Physik gemeinsam mit F. Duncan M. Haldane und J. Michael Kosterlitz für die theoretischen Entdeckungen der topologischen Phasenübergänge und Phasen der Materie.

1986 erhielt er einen Sc.D. der Universität Cambridge.

1958 heiratete er Margaret Scrase. Er hatte mit ihr zwei Söhne und eine Tochter.







Chiralität (Physik)

Chiralität ("Händigkeit", Kunstwort, abgeleitet von griechisch "χειρ~, ch[e]ir~ - hand~"), bezeichnet in der Physik ein abstraktes Konzept im Rahmen der relativistischen Quantenmechanik und der Quantenfeldtheorie.

Die Chiralität eines Teilchens ist entscheidend bei Prozessen der schwachen Wechselwirkung, da W-Bosonen nur an Teilchen mit negativer (linkshändiger) Chiralität und an Antiteilchen mit positiver (rechtshändiger) Chiralität koppeln.

Die Chiralität ist zu unterscheiden von der Helizität. Die Chiralität physikalischer Größen lässt sich im Gegensatz zur Chiralität in der Chemie auch "nicht" durch eine Spiegelung am ebenen Spiegel veranschaulichen. Stattdessen beschreibt sie die Zerlegung von Dirac-Spinoren in orthogonale Zustände, die unter Paritätsoperationen ineinander übergehen.

Die fünfte Gamma-Matrix formula_1 heißt "Chiralitätsoperator"; er ist hermitesch und selbstinvers. Seine Eigenwerte sind daher formula_2:

Aus der Dirac-Gleichung lässt sich im Grenzfall masseloser Fermionen wie Neutrinos die Weyl-Gleichung formula_3 erhalten. Im Rahmen der Weyl-Gleichung bietet es sich an, die Dirac-Matrizen nicht in Dirac-, sondern in Weyl-Darstellung zu notieren, sodass auf der Nichtdiagonalen nur Blockmatrizen auftreten. Durch das Fehlen des Masseterms entkoppeln somit die vier Komponenten der Dirac-Spinoren zu zwei unabhängigen Zweierspinoren:

Der Chiralitätsoperator kommutiert mit dem Weyl-Hamiltonoperator, sodass ein Satz gemeinsamer Energie- und Chiralitäts-Eigenzustände gefunden werden kann. Aufgrund der Diagonalität des Chiralitätsoperators in Weyl-Darstellung

folgt direkt, dass der obere Zweierspinor formula_6 als linkshändiger und der untere Spinor formula_7 als rechtshändiger Anteil gedeutet werden kann. Da Neutrinos nur schwach wechselwirken, sind rechtshändige Neutrinos bzw. linkshändige Antineutrinos hypothetische sterile Teilchen. Im Rahmen des Standardmodells dagegen sind alle Neutrinos negativer Chiralität und Antineutrinos positiver Chiralität.

Da der Dirac-Hamiltonoperator einen Masseterm besitzt, kommutiert er "nicht" mit dem Chiralitätsoperator; es lassen sich daher keine gemeinsamen Eigenzustände konstruieren. Insbesondere folgt daraus auch, dass die Chiralität eines massiven Objektes "keine" Erhaltungsgröße darstellt, da der Chiralitätsoperator auch nicht mit dem Zeitentwicklungsoperator als Exponential des Hamiltonoperators kommutiert.

Aus der Eigenschaft des Chiralitätsoperators bzw. der fünften Gamma-Matrix, ihr Selbstinverses zu sein, folgt jedoch, dass die Operatoren formula_8 und formula_9 einen vollständigen Satz von Projektionsoperatoren bilden. Sie projizieren die Anteile positiver bzw. negativer Chiralität aus dem Dirac-Spinor hinaus:

Auf diese Weise kann jeder Dirac-Spinor in einen Anteil rechts- beziehungsweise linkshändiger Chiralität zerlegt werden.

In der schwachen Wechselwirkung spielt das Konzept der Chiralität eine entscheidende Rolle. Im Rahmen der historischen V-A-Theorie projizieren die geladenen Ströme der schwachen Wechselwirkung nur den linkshändigen Anteil der Fermionen heraus, sodass nur dieser an der Wechselwirkung teilhat.

In der Glashow-Salam-Weinberg-Theorie der elektroschwachen Vereinigung werden die linkshändigen Anteile einer Teilchengeneration zu Dubletts unter dem schwachen Isospin zusammengefasst (z. B. formula_11 bzw. formula_12), während die rechtshändigen Anteile als Singuletts betrachtet werden (formula_13). Dadurch wirkt die kovariante Ableitung unterschiedlich auf die links- bzw. rechtshändigen Komponenten, sodass

Der Helizitätsoperator betrachtet die Projektion des Spins in Bewegungsrichtung eines Teilchens und ist daher im Gegensatz zum Chiralitätsoperator "nicht" lorentzinvariant. Im Gegensatz zum Chiralitätsoperator kommutiert der Helizitätsoperator jedoch mit dem Dirac-Hamiltonoperator, sodass die Helizität eine Erhaltungsgröße darstellt.

Im Falle masseloser Fermionen stimmen Helizität und Chiralität bis auf einen (Spin-)Faktor überein.

Die Chiralität von Teilchen ist aufgrund der Tatsache, dass der Chiralitätsoperator mit den Gamma-Matrizen antikommutiert, "nicht" invariant unter Paritätsoperationen formula_14 (Paritätsverletzung):

Ebenso ändert die Ladungskonjugation (Charge conjugation) formula_16 die Chiralität, da der Chiralitätsoperator zudem gleich seines komplex Konjugierten ist:

Da somit Paritätsoperation und Ladungskonjugation gleichermaßen die Chiralität umkehren, bleibt die Chiralität unter einer Nacheinanderausführung beider Operationen erhalten. Diesen Fakt bezeichnet man als CP-Invarianz.





Gordon Baym

Gordon Alan Baym (* 1. Juli 1935 in New York City) ist ein US-amerikanischer theoretischer Physiker.

Baym studierte an der Cornell University (Bachelor 1957) und an der Harvard University Physik und Mathematik (Master-Abschluss in Mathematik in Harvard 1957). 1960 wurde er in Harvard bei Julian Schwinger promoviert und war dann zwei Jahres als Post-Doc am Niels-Bohr-Institut in Kopenhagen (er war dort und an der Nordita auch später häufig als Gastwissenschaftler, so 1970, 1976) und ein Jahr an der Universität Berkeley. Ab 1963 war er an der University of Illinois at Urbana-Champaign als Assistenzprofessor und 1968 als ordentlicher Professor. Er schrieb ein in den USA bekanntes Quantenmechanik-Lehrbuch. Baym war u. a. Gastwissenschaftler in Kyoto (1968), Nagoya und Tokio (2001), der École normale supérieure (1999) und der Academia Sinica (1979).

Baym befasste sich insbesondere mit dem Verhalten von Materie unter extremen Bedingungen z. B. in Neutronensternen (teilweise in Zusammenarbeit mit Hans Bethe, Pethick) und Schwerionenstößen (Pionenkondensat, Quark-Gluon-Plasma). In diesem Zusammenhang war er auch am Zustandekommen des Schwerionenbeschleunigers RHIC am Brookhaven National Laboratory wesentlich beteiligt. Außerdem befasste er sich mit kernphysikalischer Vielteilchentheorie und Bose-Einstein-Kondensaten.

1965 wurde er Sloan Research Fellow und 1969 Fellow der American Physical Society. Er ist seit 1982 Mitglied der National Academy of Sciences (wo er Chairman der Physik-Sektion war), seit 2000 der American Philosophical Society und seit 1981 der American Academy of Arts and Sciences. 1985 wurde er zum Fellow der American Association for the Advancement of Science gewählt. 2002 erhielt er den Hans-A.-Bethe-Preis. 2008 erhielt er den Lars-Onsager-Preis mit Christopher Pethick und Tin-Lun Ho. 2011 wurde ihm die Eugene Feenberg Memorial Medal zugesprochen.

Baym war zuerst mit Nina Zippin und später mit Lillian Hoddeson verheiratet. Seine vier Kinder sind ebenfalls in Forschung und Lehre tätig.






Wolfgang Schreier

Wolfgang Schreier (* 27. November 1929 in Leipzig) ist ein deutscher Physiker und Historiker, der auf die Geschichte der Naturwissenschaften, speziell der Physik, spezialisiert ist.

Wolfgang Schreier war von 1969 bis 1975 wissenschaftlicher Assistent und anschließend bis 1987 wissenschaftlicher Oberassistent am Karl-Sudhoff-Institut für Geschichte der Medizin und der Naturwissenschaften an der Karl-Marx-Universität Leipzig. Im November 1970 promovierte er in Leipzig an der Sektion Physik mit der Arbeit "Historisch-kritischer Vergleich der Entwicklungslinien der Elektrodynamik bis zum Aufbau der elektromagnetischen Lichttheorie. Ein Beitrag zum Strukturwandel der Physik im 19. Jahrhundert" bei Hans Wußing. Die Promotion B folgte im Juni 1984 mit der Schrift "Zu den Wechselbeziehungen von Physik, insbesondere Elektrophysik, und entstehender Elektrotechnik in der Zeit der Ausbreitung der Industriellen Revolution (1820 bis 1870). Eine Studie zum frühen wissenschaftlich-technischen Fortschritt im Vorfeld der Elektrifizierung". 1987 wurde Schreier außerordentlicher Hochschuldozent für die Geschichte der Naturwissenschaften, 1992 Dozent an der Universität Leipzig. Seine Biografie zu Thomas Alva Edison erlebte in der DDR vier Auflagen, seine "Geschichte der Physik" vor und nach der Wende drei Auflagen.






Georg Friedrich Pohl

Georg Friedrich Pohl (* 24. Februar 1788 in Stettin; † 10. Juni 1849 in Breslau) war ein deutscher Naturwissenschaftler und Naturphilosoph.

Pohl studierte von 1805 bis 1808 an der Friedrichs-Universität Halle und der Brandenburgischen Universität Frankfurt Theologie – von der er eine gesicherte Lebensstellung erwartete – und Philosophie, Mathematik und Naturwissenschaften. Von Henrich Steffens ganz für die Naturphilosophie gewonnen, trat er 1809 in das Gymnasiallehrer-Seminar in Stettin ein. Schon im nächsten Jahr wurde er Lehrer am dortigen Vereinigten Gymnasium. 1813 wollte er sich an den Befreiungskriegen beteiligen, was aber durch eine Augenkrankheit verhindert wurde. Er begann in Berlin als Hilfslehrer am Friedrichwerderschen Gymnasium und an der Plamannschen Erziehungsanstalt. 

1820 wurde er Gymnasialprofessor für Mathematik und Physik am 
Friedrich-Wilhelms-Gymnasium (Berlin). Er nutzte die Zeit, um bei Georg Wilhelm Friedrich Hegel Philosophie und bei Christian Samuel Weiss Mineralogie zu hören. Von 1829 bis 1832 war er zusätzlich a.o. Professor an der Friedrich-Wilhelms-Universität. In dieser Zeit machte er seine ersten wichtigen Arbeiten zum Elektromagnetismus, worauf die Friedrich-Alexander-Universität Erlangen ihn 1826 den Dr. phil. h. c. verlieh. 1832 wurde er o. Professor für Physik an der Schlesischen Friedrich-Wilhelms-Universität, deren Rektor er 1844/45 war. Im Jahr 1843 wurde er zum Mitglied der Leopoldina gewählt. Seine Tochter, Maria Pohl (1816–1882), konvertierte unter Einfluss von Heinrich Förster (Bischof) 1844 vom evangelischen zum katholischen Glauben, wurde Nonne und war von 1865 bis zu ihrem Tod Dominikanerin im Kloster von Lienz. 1849 starb Georg Friedrich Pohl mit 61 Jahren an der Cholera.

Pohl war mehr der Naturphilosophie und der philosophischen Spekulation als der Physik zugewandt. Für ihn waren „Elektricität und Magnetismus nichts anderes als modificirte, polare Thätigkeitsformen des Chemismus“. Daher interessierte ihn mehr die Elektrochemie als der Elektromagnetismus. Dennoch stellte er 1823 einen elektromagnetischen Rotationsapparat vor, der zum Nachweis der erdmagnetischen Ströme dienen sollte. 1825 erfand er das Gyrotrop. 1828 führte er den ersten Elektromagneten in Deutschland vor, der eine Tragkraft von etwa 5 kg besaß. 1835 stellte er einen der ersten Induktionsapparate her. Daneben war er ein scharfer Gegner der Newtonschen Mechanik; z. B. lehnte er das Trägheitsgesetz ab und schrieb stattdessen den materiellen Körpern eine innere Kraft der Fortbewegung zu. Auch wollte er das Newtonsche Gravitationsgesetz durch ein zum Elektromagnetismus analoges Grundgesetz ersetzen, demzufolge es im Sonnensystem keine anderen Bahnen als elliptische mehr geben könnte.





UA1-Detektor

Der UA1-Detektor war ein Teilchendetektor am Super Proton Synchrotron am CERN.

Auf der Suche nach einem schnellen und kostengünstigen Weg zur Erzeugung der bis dahin unentdeckten W- und Z-Bosonen wurde im Jahr 1977 am CERN ein Vorschlag eingereicht, das gerade ein Jahr vorher in Betrieb gegangene Super Proton Synchrotron (SPS) in einen Protonen-Antiprotonen-Collider umzubauen, um so genügend Energie zur Erzeugung der neuen Teilchen zur Verfügung zu haben. Zum Nachweis der Teilchen wurden zwei Detektoren geplant, UA1 und UA2. Im Jahr 1978 wurden die Pläne zum Umbau des SPS und zum Bau des UA1-Detektors beschlossen. Die Entwicklung wurde von Carlo Rubbia geleitet.

Das umgebaute SPS und die neuen Detektoren gingen im Juli 1981 in Betrieb.

Im Jahr 1983 wurde die Beobachtung der W- und Z-Bosonen veröffentlicht, für die im darauf folgenden Jahr der Physik-Nobelpreis an Carlo Rubbia und Simon van der Meer vergeben wurden.

Im Jahr 1987 wurde zur Steigerung der Luminosität des SPS um etwa Faktor 10 der „Antiproton Collector“ in Betrieb genommen.

Mit dem UA1-Detektor beteiligte man sich an der Suche nach dem bis dahin nicht entdeckten Top-Quark. Der abgedeckte Energiebereich entsprach einer Quark-Masse von 60 GeV/c².

Der Betrieb lief bis zum Jahr 1989.
Nach dem Betriebsende des UA1-Detektors am SPS wurde der Magnet in den Jahren 1991 bis 1999 im NOMAD Neutrinooszillations-Experiment am CERN weiter genutzt. Im Jahr 2005 wurde beschlossen, den inzwischen im freien gelagerten Magneten für das T2K-Experiment zur Untersuchung der Neutrinooszillation dem J-PARC bei Tokai zu spenden.
Der Magnet wurde inzwischen in Japan wieder aufgebaut.

Der UA1-Detektor hatte die Abmessungen von etwa 6x6x10 m, wog ca. 2000 t und bestand aus mehreren konzentrisch um das Kollisionszentrum angeordneten Systemen.

Ab dem Jahr 1985 verfügte der UA1-Detektor über eine Driftkammer innerhalb des zentralen Detektors, den Micro Vertex Detektor (MVD). Mit dem MVD konnten Teilchenspuren mit einer Genauigkeit von bis zu 65 µm rekonstruiert werden.

Der MVD hatte einen Außendurchmesser von 18 cm, zentral hindurch führte das Strahlrohr aus Beryllium mit 5 cm Durchmesser und 1 mm Wandstärke.
Die Kammer hatte eine Länge von 8 m und war mit einer Mischung aus 53 % Argon und 47 % Ethan bei 3 Bar befüllt. Die Drähte verliefen jeweils parallel zum Strahlrohr, wobei zur longitudinalen Ortsauflösung jeweils ein schneller Komperator zur Auswertung des Differenzsignals zwischen die beiden Enden der 256 Signaldrähte geschaltet war.

Später wurden die Metallrohre des Detektors zur Verringerung der Streustrahlung durch Exemplare aus Kohlenstofffaser ersetzt.

Die Zentrale Driftkammer diente der Rekonstruktion von Teilchenspuren und ermöglichte eine Ortsauflösung der Trajektorien von 100–300 µm. Die Funktionsweise der Driftkammer orientierte sich stark an den bis dahin üblichen Blasenkammern.

Die zentrale Kammer war 6 m lang, hatte 2,2 m Außendurchmesser und war bei Umgebungsdruck mit 60 % Ethan und 40 % Argon gefüllt. Die Äußere Hülle hatte eine Dicke von 5 cm und war in Kunststoff-Sandwichbauweise gebaut. Durch den Zug der Drähte kam es zu einer Verformung der Hülle um 8 cm. Die korrekte Spannung jedes einzelnen Drahtes wurde wie beim Stimmen eines Klaviers eingestellt, indem der Draht mechanisch in Schwingung versetzt und auf die richtige Frequenz gestimmt wurde.

6000 Signalaufnahmedrähte verliefen parallel zum magnetischen Feld, angeordnet in Ebenen und gleichmäßig verteilt in 25 m Kammervolumen.
Der Abstand der Drahtebenen richtete sich nach der Driftgeschwindigkeit der Ionen und der Frequenz, mit der beim SPS Teilchenpakete zur Kollision gebracht werden. Die Wiederholungszeit des SPS betrug 3,8 µs, die maximale Driftzeit 3,6 µs und der Abstand der Drahtebenen 18 cm.

Die Kammer kann seit dem Jahr 1999 im CERN-Museum Microcosm besichtigt werden.

Kalorimeter dienen der Bestimmung von Teilchenenergien, dazu müssen die zu messenden Teilchen im Detektor absorbiert werden.

Zu Betriebsbeginn im Jahr 1981 war der UA1-Detektor mit einem inneren elektronischen und einem äußeren hadronischen Kalorimeter ausgestattet. Die Kalorimeter waren aus Szintillatoren und Blei-Absorbern aufgebaut und über Faseroptiken mit oberhalb des Magnetkerns angeordneten Photomultipliern verbunden.

Da 1984 in den Szintillatoren schon erhebliche Strahlenschäden erkennbar waren, wurde ein Ersatz durch strahlungsresistente Kalorimeter geplant, insbesondere da weitere Luminositätssteigerungen geplant waren. In den Jahren 1987 bis 1989 wurde der Detektor ohne die elektromagnetischen Kalorimeter betrieben, der Umbau auf die neuen Kalorimeter dauerte bis zum Jahr 1989.
Die neuen Kalorimeter arbeiteten nach dem Prinzip einer Ionisationskammer und bestanden aus Schichten von 3,3 mm dicken, mit Tetramethyl-Pentan gefüllter Zellen, abwechselnd mit Schichten aus 2 mm abgereichertem Uran, eine äußere Kalorimeterschicht verwendete 5 mm dicke Uran-Absorberplatten.

Mit den Kalorimetern wurde nahezu der gesamte Raum um das Kollisionszentrum herum verschlossen, lediglich ein Öffnungswinkel von 0.2° am Strahlrohr blieb unbeobachtet.

Im UA1-Detektor wurde in einem Volumen von 80 m ein magnetisches Feld von 0,7 T senkrecht zur Strahlrichtung erzeugt. Die Wicklung des Magneten bestand aus Aluminium. In den Eisenkern wurden später großflächige Myonen-Detektoren eingebaut.

Myonen können die Kalorimeter durchdringen und werden durch eine weitere Schicht Detektoren erfasst. Der gesamte Magnet war von mehrlagigen, zur zweidimensionalen Ortsbestimmung gekreuzten Myonen-Driftkammern umgeben.

Ab Ende 1984 bis zum August 1985 wurde das Myonen-Detektierungssystem um zusätzliche Driftkammern mit einer Gesamtfläche von 800 m und 50000 Kanälen erweitert. Die neuen Kammern wurden dabei in den Eisenkern des Magneten eingebaut. Die Wände des Eisenkerns bestehen aus drei aufeinander genieteten Eisenblöcken von jeweils 20 cm Dicke, die Driftkammern wurden zwischen diesen Blöcken, und zusätzlich auf der Innenseite des Magneten angebracht. Mit den neuen Kammern war unter anderem eine Verfolgung der gekrümmten Flugbahn der Myonen im Eisenkern möglich. Die Auflösung des Systems lag zwischen 300 µm und 1,2 mm. Die Kammern wurden mit einem Füllgas aus 75 % Isobutan und 25 % Argon
bei 5 mbar Überdruck betrieben.





Spreitung

Unter Spreitung (engl. "Spreading") versteht man die Ausbreitung und flächige Verteilung unlöslicher Stoffe auf Oberflächen.

Es handelt sich dabei um eine Erscheinung, die im Fachgebiet der Oberflächenphysik beobachtet und erforscht wird.

Wird eine Flüssigkeit auf eine Oberfläche getropft, bildet sich zunächst eine halbkugelförmige Struktur. Diese Struktur wird im Verlauf der Zeit flacher (der Kontaktwinkel wird kleiner), die Flüssigkeit bedeckt eine zunehmend größere Fläche.

Die Spreitung kann soweit fortschreiten, dass die Flüssigkeit nur noch als Schicht in Molekularausdehnung besteht.

Die Spreitung ist abhängig von


Dort, wo Aussagen über die Spreitung zu machen sind, verwendet man Standardflüssigkeiten definierter Menge auf bestimmten Oberflächen (Stahl, Glas, Messing) mit definierter Oberflächenbeschaffenheit und misst die Zeit, in der bei einer bestimmten Temperatur eine bestimmte Fläche bedeckt wird. Die gewonnenen Daten werden als Vergleiche herangezogen.

Eine besondere Bedeutung hat die Spreitung bei Schmierölen, da hier oft gewünscht wird, dass das Öl sehr an der Schmierstelle verbleibt und sich nicht über das gesamte Gerät verteilt.






Dioptrik

Dioptrik (früher "Anaklastik") ist das Teilgebiet der Optik, das sich mit lichtbrechenden Systemen beschäftigt. Es grenzt sich ab von der Katoptrik, die sich mit spiegelnden Systemen befasst. Die lateinische Form „Dioptrice“ ist der Titel eines bedeutenden Werks von Johannes Kepler über das Fachgebiet. 

In der Lichtmikroskopie werden Kondensoren, die aus Glaslinsen bestehen, als dioptrisch bezeichnet, um sie von katoptrischen abzugrenzen, die aus Spiegeln bestehen. Dioptrische Kondensoren sind also der Normalfall. Allvar Gullstrand erhielt 1911 den Nobelpreis für Physiologie oder Medizin „für seine Arbeiten über die Dioptrik des Auges“.

Ein katadioptrisches System, zum Beispiel ein katadioptrisches Teleskop, hat sowohl reflektierende (katoptrische) als auch brechende (dioptrische) Elemente. Weitere Beispiele sind Spiegellinsenobjektive und Katadioptrische Dialyte.




Elementarisierungsstufe

Als Elementarisierungsstufen bezeichnet man die Arten der Vereinfachung eines Sachverhaltes, die in der Physikdidaktik erlaubt sind.

Es gibt drei Elementarisierungsstufen:

Beispiel für Elementarisierungsstufen am Beispiel der Physikalischen Arbeit:






Christian Ludolph Reinhold

Christian Ludolph Reinhold (* 11. November 1739 in Wunstorf; † 19. April 1791 in Versmold) war ein deutscher Zeichner und Kupferstecher, Mathematiker, Physiker und Lehrer.

Reinholds Geburtsort und -jahr werden zwar unterschiedlich angegeben, seine Taufe am 20. November 1739 steht aber im ev. Kirchenbuch von Wunstorf (Reinholtz). Reinhold studierte an der Universität Duisburg und wurde 1768 von der Universität Göttingen mit einer Dissertation "Ober das Wesen und die Geschichte der Mathematik" zum Dr. phil. promoviert. Bereits seit 1763 war er in Osnabrück als Feldmesser, Autor und Lehrer für Mathematik und Physik am Ratsgymnasium Osnabrück tätig und korrespondierte mit Georg Christoph Lichtenberg über Vermessungsfragen. Er schuf Landkarten von Osnabrück und Umgebung, fertigte aber auch die Illustrationen zu seinen Schriften. 1790 wurde Reinhold preußischer Markenteilungs- und Ökonomiekommissar im Amt Ravensberg.

Sein Schüler Johann Georg Christian Frye stach 1775 ein Porträt Reinholds. Reinholdstraßen, die nach ihm benannt wurden, gibt es in Osnabrück und Wiedenbrück.





Poynting-Effekt

Als Poynting-Effekt werden zwei nach John Henry Poynting benannte Phänomene der Physik bezeichnet:

Diese Poynting-Effekte sind vom Poynting-Robertson-Effekt zu unterscheiden.




Kuboformel

Die Kuboformel (nach Ryōgo Kubo) ist ein Resultat der Quantenstatistik. Sie gibt die Lineare Antwortfunktion einer messbaren Größe (Observable) in zeitabhängiger Störungstheorie bei endlicher Temperatur als thermischen Erwartungswert hermitescher Operatoren im Wechselwirkungsbild an .

Zu den zahlreichen Anwendungen der Kubo-Formel gehört die Berechnung magnetischer und elektrischer Suszeptibilitäten und abstrakter Verallgemeinerungen davon als Folge einer zeitabhängigen Störung des Hamiltonoperators des Systems.

Die Kuboformel führt auf eine Beziehung zwischen

Dabei bezeichnen

Ein Quantensystem habe den zeitunabhängigen Hamiltonoperator formula_12 mit den als diskret angenommenen Energiewerten formula_13. Der quantenmechanische und thermische Erwartungswert einer physikalischen Größe mit dem hermiteschen Operator formula_14 ist dann:

wobei formula_17 die Zustandssumme und formula_18 die reziproke absolute Temperatur formula_19 mit der Boltzmann-Konstanten formula_20 und der Temperatur formula_21 ist. Im letzten Gleichheitszeichen wurde dabei nach den ungestörten Energieeigenzuständen formula_22 mit formula_23 entwickelt und deren Vollständigkeit ausgenutzt.

Wenn zur Zeit formula_24 eine externe Störung eingeschaltet wird, verlässt das System das thermische Gleichgewicht. Die Störung wird durch einen zeitabhängigen Zusatz zum Hamiltonoperator beschrieben:
Dabei bezeichnet formula_26 die Heaviside-Funktion, die für nichtnegative Werte von formula_27 den Wert Eins annimmt und für alle anderen formula_28 den Wert Null. Damit wird dem instantanen „Einschaltprozess“ zum Zeitpunkt formula_29 Rechnung getragen. formula_30 ist ein für alle formula_31 definierter hermitescher Operator, sodass formula_32 für alle formula_31 ein vollständiges Orthonormalsystem von Eigenfunktionen formula_34 und Eigenwerten formula_35 besitzt.

Aus der Zeitentwicklung der Dichtematrix formula_36
folgt unter der Annahme, dass zu jedem Zeitpunkt der quantenstatistische Gleichgewichtsformalismus gültig bleibt
, der thermische Erwartungswert der Operatoren formula_14:

mit der Zustandssumme formula_40.

Hier wird noch das quantenmechanische Schrödingerbild benutzt, allerdings mit zeitabhängigen Hamiltonoperatoren. Es wird aber an dieser Stelle darauf hingewiesen, dass sich im Allgemeinen sowohl die Eigenfunktionen formula_34 als auch die Eigenwerte formula_42 des Hamiltonoperators mit formula_7 ändern werden. Die Zeitabhängigkeit der formula_44 folgt aus der Schrödingergleichung formula_45 Da formula_46 „schwach“ sein soll, liegt es nahe, die niedrigste Ordnung der zeitabhängigen Störungstheorie zu benutzen und zum Wechselwirkungsbild überzugehen (Zustände formula_47). Das Ergebnis ist:

In linearer Ordnung in formula_46 gilt:

Auf diese Weise erhält man für formula_52 in linearer Ordnung das Endresultat (in dieser Ordnung sind ferner alle oben angesprochenen Probleme beseitigt, weil bei Störungsrechnungen erster Ordnung nur die Eigenfunktionen nullter Ordnung benötigt werden):

Hier bedeutet der Ausdruck formula_54 einen mit dem Hamiltonoperator formula_12 berechneten quantenstatistischen Erwartungswert, bei der Temperatur formula_21, während die Ausdrücke darüber, formula_57 gewöhnliche quantenmechanische Erwartungswerte sind, welche die Temperatur nicht berücksichtigen. Ferner sind in formula_58 mit formula_13 die Eigenwerte von formula_60 gemeint.

Da zum Zeitpunkt formula_4 die verschiedenen Bilder identisch sind, gilt dasselbe auch für obiges Endresultat.

Hier wurden bosonische Zustände betrachtet. Für fermionische Zustände ergeben sich zusätzliche Besonderheiten. Die reduzierte Planck'sche Konstante formula_62 wurde Eins gesetzt.




Analogie elektrischer und magnetischer Größen

Die Analogie elektrischer und magnetischer Größen ist eine Folge der starken Symmetrie in den Maxwellschen Gleichungen zwischen den auftretenden elektrischen und magnetischen Größen. Diese Analogien sind für das Verstehen elektromagnetischer und elektrotechnischer Zusammenhänge und Erscheinungen hilfreich und werden in Lehrbüchern häufig angegeben. 

So haben die Größen des stationären Strömungsfeldes eine starke Analogie zur Strömungsmechanik sowie zur Thermodynamik und sind recht anschaulich erklärbar (siehe auch Elektro-Hydraulische Analogie). Die Größen des elektrostatischen und des magnetischen Feldes sind eher abstrakt, können aber über die Analogie gut verstanden werden. Darüber hinaus wird der Unterschied zwischen elektrischem und magnetischem Feld (z. B. elektrische und magnetische Monopole, Lenzsche Regel) in den Analogien sehr deutlich.




Langmuir-Taylor-Detektor

Ein Langmuir-Taylor-Detektor (nach Irving Langmuir und John Bellamy Taylor) ist ein Gerät zur Detektion von neutralen Atomen. Er kann daher zum Beispiel in einem Stern-Gerlach-Versuch oder beim Nachweis des Lamb-Shifts eingesetzt werden. Der zugrunde liegende physikalische Effekt wird auch Langmuir-Taylor-Effekt genannt. 

Der Langmuir-Taylor-Detektor basiert auf dem Tunneleffekt. Dazu wird ein Draht aus einem Metall mit hoher Austrittsarbeit (z. B. Wolfram) erhitzt. Kommt diesem Draht ein Atom nahe, dessen Ionisierungsenergie kleiner ist als die Austrittsarbeit des Drahtmaterials, so kann ein Valenzelektron dieses vormals neutralen Atoms in das Material mit hoher Austrittsarbeit tunneln. Wenn der Draht nun heiß genug ist, werden die positiven Ionen anschließend wieder vom Draht verdampft und können mit einer Beschleunigungsspannung abgesaugt werden. Der so entstehende Ionen-Strom ist proportional zur Anzahl der einfallenden neutralen Atome. Bei Alkali-Atomen können sogar so gut wie alle eintreffenden Atome ionisiert werden.

Prinzipiell ist auch ein Elektronenaustauch in Richtung des eintreffenden Atoms möglich (abhängig von der Elektronenaffinität), sodass negative Ionen entstehen. Der Anteil der ionisierten Teilchen hängt in beiden Fällen exponentiell vom Verhältnis der thermischen Energie formula_1 (mit der Boltzmann-Konstante formula_2 und der thermodynamischen Temperatur formula_3) zur Energiedifferenz (Austrittsarbeit vs. Ionisierungsenergie) der beiden Materialien ab. Außerdem sind Effekte wie eine mögliche Oxidation der Drahtoberfläche zu berücksichtigen.




Schrödinger-Newton-Gleichung

Die Schrödinger-Newton-Gleichung (auch Newton-Schrödinger- oder Schrödinger-Poisson-Gleichung) ist eine nichtlineare Modifikation der Schrödingergleichung unter Berücksichtigung des Newtonschen Gravitationsgesetzes. Dabei ergibt sich eine Selbstwechselwirkung, da die Wellenfunktion massebehaftet angenommen wird. Die Gleichung kann entweder als eine Integro-Differentialgleichung oder als ein Gleichungssystem bestehend aus Schrödinger- und Poissongleichung geschrieben werden.

Die Schrödinger–Newton-Gleichung wurde als erstes von Remo Ruffini und Silvano Bonazzola in Verbindung mit der Eigengravitation von Bosonensternen betrachtet.

Später wurde von Lajos Diósi und Roger Penrose
diskutiert, dass die Schrödinger-Newton-Gleichung eine Erklärung für den Kollaps der Wellenfunktion sein kann.
Dabei hat Materie Quanteneigenschaften, wohingegen die Gravitation eine klassische Theorie bleibt.

Außerdem wird die Schrödinger-Newton-Gleichung als Hartree-Approximation für die gegenseitige gravitative Anziehung in einem System mit einer großen Anzahl Teilchen verwendet.

Als Gleichungssystem geschrieben ergibt sich die Schrödinger-Newton-Gleichung aus der linearen Schrödingergleichung, erweitert um ein Gravitationspotential formula_1
hier ist formula_3 das nicht gravitative Potential; das Gravitationspotential formula_1 erfüllt die Poisson-Gleichung
Aufgrund der Kopplung der Wellenfunktion und des Gravitationspotentials und wegen des Terms formula_6 ist das Gleichungssystem nichtlinear.

Die Integro-Differentialform der Gleichung ist
Diese Gleichung ergibt sich aus dem oben angegebenen Gleichungssystem unter der Annahme, dass das Gravitationspotential im Unendlichen verschwindet.

Mathematisch gesehen ist die Schrödinger-Newton-Gleichung eine Hartree-Gleichung für den Fall formula_8. Die Gleichung hat viele Eigenschaften der linearen Schrödinger-Gleichung. Insbesondere bleibt die totale Wahrscheinlichkeit sowie die Energie erhalten; weiterhin ist die Gleichung invariant bezüglich einer Galilei-Transformation.
Lösungen der Schrödinger-Newton-Gleichung wurden bereits analytisch und numerisch untersucht; die stationäre Gleichung, die sich durch Separation der Variablen ergibt, hat eine unendliche Menge von Lösungen, von denen lediglich der stationäre Grundzustand stabil ist.

Die Schrödinger-Newton-Gleichung ergibt sich aus der Annahme, dass die Gravitation sich auch auf fundamentaler Ebene klassisch verhält und dass die Wellenfunktion massebehaftet ist. Effekte der Allgemeinen Relativitätstheorie werden dabei vernachlässigt. Für den Fall, dass die Annahme korrekt ist, ist die Schrödinger-Newton-Gleichung eine fundamentale Gleichung für ein einzelnes Teilchen; eine Verallgemeinerung auf Mehrteilchensysteme wird weiter unten beschrieben. Für den Fall, dass die Annahme nicht korrekt ist, ist die Schrödinger-Newton-Gleichung lediglich eine Näherung für die gravitative Anziehung in einem System mit einer großen Anzahl von Teilchen.

Für den Fall, dass die Schrödinger-Newton-Gleichung eine fundamentale Gleichung ist, existiert eine entsprechende Gleichung für Mehrteilchensysteme, die von Diósi analog zur Einteilchengleichung unter der Annahme semi-klassischer Gravitation abgeleitet wurde:
Das Potential formula_10 enthält alle gegenseitigen linearen Wechselwirkungen, z. B. das Coulomb-Potential, wohingegen das Gravitationspotential sich aus der Masseverteilung aller Teilchen ergibt.

Bei einer Born-Oppenheimer-Näherung kann die formula_11-Teilchen-Gleichung separiert werden. Eine Gleichung beschreibt die relative Bewegung, die andere beschreibt die Dynamik des Schwerpunkts der Wellenfunktion. Für die relative Bewegung spielt die gravitative Wechselwirkung nur eine geringe Rolle, das sie üblicherweise schwach im Vergleich zu den anderen Wechselwirkungen ist. Sie hat aber einen signifikanten Einfluss auf die Bewegung des Schwerpunkts.

Eine grobe Bestimmung der Größen, bei der sich Unterschiede zwischen der Schrödinger-Gleichung und der Schrödinger-Newton-Gleichung ergeben, ist durch Einsetzen einer Gauß-Verteilung möglich.

Für eine radialsymmetrische Gauß-Verteilung
hat die lineare Schrödingergleichung die Lösung
Das Maximum der Wahrscheinlichkeitsdichte formula_14 befindet sich bei
Für die Beschleunigung, das heißt die zweite Ableitung nach der Zeit formula_16, erhält man hieraus nach kurzer Rechnung
Dies wird mit der Beschleunigung durch die Gravitation
verglichen. Zur Zeit formula_19 ist formula_20 und die Gleichsetzung der Beträge der Beschleunigungen in diesem Abstand ergibt formula_21 und damit
Diese Gleichung erlaubt es, mit formula_23 eine kritische Abmessung für eine gegebene Masse zu bestimmen, und umgekehrt.

Numerische Berechnungen zeigen, dass diese Gleichung eine gute Abschätzung des Parameterbereichs ergibt, bei dem gravitative Einflüsse signifikant werden.

Für ein Wasserstoffatom (formula_24 = atomare Masseneinheit) beträgt die kritische Größe ungefähr 10 Meter; bei einem Teilchen mit einer Masse von einem Mikrogramm erhält man 10 Meter. Im Bereich von 10 atomaren Masseneinheiten liegt die kritische Größe im Bereich von Mikrometern, so dass möglicherweise in Zukunft eine experimentelle Prüfung der Schrödinger-Newton-Gleichung möglich ist.

Die Idee, dass Gravitation den Kollaps der Wellenfunktion hervorruft (oder zumindest beeinflusst), wurde schon in den 1960er Jahren von Károlyházy vorgeschlagen.

Als mathematische Beschreibung wurde in diesem Zusammenhang die Schrödinger–Newton-Gleichung von Diósi vorgeschlagen.

Roger Penrose diskutierte, dass eine Superposition von zwei oder mehr Quantenzuständen, welche sich signifikant in der Masseverteilung unterscheiden, instabil ist und daher in einen der Zustände übergeht. Seine Hypothese ist, dass es eine bevorzugte Menge von Zuständen (die stationären Zustände der Schrödinger-Newton Gleichung) gibt, die nicht weiter kollabieren, sondern stabil sind. Ein makroskopisches, massives System kann sich daher niemals in einer Superposition von Zuständen befinden, da die nichtlineare gravitative Selbstwechselwirkung sofort zu einem Kollaps in einen stationären Zustand der Schrödinger-Newton-Gleichung führt. Nach Penrose's Auffassung führt eine Messung eines Quantensystems einerseits zu einer Verschränkung mit der makroskopischen Umgebung und damit zur Dekohärenz, gleichzeitig führt die Verschränkung mit dem massiven Messsystem durch die gravitative Selbstwechselwirkung zur Reduktion zu einem bestimmten (dem gemessenen) Zustand.

Es existieren drei grundsätzliche Probleme bei der Interpretation der Schrödinger-Newton-Gleichung als Ursache für den Kollaps der Wellenfunktion.

Numerische Simulationen zeigen, dass beim „Kollaps“ der Wellenfunktion zu einer stationären Lösung ein kleiner Teil der Wellenfunktion zum Unendlichen strebt. Dies würde bedeuten, dass auch im Fall eines komplett reduzierten Zustands ein Teilchen mit einer geringen Wahrscheinlichkeit an einem entfernten Ort gemessen werden kann. Die Schrödinger-Newton-Gleichung kann damit nur teilweise als Erklärung herangezogen werden und der Effekt der Umgebung durch Dekohärenz muss berücksichtigt werden.

Ein zweites Problem ist, dass die Bornsche Wahrscheinlichkeitsinterpretation nicht erklärt wird. Zur Lösung des Messproblems ist es nicht ausreichend, dass ein Kollaps der Wellenfunktion auftritt. Es muss auch erklärt werden, dass die Wahrscheinlichkeitsdichte, mit der ein Teilchen an einem bestimmten Ort gemessen wird, sich durch das Betragsquadrat der Wellenfunktion berechnen lässt. Es ist unklar, ob sich bei genauerer Analyse zeigen lässt, dass sich diese Wahrscheinlichkeitsdichte einstellt.

Ein letztes Problem ergibt sich durch die Interpretation der Wellenfunktion als reales physikalisches Objekt. Damit kann die Wellenfunktion eine Größe sein, die zumindest im Prinzip gemessen werden kann. Durch die nichtlokale Natur der Wellenfunktion könnte es daher möglich sein, Information mit Überlichtgeschwindigkeit zu übertragen, was im Widerspruch zur Relativitätstheorie steht. Es ist unklar, ob dieses Problem bei genauerer Betrachtung tatsächlich relevant ist.




Peter Luger

Peter Luger (* 2. Juli 1943 in Berlin) ist ein deutscher Chemiker (Anorganische Chemie) und Kristallograph.

Luger studierte an der FU Berlin mit der Promotion 1970 (Röntgenographische Untersuchungen an Cyclohexanderivaten) und der Habilitation 1974. Dort ist er seit 1979 Professor. 

1985 war er am Brookhaven National Laboratory und 1995 an der University of Okayama.

Er befasst sich mit der Strukturbestimmung kleiner organischer und anorganischer Moleküle mit Röntgen-, Synchrotron- und Neutronenstrahlung, insbesondere bei tiefen Temperaturen (Einkristall-Beugungsexperimente bei 15 Kelvin).

2015 erhielt er die Carl-Hermann-Medaille. 





William Mitchinson Hicks

William Mitchinson Hicks, FRS (* 23. September 1850 in Launceston, Cornwall; † 17. August 1934 in Crowhurst, Sussex) war ein britischer Mathematiker und Physiker.

Hicks wurde zuerst an einer Privatschule in Devonport unterrichtet. 1870 ging er zur weiteren Ausbildung nach Cambridge, wo er 1873 siebter in den Tripos-Prüfungen wurde. 1874 wurde er eines der ersten Mitglieder des Cavendish Laboratory unter James Clerk Maxwell. 1876 wurde er Fellow des St. John's College. 1892 bis 1897 war er Principal des Firth College in Sheffield, das 1897 im University College Sheffield aufging, und danach dessen Principal. Als daraus 1905 die Universität Sheffield wurde, war er deren Vizekanzler.

Hicks befasste sich mit Wirbeln in Flüssigkeiten. Nach einer Idee von Lord Kelvin (1867) galten damals Wirbelringe im Äther, den man sich als Flüssigkeit vorstellte, als mögliche Modelle von Atomen. Viele bedeutende theoretische Physiker in Großbritannien arbeiteten an dieser Theorie, zum Beispiel der spätere Nobelpreisträger J. J. Thomson in seinem Adams-Preis Essay von 1882, der die Theorie auch in der Chemie anwandte. Hicks bewies, dass hohle Wirbelringe Lösungen der hydrodynamischen Gleichungen waren und Vortex-Filamente. Noch 1895 gab Hicks einen optimistischen Bericht über die Vortextheorie als grundlegender Theorie der Physik vor der British Association.

Nachdem Joseph Larmor 1900 sein Buch "Aether and Matter" veröffentlicht hatte und darin den negativen Ausgang des Michelson-Morley-Experiments durch Längenkontraktion bei Bewegung im Äther erklärt hatte, die bei ihm Folge seiner eigenen elektrodynamischen Theorie war, wurde dies von Hicks kritisiert, indem er die Aussagefähigkeit des Michelson-Morley-Experiments und dessen Erklärung durch Längenkontraktion bestritt. Das war der Grund, warum Larmor eine Bestätigung durch ein anders aufgebautes Experiment anstrebte und das Trouton-Noble-Experiment anregte.

Hicks erhielt 1912 die Royal Medal für seine Forschungen in der Mathematischen Physik („On the ground of his researches in mathematical physics“) und 1920 den Adams-Preis. 1885 wurde er Fellow der Royal Society. Das "Hicks Building" der Universität Sheffield, in der unter anderem die Physik und Mathematik untergebracht sind, ist ihm zu Ehren benannt.





Ole Krogh Andersen

Ole Krogh Andersen (* 11. Mai 1942 in Kopenhagen) ist ein dänischer Physiker und ehemaliger Direktor am Max-Planck-Institut für Festkörperforschung in Stuttgart.

Ole Krogh Andersen studierte Elektrotechnik und Festkörperphysik und erhielt 1969 seine Promotion an der Technischen Universität Dänemark. Im selben Jahr wurde er Research Associate an der University of Pennsylvania. 1970 wurde er Adjunct Professor an der Universität Kopenhagen, zwei Jahre später Lektor an der Technischen Universität Dänemark. 1978 wurde er Gastprofessor an der Universität Genf, im gleichen Jahr wurde er Direktor und Wissenschaftliches Mitglied am Max-Planck-Institut für Festkörperforschung. Seit 1982 ist er Honorarprofessor an der Universität Stuttgart. Im Jahre 2012 wurde er emeritiert.

Ole Krogh Andersen leistete einflussreiche Beiträge zur numerischen Berechnung der elektronischen Bandstruktur von Festkörpern im Rahmen der Dichtefunktionaltheorie (DFT). Insbesondere entwickelte er die sogenannten linearen Methoden "Linear Augmented Plane Waves" (LAPW) und "Linear Muffin Tin Orbitals" (LMTO). Auch die sogenannte "Tetraeder-Methode" geht auf ihn und Ove Jepsen zurück. Für stark korrelierte elektronische Materialien schlug er mit Vladimir I. Anisimov und Jan Zaanen die "DFT+U-Methode" vor. Diese Methoden wandten er und seine Mitarbeiter auf eine Vielzahl von Materialien an, wie zum Beispiel Hochtemperatursupraleiter und andere Metalloxide. Seine etwa 250 wissenschaftlichen Veröffentligungen wurden bis 2019 laut Google Scholar 46,000 Mal zitiert. 

1978 erhielt Ole Krogh Andersen den Brinch Prize der Dänischen Physikalischen Gesellschaft. Im selben Jahr wurde er Wissenschaftliches Mitglied der Max-Planck-Gesellschaft. Zwei Jahre später erhielt er den Hewlett-Packard Europhysics Prize der European Physical Society. 1982 wurde er Ausländisches Mitglied der Königlich Dänischen Akademie der Wissenschaften und Honorarprofessor für Physik an der Universität Stuttgart. 1992 wurde er Ausländisches Mitglied der Ukrainian Academy of Sciences, ein Jahr später wurde er zum Fellow der American Physical Society ernannt. 2002 wurde er mit der Ernst-Mach-Ehrenmedaille der Tschechischen Akademie der Wissenschaften ausgezeichnet. 2008 wurde ihm die Ehrendoktorwürde der Universität Uppsala verliehen.




Alexander Leonidowitsch Tschischewski

Alexander Leonidowitsch Tschischewski (; * in Ciechanowiec, Gouvernement Grodno; † 20. Dezember 1964 in Moskau) war ein sowjetischer Biophysiker, Dichter und Maler.

Tschischewskis Vater Leonid Wassiljewitsch Tschischewski (1861–1929) war Artillerieoffizier und Erfinder (Winkelmessung aus verdeckter Position, Zerstörung von Drahtzäunen). Tschischewskis Mutter Nadeschda Alexandrowna geborene Neviandt (1875–1898) war die Schwester des Dragoneroffiziers und späteren Staatsduma-Abgeordneten Konstantin Alexandrowitsch Neviandt, Nichte des Generalmajors Alexei Petrowitsch Delssal und Cousine des Generals Pjotr Alexejewitsch Delssal. Nach dem frühen Tod der Mutter wuchs Tschischewski bei seiner Tante Olga Wassiljewna Tschischewskaja-Lesli (1863–1927) und seiner Großmutter Jelisaweta Semjonowna Tschischewskaja geborene Oblatschinskaja (1828–1908, Großnichte Admirals Pawel Stepanowitsch Nachimow) auf.

Tschischewski erhielt eine häusliche Erziehung mit Unterricht in Fremdsprachen, Geschichte und Musik. Im Alter von 7 Jahren bekam er Privatstunden bei Edgar Degas' Studenten Gustave Nodier in der Pariser Académie des Beaux-Arts. 1907 trat er in das Jungengymnasium Białystok ein. Nach Versetzung seines Vaters in die Festung "Zegrze" bei Serock erhielt Tschischewski wieder Hausunterricht. Von Januar 1914 bis April 1915 besuchte er in Kaluga die private Schachmagonow-Realschule für den Schulabschluss. Er beherrschte Französisch, Deutsch, Englisch und Italienisch. In Kaluga entstand der enge Kontakt mit Konstantin Eduardowitsch Ziolkowski, woraus sich später eine ständige wissenschaftliche Kooperation und gegenseitige Unterstützung entwickelte.

Im Juli 1915 wurde Tschischewski als Student im Moskauer Handelsinstitut angenommen. Im September 1915 wechselte er als Gasthörer zum Moskauer Archäologischen Institut. Jedoch ging er als Freiwilliger an die Front und nahm in der zweiten Jahreshälfte 1916 und von Mai bis September 1917 an den Kämpfen in Galizien teil. Nach Verwundung wurde er demobilisiert und erhielt das Georgskreuz IV. Klasse.

1917 schloss Tschischewski das Studium am Moskauer Archäologischen Institut ab und verteidigte im Mai desselben Jahres seine Dissertation über die russische Lyrik des 18. Jahrhunderts und Michail Wassiljewitsch Lomonossow. Im Dezember desselben Jahres verteidigte er seine Dissertation über die Evolution der physikalisch-mathematischen Wissenschaften in der Antike für die Promotion zum Magister der Allgemeinen Geschichte. Er war nun wissenschaftlicher Mitarbeiter des Moskauer Archäologischen Instituts.

1918, nach der Oktoberrevolution, verteidigte Tschischewski in der historisch-philologischen Fakultät der Universität Moskau seine Dissertation über die Periodizität der weltweiten historischen Prozesse für die Promotion zum Doktor der Allgemeinen Geschichte Er war Gasthörer an der physikalisch-mathematischen Fakultät in der naturwissenschaftlichen Abteilung und an der medizinischen Fakultät der Universität Moskau und hörte Vorlesungen an der Städtische Moskauer Schanjawski-Volksuniversität.

1918 ging Tschischewski nach Kaluga und gab Unterricht bei den von seinem Vater gegründeten und dann geleiteten Kursen der Roten Infanteriekommandeure (bis 1920) und danach an der 4. Sowjetischen Arbeitereinheitsschule (bis 1921). Bereits 1915 hatte sich Tschischewski im Hause seines Vaters mit der Heliobiologie beschäftigt, die sich mit dem Einfluss der Sonne auf die Biosphäre befasst. Ab 1918 führte er drei Jahre lang erste Experimente zur Untersuchung der Effekte ionisierter Luft auf lebende Organismen durch. Nach seinen Ergebnissen wirkten positiv geladene Luftionen negativ auf lebende Organismen, während negativ geladene Luftionen positiv wirkten. Darüber berichtete er in der Kalugaer Naturkundegesellschaft. Über diesen Bericht kam er in Kontakt mit dem Physiker Pjotr Petrowitsch Lasarew. Im Dezember 1921 verfasste er eine Arbeit über den Anfang des Weltalls, das System des Kosmos und dessen Probleme. Er gehörte zu den Vertretern des Kosmismus.

In Kaluga betätigte sich Tschischewski auch als Landschaftsmaler und verkaufte mehr als 100 Bilder, meist Aquarelle, zur Finanzierung seiner Experimente. Dazu veröffentlichte er seine Gedicht, die er seit seiner Kindheit verfasst hatte, sowie Übersetzungen von Gedichten beispielsweise von Ludwig Uhland. Er wurde auf Empfehlung Anatoli Wassiljewitsch Lunatscharskis Instrukteur der Literaturabteilung des Volkskommissariats für Bildung der RSFSR, worauf er zum Vorsitzenden der Dichterunion des Gouvernements Kaluga gewählt wurde.

1921 wurde Tschischewski Professor am Moskauer Archäologischen Institut. Von 1922 bis 1923 war Tschischewski außerplanmäßiger wissenschaftlicher Berater des Instituts für Physik und Biophysik des Volkskommissariats für Gesundheit der UdSSR, wo er mit Sergei Iwanowitsch Wawilow bekannt wurde. Zu Tschischewskis Bekannten zählten auch die Literaten Leonid Nikolajewitsch Andrejew, Alexander Iwanowitsch Kuprin, Alexei Nikolajewitsch Tolstoi, Igor Sewerjanin, Sergei Alexandrowitsch Jessenin, Wladimir Wladimirowitsch Majakowski, Iwan Alexejewitsch Bunin, Maxim Gorki und Waleri Jakowlewitsch Brjussow. Mit dem Komponisten Nikolai Petrowitsch Rakow war Tschischewski befreundet. Er besuchte den literarischen Salon von A. I. Holmberg (Lew Nikolajewitsch Tolstois Enkelin) und die Musikabende von T. F. Dostojewskaja (Fjodor Michailowitsch Dostojewskis Großnichte).

1924 erschien in Kaluga Tschischewskis grundlegendes Werk über die physikalischen Faktoren der historischen Prozesse. Im selben Jahr wurde er in Moskau wissenschaftlicher Senior-Mitarbeiter (im Rang eines Professors) im Laboratorium für praktische Tierpsychologie der Hauptwissenschaftsverwaltung des Volkskommissariats für Bildung der RSFSR. Hier führte er Experimente zur Untersuchung der biologischen und physiologischen Wirkung von Luftionen auf Tiere durch. Dazu konstruierte er 1927 einen speziellen Ionisator. Er stand in Kontakt mit Svante Arrhenius, Fridtjof Nansen, Charles Richet und Jacques-Arsène d’Arsonval. Er wurde zu Vorlesungen in Paris und New York eingeladen. Seine Rechte an seinen Erfindungen und Entdeckungen verkaufte er nicht an ausländische Interessenten, sondern überließ sie der sowjetischen Regierung.

1931 wurde Tschischewski Leiter des auf Veranlassung des Volkskommissars für Landwirtschaft Jakow Arkadjewitsch Jakowlew gegründeten Zentralen Forschungslaboratoriums für Ionifizierung (ZNILI) im Forschungsinstitut für Tierzucht der Sowjetischen Akademie für Landwirtschaftswissenschaften in Woronesch. Dazu gründete er eine Versuchsstation beim Geflügeltrust. Die Arbeitsergebnisse wurden regelmäßig veröffentlicht und auch in andere Sprachen übersetzt. Nach Kritik dieser Arbeiten insbesondere von dem Biologen Boris Michailowitsch Sawadowski, der Tschischewski als Scharlatan bezeichnete, verbot das Volkskommissariat für Landwirtschaft im Januar 1933 weitere Veröffentlichungen und entließ 1936 Tschischewski mit Auflösung des ZNILI. Abram Fjodorowitsch Joffe stellte 1940 in seinem Untersuchungsbericht fest, dass Tschischewskis Hypothesen auf unbewusst oder bewusst gefälschten Versuchsergebnissen beruhten, zumal ihm die physikalischen und biologischen Grundlagenkenntnisse für die nötigen wissenschaftlichen Überprüfungen fehlten, und dass seine Entlassung als wissenschaftlicher Leiter gerechtfertigt sei.

Ende 1938 wurde Tschischewski wieder beschäftigt als wissenschaftlicher Leiter für Luftionifizierung des geplanten Palasts der Sowjets. 1939–1941 leitete er die beiden Laboratorien für Luftionifizierung am Lehrstuhl für allgemeine und experimentelle Hygiene des 3. Moskauer Staatlichen Medizin-Instituts und am Leningrader Pädagogischen. Institut

1941 zog Tschischewski zu Beginn des Deutsch-Sowjetischen Krieges mit seiner Familie nach Tscheljabinsk. Am 22. Januar 1942 wurde er verhaftet und am 20. März 1943 nach Artikel 58 des Strafgesetzbuches der RSFSR zu 8 Jahren Lagerhaft verurteilt, die er in Lagern im Ural und Kasachstan und schließlich im StepLag verbrachte. In den Lagern arbeitete er wissenschaftlich, dichtete mehr als 100 Gedichte und malte. Im KarLag richtete er ein Kabinett für Luftionifizierung ein. Nach seiner Entlassung aus der Lagerhaft im Januar 1950 wurde er in einer Siedlung in Karaganda untergebracht, die er im Juni 1954 verlassen konnte. Er lebte nun in Karaganda und arbeitete als Berater für Lufttherapie und Leiter eines Laboratoriums für Blutanalyse im Klinikum der Oblast Karaganda. Nach seiner Rückkehr nach Moskau arbeitete er 1958–1961 bei der Sanitärtechnikfirma "SojusSanTechnik" als Berater für Lufttherapie und Laboratoriumsleiter. Seine Arbeiten in Karaganda wurden nun veröffentlicht. 1962 wurde er teilweise rehabilitiert (und vollständig erst nach seinem Tode). In seinen letzten Jahren arbeitete er an seinen Erinnerungen an die Jahre der Freundschaft mit Ziolkowski, dessen Tochter Marija Konstantinowna Ziolkowskaja-Kostinaja er wiederholt in Kaluga besuchte.

Tschischewskis Namen trägt der 1978 von Nikolai Stepanowitsch Tschernych entdeckte Asteroid (3113) Chizhevskij. 2012 wurde vor dem Gebäude der Staatlichen Pädagogischen Ziolkowski-Universität Kaluga ein Tschischewski-Denkmal aufgestellt. 2013 wurde auf der "European Space Weather Week" Dr. Gaël Cessateur mit der "International Alexander Chizhevsky medal for Space Weather and Space Climate" ausgezeichnet.




Libration

In der Astronomie bezeichnet Libration eine echte oder scheinbare Taumelbewegung eines Mondes, gesehen von seinem Zentralkörper.

Fast alle größeren Monde des Sonnensystems befinden sich in einer gebundenen Rotation um ihren Zentralplaneten, das heißt, sie drehen sich während eines Umlaufs um den Planeten auch einmal um die eigene Achse. Deshalb wenden diese Monde ihrem Planeten im Prinzip immer dieselbe Seite zu. Da die Monde allerdings nicht auf exakten Kreisbahnen mit konstanter Winkelgeschwindigkeit ihre Planeten umkreisen, während die Eigenrotation eine konstante Winkelgeschwindigkeit aufweist, und da sich ein Beobachter auf dem Planeten nicht exakt auf der Verbindungslinie der Massenzentren befinden muss, sieht der Beobachter im Laufe eines „Monats“ nicht immer "exakt" dieselbe Seite des Mondes. Beim Erdmond sind durch die verschiedenen Effekte, die zu dieser Taumelbewegung führen, von der Erdoberfläche aus im Laufe der Zeit insgesamt 59 Prozent der Mondoberfläche zu sehen.

Man unterscheidet folgende Arten der Libration, hier am Beispiel des Erdmondes:


Die optische Libration lässt sich in guter Näherung aus den himmelsmechanischen Eigenschaften des Erde-Sonne-Mond-Systems berechnen. Vernachlässigt man in erster Näherung den Einfluss der Sonne, so erhält man aus der Lösung des Zweikörperproblems Erde-Mond folgende Werte:
Die großen Störungen des Erde-Mond-Systems vornehmlich durch die Sonne bewirken zusätzliche Abweichungen, deren wichtigste die folgenden sind:





François Arago

Dominique François Jean Arago (; * 26. Februar 1786 in Estagel bei Perpignan; † 2. Oktober 1853 in Paris) war ein französischer Astronom, Physiker und Politiker.

François Aragos Vater war François "Bonaventure" Raymond Arago (1754–1814), der Bürgermeister von Estagel, einer kleinen Stadt etwa 18 km nordwestlich von Perpignan, und seine Mutter war Marie Anne Agathe Roig (1755–1845). Die Familie Arago kam aus den östlichen Pyrenäen. François Aragos Eltern hatten elf Kinder, sechs Jungen und fünf Mädchen. François Aragos Brüder waren Jean Martin Arago (1788–1836), er kämpfte in den Reihen der Rebellen während des mexikanischen Unabhängigkeitskriegs und hatte eine militärische Laufbahn eingeschlagen; Jacques Étienne Victor Arago (1790–1854), ein Schriftsteller und Entdecker, so nahm er an einer Forschungsreise der "l'Uranie" unter der Leitung von Louis Claude Desaulces de Freycinet (1779–1842) teil, um magnetische und ozeanographische Untersuchungen im Pazifik durchzuführen und Etienne Vincent Arago (1802–1892), Schriftsteller, Dramatiker und Politiker, dann noch Victor Arago (1792–1867), "Officier de la Légion d'Honneur", Joseph Honoré José Arago (1796–1860). Ferner noch die Schwestern Marie Rose Arago (1779–1780), Marie Thérèse Arago (1780–1780), Rose Rosette Arago (1782–1832), Marie Victoire Françoise Arago (1783–1783) und Marguerite Guidette Arago (1798–1859).

François Arago wurde mit siebzehn Jahren zur École polytechnique in Paris zugelassen. Nach dem Studium holte ihn Pierre Simon de Laplace an die Pariser Sternwarte. 1805 wurde er Sekretär am Bureau des Longitudes. In dessen Auftrag führte er ab 1806 unter großen Schwierigkeiten und Gefahren während des spanischen Aufstands gegen Napoleon gemeinsam mit Jean-Baptiste Biot die Meridianmessungen von Pierre Méchain auf dem Meridian von Paris in Spanien und auf den Balearen zu Ende. Er berichtet darüber in seinen autobiographischen Aufzeichnungen "Geschichte meiner Jugend". 1808 wurde Arago auf Mallorca wegen des in der damaligen Lage extrem verdächtigen Entzündens eines Leuchtfeuers auf einer Bergspitze festgenommen, konnte jedoch entkommen. Biot war schon vorher nach Frankreich zurückgekehrt. Erst 1809 kehrte Arago nach einer weiteren Gefangennahme nach Frankreich zurück und wurde im selben Jahr – 23 Jahre alt – zum Mitglied der Pariser Akademie der Wissenschaften gewählt und zum Professor für Geodäsie und analytische Geometrie an der "École polytechnique".
Er wird in den "Mémoires de Physique et de Chimie de la Société d’Arcueil" als Mitglied der Société d’Arcueil erwähnt.

Arago hatte mit Lucie Carrier Besomes (1788–1829) – beide waren seit dem 11. August 1811 miteinander verheiratet – insgesamt drei Söhne. So den François Victor Emmanuel Arago (1812–1896), dieser war während der Zweiten Französischen Republik ("Deuxième République") in verschiedenen politischen Positionen, so etwa als Justizminister, tätig. Gefolgt von Alfred Arago (1815–1892) und dem jüngsten Sohn Gabriel Arago (1816–1832).

Seine letzte Ruhestätte fand er auf dem Pariser Friedhof Père Lachaise (Division 4).

In Paris lernte Alexander von Humboldt 1809 François Arago kennen, mit dem er alsbald sowohl menschlich als auch wissenschaftlich eine besondere Freundschaft pflegte. Beide waren an dem Phänomen Erdmagnetismus interessiert.

Ausgehend von den Doppelspaltexperimenten zur Interferenz von natürlichem (unpolarisiertem) Licht von Thomas Young, führte Arago zusammen mit Augustin Jean Fresnel um 1817 verschiedene Experimente zur Interferenz von polarisiertem Licht durch. Ziel war es, weitere Erkenntnisse zur Natur des Wellencharakers von Licht zu gewinnen (nach der sogenannten Undulationstheorie). Dabei gingen sie am Anfang von der Annahme aus, Licht sei eine Longitudinalwelle. Ihre Ergebnisse waren jedoch nicht mit dem Verhalten longitudinaler Schwingungen vereinbar. Zusammen mit Young, dem Arago die Ergebnisse zur Diskussion zusandte, kamen sie abschließend zu der Erkenntnis, dass es sich bei Licht um zwei senkrecht zueinander orientierte Transversalwellen handeln muss. Fresnel und Arago fassten ihre Ergebnisse in vier Aussagen zur Interferenz von polarisiertem Licht zusammen, den Fresnel-Arago-Gesetzen.

Alexis Thérèse Petit (1791–1820) heiratete eine Schwester von Arago, beide führten gemeinsam Experimente zur Lichtbrechung und dem Einfluss der Temperatur auf den Brechungsindex in Gasen durch. Zusammen mit diesem fand er 1816 das Gesetz, nach dem zwei in einer Ebene polarisierte Strahlen miteinander interferieren können, zueinander senkrecht polarisierte Strahlen jedoch nicht.

1818 wurde von der französischen Académie des sciences ein Wettbewerb ausgeschrieben, dessen Jury von Arago geleitet wurde. Der 30-jährige Augustin-Jean Fresnel beteiligte sich mit einer neuartigen Arbeit über Wellenoptik. Nachdem Siméon Denis Poisson die Vorhersagen der Theorie mit seinem scheinbar abwegigen Gedankenexperiment zur Entstehung von Poisson-Flecken angezweifelt hatte, konnte Arago diesen Effekt experimentell nachweisen. In der englischsprachigen Fachliteratur wird der Fleck daher auch als "Arago spot" bezeichnet.

1820 bemerkte er die Magnetisierung von Eisen (Stahlnadeln) durch einen stromdurchflossenen Leiter. 1824 führte er ein Experiment durch, bei dem eine frei rotierbare Magnetnadel über einer rotierenden Kupferscheibe schwebt und durch die Rotation der Scheibe ebenfalls in Bewegung gesetzt wird. Michael Faraday wies einige Jahre später nach, dass es sich hierbei um die Induktion handelt. Arago nannte dieses Phänomen "Rotationsmagnetismus". Es war auch der erste Versuch, der den Wirbelstrom nachweisen konnte.

Seit 1830 war Arago Direktor der Pariser Sternwarte und nutzte hier alle modernen Möglichkeiten der Astronomie und Physik. Als erster führte er die Szintillation der Sterne auf Interferenzen in der Erdatmosphäre zurück, die aufgrund der Luftunruhe entstehen. Im Dezember 1829 wurde er Ehrenmitglied der Russischen Akademie der Wissenschaften in Sankt Petersburg. 1832 wurde Arago in die American Academy of Arts and Sciences gewählt. Ab 1835 war er auswärtiges Mitglied der Göttinger Akademie der Wissenschaften.

Am 19. August 1839 stellte Arago die Erfindung der Fotografie durch Louis Daguerre und Joseph Nicéphore Nièpce offiziell der französischen Akademie der Wissenschaft und der Öffentlichkeit vor. Zuvor hatte die Deputiertenkammer und das Haus der Pairs einem Gesetz zugestimmt, wonach die Rechte an der Erfindung vom französischen Staat aufgekauft und als Geschenk der Welt zur Verfügung gestellt wurden. Daguerre erhielt eine lebenslange, monatliche Rente von 6.000 Francs und Isidore Nièpce, der Sohn von Joseph Nicéphore Nièpce, eine solche Rente von 4.000 Francs.

Am 31. Mai 1842 wurde er als ausländisches Mitglied in den preußischen Orden Pour le Mérite für Wissenschaften und Künste aufgenommen. Bereits seit 1815 war er Ehrenmitglied ("Honorary Fellow") der Royal Society of Edinburgh und seit 1828 auswärtiges Mitglied der Königlich-Preußischen Akademie der Wissenschaften. 1843 wurde er zum auswärtigen Mitglied der Bayerischen Akademie der Wissenschaften berufen. 1850 wurde nach ihm das Cape Arago in Oregon benannt. Er ist ferner namentlich auf dem Eiffelturm verewigt, siehe: Die 72 Namen auf dem Eiffelturm. Auf dem Meridian von Paris wurden 1995 vom niederländischen Konzeptkünstler Jan Dibbets bronzene "Arago-Medaillons" in Straßenpflaster, Bürgersteige, Höfe und verschiedene Gebäude eingelassen. Von den ursprünglich 135 bisher wenig beachteten Plaketten sind einige seit der Veröffentlichung von Dan Browns Bestseller "Sakrileg" gestohlen worden.

Der Arago-Gletscher in der Antarktis, der Asteroid (1005) Arago, der Mondkrater Arago und ein Marskrater wurden nach ihm benannt.

Nach der Julirevolution von 1830 hatte sich Arago in der Politik engagiert. Als republikanischer Abgeordneter der "Pyrénées-Orientales" redete er während der "Juli-Monarchie" in der Abgeordnetenkammer, insbesondere zu Fragen der Seefahrt, des Kanalbaus und der Eisenbahnen. Nach der Revolution von 1848 holte ihn Alphonse de Lamartine in die provisorische Regierung als Kriegs- und Marineminister. Zusammen mit seinem Untersekretär Victor Schoelcher spielte er eine ausschlaggebende Rolle bei der endgültigen Abschaffung der Sklaverei in Frankreich. Für anderthalb Monate war er Vorsitzender der Exekutivkommission der Republik, bevor die Versammlung den General Louis-Eugène Cavaignac an die Spitze der Macht stellte. Er weigerte sich, sich Napoleon III. nach dessen Staatsstreich 1851 anzuschließen und legte seine öffentlichen Ämter nieder.







Clinton Davisson

Clinton Joseph Davisson (* 22. Oktober 1881 in Bloomington, Illinois; † 1. Februar 1958 in Charlottesville, Virginia) war ein US-amerikanischer Physiker und Nobelpreisträger.

Clinton Joseph Davisson wurde am 22. Oktober 1881 als Sohn des Handwerkers "Joseph Davisson" und der Lehrerin "Mary Calvert" in Bloomington im US-Bundesstaat Illinois geboren. Er begann im September 1902, nach dem Abschluss der High School in Bloomington, ein Studium der Mathematik und Physik an der University of Chicago, musste jedoch das Studium nach einem Jahr aus finanziellen Gründen abbrechen und nahm eine Stelle bei einer Telefongesellschaft in Bloomington an. Auf Empfehlung von Millikan, den er während seines Jahres in Chicago kennengelernt hatte, erhielt er im Januar 1904 eine Assistentenstelle an der Purdue University, von Juni 1904 bis August 1905 konnte er sein Studium in Chicago fortsetzen. Im September 1904 wurde er, erneut auf Empfehlung von Millikan, auf eine Teilzeitstelle als Physikausbilder an der Princeton University berufen, die er bis 1910 innehatte. Während seiner Freizeit besuchte er Vorlesungen bei Francis Magie, Edwin Plimpton Adams, James Jeans und Owen Willans Richardson. In den Sommersemestern besuchte er mehrmals die Vorlesungen in Chicago und erhielt dort 1908 den Bachelor of Science. 1910/11 erhielt er ein Stipendium für Physik an der Princeton University und promovierte dort 1911 unter Professor Richardson "über die thermische Emission positiver Ionen der Erdalkalisalze". Er war von September 1911 bis April 1917 Ausbilder im Physik-Department am Carnegie Institute of Technology in Pittsburgh, im Juni 1917 nahm er für die Zeit des Krieges eine Stelle in der Ingenieurabteilung der "Western Electric Company" (den späteren Bell Telephone Laboratories) in New York City an, nachdem er zuvor von der US-Army abgelehnt worden war. Nach Kriegsende lehnte er eine Assistenzprofessur am Carnegie Institute of Technology ab und blieb bei Western Electric.
Er wurde 1946 bei den Bell Telephone Laboratories nach 29 Dienstjahren pensioniert und war von 1947 bis 1949 Visiting Professor für Physik an der University of Virginia in Charlottesville, Virginia.

Davisson heiratete 1911 "Charlotte Sara Richardson", die Schwester seines Doktorvaters Professor Richardson, und hatte vier Kinder, drei Söhne und eine Tochter. Er starb am 1. Februar 1958 in Charlottesville.

Davisson erhielt 1937 den Physik-Nobelpreis für die experimentelle Bestätigung der von de Broglie vorhergesagten Materiewellen, die ihm 1926 zusammen mit Lester Germer durch den Nachweis der Diffraktion von Elektronen an Kristallen gelungen war. LEED ist heute eine wichtige analytische Methode in der Oberflächenchemie. Die zweite Hälfte des Preises ging an George Paget Thomson.






Kristallisation

Als Kristallisation bezeichnet man den physikalischen Vorgang der Verhärtung bei der Bildung und beim Wachstum von Kristallen. Bei diesem Prozess wird Kristallisationswärme freigesetzt. Bei der Züchtung von Kristallen werden künstliche Bedingungen geschaffen, unter denen die Kristallisation beschleunigt ablaufen kann.

Kristalle können in einer Lösung, einer Schmelze, einer Gasphase, einem amorphen Festkörper oder auch durch Umkristallisation aus einem anderen Kristall entstehen. Auf die anfängliche Kristallbildung an einem Kristallisationskeim folgt das weitere Kristallwachstum. Die Entstehung des typischen Gefüges nennt man „Kristalloblastese“.

Damit sich ein Kristall bilden kann, muss der auszukristallisierende Stoff zunächst in Übersättigung gebracht werden. Dies geschieht zum Beispiel durch Abkühlungsprozesse von Lösungen oder von Schmelzen, oder durch Verdampfen des Lösungsmittels. Bei Kristallen, die aus mehreren Komponenten bestehen (zum Beispiel Ionenkristalle), kann die Übersättigung auch durch Mischen von zwei Lösungen hergestellt werden, die jeweils eine der Komponenten enthalten. Außerdem ist es möglich, durch Hinzufügen einer dritten Komponente die Löslichkeit der bereits gelösten Komponenten zu verringern und so die Übersättigung zu erzeugen.

Die zuvor gelösten Moleküle, Ionen oder Atome ordnen sich in einer regelmäßigen, teils stoffspezifischen Form an einen Kristallisationskeim an und bilden einen Keimling, der dann in der übersättigten Lösung weiterwächst. Die Kristallbildung kann daher beschleunigt werden, wenn Impfkristalle als Kristallisationskeime hinzugefügt werden.

Die Kristalle können dann durch Filtration, Flotation, Zentrifugation oder Siebung von der Lösung getrennt werden. Ein Beispiel für die Gewinnung eines Massenproduktes durch Kristallisation ist die Gewinnung von Salz in Salinen.

Die fraktionierende Kristallisation ist ein Verfahren zur Stofftrennung. Dabei wird ein Stoffgemisch (z. B. zwei oder mehr Isomere) in einem geeigneten Lösungsmittel durch Erhitzen gelöst. Anschließend wird durch Abkühlen dieser Lösung oder durch langsames Verdunsten des Lösungsmittels eine übersättigte Lösung hergestellt, aus der ein Stoff bevorzugt auskristallisiert.

In einigen Fällen ist sogar eine Enantiomerentrennung möglich, also die Trennung der beiden Enantiomere eines chiralen Stoffes. Ein weiterer Spezialfall der fraktionierenden Kristallisation ist die Rieselfilmkristallisation.






Spektrograf

Ein Spektrograf ist ein optisches Instrument, das Licht verschiedener Wellenlänge in sein Spektrum (d. h. in seine verschiedenen Farben) zerlegt und das erzeugte Spektrum mittels geeigneter Detektoren registriert. Die Zerlegung des Lichts nach seiner Wellenlänge geschieht mit Hilfe von optischen Elementen, die Dispersionseigenschaften haben, meist entweder ein Prisma, ein Beugungsgitter oder ein so genanntes Grism, das Gitter und Prisma in einem Element kombiniert.

Spektrografen dienen unter anderem als Beobachtungsgerät der Astronomie, welches das aus einem Fernrohr austretende Licht von Sternen oder Galaxien in sein Spektrum zerlegt und auf einen Detektor leitet.

Solche Detektoren waren früher meist Fotoplatten und Filme. Heute werden vor allem CCD-Elemente oder Photodiodenzeilen zum Zweck des Nachweises des durch den Spektrografen erzeugten Spektrums verwendet.

Instrumente zur Spektroskopie, der visuellen Betrachtung von Spektren, heißen Spektroskope.





Anastigmat

Als Anastigmat wird ein optisches Linsensystem (z. B. ein Objektiv) bezeichnet, das einen früher berüchtigten Abbildungsfehler, den Astigmatismus (Punktlosigkeit) nicht aufweist. Dazu werden die sagittale und die meridionale Bildschale zur Deckung gebracht und auch die Bildfeldwölbung beseitigt, was mit einer Einzellinse physikalisch nicht erreichbar ist.

Das erste anastigmatische Objektiv war das von Paul Rudolph 1891 für Carl Zeiss entwickelte, vierlinsige "Protar". Der einfachste Anastigmat ist das wenig später entwickelte "Cooke-Triplet" aus einer konvexen, einer konkaven und einer weiteren konvexen Linse. Auf dem Cooke-Triplet bauen weitere wichtige Objektive auf, beispielsweise vom vierlinsigen Tessar-Typ, bei dem die Hinterlinse durch eine verkittete Linsenkombination (positiv brechende Konkav-konvex-Kombination) ersetzt worden ist, und ebenso das Heliar von Voigtländer, bei dem auch die Vorderlinse durch ein solches Kittglied ersetzt ist.

Objektive nach Art eines Gaußschen Doppelobjektivs sind ebenfalls in der Regel Anastigmaten. Ursprünglich handelte es sich dabei um streng symmetrische, später auch um komplexere, nicht-symmetrische Systeme.

Alle hochwertigen Linsensysteme sind heute als Anastigmaten ausgeführt. Oft handelt es sich dabei um Varianten des Triplet- oder des Gauß-Typs. Ausnahme sind Sonderfälle, bei denen nur ein kleines Gesichtsfeld abgebildet wird, wie Objektive von Fernrohren und Ferngläsern.

Insbesondere ältere Kamera- und Projektionsobjektive tragen entweder direkt die Bezeichnung „Anastigmat“ (z. B. der "Novotrinast-Anastigmat" der Ed. Liesegang oHG) oder lassen eine entsprechende Eigenschaft anklingen ("Patrinast", ebenfalls ein Projektionsobjektiv von Liesegang; "Trinast" von Carl Zeiss). Oft handelt es sich dabei um Objektive nach Art des Cooke-Triplets. Der abgebildete "Unofocal" Doppel-Anastigmat von C.A. Steinheil folgt jedoch einem Gauß-Typ.





Paul Scherrer Institut

Das Paul Scherrer Institut (PSI) ist ein multidisziplinäres Forschungsinstitut für Natur- und Ingenieurwissenschaften in der Schweiz. Es liegt auf dem Gebiet der Gemeinden Villigen und Würenlingen im Schweizer Kanton Aargau beidseits der Aare und gehört zum ETH-Bereich der Schweizerischen Eidgenossenschaft. Das Institut beschäftigt rund 2100 Mitarbeiter und betreibt auf einem Areal von über 35 Hektaren Grundlagenforschung und Angewandte Forschung in den Bereichen Materie und Material, Mensch und Gesundheit sowie Energie und Umwelt. Im Einzelnen verteilen sich die Forschungsaktivitäten auf folgende Schwerpunkte: Festkörperforschung 35 %, Lebenswissenschaften 24 %, Allgemeine Energie 20 %, Nukleare Energie und Sicherheit 13 %, Teilchenphysik 8 %.

Das PSI entwickelt, baut und betreibt grosse und komplexe Forschungseinrichtungen und stellt sie der nationalen und internationalen Wissenschaftsgemeinschaft zur Verfügung. Im Jahr 2017 etwa kamen mehr als 2500 Forscher aus 60 verschiedenen Nationen an das PSI, um die weltweit einmalige Kombination an Grossforschungseinrichtungen am selben Ort zu nutzen. Rund 1900 Experimente werden jedes Jahr an den etwa 40 Messplätzen der Anlagen durchgeführt.

Das nach dem Schweizer Physiker Paul Scherrer benannte Institut entstand 1988 aus dem Zusammenschluss des 1960 gegründeten EIR (Eidgenössisches Institut für Reaktorforschung) und dem 1968 gegründeten SIN (Schweizerisches Institut für Nuklearforschung). Die beiden Institute lagen einander gegenüber an der Aare und dienten auch zuvor schon als eine Art nationales Zentrum zur Erforschung der Teilchenphysik. Mit den Jahren wurde die Forschung allerdings erheblich auf andere Bereiche ausgeweitet, sodass Nuklear- und Reaktorphysik heute nur noch einen kleinen Teil der Arbeit am PSI ausmachen. Auch infolge des Schweizer Atomausstiegsbeschlusses im Jahr 2011 beschäftigt sich dieser Bereich vornehmlich mit Fragen der Sicherheit etwa im Umgang mit der Lagerung von radioaktivem Abfall in einem Tiefenlager.

Seit 1984 betreibt das PSI (anfangs noch als SIN) das Zentrum für Protonentherapie zur Behandlung von Patienten mit Augenmelanomen und anderen tief sitzenden Tumoren. Über 7200 Patienten wurden seither behandelt.

Auch in der Weltraumforschung ist das Institut aktiv. Zum Beispiel bauten PSI-Ingenieure 1990 für den russischen Satelliten Spectrum X-G den Detektor des Teleskops EUVITA und später auch Detektoren für NASA und ESA, welche die Strahlung im All analysieren.
Am Tandembeschleuniger des PSI bestimmten Physiker 1992 per Beschleuniger-Massenspektrometrie und Radiokarbonmethode aus Knochen-, Gewebe- und Grasproben von wenigen Milligramm das Alter des Gletschermanns Ötzi, den man ein Jahr zuvor in den Ötztaler Alpen gefunden hatte.

Im Jahr 2009 erhielt der aus Indien stammende britische Strukturbiologe Venkatraman Ramakrishnan den Chemie-Nobelpreis auch für seine Studien an der Synchrotronlichtquelle Schweiz (SLS), mit denen er klären konnte, wie Ribosomen aussehen und auf der Ebene einzelner Moleküle funktionieren. Ribosomen stellen anhand der in den Genen kodierten Informationen Proteine her, die viele chemische Prozesse in Lebewesen kontrollieren.

2010 führte ein internationales Forscherteam am PSI eine neue Vermessung des Protons durch und stellte fest, dass sein Radius deutlich kleiner ist als bis dahin angenommen: 0,84184 Femtometer statt 0,8768. Laut Presseberichten war dieses Ergebnis nicht nur überraschend, es könnte auch bisherige Modelle der Physik in Frage stellen. Die Messungen waren einzig mit dem Protonenbeschleuniger des PSI möglich, weil weltweit nur dessen Myonenstrahl stark genug ist, um genügend Myonen dafür zu produzieren.

2011 gelang es Forschern unter anderem vom PSI mithilfe der SLS, die Struktur des Proteins Rhodopsin zu entschlüsseln. Dieses Sehpigment ist als eine Art Lichtsensor entscheidend am Vorgang des Sehens beteiligt.

Ein am PSI gebauter sogenannter Barrel-Pixel-Detektor war als Kernelement des CMS-Detektors am Genfer Kernforschungszentrum CERN daran beteiligt, das Higgs-Boson nachzuweisen. Für diese am 4. Juli 2012 verkündete Entdeckung wurde ein Jahr später der Physik-Nobelpreis verliehen.

Laut einem Zeitungsbericht sind im Januar 2016 20 Kilogramm Plutonium aus dem PSI in die USA gebracht worden. Das Material soll seit den 60er Jahren in einem geheimen Plutoniumlager des Bundes für den damals angedachten Bau einer Atombombe vorgehalten worden sein. Der Bundesrat widersprach dieser Darstellung: Der Plutonium-239-Gehalt des Materials habe unter 92 Prozent gelegen, daher sei es nicht waffenfähig gewesen. Vielmehr sollte das Material, nachdem man es aus wiederaufbereiteten Brennstäben des 1960 bis 1977 betriebenen Forschungsreaktors Diorit gewonnen hatte, zur Entwicklung einer neuen Generation von Brennelementtypen für Kernkraftwerke verwendet werden. Dazu kam es jedoch nie. Spätestens nach dem Atomausstiegsbeschluss 2011 war klar, dass es in der Schweiz keine Verwendung für das Material mehr geben würde. Der Bundesrat entschied 2014 im Rahmen des Nuklearen Sicherheitsgipfels, das Schweizer Plutoniumlager aufzulösen und überführte es auf Basis eines bereits existierenden bilateralen Abkommens zur weiteren Lagerung in die USA.

Im Juli 2017 gelang es mit der SLS, die dreidimensionale Ausrichtung der Magnetisierung im Inneren eines Materials zu untersuchen und zu visualisieren, ohne das Material dabei zu beeinträchtigen. Die Technologie soll helfen, bessere Magnete etwa für Motoren oder die Datenspeicherung zu entwickeln.

Der langjährige Direktor des PSI Joël François Mesot (seit August 2008) wurde Ende 2018 zum Präsidenten der ETH Zürich gewählt. Seinen Posten übernimmt ab Januar 2019 übergangsweise der Physiker und Stabschef des PSI Thierry Strässle.

Aus dem PSI sind im Laufe der Jahre zahlreiche Unternehmen ausgegründet worden, um die Forschungserkenntnisse für die Gesellschaft nutzbar zu machen. Das mit 110 Mitarbeitern grösste Spin-off ist die 2006 gegründete DECTRIS AG im nahe gelegenen Baden, die sich auf die Entwicklung und Vermarktung von Röntgendetektoren spezialisiert hat. Bereits 1999 gegründet wurde die SwissNeutronics AG in Klingnau, die optische Komponenten für Neutronenforschungsanlagen vertreibt. Gleich mehrere neue PSI-Ableger wie etwa der Hersteller metall-organischer Gerüste novoMOF oder der Medikamente-Entwickler leadXpro haben sich in dem 2015 gemeinsam mit dem Kanton Aargau und mehreren Unternehmen gegründeten PARK INNOVAARE in Nachbarschaft des PSI angesiedelt.

Das PSI entwickelt, baut und betreibt mehrere Beschleunigeranlagen, z. B. ein 590-MeV-Hochstromzyklotron, das aktuell (2019) einen Strahlstrom von etwa 2,2 mA liefert. Ausserdem betreibt das PSI vier Großforschungsanlagen: eine Synchrotronlichtquelle (SLS) von besonderer Brillanz und Stabilität, eine Spallations-Neutronenquelle (SINQ), eine Myonenquelle (SμS) und einen Freie-Elektronen-Röntgenlaser (SwissFEL). Damit ist das PSI zurzeit (2019) weltweit das einzige Institut, das die vier wichtigsten Sonden zur Erforschung der Struktur und der Dynamik kondensierter Materie (Neutronen, Myonen und Synchrotronstrahlung) auf einem Campus der internationalen Nutzergemeinschaft anbietet.

Nicht zuletzt mithilfe dieser Anlagen wird am PSI unter anderem auf folgenden Gebieten geforscht:

Alle Materialien, mit denen der Mensch arbeitet, setzen sich aus Atomen zusammen. Das Zusammenspiel der Atome untereinander und ihre Anordnung bestimmen, welche Eigenschaften ein Material hat. Die meisten Forscher auf dem Gebiet Materie und Material des PSI wollen diesen Zusammenhang zwischen innerem Aufbau und beobachtbaren Eigenschaften für unterschiedliche Stoffe aufklären. Die Grundlagenforschung innerhalb dieses Bereiches trägt dazu bei, neue Materialien für verschiedenste Anwendungen zu entwickeln, beispielsweise für die Elektrotechnik, die Medizin, die Telekommunikation, alle Bereiche der Mobilität oder die Entwicklung neuer Energiespeicher.

Innerhalb der Teilchenphysik untersuchen die Forscher des PSI Aufbau und Eigenschaften der Elementarteilchen.

In diesem Bereich behandeln die Forscher unter anderem erneuerbare Energien, schadstoffarme Verbrennungstechnik, Brennstoffzellenentwicklung, Bewertung von Energie- und Stoffkreisläufen, Umwelteinflüsse von Energieproduktion und -verbrauch, nukleare Energieforschung (insbesondere Reaktorsicherheit und Entsorgung), Energieeffizienz und -speicherung.

Zu letzterem Punkt betreibt das PSI die Versuchsplattform ESI (Energy System Integration), auf der Forschung und Industrie vielversprechende Lösungsansätze zur Integration erneuerbarer Energien ins Energiesystem testen können – zum Beispiel das Speichern von Stromüberschüssen aus Solar- oder Windkraft in Form von Wasserstoff oder Methan.

Eine mit Hilfe der ESI-Plattform am PSI entwickelte und gemeinsam mit dem Zürcher Energieversorger Energie 360° erfolgreich getestete Technologie, die deutlich mehr Methangas aus Bioabfällen gewinnt, wurde 2018 mit dem Watt d'Or 2018 des Schweizerischen Bundesamtes für Energie ausgezeichnet.

Weiterhin betreibt das PSI unter anderem eine Smog-Kammer, mit deren Hilfe die Schadstoffemissionen verschiedener Energiegewinnungsverfahren und das Verhalten der entsprechenden Substanzen in der Atmosphäre getestet werden können.

PSI-Forscher untersuchen die Auswirkungen der Energiegewinnung auf die Atmosphäre auch vor Ort, unter anderem in den Alpen, in den Polregionen der Erde oder in China.

Das PSI ist eine der führenden Institutionen weltweit im Bereich der Protonentherapie zur Behandlung von Krebserkrankungen. Seit 1984 wurden bei mehr als 7200 Patienten Augentumoren bestrahlt (Stand 2019). Die Erfolgsrate bei der Augentherapie (OPTIS) liegt bei über 98 %.

Pro Jahr zählt das Zentrum für Protonentherapie rund 5900 Patientenbesuche. 1996 wurde erstmals ein Bestrahlungsgerät (Gantry) für die am PSI entwickelte sogenannte Spot-Scanning-Protonen-Technik ausgerüstet. Bei dieser Technik werden die Tumoren mit einem circa 5 bis 7 mm breiten Protonenstrahl dreidimensional abgescannt. Computer steuern diesen Strahl so, dass sich der Hochdosis-Spot für eine präzise vorgegebene Zeit sehr genau am gewünschten Ort im Tumor befindet. Durch Überlagern vieler einzelner Spots – für ein Volumen von 1 Liter sind es ca. 10.000 – wird der Tumor gleichmässig mit der nötigen Strahlendosis belegt, wobei diese für jeden einzelnen Spot individuell überwacht wird. Das erlaubt eine äusserst präzise, homogene Bestrahlung, die an die meist unregelmässige Form des Tumors optimal angepasst ist.

Auf dem Gebiet der Radiopharmazie befassen sich die Forscher des PSI mit sehr kleinen und im ganzen Körper verteilten Tumoren. Diese können mit der üblichen Strahlentherapie nicht behandelt werden. Für eine Therapie kombiniert das PSI spezielle Biomoleküle – sogenannte Antikörper – mit einem radioaktiven Atomkern zu Therapiemolekülen. Solche Präparate können Tumorzellen selektiv und gezielt finden und zerstören. Um seine Grundlagenforschung auf diesem Gebiet mit der klinischen Erprobung in den Spitälern zu koordinieren, arbeitet das PSI sehr eng mit Hochschulen, Kliniken und der Pharmaindustrie zusammen.

Ein weiterer Schwerpunkt der Forschung im Bereich Mensch und Gesundheit liegt bei der Struktur und Funktionsweise von Proteinen. Jede lebende Zelle benötigt eine Unzahl dieser Moleküle, zum Beispiel um Stoffwechsel betreiben zu können, Signale aufzunehmen und weiterzuleiten oder um sich zu teilen. Um all diese Lebensvorgänge besser zu verstehen und für die Bekämpfung oder Vermeidung von Erkrankungen nutzen zu können, werden die beteiligten Proteine genau untersucht.

Diente der 1974 in Betrieb genommene Protonenbeschleuniger des PSI in seinen Anfängen noch vornehmlich der Elementarteilchenphysik, stehen heute Anwendungen für Festkörperphysik, Radiopharmazie und Krebstherapie im Vordergrund. Von anfangs 100 µA wurde die Leistungsfähigkeit durch konstante Weiterentwicklung um den Faktor 24 auf mittlerweile 2,4 mA erhöht.
Im Prinzip werden die Protonen durch drei aufeinander folgende Geräte auf rund 80 Prozent der Lichtgeschwindigkeit beschleunigt: Linearbeschleuniger, kleiner Ringbeschleuniger, grosser Ringbeschleuniger.

In einem Cockcroft-Walton-Beschleuniger werden mittels Mikrowellen die Elektronen von Wasserstoffatomen abgeschält. Übrig bleiben die Wasserstoff-Atomkerne, die jeweils aus nur einem Proton bestehen. Diese Protonen werden einer Spannung von 810 Kilovolt ausgesetzt und so linear auf immerhin 46 Millionen km/h oder 4 % der Lichtgeschwindigkeit beschleunigt.

Die zweite Stufe leistet ein kleiner Ringbeschleuniger, auch Zyklotron genannt. Er übernimmt die Protonen aus dem Cockcroft-Walton-Beschleuniger und beschleunigt sie weiter auf etwa 38 % der Lichtgeschwindigkeit. Das PSI verfügte über zwei derartige Vorbeschleuniger. Der erste, „Injektor-1“, wurde inzwischen ausser Betrieb gesetzt, seine Funktion von „Injektor-2“ übernommen.

Das 1974 in Betrieb genommene Injektorzyklotron wurde von Philips erbaut und erfüllte anfänglich zwei Funktionen: Zu 75 % der Strahlzeit diente es als Injektor zur Beschleunigung eines Protonenstrahls auf 72 MeV, der anschliessend durch das Ringzyklotron auf seine Endenergie von 590 MeV gebracht wurde. Die restlichen 25 % der Strahlzeit diente diese Maschine zur Beschleunigung von Teilchen auf verschiedene Energien für Niederenergie-Experimente.

Das Hochfrequenz-Beschleunigungssystem wurde für die beiden Einsatzzwecke verschieden ausgelegt. Die Beschleunigung im Injektormodus erfolgte bei 50 MHz mit einer Spannung von 70 kV. Sie erfolgte zweimal pro Umlauf, so dass die Teilchen 500 Mal kreisten, bis sie die Extraktionsenergie von 72 MeV erreicht hatten. Dieser Teil wurde jedoch 1991 ausser Betrieb genommen.

Für den Betrieb mit variabler Energie kann die Frequenz mittels einer verschiebbaren Kurzschlussplatte zwischen 4,6 und 17 MHz der gewünschten Energie angepasst werden.
Der Injektor-1 verfügt über drei Ionenquellen: Eine interne Quelle (RIQ) für Protonen, Deuteronen, Alpha-Teilchen und schwere Ionen; eine externe polarisierte Quelle (PIQ) für Protonen und Deuteronen (welche jedoch mittlerweile ausser Betrieb ist) und eine ebenfalls externe Mikrowellenquelle (ECR), mit welcher Protonen, Argon-Ionen und Xenon-Ionen erzeugt werden. Diese wurde 1997 nachträglich eingebaut.

Mit dem Injektor-1 wurden Betriebsströme um 170 µA und Spitzenströme um 200 µA erreicht.
Injektor-1 wurde ebenfalls für Niederenergie-Experimente, für die OPTIS-Augentherapie und für das LiSoR-Experiment im Rahmen des MEGAPIE-Projekts genutzt. Seit dem 1. Dezember 2010 ist dieses Zyklotron ausser Betrieb.

Der 1984 in Betrieb genommene Injektor-2, eine Eigenentwicklung des damaligen SIN, löste den Injektor-1 als Einschussmaschine für das 590 MeV Ringzyklotron ab. Anfänglich war noch ein wechselnder Betrieb zwischen Injektor-1 und Injektor-2 möglich, inzwischen wird nur noch der Injektor-2 zur Injektion des Protonenstrahles in den Ring genutzt. Durch das neue Zyklotron wurde es möglich, den Strahlstrom auf 1 bis 2 mA anzuheben, für die 1980er Jahre ein absoluter Spitzenwert. Aktuell (2019) liefert der Injektor-2 einen Strahlstrom von ≈ 2,2 mA im Routinebetrieb, 2,4 mA im Testbetrieb.

Ursprünglich wurden zwei Resonatoren mit 150 MHz im Flattop-Betrieb betrieben, um eine saubere Trennung der Protonenbahnen zu erhalten, inzwischen werden jedoch auch diese mit 50 MHz zur Beschleunigung eingesetzt. Aus dem extrahierten 72 MeV Protonenstrahl kann ein Teil zur Isotopenproduktion oder für Experimente abgeschnitten werden.

Das 1974 in Betrieb genommene Ring-Zyklotron ist wie der Injektor-2 eine Er hat einen Umfang von rund 48 Metern. Auf der ca. 4 km langen Strecke, welche die Protonen auf 186 Runden im Ring zurücklegen, werden sie auf 80 % der Lichtgeschwindigkeit beschleunigt. Das entspricht einer Bewegungsenergie von 590 MeV und ermöglicht es, einen Protonenstrom von 2,2 mA (testweise bis zu 2,4 mA) zu extrahieren. Aufgrund dieses hohen Strahlstromes wird das PSI auch als Mesonenfabrik bezeichnet. Weltweit gibt es nur deren drei, nämlich: TRIUMF in Vancouver, Kanada; LAMPF in Los Alamos, USA; und das PSI. Die beiden erstgenannten erreichten nur Strahlströme von 500 µA bzw. 1 mA.

Die 1979 zusätzlich eingebaute, kleinere, fünfte Kavität wird mit 150 Megahertz als Flattop-Kavität betrieben, wodurch die Anzahl der extrahierten Teilchen deutlich gesteigert werden konnte. Seit 2008 sind alle alten Aluminiumkavitäten des Ringzyklotrons durch neue Kupferkavitäten ersetzt worden. Diese ermöglichen höhere Spannungsamplituden und somit eine grössere Beschleunigung der Protonen pro Umlauf. Die Anzahl der Umläufe der Protonen im Zyklotron konnte so von ca. 200 auf 186 verringert werden, und der im Zyklotron zurückgelegte Weg der Protonen sank von 6 km auf 4 km. Mit dem Strahlstrom von 2,2 mA stellt diese Protonenanlage des PSI den zurzeit leistungsfähigsten Teilchenbeschleuniger der Welt dar. Der 1,3 MW starke Protonenstrahl wird zur Myonenquelle (SμS) und zur Spallations-Neutronenquelle (SINQ) gelenkt.

In der Myonenquelle (SμS) stösst der Protonenstrahl des Ringzyklotrons auf zwei Targets – Ringe aus Kohlenstoff. Bei den Kollisionen der Protonen mit den Atomkernen des Kohlenstoffs entstehen Pionen, die nach rund 26 Milliardstel Sekunden zu Myonen zerfallen. Diese Myonen werden dann von Magneten zu einem von sechs Messplätzen geleitet, um Experimente durchzuführen. Der Beschleuniger des PSI ist der einzige weltweit, der so viele Myonen erzeugt (bis zu 500 Millionen pro Sekunde), dass die Suche nach bestimmten seltenen Myonenzerfällen innerhalb eines Menschenlebens Ergebnisse liefern kann. Und nur hier können dank eines speziellen Verfahrens Myonen erzeugt werden, die langsam genug sind, um damit dünne Materialschichten und Oberflächen zu untersuchen.

Die seit 1996 in Betrieb befindliche Neutronenquelle SINQ ist die erste und gleichzeitig die stärkste ihrer Art. Sie liefert einen kontinuierlichen Neutronenfluss von 10 n/cm/s. Die Protonen aus dem grossen Teilchenbeschleuniger treffen hier auf ein Bleitarget und schlagen aus den Bleikernen die Neutronen heraus, die dann für Experimente zur Verfügung stehen. Neben thermischen Neutronen liefert ein Moderator aus flüssigem Deuterium auch langsame Neutronen, welche ein tieferes Energiespektrum besitzen.

Durch die Inbetriebnahme des MEGAPIE Targets (Megawatt Pilot-Experiment) im Sommer 2006, bei dem das Feststofftarget durch eines aus einer Blei- und Bismut-Legierung ersetzt wurde, konnte die Neutronenausbeute um ca. weitere 80 % gesteigert werden.

Das MEGAPIE-Projekt wurde durch die CEA in Cadarache und das Forschungszentrum Karlsruhe in Zusammenarbeit mit dem PSI initiiert, um die Machbarkeit eines Flüssigmetall-Targets bei 1 MW Leistung zu demonstrieren. Beschleunigergetriebene Kernreaktoren (ADS) mit derartiger Neutronenquelle stehen zur Diskussion für die Transmutation von nuklearen Abfällen.

Dieses supraleitende 250-MeV-Zyklotron ist seit 2007 für die Protonentherapie in Betrieb und liefert den Strahl für die Tumorbekämpfung an Krebspatienten. Die Anlage produziert also unabhängig vom grossen Protonenbeschleuniger einen eigenen Protonenstrahl, der mehrere Bestrahlungsgeräte versorgt.

Die Synchrotron-Lichtquelle Schweiz (Swiss Light Source, SLS), ein Elektronen-Synchrotron, ist seit dem 1. August 2001 in Betrieb. Sie funktioniert wie eine Art Kombination aus Röntgengerät und Mikroskop, um verschiedenste Substanzen zu durchleuchten. In dem runden Bauwerk bewegen sich die Elektronen auf einer Kreisbahn von 288 Metern Umfang, wobei sie die Synchrotronstrahlung in tangentialer Richtung emittieren. Insgesamt 350 Magnete halten den Elektronenstrahl auf seiner Bahn und fokussieren ihn; Beschleunigungskavitäten sorgen für gleichbleibende Geschwindigkeit.
Seit 2008 ist die SLS der Beschleuniger mit dem dünnsten Elektronenstrahl weltweit – dafür haben die Forscher und Techniker des PSI acht Jahre gearbeitet und jeden einzelnen der vielen Magnete immer wieder justiert. Die SLS bietet ein sehr breites Spektrum von Synchrotronstrahlung von infrarotem Licht bis zu harten Röntgenstrahlen. Damit können Forscher mikroskopische Aufnahmen im Innern von Objekten, Materialien und Gewebe machen, um zum Beispiel Werkstoffe zu verbessern oder Medikamente zu entwickeln.

2017 gelang es mit einem neuen Instrument an der SLS erstmals, in einen Teil eines Computerchips hineinzuschauen, ohne ihn zu zerstören. Dabei wurden Strukturen wie 45 Nanometer schmale Stromleitungen und 34 Nanometer hohe Transistoren sichtbar. Mit dieser Technologie können zum Beispiel Chip-Hersteller besser prüfen, ob ihre Produkte genau den Vorgaben entsprechen.

Derzeit laufen unter dem Arbeitstitel „SLS 2.0“ Planungen, die SLS aufzurüsten und dadurch eine Synchrotron Lichtquelle der vierten Generation zu schaffen.

 Der Freie-Elektronen-Laser SwissFEL wurde am 5. Dezember 2016 durch Bundesrat Johann Schneider-Ammann symbolisch eröffnet. Im Jahr 2018 wurde die erste Strahllinie ARAMIS in Betrieb genommen. Bis Herbst 2020 soll die zweite Strahllinie ATHOS folgen. Weltweit sind nur vier vergleichbare Anlagen in Betrieb.

Das PSI Bildungszentrum – bestehend aus der Reaktorschule, der Schule für Strahlenschutz und der PSI Akademie – hat über 50 Jahre Erfahrung in der Aus- und Weiterbildung und bildet jährlich über 2000 Teilnehmende aus.

Die Schule für Strahlenschutz bietet sowohl Fachkräften als auch anderen Personen, welche mit ionisierender Strahlung oder radioaktivem Material arbeiten, eine breite Palette von Grund- und Fortbildungskursen an. Die Kurse zur Erlangung des entsprechenden Sachverstandes sind vom Bundesamt für Gesundheit (BAG) und vom Eidgenössischen Nuklearsicherheitsinspektorat (ENSI) anerkannt.

Die PSI Akademie bietet den Mitarbeitern des Instituts wie auch interessierten Personen aus dem ETH-Bereich Aus- und Weiterbildungskurse an. Seit 2015 werden an der Akademie Kurse zur Personalentwicklung (wie Konfliktmanagement, Führungsworkshops, Kommunikation, Transferable Skills etc.) durchgeführt.

Dem Institut ist die Reaktorschule als Sektion angeschlossen. Die Reaktorschule bildet zum Reaktoroperateur aus. Rechtlich handelt es sich bei der Schule um eine höhere Fachschule für Technik, womit sie auch als Technikerschule bezeichnet werden kann.





Ponderomotorische Kräfte

Die ponderomotorische Kraft (englisch: "", deshalb manchmal auch "ponderomotive Kraft" genannt) ist der niederfrequente Anteil der Kraft eines räumlich inhomogenen, hochfrequenten elektromagnetischen Feldes auf ein System von (sich in diesem Feld bewegenden) elektrischen Ladungen.

Wenn ein homogenes (also nicht ortsabhängiges) Wechselfeld auf ein geladenes Teilchen wirkt, schwingt das Teilchen durch die wechselnde Kraft um seine Ruhelage. Da das Feld abwechselnd mit der gleichen Stärke in zwei entgegengesetzte Richtungen wirkt, ist die Teilchen-Trajektorie streng periodisch, das heißt, es wirkt im zeitlichen Mittel auch keine Kraft auf das Teilchen.

Bei einem inhomogenen (ortsabhängigen) Feld erfährt das Teilchen bei der Auslenkung auf eine Seite eine andere Feldstärke als bei Auslenkung auf die andere Seite. Dadurch ist die Kraft im zeitlichen Mittel nicht mehr null; die mittlere Kraft wird als ponderomotorische Kraft bezeichnet. Da bei hochfrequenten Wechselfeldern die Kraft sehr rasch variiert und die Schwingungsamplituden der Teilchen demnach sehr gering sind, ist für eine makroskopische Betrachtung der Teilchenbahnen nur die gemittelte, also die ponderomotorische Kraft entscheidend.

Für ein geladenes Teilchen ist die ponderomotorische Kraft proportional zum Gradienten der Intensität des elektromagnetischen Feldes und wirkt bei einem "freien" Teilchen in Richtung geringerer Feldstärke. Die Intensität des elektromagnetischen Feldes ist im Wesentlichen das Quadrat der Feldstärke. Die Kraft ist umgekehrt proportional zur Masse des Teilchens. Dies erklärt sich dadurch, dass leichte Teilchen größere Schwingungsamplituden haben und daher eher auf räumlich unterschiedliche Feldstärken empfindlich sind, als massereiche Teilchen.

Auf ein "gebundenes" Teilchen wirkt die ponderomotorische Kraft oberhalb der Bindungs-Resonanzfrequenz ebenfalls "entgegen" dem räumlichen Intensitäts-Gradienten. Unterhalb der Resonanzfrequenz wirkt die ponderomotorische Kraft "in Richtung" des räumlichen Intensitäts-Gradienten des Feldes.

Im Grunde ist der Effekt nur von der Teilchen-Dynamik abhängig – nicht davon, welche Art von Wechselwirkung die Ursache der Kraft ist. Beispielsweise beruht die Schallstrahlungskraft (acoustic radiation force), die im Ultraschall auf Partikel (oder Gasblasen in Flüssigkeiten) wirkt, auf demselben Phänomen, jedoch im Schallfeld.

Bei einem Vielteilchen-System sorgt nicht nur die Detektor-Trägheit für eine Tiefpass-Filterung, sondern meist bereits die thermodynamische Ensemble-Mittelung beim Übergang von der mikroskopischen zur makroskopischen Perspektive.

Die ponderomotorische Kraft ergibt sich aus
wobei formula_2 die elektrische Ladung und formula_3 die Masse des Teilchens ist; formula_4 und formula_5 sind die Kreisfrequenz und Feldstärke des elektrischen Felds. Die Linie oberhalb des Quadrats der Feldstärke steht für den Mittelwert über die Zeit. Wird die ortsabhängige Amplitude der Feldstärke eingesetzt, ist der Vorfaktor formula_6 statt formula_7 zu verwenden.

Ponderomotorische Kräfte werden in Ionenfallen, zum Beispiel in der Paul-Falle verwendet, um Ionen ohne Kontakt mit den Wänden des Vakuumgefäßes zu speichern. Eine ähnliche Anordnung wie die Paul-Falle ist der Quadrupol-Massenanalysator, der für die Massenspektrometrie verwendet wird. Dabei wird die Tatsache ausgenutzt, dass Ionen zu hoher Masse nur eine geringe Schwingungsamplitude aufweisen und daher die ponderomotorische Kraft nicht ausreicht, um ein ihr entgegenwirkendes konstantes Feld zu überwinden.

Ponderomotorische Kräfte wirken auch auf Elektronen in Laserstrahlen extrem hoher Energiedichte (Femtosekundenlaser).

Des Weiteren spielen ponderomotive Kräfte beim Freien Elektronen Laser (FEL) eine entscheidende Rolle, denn sie sind die Ursache für den als "Microbunching" bezeichneten Effekt, der zur Photonen-Erzeugung führt.




Dissipative Struktur

Eine dissipative Struktur (engl. "dissipative structure" ‚zerstreuende Struktur‘) bezeichnet das Phänomen sich selbstorganisierender, dynamischer, geordneter Strukturen in nichtlinearen Systemen fern dem thermodynamischen Gleichgewicht. Dissipative Strukturen bilden sich nur in offenen Nichtgleichgewichtssystemen, die Energie, Materie oder beides mit ihrer Umgebung austauschen. Beim Aufbau geordneter Strukturen nimmt die Entropie lokal ab; diese Entropieminderung des Systems muss durch einen entsprechenden Austausch mit der Umgebung ausgeglichen werden.

Die Ausprägung geordneter Strukturen hängt entscheidend von den Systemparametern ab, wobei der Übergang vom ungeordneten zum geordneten Zustand sprunghaft erfolgt. Dissipative Strukturen zeigen eine gewisse Stabilität (Nichtgleichsgewichtsstabilität) gegenüber Störungen von außen, zerfallen jedoch, sobald der Austausch mit der Umgebung unterbrochen wird oder allgemein bei größeren Störungen der Systemparameter.

Bereits ab 1950 arbeitete Alan Turing an einer neuen mathematischen Theorie der Morphogenese, welche die Auswirkungen nichtlinearer chemischer Reaktions- und Diffusionsfunktionen auf spontane Strukturbildungen zeigt. Die Ergebnisse dieser Arbeit hat er 1952 unter dem Titel "The chemical basis of morphogenesis" veröffentlicht. Diese Arbeit ("Turing-Mechanismus") wird als wegweisend für die spätere Entdeckung dissipativer Strukturen angesehen.

Der Begriff selbst wurde 1967 vom Physikochemiker Ilya Prigogine vorgeschlagen, der ab den 1940er Jahren an der Entwicklung der Theorie der Nichtgleichgewichtsthermodynamik beteiligt war. Prigogine untersuchte mit Grégoire Nicolis und später mit R. Lefever die Kinetik von offenen Systemen, die durch Energie- und Stoffdurchsatz fern vom thermodynamischen Gleichgewicht gehalten wurden. Basierend auf den Arbeiten Alan Turings und Lars Onsagers zeigte er, dass in offenen Systemen, in welchen autokatalytische chemische Reaktionen ablaufen, in der Nähe des thermodynamischen Gleichgewichts zunächst Inhomogenitäten auftreten, die durch Diffusion oder Strömungsprozesse aufrechterhalten werden können. Bei Erreichen eines Übergangspunkts fern vom Gleichgewicht kann das System Symmetriebrüche zeigen, indem es zur Ausbildung einer stationären, geordneten dissipativen Struktur kommt.

Ilya Prigogine erhielt 1977 den Nobelpreis für Chemie "für seinen Beitrag zur irreversiblen Thermodynamik, insbesondere zur Theorie der „dissipativen Strukturen“".

Beim Aufbau geordneter Strukturen nimmt die Entropie lokal ab, was nur in offenen Systemen möglich (bzw. wahrscheinlich) ist. Die Änderung der Entropie in einem Zeitintervall 

teilt sich in einen inneren (formula_2) und äußeren (formula_3, Austausch mit der Umgebung) Anteil auf. In abgeschlossenen Systemen findet kein Austausch statt (formula_4) und nach dem zweiten Hauptsatz ist immer formula_5 (gleich Null im Gleichgewicht), also formula_6. In offenen Systemen dagegen kann Entropie mit der Umgebung ausgetauscht werden, und es können geordnete stationäre (in der Zeit konstante) Strukturen entstehen, vorausgesetzt (es gilt formula_7 bei einem stationären Zustand; nach dem zweiten Hauptsatz gilt hier ebenso formula_5) 

Beispiele für dissipative Strukturen sind die Ausbildung von wabenförmigen Zellstrukturen in einer von unten erhitzten Flüssigkeit ("Bénard-Effekt") oder an Phasengrenzen bei Strömungsvorgängen, Fließgleichgewichte in der Biochemie, Hurrikane, chemische Uhren und Kerzenflammen. Dissipative Strukturen besitzen viele Gemeinsamkeiten mit biologischen Organismen, weshalb Lebewesen auch meist zu diesen gezählt werden.

Die Erdoberfläche inklusive der Atmosphäre bildet ein gleichgewichtsfernes energieumsetzendes (dissipatives) System, das durch die Sonneneinstrahlung Energie aufnimmt und durch Wärmeabstrahlung in den Weltraum abgibt. Innerhalb dieses Systems kann sich eine Vielzahl dissipativer Strukturen bilden, wie zum Beispiel Wolken, Flüsse oder Wirbelstürme.

Auch eine Volkswirtschaft bildet ein dissipatives System, bei dem die Erhöhung des Komplexitätsgrades den Durchsatz von Energie sowie die Entropieproduktion steigert. Der sogenannte technische Fortschritt im Sinne des Solow-Residuums kann somit durch eine Komplexitätserhöhung zur Steigerung der Leistungsfähigkeit erklärt werden, Primärenergie in nützliche Arbeit für den volkswirtschaftlichen Produktionsprozess umzuwandeln. Dissipative Strukturen sind hierbei Kapitalgüter (Maschinen) und Organisationsformen (Unternehmen).






Robert Wichard Pohl

Robert Wichard Pohl (* 10. August 1884 in Hamburg; † 5. Juni 1976 in Göttingen) war ein deutscher Physiker. Er ist bekannt für seine Experimentalphysik-Vorlesung, die auch als Lehrbuch erschien, und als Pionier der Festkörperphysik. Nevill Francis Mott beschrieb ihn sogar als den „wahren Vater der Festkörperphysik“.

Nach Robert Wichard Pohl ist das Pohlsche Rad benannt.

Robert Wichard Pohl wurde in Hamburg geboren als Sohn des Schiffbau-Ingenieurs Eugen Robert Pohl und dessen Frau Martha, Tochter von Wichard Lange, dem Gründer der Privatschule Dr. Wichard Lange, und Enkelin von Wilhelm Middendorff, der zusammen mit Friedrich Fröbel den ersten Deutschen Kindergarten gegründet hatte. 

Nach Besuch der Dr. Wichard Lange Schule trat er 1895 in die Gelehrtenschule des Johanneums ein, die er 1903 mit dem Abitur verließ, um im Sommersemester 1903 in Heidelberg Naturwissenschaften zu studieren. Dort lernte er auch James Franck kennen, mit dem ihn bis zu dessen Tod im Jahr 1964 eine enge Freundschaft verband. Zum Wintersemester 1903 ging er an die Universität Berlin, um Physik zu studieren. Schon im Sommersemester 1904 arbeitete er im Physikalischen Institut bei Emil Warburg an dem Thema, das seine Doktorarbeit wurde. Dort entstand auch seine erste Veröffentlichung angeregt durch Bernhard Walter vom Hamburger Physikalischen Staatslaboratorium, bei dem er im Folgenden in den Ferien arbeitete, vor allem bei der Suche nach der Beugung von Röntgenstrahlen.
Er wurde im Sommer 1906 zum Dr. phil. promoviert und unterrichtete danach als Assistent im physikalischen Praktikum unter dem Direktor des Instituts Heinrich Rubens. Es entstanden gemeinsame Arbeiten mit James Franck über die Ionenbeweglichkeit in Gasen und zur Frage der Geschwindigkeit von Röntgenstrahlen. Ab 1909 arbeitete er über den normalen und den selektiven photoelektrischen Effekt von Metallen, ab 1910 gemeinsam mit Peter Pringsheim, darunter die praktisch wichtige Arbeit zur Herstellung von Metallspiegeln. 1910 erschien eine Monographie über die Fernübertragung von Bildern und 1912 erfolgte die Habilitation. In einem Nachtrag enthält die Habilitationsschrift eine Besprechung der Laueschen Entdeckung der Röntgenbeugung. 

Nach der Habilitation begann Pohl Experimentalphysik-Vorlesungen zu halten, die er auch dazu benützte, privat eine Sammlung von Vorführungs-Instrumenten anzulegen. Auch wurden von ihm Experimente in Sitzungen der Physikalischen Gesellschaft vorgeführt. Bis zum Anfang des Krieges hatte er 54 Arbeiten veröffentlicht, zusätzlich zu den drei Büchern. 

Bei Kriegsausbruch versuchte Pohl vergeblich, als Freiwilliger angenommen zu werden, wurde aber aus gesundheitlichen Gründen abgelehnt. Sein Angebot, zusammen mit Erich Regener in zwei Reservelazaretten aus eigenen Mitteln Röntgen-Apparaturen aufzustellen und zu betreiben, wurde dankbar angenommen. Anfang November fand er die Möglichkeit bei den Funkern an der Entdeckung feindlicher Sendestationen zu arbeiten. Daraus wurde eine Anstellung als Oberingenieur bei der Verkehrstechnischen Prüfungskommission (VPK) im Hauptmannsrang, die er bis Kriegsende ausfüllte.

Im Februar 1916 erhielt er den Ruf nach Göttingen als außerordentlicher Professor (und Nachfolger von Eduard Riecke), konnte die Professur wegen des Krieges aber erst Anfang 1919 antreten. Heinrich Rausch von Traubenberg vertrat ihn in dieser Zeit. Im Umzugsgepäck waren mehr als 40 Kisten gefüllt mit Geräten für seine Vorlesungen. Aufgrund eines Rufs an die Technische Hochschule Stuttgart im September 1919 erhielt er in Göttingen im Dezember 1920 ein planmäßiges Ordinariat und wurde Direktor des 1. Physikalischen Instituts. Im Juni 1922 folgte ein Ruf aus Würzburg, den er auch ablehnte. Er hatte damit in der Blütezeit der Physik in Göttingen in den 1920er Jahren eines der drei Ordinariate für Physik neben James Franck (Direktor des 2. Physikalischen Instituts) und dem Theoretischen Physiker Max Born.

Weihnachten 1922 heiratete er Tussa Madelung, die Schwester von Erwin Madelung, der im Physikalischen Institut in Göttingen Assistent gewesen war, als Tussa mit ihrer Familie im Mai 1920 von Straßburg nach Göttingen zog. Sie hatten drei Kinder: Ottilie, Eleonore und Robert Otto, später Professor für Physik an der Cornell University.
Lichtelektrische Beobachtungen – allerdings nicht an Oberflächen wie in Berlin, sondern im Inneren von Isolatoren – begann Pohl mit seinem Assistenten Bernhard Gudden im Jahr 1919. (Die Arbeiten dieser Periode werden in ausführlich beschrieben.) So entdeckten sie, dass Diamant-Kristalle bei Beleuchtung elektrisch leitend wurden. Danach beobachteten sie denselben Effekt in dem Alkalihalogenid Natriumchlorid, allerdings erst nachdem dieses durch Bestrahlung mit Röntgenlicht eine Farbe angenommen hatte. Eine systematische Untersuchung dieser Färbungszentren an künstlich hergestellten Kristallen führte zur Entdeckung der Farbzentren, die in der Folgezeit ausführlich untersucht wurden. Durch den Einbau von drei Elektroden in einen Kaliumbromid-Kristall konnte 1938 mit Rudolf Hilsch das erste Modell eines Transistors mit Farbzentren gezeigt werden.

Neben diesen Arbeiten in seinem Institut half er auch seinen wissenschaftlichen Kollegen bei ihrer Arbeit. Mit dem Zoologen Alfred Kühn untersuchte er den Farbensinn der Bienen, für den Chemiker Adolf Windaus verwendete er optische Spektroskopie bei der Trennung des Ergosterins vom Cholesterin. Dem Archäologen Kurt Müller half er, antike Vasen ohne störende Reflexe zu fotografieren. Seinen Studenten Hans Joachim Pabst von Ohain unterstützte er tatkräftig, als dieser im Anschluss an seine Dissertation die ersten Versuche zum Strahlantrieb mit eigenen Mitteln im Institut anstellte.

Die Einführungsvorlesung in die Physik war für ihn von Anfang an sehr wichtig, immer wieder beschrieb er in den wissenschaftlichen Zeitschriften neue Experimente, die er dafür ersonnen hatte und die er dann in seinen Lehrbüchern verwendete. Das erste Buch seiner berühmten Einführung in die Physik, die „"Elektrizitätslehre"“, erschien im Jahr 1927. 1930 erschien dann die „"Mechanik und Akustik"“, in der dritten Auflage erweitert durch die „"Wärmelehre"“. Der dritte Band, die „"Optik"“, wurde 1941 veröffentlicht, die 1954 in der neunten Auflage durch die „"Atomphysik"“ erweitert wurde. 

In einem neuen Kapitel "„Quantenoptik fester Körper“" werden die Arbeiten im Göttinger Institut zusammengefasst. Auch in der "„Elektrizitätslehre“" werden von der 15. Auflage (1955) an elektrische Eigenschaften fester Körper behandelt, darunter auch Ergebnisse seiner Göttinger Arbeiten. Nach seinem Tod wurden die drei Bände auf zwei reduziert, wobei diese Kapitel entfernt wurden. Beide Bände enthalten jetzt Videos mit insgesamt 110 Experimenten, durchgeführt mit den ursprünglichen Geräten, Der zweite Band enthält außerdem ein Video mit einer Biographie von Pohl, deren Autor Ekkehard Sieker ist. Weitere Videos im selben Band zeigen die Stromverstärkung in einem Drei-Elektroden–Kristall sowie eine Audioaufnahme der Verleihung der Ehrendoktorwürde an Ernest Rutherford durch den Dekan Max Born (1931).

Seine Haltung zum Nazi-Regime hat Pohl in einem Lebenslauf auf Verlangen der Militärregierung beschrieben. Danach gehörte er nie einer politischen Partei an, stand den Nationalsozialisten reserviert bis ablehnend gegenüber (er hatte Kontakte zum Goerdeler-Kreis, sein Kontaktmann, der Studienrat Hermann Kaiser, wurde im Januar 1945 hingerichtet) und war von Anfang des Zweiten Weltkrieges an von einer Niederlage Deutschlands überzeugt. Nach dem Krieg arbeitete er bis 1948 als Mitglied des Entnazifizierungs-Ausschusses am Wiederaufbau der Universität Göttingen. 

Die Arbeiten des Pohlschen Instituts wurden erst kurz vor Ausbruch des Krieges im Ausland bekannt, als Pohl und sein Assistent Hilsch 1937 zu einer Konferenz über "The conduction of electricity in solids" nach Bristol eingeladen wurden. 1946 erschien in den USA die erste zusammenfassende Veröffentlichung über Farbzentren. Ihr folgten 1951 eine Einladung nach Urbana an die University of Illinois mit Besuchen bei den Bell Telephone Laboratories, dem Naval Research Laboratory und anderen Forschungseinrichtungen. Bei dieser Gelegenheit traf er wieder mit Franck zusammen, nach der erzwungenen Emigration Francks 1933 wurde ihre Freundschaft erneuert. 1956 fand die erste "International Color Center Conference" am Argonne National Laboratory statt, der in den folgenden Jahren bis 1977 in dreijährigem Rhythmus weitere folgten.

Pohl hat sein Institut stets klein gehalten. Von seinen 55 deutschen Doktoranden sind spaeter 11 ordentliche Professoren an einer deutschen Hochschule geworden, von seinen 7 auslaendischen Doktoranden haben 6 Professuren im Ausland erhalten.

Die Bedeutung der Göttinger Arbeiten wurde 1980 vom Nobelpreisträger Nevill Mott zusammengefasst:

"„R. W. Pohl of Göttingen is in my view the real father of solid state physics.“"

Nach seiner Emeritierung im Jahr 1952 widmete er sich weiter der Bearbeitung seiner Lehrbücher. In einem Interview mit seinem ehemaligen Schüler Heinz Pick im Jahr 1974 beschrieb Pohl einige seiner weiteren Erlebnisse in Göttingen im Einzelnen.


Nach ihm benannt ist der seit 1979 verliehene Robert-Wichard-Pohl-Preis für Experimentalphysik und Physikdidaktik, ausgeschrieben von der Deutschen Physikalischen Gesellschaft. Außerdem ist seit 1979 das Robert-Wichard-Pohl Institut an der Tongji-Universität, Shanghai, das durch die Stiftung Volkswagenwerk unterstützt wird, nach ihm benannt. Seit 1995 befindet sich an seinem Wohnhaus in Göttingen, Klopstockstr. 4, eine Gedenktafel. Seit 2007 vergibt die Fakultät Physik der Uni Göttingen außerdem die Robert-Wichard-Pohl-Medaille an Dozenten für die beste Lehrleistung.



Pohl unterstützte die von Martin Luserke gegründete und geleitete reformpädagogische "Schule am Meer" auf der Nordseeinsel Juist mit einer großzügigen Spende einer Vielzahl von Musikinstrumenten für das von Eduard Zuckmayer geleitete Schulorchester bzw. den Musiksaal der deutschlandweit damals einzigen Theaterhalle einer Schule.






Blase (Physik)

Eine Blase ist ein gasförmiger Körper innerhalb einer Flüssigkeit. Er ist von dieser durch eine in sich geschlossene Phasengrenzfläche getrennt. Das Komplement zur Blase ist der Tropfen (Flüssigkeit in Gas); siehe aber auch Antibläschen.

Befinden sich Blase und Flüssigkeit in Ruhe zueinander, so ist die Blase kugelförmig. Bewegt sich die Blase gegenüber der Flüssigkeit, z. B. unter dem Einfluss der Schwerkraft, so plattet die Blase an der in Bewegungsrichtung liegenden Seite ab, und zwar mit der Geschwindigkeit zunehmend.

Im kugelförmigen Fall lässt sich der Blaseninnendruck formula_1 berechnen aus

Kleine Blasen haben deshalb einen hohen Innendruck.

In strömenden, turbulenten oder siedenden Flüssigkeiten können sich Dampfblasen der jeweiligen Flüssigkeit dort bilden, wo der Dampfdruck den Umgebungsdruck übersteigt und außerdem die Oberflächenspannung überwinden kann. Gerät die Blase in einen Bereich mit anderem thermodynamischen Zustand, wo die o. g. Bedingungen nicht mehr vorliegen, so fällt die Blase durch Umgebungsdruck und Ober(Grenz-)flächenspannung unter Kondensation wieder zusammen. Durch die Trägheit der zusammenstürzenden Flüssigkeit kann an der Stelle, wo das Zentrum der Blase war, ein Bereich sehr hohen Drucks entstehen, welcher sich als Stoßwelle ausbreitet. Ist die Blase nur ein winziges bisschen abgeplattet und leicht asymmetrisch, so entsteht durch das Zusammenstürzen ein kumulativer Strahl mit dynamischen Drücken, die jedes Material zerstören können. 

Dieser Vorgang wird als Kavitation bezeichnet und spielt u. a. bei der Auslegung von hydraulischen Anlagen, Rohreinbauten wie Pumpen oder Ventilen und bei Schiffspropellern eine wichtige Rolle.





Dichteoperator

Der Dichteoperator (auch statistischer Operator) beschreibt den Zustand eines physikalischen Systems oder eines Ensembles solcher Systeme. Diese Beschreibung ist in physikalischer Hinsicht vollständig, denn mithilfe des Dichteoperators lässt sich für jede am System bzw. Ensemble mögliche Messung der Erwartungswert vorhersagen. Der Dichteoperator gibt insbesondere an, mit welcher Wahrscheinlichkeit sich das System oder ein aus dem Ensemble herausgegriffenes System in einem bestimmten eindeutig definierten Zustand befindet. Er kann den Zustand eines einzelnen Systems oder Ensembles auch dann beschreiben, wenn dieser nur unvollständig bekannt ist. Der Dichteoperator kann durch die Dichtematrix (bzw. "statistische Matrix") dargestellt werden. Er wird daher auch in der Quantenstatistik viel verwendet.

Der Dichteoperator wurde ursprünglich im Rahmen der klassischen Physik von George Gabriel Stokes für den Polarisationszustand eines Lichtstrahls entwickelt (Stokes-Parameter). In die Quantenmechanik wurde er 1927 von Lew Landau und John von Neumann eingeführt und dann ausführlich von Paul Dirac in "Principles of Quantum Mechanics" (1930) und von John von Neumann in "Mathematische Grundlagen der Quantenmechanik" (1932) dargestellt.

Für einen reinen Zustand mit (normiertem) Zustandsvektor formula_1 heißt der Dichteoperator (in Bra-Ket-Schreibweise)
Dieser Operator bleibt ungeändert, wenn man denselben Zustand durch einen Zustandsvektor formula_3 beschrieben hätte. Daher besteht, anders als beim Zustandsvektor, eine in beiden Richtungen eindeutige Zuordnung zwischen dem physikalischen Zustand und seinem Dichteoperator.

Dieser Operator ist ein Projektionsoperator formula_4, denn angewendet auf einen beliebigen Zustandsvektor formula_5, projiziert er diesen auf den durch formula_1 bestimmten 1-dimensionalen Unterraum des Hilbertraums:
wobei der Zahlenfaktor formula_8 das Skalarprodukt beider Vektoren ist. formula_9 ist hermitesch, unitär und idempotent (d. h. formula_10). Seine Eigenwerte sind 1 (für alle Vektoren desselben reinen Zustands) und Null (alle dazu orthogonalen Vektoren).

Für einen kohärenten, also reinen Überlagerungszustand
lässt sich der Dichteoperator formula_12 durch die beiden überlagerten Zustände ausdrücken:
Wenn formula_14 und formula_15 orthogonal sind und als Basisvektoren genommen werden, dann ist formula_16 durch die Matrix
gegeben. Die kohärente Linearkombination drückt sich in den Nichtdiagonalelementen aus. Alle Matrixelemente sind unabhängig davon, ob man für Überlagerungszustand anstelle von formula_18 einen Vektor formula_19 mit einer globalen Phase gewählt hat. In dieser Form treten sie auch in der Formel für den Erwartungswert eines Operators formula_20 auf und bilden hier die Interferenzterme:

Ein Ensemble, das aus Subensembles zusammengesetzt ist, in denen sich jeweils die Systeme in demselben reinen Zustand formula_22 befinden, ist in einem gemischten Zustand. Hier sind die reinen Zustände "inkohärent" überlagert. Sind die Zustände formula_22 orthogonal, so ist die jeweilige Anzahl formula_24 der betreffenden Ensembles die Wahrscheinlichkeit dafür, dass bei einer Messung ein einzelnes System im Zustand formula_22 gefunden wird. Die Gewichte sind dann auf 1 normiert: formula_26. Dann ist der Dichteoperator gegeben durch

Mit Hilfe der Projektionsoperatoren lässt sich der Dichteoperator auch schreiben als

Der Erwartungswert eines beliebigen Operators formula_29 ist dann

also die inkohärente Summe der Erwartungswerte für die einzelnen Subensembles, jeweils gewichtet mit der relativen Anzahl der darin enthaltenen Einzelsysteme. Es gibt keine Interferenz zwischen den Zuständen verschiedener Einzelsysteme.

Wurde zum Beispiel das Ensemble aus zwei Subensembles zusammengesetzt, die jeweils nur Systeme in dem einen oder dem anderen von zwei orthogonalen Zuständen formula_1 und formula_5 haben, so ist der Dichteoperator
formula_34 und formula_35 mit formula_36 sind dabei die relativen Häufigkeiten.

Mit formula_14 und formula_15 als Basisvektoren ist die Dichtematrix dieses Zustandsgemischs durch die Diagonalmatrix
gegeben. Die inkohärente Überlagerung von Systemen drückt sich im Verschwinden der Nichtdiagonalelemente aus, wenn (wie hier) die Systeme jeweils einen der Basiszustände besetzen.

In einer anderen Basis hat derselbe Dichteoperator im Allgemeinen eine Nichtdiagonalmatrix, ausgenommen der Fall, dass alle Basiszustände mit gleicher Häufigkeit vertreten sind.

Diese Matrix ist unabhängig davon, ob innerhalb des von den beteiligten Zuständen definierten Unterraums eine andere Basis gewählt wurde. Darin drückt sich die Tatsache aus, dass inkohärente Ensembles physikalisch identisch sind, wenn sie aus orthogonalen Zuständen mit jeweils gleicher Häufigkeit, aber verschieden gewählter Basis des durch die überlagerten Zustände gebildeten Unterraums gebildet sind.

Der Dichteoperator für das kanonische Ensemble ist:
In der Eigenbasis des Hamiltonoperators nimmt formula_9 die Form (1) an. Analog erhält man für den Dichteoperator des großkanonischen Ensembles

Ein Zustandsgemisch liegt auch bei nur einem einzigen System vor, wenn es vor einer Messung mit einem zweiten System verschränkt war, so dass bestimmte reine Zustände des ersten Systems mit bestimmten reinen Zuständen des zweiten System vollständig korreliert waren. Wenn dann durch diese Messung, die gar nicht auf das erste System einwirkt, der Zustand des zweiten Systems zu einem bestimmten reinen Zustand reduziert wurde, der nicht als solcher zu den korrelierten Zuständen gehört hatte, muss anschließend das erste System als Zustandsgemisch behandelt werden.

Dieser Fall ist häufig, zum Beispiel wenn ein Atom ein anderes stößt, dabei mit gewisser Wahrscheinlichkeit eine Anregung verursacht und dann unter einem bestimmten Ablenkwinkel auf einen Detektor trifft. Das getroffene Atom befindet sich danach in einem Zustandsgemisch in Form einer inkohärenten Überlagerung von angeregtem Zustand und Grundzustand. Wenn man durch eine Messung am gestoßenen Atom die Richtung seines Rückstoßes festgestellt hätte, würde sich umgekehrt das stoßende Atom nun in einem Zustandsgemisch befinden, gebildet aus einer inkohärenten Überlagerung der gestreuten Wellen verschiedener Energie. Zur Beschreibung benutzt man den Reduzierten Dichteoperator, der sich aus dem Dichteoperator des ursprünglichen Gesamtsystems durch partielle Spurbildung ergibt und keine Informationen zu dem Teilsystem, an dem gemessen wurde, mehr enthält. Diese durch Verschränkung vermittelte Veränderung des Zustands eines Systems, ohne dass es Objekt einer physikalischen Einwirkung geworden wäre, stellt einen der für die Anschauung schwierigsten Aspekte der Quantenphysik dar (siehe z. B. Quantenverschränkung, EPR-Paradoxon, Quantenradierer).

Für jeden einzelnen Bestandteil formula_22 des Zustandsgemischs ist der Mittelwert der Messergebnisse einer physikalischen Größe formula_49 gegeben durch den Erwartungswert
formula_50
Darin ist formula_51 der zu formula_49 gehörige Operator (s. Quantenmechanik, Observable).

Da das Ensemble ein Gemisch von Systemen in den verschiedenen beteiligten Zuständen formula_22 ist, ist der Mittelwert aller Messungen an den einzelnen Systemen die gewichtete Summe der einzelnen Erwartungswerte:

Dies ist gleich der Spur
wie man mit Hilfe eines vollständigen Systems von orthonormierten Basisvektoren formula_56 sehen kann: Wegen
formula_57 (Einheitsoperator) ist

Sind die formula_56 gerade die Eigenzustände zur Observable formula_49 (d. h. formula_61 mit den Eigenwerten formula_62), dann gilt weiter

Darin ist formula_64 das über das Ensemble gewichtete Mittel für die Wahrscheinlichkeit, ein herausgegriffenes System im Eigenzustand formula_56 anzutreffen. formula_66 ist also auch die Wahrscheinlichkeit, bei einer einzelnen Messung den Eigenwert formula_62 als Ergebnis zu erhalten. Charakteristisch ist, dass formula_66 durch eine inkohärente Summe gegeben wird, die von den relativen Phasen der am Ensemble beteiligten Zustände formula_22 unabhängig ist.

Umgekehrt lässt sich der Operator formula_51 durch die aus seinen Eigenwerten formula_62 und den Dichteoperatoren der Eigenzustände formula_72 gebildete Summe darstellen:

Die Dichtematrix ist die Matrix, mit der der Operator formula_9 in Bezug auf eine orthonormierte Basis formula_75 dargestellt werden kann:

Im Folgenden bezeichnet das Zeichen „formula_77“, dass ein Bra, Ket oder ein Operator bezüglich einer Basis dargestellt wird (vergleiche auch Bra-Ket#Darstellung).
Die Zustände „Spin auf“ (bezgl. z-Achse) formula_78
und „Spin ab“ formula_79
werden als ket-Vektoren durch Spalten dargestellt. Die zugehörigen bra-Vektoren sind dann Zeilenvektoren: formula_80 bzw. formula_81. Die Projektionsoperatoren (durch Matrizenmultiplikation):
Dies sind auch die Dichtematrizen für vollständig in formula_83- bzw. formula_84-Richtung polarisierte Elektronen.

Die formula_85-Komponente des Spins hat die aus den Eigenwerten gebildete Diagonalmatrix formula_86 Für das vorausgesagte Messergebnis ergibt sich für das Ensemble formula_87 richtig

Für das Ensemble formula_89 ergibt sich
formula_90

Die Zustände von in formula_91- bzw. formula_92-Richtung polarisierten Elektronen sind
formula_93
Die Projektionsoperatoren dazu haben (in der Basis der formula_94-Eigenzustände!) die Matrizen formula_95
Charakteristisch ist, dass dies keine Diagonalmatrizen sind und dass sich die verschiedenen Phasen, mit denen die formula_94-Eigenzustände als ket-Vektoren hier überlagert wurden, in den Matrixelementen außerhalb der Hauptdiagonale wiederfinden. Das ist Ausdruck der kohärenten Überlagerung, durch die aus die formula_97-Eigenzustände gebildet werden.

Sind die Elektronen je zur Hälfte in formula_98-Richtung polarisiert, heißt die Dichtematrix:

Die gleiche Dichtematrix ergibt sich für ein Gemisch aus Elektronen, die zu je 50 % in formula_100-Richtung (oder in eine beliebige andere Richtung) polarisiert sind. Damit sind auch alle möglichen Messergebnisse identisch zu denen am Ensemble, das aus formula_98-polarisierten Elektronen gebildet wurde. Die ursprünglichen zur Definition des Ensembles benutzten Polarisationsrichtungen sind physikalisch (und damit auch begrifflich) nicht mehr zu unterscheiden: Es ist immer "ein und dasselbe" Ensemble entstanden.

Beispielsweise für ein Gemisch aus Elektronen mit Spin in formula_102-Richtung und formula_103-Richtung mit Anteilen formula_104 bzw. formula_105 heißt die Dichtematrix

Der Erwartungswert des Spins in formula_98-Richtung ist dann
Die in (formula_92)-Richtung polarisierten Elektronen tragen also erwartungsgemäß nichts zum Erwartungswert formula_110 bei.

Gegeben sei ein quantenmechanisches System, das auf einem Hilbertraum formula_111 modelliert ist.
Ein beschränkter linearer Operator formula_112 auf formula_111 ist ein Dichteoperator, wenn gilt:

Obwohl die Begriffe Dichtematrix und Dichteoperator oft synonym gebraucht werden, besteht ein mathematischer Unterschied. Genau wie in der linearen Algebra eine Matrix die Basisdarstellung eines linearen Operators ist, kann in der Quantenmechanik zwischen abstraktem "Dichteoperator" und einer konkreten "Dichtematrix" in einer bestimmten Darstellung unterschieden werden. Ist formula_114 ein Dichteoperator, so bezeichnet
die Dichtematrix in Ortsdarstellung. Sie ist allerdings keine echte Matrix, da die Ortsdarstellung über ein Kontinuum von "uneigentlichen" Basisvektoren formula_116 definiert ist, sondern ein so genannter Integralkern.

In endlichdimensionalen Hilberträumen (z. B. bei Spinsystemen) ergibt sich dagegen dann eine positiv semidefinite Matrix mit Spur 1, also eine echte Dichtematrix, wenn eine Orthonormalbasis formula_117 gewählt wird:




Ist das betrachtete Ensemble ein "reines Ensemble", besteht das System also nur aus einem reinen Zustand, so gilt für die Dichtematrix
formula_129.

Für gemischte Zustände gilt stets formula_130.

Ein formula_40-Niveau-System, bei dem alle formula_132 Zustände gleich wahrscheinlich sind, hat die Dichtematrix
wobei formula_134 die formula_40-dimensionale Einheitsmatrix bezeichnet.

Der reduzierte Dichteoperator wurde 1930 durch Paul Dirac eingeführt. Er bezieht sich auf ein herausgegriffenes Teilsystem eines zusammengesetzten Systems und dient dazu, die Ergebnisse von Messungen an dem Teilsystem vorherzusagen, wenn die übrigen Teile des Systems gar nicht mitbeobachtet werden.

Sind formula_49 und formula_137 zwei Systeme mit (normierten) Zuständen formula_138 in ihrem jeweiligen Hilbertraum formula_139, dann hat das zusammengesetzte System formula_140 den Tensorraum formula_141 zum Hilbertraum. Das Gesamtsystem befindet sich in einem "separablen" Zustand formula_142, wenn feststeht, dass die beiden Teilsysteme sich in den Zuständen formula_143 bzw. formula_144 befinden. Allgemein befindet sich das Gesamtsystem in einem Zustand
(mit orthonormierten Basisvektoren formula_146 und Konstanten formula_147 ), der als verschränkt bezeichnet wird, wenn er sich "nicht" als separabler Zustand darstellen lässt.

Für eine Observable des Teilsystems formula_49 ist der Operator formula_149 zunächst nur im Hilbertraum formula_150 definiert. Für die Messung dieser, nur das System formula_49 betreffenden Observablen am Gesamtsystem muss der Operator gemäß formula_152 zu einem Operator auf formula_141 erweitert werden, wobei formula_154 der Einheitsoperator in formula_155 ist.

Ist der Zustand des Systems separabel, dann ergibt sich der Erwartungswert
Das stimmt mit dem Ergebnis überein, das man erhält, wenn man das Teilsystem formula_49 von vornherein als ein isoliertes System betrachtet.

Im Allgemeinen hingegen folgt für den Erwartungswert:
Darin ist mit
der reduzierte Dichteoperator für das Teilsystem formula_49 definiert, wenn das Gesamtsystem im Zustand formula_161 ist. Er ist ein Operator im Raum formula_162 und entsteht, wenn in der Matrix des Dichteoperators für das Gesamtsystem
durch Summierung über den Index formula_164 der Basiszustände des Teilsystems formula_137 die partielle Spur gebildet wird.

Eine einfache Interpretation ergibt sich für den Fall, dass es sich bei der Basis formula_166 um die Eigenvektoren des Operators formula_149 handelt (mit Eigenwerten formula_168). Dann ist der Erwartungswert von formula_149 ein inkohärent gewichteter Mittelwert von dessen Eigenwerten:
Für den Fall, dass das Gesamtsystem in einem separablen Zustand vorliegt, z. B. formula_171, ergibt diese Formel das erwartete Ergebnis formula_172 denn alle Glieder mit Index formula_173 sind Null, und die Summe formula_174 ist die Norm von formula_175, also gleich 1.

Der Einteilchendichteoperator ist bei einem Vielteilchensystem der auf den Hilbertraum eines Teilchens reduzierte Dichteoperator. Bei Systemen identischer Teilchen genügt die Kenntnis des Einteilchendichteoperators, um Erwartungswerte und Übergangsmatrixelemente jedes Operators auszurechnen, der die Summe von Einteilchenoperatoren ist. Das betrifft z. B. die kinetische Energie und die potenzielle Energie in einem äußeren Feld und ist daher ein wichtiges Hilfsmittel bei der Modellierung der Elektronenhülle von Atomen und Molekülen. Die Berechnungen werden häufig in Ortsdarstellung durchgeführt, also basierend auf der N-Teilchen-Wellenfunktion formula_176. Darin sind formula_177 die Orts- und Spinkoordinate des i-ten Teilchens. In der Matrixdarstellung treten sie hier als z. T. kontinuierliche Indizes auf und werden deshalb nicht als unterer Index, sondern wie das Argument einer Funktion geschrieben. Die Dichtematrix des Gesamtsystems heißt
Die Einteilchendichtematrix ist dann
Die Wahl der (N-1) Integrations- (bzw. Summations-)variablen mit den Nummern 2 bis formula_40 ist beliebig, da die Wellenfunktion bei identischen Teilchen gegenüber Umnummerierung höchstens das Vorzeichen wechselt und daher für die Einteilchendichtematrix immer dasselbe Ergebnis herauskommt.

Das Diagonalelement formula_182 gibt die Gesamtdichte an, die die formula_40 Teilchen am Ort formula_184 mit Spinrichtung formula_185 bilden.

Da der Einteilchendichteoperator formula_186 hermitesch ist, gibt es eine Basis formula_187 aus Eigenzuständen: formula_188. Für die Eigenwerte gilt formula_189 und formula_190. Die formula_40 Eigenzustände mit den größten Eigenwerten heißen "natürliche Orbitale". Wenn man jedes natürliche Orbital mit einem Teilchen besetzt, also einen Zustand in Form der Slater-Determinante bildet, stellt diese die beste Annäherung an die ursprüngliche N-Teilchen-Wellenfunktion formula_161 dar, die man im Rahmen eines Einzelteilchenmodells in Bezug auf die gesamte Teilchendichte erreichen kann.

Aus der Schrödingergleichung, die die Zeitentwicklung (Dynamik) reiner Quantenzustände beschreibt, kann man unmittelbar die Zeitentwicklung gemischter Zustände ableiten. Dazu benutzt man eine beliebige Zerlegung der Dichtematrix in reine Zustände, deren Dynamik der Schrödinger-Gleichung genügt, und berechnet daraus die Dynamik des gemischten Zustandes zu
wobei formula_194 der Hamilton-Operator des Systems ist. Diese Gleichung ist als von-Neumann’sche Bewegungsgleichung bekannt (nicht zu verwechseln mit der Heisenberg’schen Bewegungsgleichung).

Diese Differentialgleichung kann man für zeitunabhängige Hamilton-Operatoren lösen und erhält mit dem unitären Zeitentwicklungs-Operator formula_195 die Gleichung

Diese Lösung kann man durch Einsetzen leicht überprüfen.

Bemerkenswert ist hierbei, dass für den Operator formula_197 die übliche Heisenberg'sche Bewegungsgleichung "nicht" gilt, da der Zeitentwicklungsoperator der direkt aus der Schrödingergleichung abgeleiteten Dynamik formula_198 gehorcht. Auch die Zeitentwicklung des Operators formula_199 durch den Zeitentwicklungsoperator formula_197 erfolgt nicht gemäß der üblichen Zeitentwicklungsgleichung für Operatoren (formula_201 für eine gewöhnliche Observable A), was jedoch verständlich ist, da formula_202

Mit Hilfe der Dichtematrix formula_203 lässt sich die Von-Neumann-Entropie eines Systems wie folgt definieren:

wobei formula_205 die Boltzmannkonstante ist, und die Spur über dem Raum formula_111 genommen ist, in dem formula_203 operiert.

Die Entropie jedes reinen Zustands ist Null, da die Eigenwerte der Dichtematrix Null und Eins sind. Dies stimmt mit der heuristischen Argumentation überein, dass keine Unsicherheit über die Präparation des Zustandes herrscht.

Man kann zeigen, dass auf einen Zustand angewendete unitäre Operatoren (wie der aus der Schrödinger-Gleichung gewonnene Zeitentwicklungs-Operator) die Entropie des Systems nicht ändern. Das verbindet die Reversibilität eines Prozesses mit seiner Entropieänderung – ein fundamentales Ergebnis, das die Quantenmechanik mit der Informationstheorie und der Thermodynamik verbindet.






Mikrobarome

Mikrobarome (englisch "microbaroms") sind schwache niederfrequente Überlagerungen des Luftdrucks. Sie können sowohl als einzelne Druckschwankungen auftreten oder aber auch als kohärente Druckwellen über einen längeren Zeitraum. Sie sind ein Bestandteil des Hintergrundrauschens, das den Luftdruck begleitet.

Als Ursache der Mikrobarome sind sowohl das Wettergeschehen verantwortlich als auch eine Koppelung von hohem Seegang (bei Stürmen) mit der Atmosphäre. Als Infraschallereignisse haben sie aufgrund der geringen Dämpfung der Atmosphäre in diesem Frequenzbereich eine große Reichweite von tausenden Kilometern und können mit empfindlichen Mikrobarometern registriert werden. Die Hauptenergie der Mikrobarome liegt im Bereich von 0,1 bis 0,5 Hz, mit einem Maximum bei 0,2 Hz. Die Druckschwankungen liegen bei unter einem Pascal (andere Angaben: 0,1–1 Mikrobar).

Zurzeit lassen sich Daten zu Mikrobaromereignissen noch nicht praktisch anwenden. Es gibt keine Möglichkeit, diese Daten, z. B. für eine Wetterprognose, einzusetzen. Allerdings ist die Kenntnis dieses Phänomens wichtig, um Infraschallereignisse richtig deuten zu können. Mit Hilfe von Infraschallaufzeichnungen wird weltweit die Einhaltung des Verbotes von Atombombentests, im Rahmen des Vertrages zum Verbot von Nuklearwaffentests in der Atmosphäre, im Weltraum und unter Wasser, überwacht. Mikrobarome erzeugen manchmal Signale, die mit den Druckwellen von Atombombenexplosionen verwechselt werden können.





Diproton

Ein Diproton ist ein hypothetischer Atomkern des Heliums, bestehend aus zwei Protonen und ohne Neutronen. Die starke Wechselwirkung zwischen den Protonen wirkt anziehend, gleichzeitig wirkt aber auch eine elektrostatische Abstoßung. Aus dem Zusammenspiel beider Wechselwirkungen ergibt sich, dass das Diproton nicht gebunden ist und damit nicht stabil.

Wenn die Stärke der starken Wechselwirkung nur um 2 Prozent stärker wäre, wäre das Diproton stabil. Dieser Fall wird manchmal als Diprotonenkatastrophe bezeichnet: In diesem Falle wäre Leben im Universum nicht möglich. Die Fusion innerhalb der Sterne würde statt durch die schwache durch die starke Wechselwirkung bewirkt werden und etwa 10-mal schneller ablaufen. Sterne würden die Kernfusion so schnell durchführen, dass sich kein Leben entwickeln könnte (Freeman Dyson).

Dass das Diproton nicht gebunden ist, lässt sich durch das Pauli-Prinzip erklären zusammen mit der Tatsache, dass die starke Wechselwirkung für parallele Spins stärker als für antiparallele Spins ist. Dies ist auch der Grund, warum das Deuteron nur im Triplett-Zustand (S=1) existiert. Da die Protonen im hypothetischen Diproton identische Fermionen sind, unterliegen sie dem Pauli-Prinzip, d. h., sie können nicht dieselben Quantenzahlen haben. Die Spins müssen antiparallel ausgerichtet sein (Singlett). Dieser Zustand ist aber aufgrund Spin-Abhängigkeit der Kernkraft nicht gebunden (die Kernkraft ist stärker bei parallelem Spin der Nukleonen). Die gleiche Überlegung gilt auch für das Dineutron.

Diprotonen wurden 2002 bei Zweiprotonenemission als kurzlebige Zustände beobachtet.




Newton-Skala

Die Newton-Skala ist eine Temperaturskala, die um das Jahr 1700 von Isaac Newton vorgeschlagen wurde. Als sich Newton mit dem Problemfeld der Wärme beschäftigte, entwickelte er eine erste qualitative Temperaturskala, die von "kalter Luft im Winter" bis "glühende Kohlen im Küchenfeuer" etwa 20 Skalenpunkte besaß. Diese Herangehensweise war grob und ungenau, so dass auch Newton recht schnell wieder unzufrieden mit ihr wurde. Da ihm das Konzept der Wärmeausdehnung bekannt war, nutzte er ein Gefäß mit Leinöl und maß dessen Volumenänderung in Bezug auf die früheren Skalenpunkte. Dabei stellte er zwischen der Temperatur von geschmolzenem Schnee und kochendem Wasser eine Volumenzunahme von 7,25 % fest.

Nach einer Weile definierte er den Nullpunkt seiner Skala bei schmelzendem Schnee (Schmelzpunkt, 0 °C) und den 33. Grad als kochendes Wasser (Siedepunkt, 100 °C), nutzte also die gleichen Fixpunkte wie die Celsius-Skala, nur mit anderen Gradabständen. Ein Unterschied von einem Grad Newton (1 °N) entspricht daher in etwa dem von drei Grad Celsius.




Zugriffsindex für die Isotopenlisten einzelner Elemente und für die Perioden des Periodensystems

Diese Liste enthält ein Periodensystem der Elemente mit Zugriffsindex für die Isotopenlisten einzelner Elemente und für alle Elemente einer Periode.

Klicken Sie links in das PSE-Diagramm auf das Element, um zu diesem Element zu gelangen, oder rechts in die Perioden, um zu einer ganzen Periode zu gelangen.
Die Einträge zu den Elementen selbst sind in der deutschsprachigen Wikipedia periodenweise organisiert, so dass Sie innerhalb einer Periode durch Scrollen navigieren können.

Für jedes Isotop enthält die Tabelle
Nuclide: \mathrm




Anpassungsdämpfung

Anpassungsdämpfung (engl.: "matching attenuation") ist in der Hochfrequenztechnik eine Dämpfung durch Reflexion. Die Anpassungsdämpfung tritt auf, wenn eine Leitung nicht mit ihrer Wellenimpedanz abgeschlossen ist oder Leitungen mit unterschiedlichen Wellenimpedanzen gekoppelt werden. Am Ort der Fehlanpassung entsteht im Hochfrequenzbereich durch Reflexion eine zum Leitungsanfang zurück laufende Welle als Störung.
Bei jeder Schnittstelle bildet der Ausgangswiderstand der Quelle mit dem Eingangswiderstand der Last eine Anpassungsdämpfung, die auch Schnittstellendämpfung genannt wird. Mit
ist die Anpassungsdämpfung formula_3
und das Anpassungsdämpfungsmaß

Eine Anpassungsdämpfung gibt es auch im Audio-Bereich, also in der Tontechnik. Beispielsweise erleidet ein Mikrofon, das in den Eingang einer Soundkarte gesteckt wird eine Anpassungsdämpfung. Wie gut, wenn das Mikrofon niederohmig ist, also einen niedrigen Ausgangswiderstand (Quellwiderstand formula_1, Innenwiderstand) von 50 Ω besitzt und es sich um Niederfrequenz im Hörbereich bis maximal 40 kHz (Doppelte Hörgrenze) handelt. Als Lastwiderstand formula_7 tritt der Eingangswiderstand des Mischpults (Mikrofonvorverstärker) mit 1000 bis 2000 Ω auf. Eine Anpassungsdämpfung von kleiner 1 dB ist problemlos.

Besonders ist der wichtige Dämpfungsfaktor bei der Schnittstelle vom Endverstärker mit 0,1 Ω Ausgangswiderstand zum üblichen 8-Ω-Lautsprecher als Anpassungsdämpfung zu beachten. Hier gibt es keine Leistungsanpassung.





Quanten-Fouriertransformation

Die Quanten-Fouriertransformation ist ein Algorithmus aus dem Gebiet der Quanteninformatik. Sie ist eine Zerlegung der diskreten Fouriertransformation in ein Produkt unitärer Matrizen. Dadurch kann sie als Quantenschaltkreis aus Hadamard-Gattern und Phasengattern implementiert werden.

Die Quanten-Fouriertransformation ist ein wesentlicher Bestandteil eines der prominentesten Quantenalgorithmen, des Shor-Algorithmus.

Am einfachsten wird die Struktur der Quanten-Fouriertransformation anhand des entsprechenden Quantenschaltkreises sichtbar. In der folgenden Skizze findet man ihn für formula_1 (ohne die noch erforderliche Umkehrung der Reihenfolge der Zustände der Ausgaben). Dort ist die übliche Notation für die binäre Darstellung der Phasenterme genutzt, d. h. formula_2 usw.

Die Situation für 1-, 2- und 3-Qubit-Register wird auf der Seite des Wolfram Demonstrations Project dargestellt.

Daran kann man leicht erkennen, wie die Schaltkreise für größere Quantenregister aussehen. Die mit formula_3 beschrifteten Quantengatter stellen Hadamard-Gatter dar, während die mit formula_4 beschrifteten Gatter Phasengatter repräsentieren, die hier als gesteuerte Gatter eingesetzt werden (Steuer-Qubit wie üblich durch schwarzen Punkt und Verbindungslinie zum Ziel-Qubit angedeutet; Controlled Phase).

Die einzelnen Gatter werden jeweils durch folgende unitäre Matrizen beschrieben.
Dabei bezeichnet formula_6 die formula_7-te Einheitswurzel formula_8.

Eine verallgemeinerte Schaltskizze ist in folgender Grafik zu sehen, wieder ohne die erforderliche Umkehrung der Reihenfolge der Ausgabe-Zustände. Hier ist wieder die binäre Darstellung in den Ausgabezuständen genutzt.

Die Quanten-Fouriertransformation benötigt bei formula_9 Qubits insgesamt formula_10 Gatter
für den entsprechenden Schaltkreis sowie formula_11 weitere, um die Output-Qubits in die richtige Ordnung zu bringen.

In der Quanteninformatik werden Algorithmen durch ihre Wirkung auf ein Quantenregister beschrieben. Die Quanten-Fouriertransformation arbeitet auf einem Quantenregister mit formula_9 Qubits, wobei dessen formula_13 Basiszustände unter Verwendung der Bra-Ket-Notation folgendermaßen notiert werden:
Als diskrete Fouriertransformation bildet auch die Quanten-Fouriertransformation jeden Basiszustand formula_15 auf eine Überlagerung aller Basiszustände ab:
Alternativ kann die Quanten-Fouriertransformation auch mittels der folgenden Faktorisierung berechnet werden:
Berechnet man sie mit Hilfe der Verallgemeinerung der obigen Quantenschaltkreis-Idee, erhält man fast die obige Faktorisierung, allerdings in umgekehrter Reihenfolge, konkret:

Wendet man die Quanten-Fouriertransformation auf den Zustand formula_19 an, so erzeugt sie genauso wie die Hadamard-Transformation eine gleichgewichtete Superposition der Basiszustände:

Des Weiteren besitzt die Quanten-Fouriertransformation natürlich auch alle Eigenschaften der diskreten Fouriertransformation.





Querstabilität

Der Begriff Querstabilität beschreibt die Fähigkeit eines Flugzeuges, auf um die Längsachse auftretende Störungen (beispielsweise Böen) so zu reagieren, dass sich ohne Zutun des Piloten wieder die ursprüngliche Fluglage einstellt. Querstabilität wirkt der Rollneigung eines Flugzeugs entgegen.

Die Querstabilität eines Flugzeuges wird insbesondere bestimmt durch V-Stellung und Pfeilung der Tragflächen.

Außerdem beeinflusst die vertikale Anordnung der Tragflächen am Rumpf (Hochdecker, Tiefdecker, …) die Querstabilität.

Querstabilität ist ein wichtiger Aspekt im Flugzeugbau. 
Mit zunehmender Querstabilität wird das Flugverhalten 'gutmütiger' aber gleichzeitig weniger agil bzw. sensibel für Steuerbefehle um die Längsachse.
Üblicherweise werden die o. g. Merkmale kombiniert um – je nach Anforderung – einen bestimmten Grad der Stabilität zu erzielen.

Als einfache Faustregeln gelten:

Die Querstabilität eines Flugzeuges nimmt zu mit





Laboratoire Souterrain de Modane

Das Laboratoire Souterrain de Modane, bekannter unter dem englischen Namen Modane Underground Laboratory, ist eine unterirdische Versuchsstation für Experimente der Teilchenphysik.

Es ist abzweigend vom Mont-Cenis-Tunnel zwischen Modane und Bardonecchia an der französisch-italienischen Grenze eingerichtet worden. Dadurch entspricht seine Abschirmung der von 4800 m Wasser, dem üblichen Maß zur Beurteilung der Abschirmung vor kosmischer Strahlung. Diese wird dadurch auf ein Millionstel verringert.

Die wichtigsten Experimente im Laboratoire Souterrain de Modane sind EDELWEISS, zur Suche nach WIMPs, den hypothetischen Teilchen der Dunklen Materie und NEMO, zur Suche nach dem neutrinolosen doppelten Betazerfall, eine Überprüfung des Standardmodells der Elementarteilchenphysik.

Das Laboratoire Souterrain de Modane ist, wie die drei anderen europäischen Untergrundlaboratorien
Laboratori nazionali del Gran Sasso,
Laboratorio subterráneo de Canfranc und
Boulby Underground Laboratory,
der Koordinierungsgruppe ILIAS angeschlossen.





Homogenes Feld

In der Physik versteht man unter einem homogenen Feld ein Feld, dessen Feldstärke nicht vom Ort abhängt – die Kraft auf einen (Probe-)Körper in einem homogenen Feld ist also stets überall gleich groß und gleich gerichtet. 

Felder, für die das nicht gilt, heißen inhomogen.

Gibt es zwischen zwei Punkten eines Magnetfelds weder eine Flussänderung noch einen Feldstärkenunterschied und ist es ein gleichbleibendes Feld, dessen Feldlinien allesamt parallel in dieselbe Richtung verlaufen, spricht man von einem homogenen Magnetfeld.





Duffing-Oszillator

Der Duffing-Oszillator, benannt nach Georg Duffing, ist ein nichtlinearer Oszillator. Er kann als Erweiterung des harmonischen Oszillators, dessen Potential das lineare hookesche Gesetz zu Grunde liegt, um eine kubische Rückstellkraft betrachtet werden.
Sein Verhalten wird durch folgende Differentialgleichung mit den zeitlichen Ableitungen von x beschrieben:

formula_2 ist die Dämpfung, formula_3 sind die Amplitude und Frequenz der Anregung, formula_4 sind systemspezifische Parameter, welche die nichtlineare, rücktreibende Kraft charakterisieren.

Die Zustandsraumdarstellung des homogenen Duffing-Oszillators formula_5 ist

Für den stationären Fall gilt

und damit

Die Gleichung liefert für formula_10 drei stationäre Lösungen

Diese sind nur dann reell, wenn formula_12 ist. Zur Beurteilung, welche dieser stationären Lösungen stabil sind, wird das Differentialgleichungssystem um diese Punkte linearisiert. Die Jacobi-Matrix des Systems 

hat für formula_14 die Eigenwerte

und für formula_16 die Eigenwerte

Die Bedingung formula_12 liefert zwei Fälle.

Fall 1: formula_19 und formula_20

Fall 2: formula_23 und formula_24

Die Differenzialgleichung 

mit formula_28 beschreibt den stabilen Duffing-Oszillator.





Neutretto

Neutretto ist die historische Bezeichnung für ein elektrisch neutrales Meson (damit war kein Meson im heutigen Sinne gemeint; als Mesonen wurden damals alle Teilchen mit einer Masse zwischen der eines Elektrons und eines Protons bezeichnet). Der Ausdruck wurde erstmals 1938 von Walter Heitler und Niels Arley (1911–1994) für das "„neutrale Gegenstück des schweren Elektrons“" (Myons) vorgeschlagen, das als hypothetisches Teilchen in den Kaskadenzerfällen der kosmischen Strahlung vermutet wurde. 

Ab etwa 1960 war Neutretto eine Alternativbezeichnung für das Myon-Neutrino. Der Name entstand in Anlehnung an das schon länger bekannte Elektron-Neutrino, da sowohl "-ino" als auch "-etto" Verkleinerungsformen des Italienischen sind.

Im modernen Standardmodell der Elementarteilchen wird die Bezeichnung Neutretto nicht mehr verwendet.




André-Eugène Blondel

André-Eugène Blondel (* 28. August 1863 in Chaumont (Haute-Marne); † 15. November 1938 in Paris) war ein französischer Physiker. Er gilt als Erfinder des Oszillographen und hat ein System photometrischer Einheiten entwickelt, das mit kleineren Modifikationen bis heute in Gebrauch ist.

André-Eugène Blondel schloss 1888 die École des Ponts et Chaussées als Klassenbester ab. Nach einer Anstellung als Ingenieur wurde er später Professor für Elektrotechnik an seiner Alma Mater.

1893 erfand er den elektromagnetischen Oszillographen. 1894 schlug er das Lumen und andere Maßeinheiten der Photometrie vor. Diese wurden 1896 auf dem Internationalen Elektrikerkongress in Genf angenommen und sind bis heute in Gebrauch.

Blondels Beiträge zur Wissenschaft umfassen auch die drahtlose Telegrafie, die Akustik und die Mechanik, außerdem entwickelte er Vorschläge für eine Theorie des Induktionsmotors.

Eine (veraltete) Einheit der Leuchtdichte wurde nach ihm Blondel benannt.



 




Baryonische akustische Oszillation

Baryonische akustische Oszillationen () sind Dichtewellen, die sich im frühen Universum durch das Wechselspiel von Gravitation und Strahlungsdruck ausbildeten. Ihre Auswirkungen sind heute im Anisotropiespektrum der kosmischen Hintergrundstrahlung und in der Verteilung der Galaxien beobachtbar. Ihre genaue Vermessung spielt eine wichtige Rolle in der modernen Kosmologie.

Nach der gängigen Theorie besteht Materie aus baryonischer und aus dunkler Materie. Nach der Inflation ist die Materie nicht absolut homogen verteilt, es gibt Anfangsfluktuationen. Durch Gravitation wird die baryonische Materie in Gebiete mit höherer Dichte an dunkler Materie hineingezogen, die Dichte der Baryonen nimmt zu. Dadurch steigt der Photonendruck, drängt die baryonische Materie wieder auseinander und bewirkt so eine Abnahme der Baryonendichte. Durch dieses Wechselspiel aus Anziehung aufgrund Gravitation und Abstoßung aufgrund Photonendruck beginnt die baryonische Materie zu oszillieren.

Dieser Mechanismus bricht bei der Entkopplung von Strahlung und Materie ca. 380.000 Jahre nach dem Urknall zusammen. Die baryonische Materie unterliegt fortan nur noch der Gravitation, die Photonen aber „speichern“ die Information über die Dichte der Baryonen an ihrem Entstehungsort zum Zeitpunkt der Entkopplung in Form einer Temperatur.





Physikalische Zeitschrift

Die Physikalische Zeitschrift war eine von 1900 bis 1945 im S. Hirzel Verlag erscheinende deutsche Fachzeitschrift. Im Jahr 1924 erfolgte die Vereinigung mit dem "Jahrbuch der Radioaktivität und Elektronik", welches im Folgenden als Untertitel geführt wurde.

Im Vorwort der ersten Ausgabe, die auf den 1. Oktober 1899 datiert ist, nahm Herausgeber Eduard Riecke eine Zweckbestimmung der neuen Zeitschrift vor und beschrieb insbesondere die intendierte Stellung zu den renommierten Annalen der Physik. Nach seinen Vorstellungen sollte die Physikalische Zeitschrift dem interessierten Leser mit Hilfe von Referaten, Zusammenfassungen und Mitteilungen wissenschaftlicher Gesellschaften einen Überblick über die Forschungslandschaft vermitteln und – anders als die Annalen – nicht nur „zu einem gewissen Abschluss gelangte Untersuchungen“ aufgreifen.

Die Absichten spiegelten sich in den – über die Jahre leicht wechselnden – Inhaltsverzeichnissen der Zeitschrift wider, wenn es dort Rubriken wie "Originalmitteilungen, zusammenfassende Berichte, Besprechungen, Nachrichten, Vorankündigungen, Vorträge, Referate, Briefkasten" oder "Personalien" gab.

Es wurden zudem in knapper Form die physikalischen Vorlesungsverzeichnisse deutschsprachiger Universitäten abgedruckt, und es fanden sich Nachrufe auf bedeutende Kollegen, Buchvorstellungen und Grundrisse von Institutsneubauten. Gleichwohl publizierte die Physikalische Zeitschrift von Anfang an ausführliche Abhandlungen, wie sie auch in den Annalen zu finden sind.

Der Umfang der Zeitschrift wuchs schnell an, so dass sich Herausgeber (Eduard Riecke, Hermann Theodor Simon), Redaktion (E. Bose) und Verlag (S. Hirzel) im Dezember 1904 an die Leser wendeten, um für Verständnis für eine Preiserhöhung zu werben. Der Abonnementenpreis von 20 Mark sei für drei Druckbogen kalkuliert worden, und man werde den Preis im Folgenden auf 25 Mark bei vier Druckbogen pro Nummer anheben.

Der erste Jahrgang der Zeitschrift lief vom 1. Oktober 1899 bis Ende September 1900 und wurde von E. Riecke (Göttingen) und H. Th. Simon (Frankfurt) herausgegeben. Simon übernahm auch die Redaktion. Es erschienen 52 Hefte („Nummern“) mit jeweils 16 Seiten Umfang. Bereits im ersten Jahr wurden Beiträge renommierter Wissenschaftler wie Max Abraham, Ludwig Boltzmann, Friedrich Paschen, Augusto Righi, Ernest Rutherford und Wilhelm Wien veröffentlicht. Ab 1909 veröffentlichte auch Albert Einstein einige Arbeiten – beispielsweise über Relativitätstheorie und Quantenphysik.

Ab dem dritten Jahrgang 1902 erschienen unterschiedlich viele Hefte pro Jahr mit 32 oder mehr Seiten. Häufig waren es 25 Nummern, die 14-täglich erschienen. Der fünfte Jahrgang 1904 umfasste erstmals ein Kalenderjahr. Nach E. Bose (1904) und F. Krüger (1909) übernahm am 1. Juli 1913 Hans Busch die Redaktion.

1915 wurde Riecke mit Peter Debye ersetzt, die Redaktion teilten sich Busch, Max Born und Simon. Da Busch und Born zum Kriegsdienst eingezogen wurden, übernahm Simon zum 1. Januar 1916 wieder allein die Redaktion, die fortan als "Schriftleitung" bezeichnet wurde. Im Jahr 1924 erfolgte die Vereinigung mit dem "Jahrbuch der Radioaktivität und Elektronik", welches im Folgenden als Untertitel geführt wurde.

Im Jahre 1919 wurde Debye alleiniger Herausgeber, zum 22. Jahrgang 1921 kam Max Born hinzu, der 1924 von Friedrich Harms (1876–1946) abgelöst wurde. Von 1931 bis 1945 war Debye wieder alleiniger Herausgeber, ab 1934 unter „Mitwirkung der Physikalisch-Technischen Reichsanstalt“.

Als letzte Ausgabe erschienen in einem gemeinsamen Heft die Nummern 16–18 des 45. Jahrgangs, als Redaktionsschluss ist in der Vorausgabe der 15. Januar 1945 vermerkt. Anders als zuvor ist bei diesem letzten Heft kein genaues Veröffentlichungsdatum mehr genannt, sondern lediglich „im Februar 1945“ vermerkt.




Jean-Baptiste van Mons

Jean-Baptiste Ferdinand Antoine Joseph van Mons (* 11. November 1765 in Brüssel; † 6. September 1842 in Löwen) war ein belgischer Physiker, Chemiker, Botaniker, Gärtner und Pomologe, und 1817–1830 in Löwen Professor für Chemie und Agrarwissenschaften.

Jean-Baptiste van Mons wurde 1765 in Brüssel als zweitältester Sohn von Ferdinand-Philippe van Mons (1719–1794) und dessen Frau Marie Catherine Josèphe geborene Colins (1733–1798) geboren. Er hatte vier Brüder und eine Schwester und wuchs in ärmlichen Verhältnissen auf, so dass er früh eine Apothekerlehre beginnen musste.

Am 16. August 1792 heiratete van Mons Wilhelmine-Antoinette van Coeckelberghe (* 1765 in Heusden-lez-Gand), die am 26. August 1794, drei Monate nach der Geburt der ersten Tochter Antoinette (* Mai 1794, † 19. März 1795), starb.

Am 28. Februar 1795 heiratete er daraufhin in zweiter Ehe Jeanne-Agnès Dillen († 1817), mit der er vier Söhne bekam:

Van Mons hatte im Laufe seines Lebens mehrere persönliche Rückschläge zu verarbeiten. Sein jüngster Sohn starb im Jahr 1815 mit 13 Jahren, zwei Jahre später verlor er auch seine Frau. Dies bewog ihn, Brüssel zu verlassen und den Ruf an die Universität Löwen anzunehmen.
Nachdem er durch äußere Umstände dreimal einen Großteil seines Baumbestandes einbüßen musste und im Jahr 1837 auch sein Sohn Charles-Jacques starb, zog sich van Mons verbittert aus der Öffentlichkeit und von seinen Bekannten und Freunden zurück und vernachlässigte sich selbst zunehmend.

Van Mons starb am 6. September 1842 im Alter von 76 Jahren in Löwen.

Van Mons absolvierte zunächst eine Apothekerlehre und betrieb später in Brüssel eine Apotheke. Im Selbststudium lernte er Chemie und schrieb bereits im Alter von 20 Jahren einen Aufsatz über neue Erkenntnisse dieses Fachgebietes. Er stand später in regem fachlichen Austausch mit Chemikern wie Antoine Laurent de Lavoisier, Antoine François de Fourcroy, Gaspard Monge und Jean Nicolas Pierre Hachette.

Im Jahr 1797 wurde er zum Professor für Physik und Chemie an der Zentralschule des Département Dyle ernannt. Er war Herausgeber der Fachzeitschrift Annales de Chimie und begründete 1801 die Zeitschrift "Journal de chimie et de physique".
Van Mons führte als erster Impfungen in Belgien durch.

Im Jahr 1807 wurde er in Paris zum Dr. med. ernannt. Bei der Rekonstitution der Belgischen Akademie der Wissenschaften in Brüssel im Jahr 1816 wurde er zu deren Mitglied ernannt und wurde im Jahr 1817 als Professor für Chemie und Agronomie an die neu gegründete Universität Löwen berufen. Hier unterrichtete er bis 1836.

Van Mons wurde in Anerkennung seiner wissenschaftlichen Leistungen zum Membre associé des Institut de France ernannt.

Bereits als Jugendlicher begann van Mons mit Aussaatversuchen an Balsaminen und Indischen Rosen (Hibiskus). Um 1785 begann van Mons mit Versuchen zur Aussaat von Obstkernen. Außerhalb von Brüssel legte er die Baumschule "de la Fidelité" an, in der er Sämlinge aufzog. Bereits 1815 soll er einen Bestand von ca. 80.000 Birnen-Sämlingen gehabt haben. Obwohl er 1817 als Professor an die Universität Löwen berufen wurde, behielt er zunächst seinen Versuchsgarten in Brüssel. 1819 beschloss der Magistrat der Stadt Brüssel, die Stadt in dem Bereich, in dem van Mons ca. 2 ha große Baumschule lag, zu bebauen, weshalb van Mons enteignet wurde. Auf dem Gelände standen zu diesem Zeitpunkt mehr als 50.000 Gehölze, die teilweise älter als 20 Jahre waren. Van Mons musste das Gelände mitten im Winter innerhalb von nur 6 Wochen räumen, so dass er nur ca. 5 % der Pflanzen auf neue Grundstücke in Löwen, die ihm die Universität zur Verfügung stellte, umsiedeln konnte. Von den meisten Bäumen konnte er nur Edelreiser gewinnen.

In Löwen setze van Mons seine Aussaatversuche fort. 1823 hatte er wieder einen Bestand von 50.000 Bäumen aufgebaut und gab einen beschreibenden Katalog seiner Obstbaumbestände heraus, der 1.050 Birnensorten enthielt.

Im Jahr 1831 benutzten französische Truppen, die zur Unterstützung des Königreichs Belgien in Löwen stationiert wurden, seinen Obstgarten als Standquartier, wodurch van Mons die gesamte Obsternte verlor. 1834 erhob schließlich die belgische Regierung Anspruch auf die beiden größten Gärten, da auf dem Gelände eine Leuchtgasfabrik errichtet werden sollte. Dies bedeutete für van Mons erneut einen großen Verlust an Bäumen.

1835/1836 veröffentlichte van Mons das zweibändige Buch "Arbres fruitiers: leur culture en Belqique et leur propagation par la graine: ou, Pomonomie Belge, expérimentale et raisonnée", mit dem er die Verbreitung seiner Sorten förderte und in dem er die von ihm entwickelte Züchtungstheorie der "successive Regeneration" der Obstsorten propagierte. Dabei ging van Mons davon aus, dass Obstsorten im Laufe mehrerer Generationen vegetativer Vermehrung altern und dadurch einen qualitativen Leistungsabfall zeigen. Er zog diesen Schluss aus der Beobachtung, dass die Fruchtqualität alter Birnensorten, von denen während seiner Kindheit Bäume kannte, damals besser war als bei den von ihm kultivierten Bäumen. Diese Theorie wurde bald widerlegt, wurde aber noch bis in die 1940er-Jahre öfter aufgegriffen. Heute vermutet man, dass die von van Mons beobachteten Leistungseinbußen wahrscheinlich auf die Einwirkung des Frostwinters 1828/1829, auf einen Virusbefall oder auf unbemerkte Kleinmutationen zurückzuführen waren.

Van Mons versuchte durch Aussaaten, wüchsige, gut akklimatisierte und robuste Birnensorten zu züchten. Dazu säte er die Samen eines Birnbaumes mit geringer Fruchtqualität über 9 Generationen aus und selektierte die Nachkommen auf hohe Fruchtqualität. Van Mons nahm dabei an, dass mit zunehmender Generationenzahl der Reifezeitpunkt einer Sorte vorverlegt wird und die Fruchtqualität zunimmt und glaubte, dass sich die Sorteneigenschaften bei der Aussaat umso stabiler zeigen, je älter eine Obstsorte ist. Auch wenn diese grundlegenden Annahmen seiner Theorie später durch neue Erkenntnisse zu Vererbung und Sortenzüchtung widerlegt wurden, so zog er doch auch schon zu diesem frühen Zeitpunkt richtige Schlussfolgerungen aus seinen Beobachtungen. So erkannte er den Einfluss von Klima und Boden auf die Entwicklung der Sämlinge.

Nach van Mons Tod wurde seine Baumschule von dem Pomologen Alexandre Bivort übernommen, der auch dessen Aussaatversuche fortführte.

Van Mons gilt noch heute als der produktivste Birnenzüchter Belgiens. Aus seinen Aussaatversuchen gingen tausende von Sämlingen hervor, mit denen er in mehreren Aussaatgenerationen weiterarbeitete und aus denen er zahlreiche Sorten aufgrund ihrer Frucht- und Wuchseigenschaften selektierte, weiter vermehrte und verbreitete.
Er arbeitete vor allem mit Birnen, züchtete aber auch einige Apfel- und Steinobstsorten. Van Mons war dafür bekannt, dass er Saatgut, junge Sämlinge und Edelreiser in großem Umfang an pomologische Kollegen abgab, zum einen um von ihm für besonders vermehrungswürdig befundene Sortenzüchtungen zu verbreiten, zum anderen um weitere Kollegen für die Idee der Verbesserung der Obstsorten durch Aussaat und Auslese zu begeistern. Die aus dem Vermehrungsmaterial hervorgegangene Bäume wurden dann zum Teil als Züchtungen der neuen Besitzer angesehen, von diesen mit Sortennamen benannt und pomologisch beschrieben, obwohl sie eigentlich auf van Mons zurückgehen. Da van Mons enge Kontakte mit Pomologen vor allem auch im Ausland unterhielt, mit denen er Reiser seiner Züchtungen austauschte, erlangten seine Züchtungen internationale Verbreitung, in Deutschland vor allem über Oberdieck und Diel, mit denen van Mons in regem fachlichen Austausch stand.

Zahlreiche der van Mons gezüchteten Obstsorten sind heute noch verbreitet.










Funkenspektrometer

Ein Funkenspektrometer (, "OES"; deutsch: "Optisches Emissionsspektrometer") ist ein Gerät zur chemischen Analyse und dient der Darstellung des Emissionspektrums von chemischen Stoffen. Die hauptsächliche Verwendung liegt in der schnellen Analyse von Metalllegierungen. Verwendet wird für einfachere, extrem schnelle Analysen (z. B. Massentest in der Qualitätssicherung, Sortierung von Schrott) ein Lichtbogen zwischen einer Kupfer- oder Silberelektrode und einer grob geschliffenen Materialprobe. Dadurch wird Probenmaterial verdampft und die freigesetzten Atome und Ionen durch Elektronenstoß angeregt. Die emittierte Strahlung wird über Lichtleiter an die optischen Systeme geleitet, wo diese dann in ihre einzelnen spektralen Komponenten zerlegt wird. Jedes Element, das in der Probe enthalten ist, emittiert auf mehreren Wellenlängen und kann somit über Photomultiplier gemessen werden. Die so gemessene Strahlungsintensität verhält sich proportional zur Konzentration des Elements in der Probe.
Für genauere Analysen (z. B. Werkstoffprüfung, Schmelzenführung in Gießereien und Hüttenwerken) wird ein periodischer Funke unter Argon zwischen einer Wolframelektrode und einer sehr sorgfältig aufbereiteten Materialprobe verwendet.

Im Gegensatz zur Röntgenfluoreszenzanalyse können auch die Gehalte von leichteren Elementen wie Kohlenstoff gemessen werden. Bei höheren Gehalten können abhängig von der Existenz geeigneter Eichproben Messgrenzen vorliegen.

Weiterführende Informationen siehe Atomspektroskopie.




Spiralwelle

Spiralwellen, in der englischsprachigen Literatur auch als oder bezeichnet, sind eine spezielle Form von 2-dimensionalen chemischen Wellen. Dreidimensionale Spiralwellen werden auch als bezeichnet. Die Möglichkeit der Entstehung von Spiralwellen wurde erstmals 1946 durch Norbert Wiener und Arturo Rosenblueth theoretisch vorhergesagt. Beobachten lassen sich Spiralwellen beispielsweise bei der Anregung des Herzmuskels.

Autokatalytische Wellen treten entweder als in sich geschlossene zirkuläre Wellenfronten auf, oder sie besitzen zwei Enden, welche die Grenzen des Ausbreitungsmediums berühren. Die dritte Erscheinungsform, nämlich die Spiralwellen, sind hingegen dadurch gekennzeichnet, dass ein Ende der Wellenfront stets an der Mediumgrenze entlang läuft, während das andere, der Kern, sich frei im Medium bewegt. Diese Bewegung kann auf unterschiedlichen Bahnen verlaufen, die von der Art des Mediums und von den speziellen Diffusions- und Reaktionsparametern bestimmt werden. Man kann in sich geschlossene und offene Trajektorien unterscheiden. Bei den in sich geschlossenen Trajektorien findet man Kreisbahnen und elliptische Bahnen. Je nach Konstellation der Parameter können die Kerne aber auch driften. Auf diese Weise entstehen häufig in sich geschlossene Rosetten. Es kann aber auch zu quasiperiodischen Bewegungen kommen, wobei offene Rosetten entstehen. Kollidiert der Kern der Spiralwelle mit einer Mediumgrenze, so entsteht wieder eine geschlossene Wellenfront, indem nun beide Enden der Welle die Mediumgrenze berühren, womit die Existenz der Spiralwelle beendet ist.

Im Prinzip entstehen Spiralwellen durch Aufbrechen einer geschlossenen Wellenfront. Dies kann auf unterschiedliche Weise geschehen.




Nuclear Physics

Nuclear Physics ist eine seit 1956 bei North Holland und nach deren Übernahme 1970 bei Elsevier erscheinende Zeitschrift für Kernphysik und Elementarteilchenphysik. 1967 spaltete sie sich in die Teilreihen 

Sie ist eine der führenden Zeitschriften für Kern- und Teilchenphysik. Seit 1987 erscheinen auch "Nuclear Physics/B", Proceedings Supplements für Konferenzberichte (). 

Bis 1974 war Léon Rosenfeld der Herausgeber. Bei der Gründung 1956 hatte der Verlagsleiter bei North Holland M. D. Frank große Widerstände nationaler Physik-Gesellschaften zu überwinden, gewann dann aber doch zwei namhafte französische Physiker für das Herausgebergremium. Die eigentliche Herausgabe besorgte anfangs W. H. Wimmers, ein dafür eingestellter niederländischer Physiker, der aus der niederländischen Ölbranche in Indonesien kam. North Holland hatte mit "Nuclear Forces" von Leon Rosenfeld 1948 einen ersten Erfolg im Bereich der Physik.

Nuclear Physics A hatte im Jahr 2012 einen Impact Factor von 1,525 und wurde damit im Science Citation Index auf Rang 11 von 21 Zeitschriften in der Kategorie "Physics, Nuclear" geführt. Der Impact Factor von Nuclear Physics B lag 2012 bei 4,327, womit die Zeitschrift Rang 9 von 27 Journals in der Kategorie "Physics, Particles & Fields" belegte.





Robert J. Birgeneau

Robert Joseph Birgeneau (* 25. März 1942 in Toronto) ist ein kanadischer Physiker, der in experimenteller Festkörperphysik arbeitet.

Birgeneau studierte an der University of Toronto (Bachelor 1963) und promovierte 1966 an der Yale University. Danach war er als Post-Doc an der Universität Oxford und ab 1968 Wissenschaftler an den Bell Laboratories. Ab 1975 war er Professor am Massachusetts Institute of Technology. Von 1991 bis 2000 war er dort Dekan (Dean of Science) und von 1988 bis 1991 Vorstand der Physik-Fakultät. Von 2000 bis 2004 war er Präsident der Universität Toronto und ab 2004 Kanzler der University of California, Berkeley, wo er auch Professor ist.

Er untersuchte unter anderem eindimensionale Heisenberg-Antiferromagneten mit Neutronenstreuung, wobei sich eine gute Übereinstimmung mit der exakten Theorie von Michael E. Fisher ergab, und zweidimensionale Antiferromagneten.

Seit 1968 ist er Gastwissenschaftler am Brookhaven National Laboratory. 1974 wurde er Fellow der American Physical Society und 1981 der American Association for the Advancement of Science. 1987 wurde er in die American Academy of Arts and Sciences gewählt, 2004 in die National Academy of Sciences und 2006 in die American Philosophical Society. Seit 2001 ist er Fellow der Royal Society. 1987 erhielt er den Oliver E. Buckley Condensed Matter Prize und 2000 den Julius-Edgar-Lilienfeld-Preis. Er erhielt den Preis der IUPAP für Magnetismus. Für 2016 wurde ihm der Vannevar Bush Award der National Science Foundation der Vereinigten Staaten zugesprochen.





Clusterzerfall

Der Clusterzerfall (auch "Clusteremission", ) ist ein sehr selten auftretender radioaktiver Zerfallstyp. Dabei wird ein leichter Atomkern emittiert, der schwerer als ein Alpha-Teilchen, aber mit 6 bis 14 Prozent der Masse des Mutterkerns wesentlich leichter als die typischen Spaltfragmente der Kernspaltung ist. Außerdem werden keine Neutronen freigesetzt.

Als emittierte Cluster beobachtet wurden bisher Kerne zwischen Kohlenstoff-14 und Silicium-34. Es handelt sich überwiegend nicht um die jeweils stabilsten Kerne zu ihrer Ordnungszahl, sondern um deren Isotope mit höherem Neutronenüberschuss, entsprechend dem Neutronenüberschuss des Mutterkerns.

Der Clusterzerfall wurde von Aureliu Săndulescu, Dorin N. Poenaru und Walter Greiner 1980 theoretisch vorhergesagt. H. J. Rose und George Arnold Jones erbrachten 1983 an der University of Oxford den ersten experimentellen Nachweis, der Anfang 1984 in der Zeitschrift "Nature" veröffentlicht wurde. Sie stellten fest, dass das Radiumisotop Radium-223 (ein Alpha-Strahler mit einer Halbwertszeit von 11,43 Tagen) unter Emission eines Kohlenstoff-14-Atomkerns direkt zu Blei-209 zerfallen kann:

In der Karlsruher Nuklidkarte von 2012 sind 20 Radionuklide aufgeführt, die neben dem dominierenden Alphazerfall auch Clusteremission aufweisen:

Bei einigen Radionukliden sind bis zu vier Möglichkeiten des Clusterzerfalls beobachtet worden, beispielsweise drei bei dem in der Natur vorkommenden Uranisotop Uran-234: die Emission eines Neon-24-, eines Neon-26- oder eines Magnesium-28-Kerns.





Deviationsgleichung

Die Deviationsgleichung oder geodätische Abweichung ist eine Gleichung der Riemannschen Geometrie bzw. Allgemeinen Relativitätstheorie und beschreibt die Änderung des Abstandes zweier benachbarter Geodäten mit Hilfe des Riemannschen Krümmungstensors. Mittels dieser Gleichung kann festgestellt werden, ob und in welcher Art ein Raum gekrümmt ist, indem die Relativbeschleunigung zweier Probekörper auf benachbarten Geodäten gemessen wird. Wird keine Relativbeschleunigung zwischen zwei Geodäten gemessen, so ist der Raum flach. Die Relativbeschleunigung zwischen den Probekörpern rührt nur von der Krümmung des Raumes her, nicht von ihrer gegenseitigen gravitativen Anziehung, die bei einem realen Experiment noch zusätzlich wirken würde.

Die mathematische Formulierung der Deviationsgleichung lautet:
und vereinfacht sich in einem torsionsfreien Raum zu
Die Symbole in den Gleichungen bedeuten dabei folgendes:
Im flachen Raum wächst der Abstand zweier sich schneidenden Geodäten formula_16 und formula_17 proportional zu formula_18. Ist dies nicht der Fall, so ist dies ein Symptom für die Krümmung des Raumes und entspricht der obigen Gleichung bei nichtverschwindendem Krümmungstensor.




Einsteinsche Postulate

Als Einsteinsche Postulate werden zwei entscheidende Grundannahmen bezeichnet, die Albert Einstein 1905 in seiner Arbeit "Zur Elektrodynamik bewegter Körper" traf, um auf diesen die spezielle Relativitätstheorie zu begründen. 

Diese Postulate bilden eine axiomatische Grundlage für die spezielle Relativitätstheorie und führten dazu, dass wesentliche Prinzipien der klassischen Physik wie die Galileitransformation erweitert und der Begriff der Gleichzeitigkeit neu definiert werden musste.




Hans H. Staub

Hans Heinrich Staub (* 20. Januar 1908 in Wald; † 21. Dezember 1980 in Zürich; heimatberechtigt in Oberrieden) war ein Schweizer Atomphysiker.

Staub studierte Physik an der ETH Zürich, wo er auch promovierte und anschliessend von 1931 bis 1937 als Assistent wirkte. 1937 kam er ans California Institute of Technology. Ab dem Wintersemester 1938/39 war er ordentlicher Professor an der Stanford University. Von 1942 bis 1946 arbeitete er am Manhattan-Projekt zur Entwicklung und zum Bau einer Atombombe. In jener Zeit war er Freund und Kollege des Nobelpreisträgers Wolfgang Pauli. Im Wintersemester 1949/50 wechselte er als ordentlicher Professor an die Universität Zürich. Im Sommersemester 1973 wurde er daselbst Honorarprofessor.
1944 wurde er Fellow der American Physical Society.




Crossover-Übergang

Als „crossover“-Übergang bezeichnet man ein in der Physik und anderen Naturwissenschaften auftretendes Phänomen, bei dem ein scharfer Phasenübergang – der nur geringfügig abgerundet oder „verschmiert“ erscheint – nur als scharf vorgetäuscht wird.

Das Übergangsphänomen äußert sich typischerweise dadurch, dass eine Größe "Y" bei doppelt-logarithmischer Auftragung über einer Variablen "X" zwei aneinander angrenzende lange Geradenabschnitte mit unterschiedlichen Steigungen aufweist, z. B. Übergang von einem verlängert gedachten Geradensegment mit Steigung 0 zu einem anderen mit Steigung 1, etwa so:    ____formula_1, was abgerundet etwa durch folgende Funktion beschrieben werden kann (für formula_2 zwischen 0 und einem sehr großen Wert):

Für sehr kleine "X" ist nur der erste Term der rechten Seite wichtig, für sehr große dagegen nur der zweite. Man erhält so für formula_4 das Ergebnis formula_5, also eine Gerade mit Steigung Null durch "Y"=1. Für formula_6 dagegen ergibt sich bei logarithmischer Auftragung eine Gerade mit der Steigung 2, oder bei Änderung der Potenz "X" ein anderer endlicher Wert. Der Übergang zwischen Geradensegmenten erfolgt jedoch nicht „scharf“, sondern „abgerundet“, entsprechend der vollen Beschreibung.

Der „crossover“-Übergang ist also nur „ungefähr“ lokalisiert, und zwar ungefähr am rechten Ende des linken Geradensegments bzw. am linken Ende des rechten Segments.

Der Übergangsbereich zwischen den unterschiedlichen Geradenabschnitten ähnelt dann einem abgerundeten (bzw. durch Störstellen „verschmierten“) echten Phasenübergang, wobei die unterschiedlichen Geradensteigungen fälschlich als „kritische Exponenten“ rechts bzw. links dieses vorgetäuschten echten Phasenübergangs interpretiert werden und der fiktive Schnittpunkt der Geraden fälschlich als Ort des vorgetäuschten Phasenüberganges.

In Wirklichkeit handelt es sich um zwei unterschiedlich charakteristische Gebiete ein-und-derselben Phase.

Als konkretes Beispiel betrachten wir einen magnetischen Kristall, etwa Eisen. Unterhalb der kritischen Temperatur "T" gilt, dass die Magnetisierung (genauer: der thermische Erwartungswert der Größe) bei Annäherung von unten an diese sog. Curie-Temperatur, mit einem charakteristischen Wurzelgesetz zunimmt (Molekularfeldtheorie), formula_7. Zugleich wachsen die Fluktuationen der Magnetisierung an, und zwar nach dem Gesetz formula_8 mit formula_9, bis diese zuletzt eine mit "M" vergleichbare Größenordnung erreichen. Wenn das der Fall ist, d. h. bei weiterer Annäherung von "T" an den kritischen Wert "T", erfolgt – wie man experimentell feststellen kann – ein Übergang zu einem Verhalten formula_10 und formula_11, mit formula_12 und formula_13 Dieser Übergang von den Molekularfeld-Steigungen, z. B. von formula_14  zu den von den Fluktuationen dominierten eigentlichen kritischen Werten, etwa zu formula_15, erfolgt genau durch einen „crossover“-Übergang vom Molekularfeld-Verhalten zum eigentlichen kritischen Verhalten.

Dieses „crossover“-Phänomen findet bei einer nur „ungefähr“ definierten „crossover“-Temperatur nahe bei "T" statt, etwa wenige Prozent darunter. Es kann beträchtliche Unterschiede umfassen, wie den Unterschied zwischen dem Molekularfeld-Exponenten β=1/2 und dem eigentlichen, nicht exakt bekannten kritischen Wert β ≈ 1/3. Der eigentliche „scharfe“ Phasenübergang ereignet sich jedenfalls bei "T" selbst.

Die logarithmische Auftragung der Variablen "X" ist wesentlich, weil nur so die geforderten langen Segmente entstehen, die i.a. viele Zehnerpotenzen umfassen sollen. Die logarithmische Auftragung der Variablen "Y" für die mathematische Umwandlung von Potenzgesetzen in Geradensteigungen ist ebenfalls wichtig. Außerdem muss das Phänomen insgesamt additiv sein, was wegen formula_16 und formula_17 erfüllt ist.

Es ist eine wichtige Aufgabe der Theoretischen Physik gegenüber z B. der Experimentalphysik solche nur „vorgetäuschten“ Übergänge von „verschmierten echten Phasenübergängen“ zu unterscheiden. Der Unterschied wird im Verhalten beim sog. Thermodynamischen Grenzfall formula_18 sichtbar, wo bei „echten“ Phasenübergängen die Verschmierung bzw. Abrundung verschwinden muss.





Hauptinvariante

Die Hauptinvarianten eines Tensors sind die Koeffizienten seines charakteristischen Polynoms.

Die Komponenten eines Tensors referenzieren auf Dyaden von Vektoren, die sich ihrerseits komponentenweise bezüglich einer Vektorraumbasis darstellen lassen. Bei einem Wechsel der Basis ändern sich die Komponenten der Vektoren in charakteristischer Weise nicht aber die Beträge der Vektoren. Der Betrag eines Vektors ist also "invariant" gegenüber einem Wechsel der Basis. In gleicher Weise sind die Hauptinvarianten des Tensors invariant gegenüber einem Wechsel der Basis, daher der Name.

Die Hauptinvarianten symmetrischer Tensoren spielen eine zentrale Rolle in der Materialtheorie. Eine wichtige Anforderung an Materialmodelle leitet sich daraus ab, dass ein bewegter Beobachter immer dasselbe Materialverhalten misst wie ein ruhender. Diese Eigenschaft wird "materielle Objektivität" genannt. Die Bewegung eines Beobachters wird mathematisch als Wechsel des Bezugssystems und somit als Wechsel der Vektorraumbasis beschrieben. Die Hauptinvarianten sind also Größen, die alle Beobachter in gleicher Weise wahrnehmen und die daher für die Materialmodellierung geeignet sind. Beispiele für Materialmodelle, die die Hauptinvarianten benutzen, sind das Hooke'sche Gesetz, die Hyperelastizität und Plastizitätstheorie.

Die Darstellung erfolgt in drei Dimensionen für Tensoren zweiter Stufe, lässt sich aber in einfacher Weise auf mehr Dimensionen verallgemeinern.

Gegeben sei ein Tensor zweiter Stufe formula_1. Dann lautet sein charakteristisches Polynom:

Darin ist formula_3 die Determinante, formula_4 der Einheitstensor, formula_5 eine reelle oder komplexe Zahl und die Koeffizienten formula_6 sind die drei Hauptinvarianten

Der Operator formula_8 liefert die Spur seines Arguments, formula_9 ist die Adjunkte und formula_10 der Kofaktor
wobei letztere Identität nur gilt, wenn der Tensor invertierbar ist und mithin formula_12 ist.

Für Tensoren zweiter Stufe ist die Addition und Multiplikation mit einem Skalar definiert weshalb die Menge aller Tensoren zweiter Stufe einen Vektorraum bildet, der Vektorraumbasen besitzt, die aus Dyaden bestehen, die sich wiederum mit dem dyadischen Produkt formula_13 zweier Vektoren berechnen. Sei formula_14 der Vektorraum der geometrischen Vektoren. Dann ist formula_15 der Vektorraum der Tensoren zweiter Stufe, die Vektoren aus formula_14 in den formula_14 abbilden. Bezüglich einer Vektorraumbasis des formula_15 kann jeder Tensor komponentenweise dargestellt werden und aus diesen Komponenten können die Hauptinvarianten berechnet werden, die ja unabhängig von der Wahl der Basis sind.

Sei formula_19 die Standardbasis des formula_14 und

ein Tensor mit den Komponenten formula_22 bezüglich dieser Standardbasis. Dann berechnet sich

Seien formula_26 und formula_27 zwei beliebige Basissysteme des formula_14 und

ein Tensor mit den Komponenten formula_30 bezüglich dieser Basen. Dann berechnet sich

Das äußere Tensorprodukt # ist mittels Dyaden definiert über

Mit diesem und dem Frobenius-Skalarprodukt „formula_35“ von Tensoren bekommen die drei Hauptinvarianten die Darstellungen

Die Eigenwerte formula_37 eines Tensors zweiter Stufe sind die Lösungen formula_38 seines charakteristischen Polynoms und ebenfalls Invarianten. Nach dem Satz von Vieta gilt:

Der Betrag eines Tensors

definiert mit der Frobeniusnorm formula_41 und dem Frobenius-Skalarprodukt „formula_35“, lässt sich im Allgemeinen nicht mit den drei Hauptinvarianten darstellen. Es gelingt aber bei symmetrischen oder schiefsymmetrischen Tensoren. Bei symmetrischen Tensoren ist formula_43, d. h. der Tensor ist mit seiner transponierten identisch, und daher

Bei schiefsymmetrischen Tensoren ist formula_45 und daher formula_46 und

Die drei Hauptinvarianten lassen sich auch mit den Spuren der Potenzen eines Tensors darstellen, die ebenfalls Invarianten sind. Sei
dann gilt

In der Hyperelastizität wird die Formänderungsenergie, die aufgebracht werden muss um einen Körper zu verformen, manchmal als Funktion der Hauptinvarianten des Verzerrungstensors modelliert. Die Spannungen ergeben sich dann aus der Ableitung der Formänderungsenergie nach dem Verzerrungstensor, wofür die Ableitungen der Hauptinvarianten nach dem Verzerrungstensor benötigt werden. Daher lohnt es sich, diese Ableitungen bereitzustellen.

Die Ableitung einer skalarwertigen Funktion formula_50 nach dem Tensor formula_1 ist der Tensor formula_52 für den gilt

Man schreibt dann auch

So berechnet sich:

Wegen

berechnet sich

Die folgenden Beispiele zeigen die Benutzung der Hauptinvarianten in Materialtheorien und oft benutzten Materialmodellen:


Es wird der Nachweis der Invarianz der Spur eines Tensors erbracht. Seien formula_26 und formula_73 zwei beliebige Basissysteme des formula_14 und

Beim Wechsel zu anderen Basen formula_76 und formula_77 mit dualen Basen formula_78 und formula_79 berechnen sich die neuen Komponenten formula_80 gemäß

Die Spur mit den neuen Komponenten formula_82 ergibt sich also zu

was zu zeigen war.






Michael Wolf (Philosoph)

Michael Wolf auch: "Wolff, Wolfius", (* 3. Oktober 1584 in Regensburg; † 2. April 1623 in Jena) war ein deutscher Mathematiker, Physiker, Logiker und Metaphysiker. 

Michael war Sohn des damaligen Rektors in Regensburg, des späteren Rektors des Gymnasiums in Weimar und dortigen Bürgermeisters Johann Wolf (* 1524 in Weimar; † 3. November 1602 ebd.) und dessen 1556 geheirateten Frau Elisabeth Schneller, Tochter des Bürgermeisters in Weimar Hans Schneller. Seine ersten Bildungsgrundlagen scheint er in Weimar gelegt zu haben. Im Sommersemester 1601 immatrikulierte er sich an der Universität Jena, wo er am 4. August 1607 den akademischen Grad eines Magisters der philosophischen Wissenschaften erwarb. Nachdem er sich am Vorlesungsbetrieb der Jenaer Hochschule beteiligt hatte, erhielt er dort 1612 die Professur für Mathematik und wurde 1613 Professor für Physik. 1616 übernahm er die Professur für Logik und Metaphysik an der Salana und suchte dabei die Ideen Aristoteles mit der Bibel zu belegen. Auch beteiligte er sich an den organisatorischen Aufgaben der Hochschule. So war er einige Male Dekan der philosophischen Fakultät und im Sommersemester 1617 Rektor der Alma Mater. Jedoch währte seine Wirkungszeit nicht lang, da er im Alter von achtunddreißig Jahren verstarb.

Wolf verheiratete sich am 6. Juni 1614 in Gotha mit Anna Wilke (* 8. Februar 1594 in Gotha; † 17. Juni 1622 in Jena), der Tochter des Rektors am Gymnasium in Gotha Andreas Wilke (* 5. Juli 1562 in Helmershausen; † 19. Juni 1631 in Gotha) und dessen am 28. Oktober 1592 geheirateten Frau Sabina Ferber (auch Färber; * Ohrdruf). Von den aus dieser Ehe stammenden Kindern kennt man die Tochter Annula (Anna, Hannula) Elisabeth Wolf und den Sohn Andreas Wolf.






Reflexionsanisotropiespektroskopie

Die Reflexionsanisotropiespektroskopie (RAS) ist eine optische Spektroskopie ähnlich der Ellipsometrie, mit der die Anisotropie von Festkörpern und deren Oberflächen untersucht wird. Hierbei wird linear polarisiertes Licht im fast senkrechten Einfall auf eine Oberfläche eingestrahlt und dessen Reflektanz gemessen:

formula_2 und formula_3 bezeichnen die komplexen Reflektanzen bezüglich zweier orthogonaler Achsen.

Besteht bei der gemessenen Probe eine optische Anisotropie, so wird das reflektierte Licht elliptisch polarisiert. Diese Polarisation wird dann in Abhängigkeit von der Photonenenergie analysiert. Wird die Anisotropie, welche zum Beispiel durch Dimere auf der Oberfläche entstehen kann, um 90° gedreht, bedeutet das einen Wechsel im Vorzeichen des Signals (siehe Bild 1). 

Die Methode wurde 1985 zur Untersuchung der optischen Eigenschaften der kubischen Halbleiter Silicium und Germanium eingeführt. Wegen ihrer Unabhängigkeit von Ultrahochvakuum-Bedingungen wurde die Anwendung der Methode auf die in situ Überwachung des epitaktischen Wachstums von Halbleitern ausgedehnt. Ist das Kristallvolumen optisch isotrop, wird RAS sehr oberflächenempfindlich und kann beispielsweise auch zur Untersuchung von Adsorbaten auf Oberflächen eingesetzt werden. Die Signale sind jedoch in der Regel sehr komplex und bedürfen theoretischer Modellierung, um die einzelnen spektralen Merkmale bestimmten elektronischen Übergängen oder Oberflächenrekonstruktionen zuordnen zu können.




Mittelpunktsgleichung

Als Mittelpunktsgleichung wird seit der antiken Astronomie die Abweichung der ungleichmäßigen Bewegung von Mond und Planeten von einer gleichmäßigen Bewegung entlang einer Kreisbahn bezeichnet. Wie Johannes Kepler 1609 zeigte, hängt sie von der Exzentrizität "e" der jeweiligen Bahnellipse ab. Ihr Maximalbetrag wird große Ungleichheit genannt.

Sie ergibt sich aus der Kepler-Gleichung als Differenz zwischen "mittlerer Anomalie M" und "wahrer Anomalie V". Letztere ist der momentane Winkelabstand des Himmelskörpers von seiner Periapsis (erd- bzw. sonnennächster Punkt der Bahnellipse), während der Winkel M gleichmäßig mit der Zeit abläuft und im Periapsis mit Null beginnt. Weil sich die Kepler-Gleichung nur iterativ lösen lässt, wird "V - M" heute meist durch eine Reihenentwicklung berechnet. Für Gradmaß ergibt sich in Näherung zweiter Ordnung

bzw. in Näherung dritter Ordnung

Das Maximum tritt bei den Winkeln 90° und 270° auf -- d. h. zum Viertel bzw. zu ¾ der Umlaufzeit -- und wird "Große Ungleichheit" genannt. Sie entspricht dem 1. Term "2e" der obigen Reihe und beträgt beim Mond ± 6,3°, bei der Erdbahn bzw. der scheinbaren Sonnenbahn ± 1,9°, beim Merkur 24°, bei Venus 0,8°, beim Mars 10,7°, bei Jupiter 5° und bei Saturn 6°. Diese Werte waren schon Claudius Ptolemäus wohlbekannt; vermutlich hat sie schon Apollonios von Perge um 200 v. Chr. aus langjährigen Beobachtungen hergeleitet. Ähnliche Untersuchungen wurden auch im alten Indien, in Babylonien und in Persien durchgeführt.

Der größte Term der Mittelpunktsgleichung, die Sinus-Schwingung "2e·sinM" der obigen Reihenentwicklung, wurde in der griechischen Planetentheorie durch Epizykel berücksichtigt. Man ließ den Epizykelmittelpunkt so auf einem Exzenter laufen, dass
die Bewegung von einem Ausgleichspunkt gesehen gleichförmig erscheint
Die Babylonier berechneten ihn jedoch nicht mittels Epizykeltheorie, sondern durch arithmetische Reihen.

Dass sich die Mondbahn damit noch nicht befriedigend berechnen lässt, schreibt aber schon Ptolemäus in seinem Almagest. Als Korrektur führt er die Evektion ein, eine Störung von 1,3°, die von der gegenseitigen Stellung Sonne-Mond abhängt. 1500 Jahre später entdeckt Tycho de Brahe in seinen 0,02° genauen Beobachtungen zwei weitere Störungen (Variation und jährliche Gleichung), die durch Newtons Gravitationsgesetz bestätigt wurden. Heute berücksichtigt die Theorie der Mondbahn weit über 1000 periodische Störungsterme, zu denen noch säkulare Effekte (z. B. Drehung der Mondbahnebene) kommen.

Auch bei den Planeten beschreibt die Mittelpunktsgleichung die ungleichförmige Geschwindigkeit infolge der Bahnelliptizität, doch übertrifft sie nur bei Merkur ("e" = 0,206) und Mars (0,093) jene des Mondes. Die sonstigen Störungen sind geringer, weil die Erde und andere Planeten weit entfernt sind.





Jewgeni Pawlowitsch Welichow

Jewgeni Pawlowitsch Welichow (; * 2. Februar 1935 in Moskau) ist ein russischer Physiker und Hochschullehrer.

Welichow absolvierte die Mittelschule Nr. 49 in Moskau und studierte ab 1953 Physik an der Lomonossow-Universität Moskau (MGU) mit Abschluss in Theoretischer Physik 1958. Anschließend arbeitete er an der MGU als Aspirant für die Promotion zum Kandidaten der physikalisch-mathematischen Wissenschaften. Er untersuchte Instabilitäten in Fluiden und Plasmen, was 1959 zur Entdeckung der Magnetorotationsinstabilität und 1962 zur Entdeckung der elektrothermischen Instabilität. führte.

Ab 1961 arbeitete Welichow im Kurtschatow-Institut für Atomenergie. 1964 wurde er zum Doktor der physikalisch-mathematischen Wissenschaften promoviert. Seine Arbeitsgebiete waren Plasmaphysik, Laser-Physik, Kernfusion und Magnetohydrodynamik mit Anwendung als Magnetohydrodynamischer Generator. 1968 wurde er Professor für Atomphysik, Plasmaphysik und Mikroelektronik. 1968 wurde er Korrespondierendes Mitglied der Akademie der Wissenschaften der UdSSR (AN-SSSR) und 1974 Vollmitglied. 1989 wurde er ausländisches Mitglied der Bulgarischen Akademie der Wissenschaften.

1970 bis 1978 war Welichow Mitglied des Zentralkomitees des Komsomol und ab 1977 Vorsitzender des "Sowjets der jungen Wissenschaftler und Spezialisten" des Zentralkomitees. 1971 trat er in die Kommunistische Partei der Sowjetunion ein und blieb ihr Mitglied bis zu ihrer Auflösung 1991.

1971 bis 1978 leitete Welichow das vom Kurtschatow-Institut abgeteilte Magnetlaboratorium der AN-SSSR, das 1991 das "Troizker Institut für innovative und thermonukleare Forschung (TRINITI)" wurde. 1972 wurde er auf den neuen Lehrstuhl für Plasma-Energietechnik im Moskauer Institut für Physik und Technologie (MFTI) berufen. Er war 1978 bis 1991 Vizepräsident der AN-SSSR und dann bis 1996 der Russischen Akademie der Wissenschaften (RAN). 

Welichow war Abgeordneter im Obersten Sowjet der UdSSR von 1984 bis 1989. 1983 bis 1988 leitete er das Komitee der sowjetischen Wissenschaftler zur Verteidigung des Friedens und gegen den Atomkrieg. 1989 bis 1990 war er Mitglied des Zentralkomitees der Kommunistischen Partei. Er war Vorsitzender der Kommission der AN-SSSR für die Arbeit mit der Jugend und der Internationalen Stiftung für das Überleben und die Entwicklung der Menschheit sowie Präsident der russischen Abteilung des Internationalen Zentrums für Wissenschaftskultur. Er war Mitglied des russischen Komitees der Pugwash Conferences on Science and World Affairs und Mitglied der "Russischen Akademie der Naturwissenschaften".

Nach der Nuklearkatastrophe von Tschernobyl 1986 war Welichow an den Arbeiten zur Bewältigung der Katastrophenfolgen beteiligt. 1988 wurde er Direktor des Kurtschatow-Instituts und Vorsitzender des internationalen Programms (Sowjetunion, USA, EWG, Japan) zum Bau eines Tokamak-Reaktors (ITER).

1992 wurde Welichow Präsident des russischen "RosSchelf-Konzerns" zur Erschließung des Schelfs (zu Gazprom gehörig). 1993 gründete er eine russisch-US-amerikanische Gesellschaft zur Vermarktung von russischen Patenten und Lizenzen. Er gehört zum Direktorenkollegium des RelKom-Computernetzwerks, das ursprünglich im Kurtschatow-Institut entstanden war.

1992 wurde Welichow Präsident des Wissenschaftszentrum "Kurtschatow-Institut". 2009 wurde er Vorsitzender des ITER-Verwaltungsrats. 2010 wurde er Mitglied des wissenschaftlichen Fachbeirats der Skolkowo-Stiftung. 2013 ging er in den Ruhestand.

2005 wurde Welichow Mitglied und Sekretär der neuen Gesellschaftlichen Kammer Russlands (ab 2014 Ehrensekretär). 2012 wurde er als Vertrauensperson des Kandidaten Putin offiziell registriert.





Juri Michailowitsch Bunkow

Juri Michailowitsch Bunkow (, englische Transkription "Yuri Bunkov"; * 29. August 1950 in Stawropol) ist ein russischer experimenteller Festkörperphysiker, der für die Entdeckung von Spin-Suprafluidität bekannt ist.

Bunkow stammte aus einer Geologen-Familie, besuchte eine Spezialschule für Physik und Mathematik in Moskau (Schule Nr. 2) und studierte am Moskauer Institut für Physik und Technologie. 1974 wurde er promoviert und 1985 habilitierte er sich (russischer Doktortitel, mit einer Dissertation über NMR-Studien an suprafluidem Helium-3). 1974 bis 1995 war er am Kapiza-Institut, ab 1986 als leitender Wissenschaftler. Er war dort in der Gruppe von Wiktor-Andrei Stanislawowitsch Borowik-Romanow. Dort entdeckte er mit Wladimir Dmitrijew und Juri Mucharski (die als Studenten dort arbeiteten) die Spin-Suprafluidität. Sie äußerte sich bei NMR-Untersuchungen an Helium-3 B als Gebiet kohärenter Larmorpräzession (HPD, Homogeneously Precessing Domains), wobei Inhomogenitäten in der Präzession durch Superströme im Spin (Magnetisierung) ähnlich denen bei Supraleitung (Ladung) und Supraflüssigkeit oder Bose-Einstein-Kondensaten (Masse) ausgeglichen wurden. Sie ist auch ein Bose-Einstein-Kondensat (BEC) von Magnonen. Eine theoretische Erklärung gab in den 1980er Jahren der Theoretiker Igor Fomin. Zuvor hatte er die erste Magnetische Kühlung mit adiabatischer Kernentmagnetisierung in der Sowjetunion installiert. 1989 bis 1995 nahm er an Experimenten an der University of Lancaster teil. Er forscht seit 1995 für das CNRS am Institut Néel in Grenoble, wo er Forschungsdirektor ist. Zuvor war er ab 1992 Gastprofessor in Grenoble. Er ist auch Professor an der Universität Kasan.

In der He 3 B Phase (die eine komplexe Phasenstruktur aufweist) entdeckte er auch experimentell Analoga zu kosmologischen und quantenfeldtheoretischen Phänomenen, so von kosmologischen Strings (als Wirbel im Spin-Superstrom) und Majorana-Quasiteilchen. Schon in den 1980er Jahren wurden Goldstone-Moden nachgewiesen (Phononen im HPD analog dem zweiten Schall bei Supraflüssigkeiten). In den 2000er Jahren entdeckte er Q-Bälle, nichttopologische Solitonen, die auch ursprünglich in der QFT eingeführt wurden.

Er entdeckte mit japanischen Kollegen 2008 auch kohärente Präzession in der Helium-3-A Phase die in von Außen deformierte Aerogele eingelagert war.

2008 erhielt er mit Wladimir Dmitrijew und Igor A. Fomin den Fritz London Memorial Prize. Außer mit dem Theoretiker Fomin arbeitete er auch mit dem Theoretiker Grigori Jefimowitsch Wolowik (Volovik) zusammen. Seit 2010 ist er ordentliches Mitglied der Academia Europaea.






Majorisierungskriterium

Das Majorisierungskriterium ist in der Quantenmechanik ein Merkmal, das der Unterscheidung verschränkter von separablen Zuständen anhand ihrer Dichtematrix dient. Es ist für separable Zustände erfüllt, aus seiner Erfüllung folgt aber nicht die Separabilität. Es ist somit das schwächste der drei Operatoren-Separationskriterien in der Quantenmechanik, zu denen auch das Reduzierungskriterium und das Peres-Horodecki-Kriterium zählen.

Das Kriterium wurde im Jahr 1999 von Michael Nielsen und 2001 zusammen mit Julia Kempe veröffentlicht, und wird daher auch oder genannt. Anschaulich gesprochen besagt das Majorisierungskriterium, dass separable Zustände global eine größere Unordnung zeigen als lokal.

Das Majorisierungskriterium ist erfüllt, wenn die Eigenwerte formula_1 der Dichtematrix formula_2 eines quantenmechanischen Zustands durch die Eigenwerte ihrer partiellen Spuren formula_3 und formula_4 bezüglich der Hilberträume formula_5 und formula_6 majorisiert sind:

Hierin steht:

Der Operator formula_14 wird als "„… wird majorisiert durch …“" (siehe Majorisierung) gesprochen und formula_15 bedeutet für zwei Eigenwertvektoren formula_16 und formula_17 der Dimension formula_18:

Die Markierung formula_20 deutet dabei an, dass die Eigenwerte im jeweiligen Eigenwertvektor formula_21 absteigend nach ihrer Größe sortiert sind. Wenn die Dimension eines Eigenwertvektors kleiner als der andere ist, so wird sie durch Anfügen von Nullen an den Größeren angeglichen.

Für separable Dichtematrizen ist das Kriterium erfüllt.
Wenn das Kriterium nicht erfüllt ist, ist die Dichtematrix nicht separabel.
Der Umkehrschluss, dass aus der Erfüllung des Majorisierungskriteriums die Separabilität der Dichtematrix folgt, bzw. dass für eine nicht-separable Dichtematrix das Majorisierungskriterium nicht erfüllt ist, gilt nicht.

Es gibt beispielsweise isospektrale Zustände, die zwar das Majorisierungskriterium erfüllen, aber nicht separabel sind (siehe unten).

Das Majorisierungskriterium ist schwächer als das Reduzierungskriterium oder das Peres-Horodecki-Kriterium, weil aus einem erfüllten Peres-Horodecki-Kriterium in 2x2 bzw. 2x3 Dimensionen (d. h. einem Zustand, der aus zwei Qubits oder einem Qubit und einem Qutrit, also einem Teilchen mit drei möglichen Zuständen, zusammengesetzt ist) die Separabilität von formula_22 folgt, während die Rückrichtung beim Majorisierungskriterium im Allgemeinen nicht erfüllt ist.

Inhaltlich orientiert sich der Beweis an einschlägiger Literatur. Um das Kriterium zu beweisen, ist es notwendig, eine bistochastische Matrix formula_6 zu finden, sodass die Eigenwerte der Dichtematrix (in beliebiger Reihenfolge) aus den Eigenwerten der reduzierten Dichtematrix folgen: formula_24. Mithilfe dieser Gleichung lässt sich zeigen, dass alle Schur-konvexen Funktionen oben gezeigte Ungleichung

erfüllen, für Schur-konkave Funktion folgt dieselbe Ungleichung, in der sich die Richtung des Ungleichheitszeichens ändert. Damit lässt sich das Majorisierungskriterium auf das Entropie-Kriterium zurückführen, bei dem aus der von-Neumann-Entropie Rückschlüsse auf die Separabilität gezogen werden können: Bei einem reinem Zustand verschwindet die von-Neumann-Entropie, wenn sie aber für ihre partiellen Spuren nicht verschwindet, ist der Zustand verschränkt.

Sei

mit den Eigenwerten der Dichtematrix formula_27 ein (nach PPT-Kriterium) verschränkter Zustand, dann sind dessen partielle Spuren gegeben als

mit den Eigenwerten formula_29. Hier ist das Majorisierungskriterium also offensichtlich erfüllt, da die Summen über die Eigenwerte identisch sind. Wäre die Rückrichtung des Majorisierungskriteriums erfüllt, unterläge man hier dem Trugschluss, dass formula_2 separabel wäre, obwohl es ein verschränkter Zustand ist.

Betrachte die Zustände der Qubit-Familie der Werner-Zustände,
formula_31 mit der Einheitsmatrix formula_32.

Die Dichtematrix lautet

Die Eigenwerte dieser Dichtematrix sind gegeben als

Die partielle Spur über formula_5 und formula_6 ist gegeben als formula_37, also sind die Eigenwerte gegeben als formula_38. Fülle diese jetzt mit Nullen auf, bis dieselbe Dimension wie für formula_39 erreicht ist und erhalte

Rechne nun das Majorisierungskriterium nach:

Daraus folgt also, dass der Werner-Zustand für formula_42 verschränkt ist, was das gleiche Ergebnis ist, was auch aus dem Peres-Horodecki-Kriterium hervorgeht.




Francesca Ferlaino

Francesca Ferlaino (* 23. Dezember 1977 in Neapel) ist eine italienische Physikerin mit den Forschungsschwerpunkten Atomphysik, Experimentalphysik und Quantenmechanik und ist seit 2006 an der Universität Innsbruck tätig.

Ferlaino wurde 1977 in Neapel geboren. Von 1996 bis zu ihrem Diplom im Jahr 2000 studierte sie an der Universität Neapel Federico II. Ursprünglich wollte sie Philosophie studieren, entschied sich nach einem Besuch bei einem Professor für ein Physikstudium, obwohl sie keinerlei Grundkenntnisse in Naturwissenschaften besaß. Sie promovierte 2004 an der Università degli Studi di Firenze. 2006 wanderte sie nach Österreich aus.

Ferlaino lehnte 2014 eine Alexander von Humboldt-Professur an der Universität Ulm ab. Sie ist seit 2011 Mitglied der Österreichische Akademie der Wissenschaften. Sie ist am Institut für Quantenoptik und Quanteninformation tätig.





Legierungsverbreiterung

Die Legierungsverbreiterung () bezeichnet eine durch die statistische Verteilung der Legierungspartner hervorgerufene Verbreiterung von Lumineszenzlinien von Legierungen.

Die Legierungsverbreiterung ist eine der Linienverbreiterungen. Die statistische Verteilung der Legierungspartner führt lokal zu einer unterschiedlichen Materialzusammensetzung, was bei Halbleitermaterialien und Isolatoren eine lokale Variation der Bandlücke zur Folge hat. Aus diesem Grund führt die Exzitonenrekombination, je nachdem in welcher Materialumgebung sie stattfindet, zur Aussendung von Photonen mit unterschiedlicher Energie. Die Legierungsverbreiterung gehört daher zu den inhomogenen Linienverbreiterungen, was bedeutet, dass die Linienverbreiterung gaußförmig ist. Die Verbreiterung ist unabhängig vom Anregungsmechanismus.

Diese Linienverbreiterung tritt nur bei Legierungen auf und ist in der Größenordnung von wenigen meV. In Festkörpern treten eine Vielzahl von Linienverbreiterungen auf. Bei hohen Temperaturen, dies können je nach Material schon Temperaturen um 20 K sein, wird die Legierungsverbreiterung von anderen Linienverbreiterungen überdeckt und spielt bei Raumtemperatur keinerlei Rolle.

Die Legierungsverbreiterung wurde zum Beispiel bei der zweiatomigen Legierung Si_{1-x}Ge_{x} experimentell nachgewiesen. Bei einer zweiatomigen Legierung vom Typ Si_{1-x}Ge_{x}, befinden sich im Volumen formula_1 im Schnitt formula_2 Ge-Atome und formula_3 Si-Atome. Dabei ist formula_4 die Teilchendichte, also die Anzahl der Atome pro Volumen. Die Wahrscheinlichkeit, im Volumen formula_1 genau formula_6 Ge-Atome zu zählen, ist durch die Binomialverteilung gegeben. Die Standardabweichung der Binomialverteilung ist

wobei formula_8 die Gesamtzahl der Atome im Volumen formula_1 ist. Um die Änderung der Bandlückenenergie zu berechnen, ist man allerdings an formula_10 interessiert. Es gilt

Daraus folgt, dass

ist. Für die Variation der Bandlückenenergie gilt daher:

Da die Lumineszenz bei der Exzitonenrekombination auftritt, muss für formula_1 das Volumen eines Exzitons verwendet werden.

Die Legierungsverbreiterung folgt einer Gaußverteilung, kann also mittels

beschrieben werden, wobei formula_16 die rein durch die Legierungsverbreiterung verbreiterte Lumineszenzintensität als Funktion der Lichtenergie formula_17 ist. Um die Legierungsverbreiterung zu untersuchen, muss das untersuchte Materialsystem bei tiefen Temperaturen (um 4 K) und geringen Ladungsträgerdichten untersucht werden.




Keith Burrell

Keith H. Burrell (* 13. April 1947 in Santa Monica) ist ein US-amerikanischer Plasmaphysiker.

Burrell studierte Physik an der Stanford University mit dem Bachelor-Abschluss 1968 und am Caltech mit dem Master-Abschluss 1970 und der Promotion 1975. Danach wirkte er bei General Atomics in der Fusionsforschung mit Tokamaks, insbesondere dem DIII-D Tokamak von General Atomics. Davor forschte er am ISX-A und B Tokamak des Oak Ridge National Laboratory.

Er spielte eine wichtige Rolle in der Erforschung der 1982 am Asdex entdeckten H-Mode in magnetisch eingeschlossenen Fusionsplasmen (H für high confinement) und der dahinter stehenden Transportmechanismen, insbesondere der Unterdrückung von Turbulenz durch Ausbildung von Scherströmen. Burrell war an der Entdeckung der "ruhigen H-Mode" ("quiescent H-mode") am DIII-D 1999 beteiligt, die die Vorteile von H-Moden haben, aber keine Randinstabilitäten (Edge Localized Modes, ELM). Burrell entwickelte auch Methoden zur Plasmadiagnostik.

2001 erhielt er den "Excellence in Plasma Physics Award" der American Physical Society. 2018 erhielt er den James-Clerk-Maxwell-Preis für Plasmaphysik für "Pionierforschung, einschließlich wesentliche experimentelle Fortschritte und diagnostische Entwicklung, die die Verbindung zwischen Plasma-Scherfluss und turbulentem Transport begründeten, was zu verbesserten Einschlusskonfigurationen magnetisierter Plasmen führte durch die Reduzierung von turbulentem Transport durch Scherflüsse" (Laudatio). Er ist Fellow der American Physical Society (1985) und des Institute of Physics.





Carl Pape

Carl Johannes Wilhelm Theodor Pape (* 20. Januar 1836 in Hannover; † 7. Mai 1906 in Steglitz) war ein deutscher Physiker.

Carl Pape besuchte das Gymnasium in Celle und studierte nach dem Abitur 1855 Naturwissenschaften bis 1857 an der Georg-August-Universität Göttingen und anschließend an der Ruprecht-Karls-Universität Heidelberg, wo er auch 1858 zum Dr. phil. promoviert wurde (Dissertation gedruckt 1861).

Nach Fortsetzung seiner Studien an der Albertus-Universität Königsberg bei Franz Neumann in Königsberg wurde er 1862 mit seiner Habilitationsschrift "über die spezifische Wärme wasserfreier und wasserhaltiger Salze" für mathematische Physik in Göttingen habilitiert. Anschließend lehrte er als Privatdozent in Göttingen und ab 1866 als Professor an der Landwirtschaftlichen Akademie in Proskau.

1877 wurde er an der Albertus-Universität Königsberg als Nachfolger von Ludwig Moser ordentlicher Professor für Experimentalphysik. Von 1878 bis 1904 war er Ordinarius für Physik in Königsberg.

Der wissenschaftliche Schwerpunkt von Carl Pape lag im Bereich der Kristallphysik. Pape war an der Edition der Schriften Neumanns beteiligt und initiierte den Neubau des physikalischen Instituts der Albertus-Universität Königsberg.

Am 28. Oktober 1887 wurde Carl Pape als Mitglied (Matrikel-Nr. 2666) in die Leopoldina aufgenommen.


als Herausgeber:






Jaume Gomis

Jaume Gomis ist ein spanischer theoretischer Physiker.

Gomis promovierte 1999 an der Rutgers University und war als Post-Doktorand (und Sherman Fairchild Senior Research Fellow) am Caltech bei Hirosi Ooguri. Er erhielt 2004 den European Young Investigator Award der European Science Foundation, den er aber ausschlug, um am Perimeter Institute zu forschen. Er ist außerdem an der University of Waterloo.

Er befasst sich mit nichtstörungstheoretischer Dynamik in Stringtheorie und Eichfeldtheorien, deren Dualitäten und holographischer Formulierung von Quantengravitation. Gomis entwickelte 2009 eine Theorie magnetischer Observabler (aufbauend auf Gerardus ’t Hooft) in supersymmetrischen Yang-Mills-Theorien, mit denen Phasen dieser Theorien unterschieden werden können und mit denen er explizit S-Dualität zeigen konnte. Er befasste sich unter anderem mit nichtrelativistischen Strings (Superstrings, Superbranes).

2019 erhielt er den CAP-CRM Prize "für sein breites Spektrum wichtiger Beiträge zu Stringtheorie und stark gekoppelten Eichfeldtheorien, einschließlich Pionierarbeiten zur Verwendung nichtlokaler Observabler, der exakte Berechnung physikalischer Größen in der Quantenfeldtheorie und der Aufdeckung der nichtstörungstheoretischen Dynamik von Eichfeldtheorien" (Laudatio). Er erhielt 2009 den Early Research Award von Ontario.





Xie Chen

Xie Chen (* um 1984) ist eine chinesische theoretische Festkörperphysikerin.

Xie Chen studierte ab 2002 an der Universität Tsinghua mit dem Bachelor-Abschluss 2006 und ging dann an das Massachusetts Institute of Technology (als Whiteman Fellow), an dem sie 2012 in theoretischer promoviert wurde. Als Post-Doktorandin war sie 2012 bis 2014 Miller Research Fellow an der University of California, Berkeley. 2014 wurde sie Assistant Professor und 2017 Associate Professor am Caltech.

Sie befasst sich mit neuartigen (topologischen) Phasen (wie Symmetrie-geschützten topologischen Ordnungen, "symmetry protected topological phases", SPT) und zugehörigen Phänomenen (wie fraktionierte Symmetrien) in Vielteilchensystemen, stark korrelierten Systemen, Dynamik von Quantenvielteilchensystemen, Tensor-Netzwerk-Darstellungen und Anwendungen in der Quanteninformationstheorie.

Tensor-Netzwerk-Darstellungen dienen der effizienten Darstellung von verschränkten Vielteilchenwellenfunktionen mit area law und wurden von Chen und Kollegen insbesondere stark wechselwirkenden Systemen und Systemen mit topologischer Ordnung benutzt. Sie untersuchte auch das Phänomen der Symmetrie-Fraktionierung bei Systemen mit innerer topologischer Ordnung, bei denen sich die fraktionierte Statistik von Anyon-Zuständen auch auf die Darstellung unter globalen Symmetrien überträgt.

SPT besitzen nicht-triviale Kantenzustände ohne Anregungsschwelle (gap) bei ansonsten normal aussehendem Volumenzuständen und werden durch globale Symmetrien wie Ladungserhaltung und Zeitumkehrinvarianz verursacht, die den topologischen "Schutz" bereitstellen. Sie verallgemeinern die in der Theorie topologischer Isolatoren und Supraleiter auftretenden Phänomene auf allgemeinere Fermion-, Boson- und Spin-Systeme. Unter anderem klassifizierte sie mit Kollegen die möglichen SPT-Phasen und konstruierte SPT-Systeme.

2017 wurde sie Sloan Research Fellow und erhielt einen National Science Foundation Early Career Award. Für 2020 erhielt sie den New Horizons in Physics Prize "für einschneidende Beiträge zum Verständnis topologischer Zustände von Materie und ihrer Verbindungen zueinander" (Laudatio).






Sternstrom

Als Sternstrom werden Sternassoziationen (also lose Gruppierungen von Sternen mit physikalisch ähnlichen Eigenschaften) oder Bewegungshaufen bezeichnet, die in einer sehr ähnlichen Bewegungsrichtung und Geschwindigkeit eine Galaxie umkreisen. Auch langgestreckte Wasserstoffwolken sind möglich. Sternströme entstehen aus Zwerggalaxien oder Kugelsternhaufen, die durch Gezeitenkräfte auseinandergerissen und entlang ihrer Bahn verteilt worden sind.

Die nahe der Milchstraße kreisenden Mitglieder sind oft nur mit Hilfe der Stellarstatistik zu entdecken, vor allem anhand ihrer Eigenbewegung und von Alter bzw. Sternpopulation.

Kathryn V. Johnston: "Kosmische Fossilienjagd". In: "Spektrum der Wissenschaft", Mai 2015, S. 38–44