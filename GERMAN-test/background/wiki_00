<doc id="8741" url="https://de.wikipedia.org/wiki?curid=8741" title="Elektromagnetische Wechselwirkung">
Elektromagnetische Wechselwirkung

Die elektromagnetische Wechselwirkung ist eine der vier Grundkräfte der Physik. Wie die Gravitation ist sie im Alltag leicht erfahrbar, daher ist sie seit langem eingehend erforscht und seit über 100 Jahren gut verstanden. Die elektromagnetische Wechselwirkung ist verantwortlich für die meisten alltäglichen Phänomene wie Licht, Elektrizität und Magnetismus. Sie bestimmt zusammen mit der Austauschwechselwirkung den Aufbau und die Eigenschaften von Atomen, Molekülen und Festkörpern.

Ausgangspunkt der Erforschung war eine Untersuchung der Kräfte zwischen elektrischen Ladungen. Das Gesetz von Coulomb von etwa 1785 gibt diese Kraftwirkung zwischen zwei punktförmigen Ladungen ganz analog zum Gravitationsgesetz an. Die Wirkung von elektrischen Kräften auf entfernte Ladungen wird durch das Konzept des elektrischen Feldes beschrieben. Dieses wird nicht nur durch elektrische Ladungen hervorgerufen, sondern auch durch zeitliche Änderungen magnetischer Felder. Diese Erkenntnis geht vor allem auf Michael Faraday zurück. Während ruhende elektrische Ladungen anscheinend nichts mit den Erscheinungen des Magnetismus zu tun haben, erweist sich eine bewegte elektrische Ladung als Ursache eines magnetischen Feldes, wie Hans Christian Ørsted 1820 erkannte. Wenn sich in diesem Feld eine zweite Ladung bewegt, so erfährt sie nach den Gesetzen der klassischen Elektrodynamik eine magnetische Kraft, die dann etwa so groß wie die elektrische Kraft ist, wenn die Relativgeschwindigkeit in der Größenordnung der Lichtgeschwindigkeit ist. Die klassische Elektrodynamik ist das erste Beispiel einer Feldtheorie, die das einsteinsche Relativitätsprinzip erfüllt. Wenn die Elektrodynamik nur invariant gegenüber Galilei-Transformationen wäre, dann gäbe es keine Induktionserscheinungen und keine Ausbreitung elektromagnetischer Wellen.

Die Theorie der klassischen Elektrodynamik geht auf James Clerk Maxwell zurück, der im 19. Jahrhundert in den nach ihm benannten Maxwell-Gleichungen die Gesetze der Elektrizität, des Magnetismus und des Lichts als verschiedene Aspekte einer grundlegenden Wechselwirkung, des Elektromagnetismus, erkannte. Die elektromagnetische Wechselwirkung, die ja selbst das Ergebnis der Zusammenfassung der Theorie elektrischer und magnetischer Wechselwirkung ist, wird seit 1967 mit der schwachen Wechselwirkung zusammen als elektroschwache Wechselwirkung beschrieben. Eine Integration der starken Wechselwirkung in die gemeinsame einheitliche Feldtheorie wird angestrebt.

Kennzeichnend für die elektromagnetische Wechselwirkung ist, dass sie eine große (prinzipiell unendliche) Reichweite hat und gleichzeitig absättigbar ist, d. h. die Wirkung einer negativen und einer positiven Ladung auf eine entfernte dritte Ladung heben sich praktisch auf. Die Stärke der elektromagnetischen Wechselwirkung wird durch die Feinstrukturkonstante bestimmt, diese Kopplungskonstante ist etwa um den Faktor 100 kleiner als die der starken Wechselwirkung, aber um mehrere Größenordnungen größer als die der schwachen Wechselwirkung und erst recht viel größer als die der Gravitation.

Erscheinungen des Elektromagnetismus können auch dann beobachtbar sein, wenn keine elektrische Ladung in greifbarer Entfernung vorhanden ist, beispielsweise bei den elektromagnetischen Wellen oder beim Zerfall formula_1 des formula_2-Pions in zwei Gamma-Photonen.

Im Bereich der kleinsten Teilchen wird die elektromagnetische Wechselwirkung durch die Quantenelektrodynamik beschrieben. Die elektromagnetischen Potentiale werden darin als Feldoperatoren aufgefasst, durch diese werden die Photonen, die Wechselwirkungsteilchen der elektromagnetischen Wechselwirkung, erzeugt oder vernichtet. Anschaulich bedeutet das, dass die Wechselwirkung zwischen geladenen Teilchen, also der Austausch von Impuls und Energie, das Ergebnis des Austausches von Photonen zwischen diesen Teilchen ist.



</doc>
<doc id="14390" url="https://de.wikipedia.org/wiki?curid=14390" title="Amplitude">
Amplitude

Amplitude ist ein Begriff aus der Mathematik sowie aus der Physik und Technik zur Beschreibung von Schwingungen. Er ist anwendbar bei Größen wie beispielsweise einer Wechselspannung und deren Verlauf über der Zeit. Dabei wird er definiert als die maximale Auslenkung einer sinusförmigen Wechselgröße aus der Lage des arithmetischen Mittelwertes.
Der Begriff ist auch anwendbar auf Wellen, wenn sich die Schwingung mit einer konstanten Geschwindigkeit örtlich ausbreitet (Sinuswelle).
In DIN 40110-1 wird unterschieden zwischen
Für weitere Benennungen, die nicht auf Wechselgrößen beschränkt sind, aber allgemein für periodische Vorgänge verwendet werden, z. B. bei Mischspannung, siehe unter Scheitelwert.

Der Abstand zwischen Maximum und Minimum wird bei Schwingungen als "Schwingungsbreite" oder auch als "Spitze-Tal-Wert" bezeichnet (früher als Spitze-Spitze-Wert).

Eine ungedämpfte sinusförmige oder harmonische Schwingung wird durch

mit der "Amplitude" formula_4, Kreisfrequenz formula_5 und Nullphasenwinkel formula_6 beschrieben. Die Amplitude ist zeitunabhängig und damit konstant.

Eine andere Möglichkeit der Beschreibung ist die komplexe Darstellung mittels der Eulerschen Formel (mit dem in der Elektrotechnik üblichen Formelzeichen formula_7 für die imaginäre Einheit:)

Diese Form erleichtert viele Berechnungen, siehe Komplexe Wechselstromrechnung. Der Ausdruck
ist die komplexe Amplitude, deren Betrag gleich der Amplitude formula_4 und deren Argument gleich dem Nullphasenwinkel formula_6 ist.

In bestimmten Zusammenhängen kann sich die Amplitude auch langsam gegenüber der zugehörigen Schwingung ändern, z. B. bei Dämpfung oder Modulation.

Eine schwach gedämpfte, nicht periodische Schwingung wird mit dem Abklingkoeffizienten formula_12 durch
beschrieben. Der Ausdruck
ist die zeitveränderliche "Amplitudenfunktion".

Zur gezielten Beeinflussung der Amplitude siehe Amplitudenmodulation.

Gerne wird die Amplitude an mechanischen Beispielen veranschaulicht, insbesondere am Pendel.

Ein Federpendel führt im Idealfall (ungedämpft) eine Sinusschwingung aus. Die Distanz zwischen
ist die Amplitude.

Ein ebenes Physikalisches Pendel schwingt auch bei ungedämpfter Bewegung weder im Winkel noch in der horizontalen Auslenkung sinusförmig. Die horizontale Distanz zwischen Umkehrpunkt und Ruhepunkt ist ein "Scheitelwert". Nur bei geringer Auslenkung, wenn der Scheitelwert sehr viel kleiner ist als die Pendellänge, also wenn die Kleinwinkelnäherung angewendet werden kann, wird die Schwingung sinusförmig, und der Scheitelwert wird zur Amplitude.

Als Amplitude im weiteren Sinne werden auch die Grenzwerte der Abweichungen vom jeweiligen Mittelwert bei anderen Kurven in grafischen Darstellungen bezeichnet. Teilweise wird der Amplitude auch eine andere Bedeutung wie "Differenz zwischen dem Minimum und dem Maximum" zugeordnet. Hier hat eine Übernahme des Fachbegriffes in die Fachsprache anderer Fachwissenschaften stattgefunden, die ihn "nicht" der oben definierten Norm entsprechend verwenden, so dass die spezielle Bedeutung fallweise ungewiss ist, zum Beispiel in der Pneumologie bei der Spirometrie, in der Seismologie beim Seismogramm oder auch in der Meteorologie und Klimageographie beim Klimadiagramm.




</doc>
<doc id="26987" url="https://de.wikipedia.org/wiki?curid=26987" title="Skalengesetz">
Skalengesetz

Unter Skalengesetzen oder Skalierungsgesetzen versteht man die Manifestationen von mathematischen Beziehungen der Art

d. h. exponentielle Beziehungen, oder

d. h. Potenz- oder polynomiale Beziehungen, wobei formula_3 und formula_4 reelle Konstanten darstellen. Potenzgesetze sind häufiger anzutreffen als exponentielle Beziehungen.

Derartige Beziehungen sind in der Natur und Gesellschaft so verbreitet, dass man von einem strukturbildenden Prinzip sprechen kann. Teilweise handelt es sich um rein empirisch gefundene Verteilungen, teilweise konnten diese aber auf eine solide theoretische Basis gestellt werden, so dass im naturwissenschaftlichen Sinne von »Gesetzen« gesprochen werden kann. Das begründet sich unter anderem darin, dass
die Lösung der simpelsten linearen Differentialgleichung
ist, die einen sich selbst beschleunigenden Prozess beschreibt, z. B. das Wachstum einer Population ohne Ressourcenbeschränkung.

Skalenbeziehung, die auf Potenzgesetzen beruhen, sind skaleninvariant aufgrund der Beziehung
d. h., dass formula_8 proportional formula_9 ist und sich die Charakteristika von formula_10 nicht verändern. Exponentielle Beziehungen zeigen diese Skaleninvarianz nicht.


Geoffrey West führt die Universalität von Skalengesetzen in der Biologie auf folgende Punkte zurück:

Aus diesen Prinzipien scheinen sich wenigstens die Allometrien mit sehr einfachen Skalengesetzen (die Exponenten tendieren dazu, ganzzahlige Vielfache von 1/4 zu sein) ableiten zu lassen.

Beispiele sind die Beziehungen zwischen


Das Internet ist ein riesiges Netzwerk mit emergenten Phänomenen wie selbstähnlicher Skalierung in den Burst-Mustern seines Datenverkehrs und skalenfreier Struktur in der Verbindungstopologie.

Auch andere selbstlinkende Internet-Plattformen wie Weblogs zeigen einen bestimmten Zusammenhang: neue Weblogs linken bevorzugt – d. h. mit höherer Wahrscheinlichkeit – auf schon beliebte Weblogs und machen diese noch beliebter.
Dieser Verlinkungs-Algorithmus ist übrigens auch die Regel für die Erstellung eines skalenfreien Netzes.

"Hauptartikel:" Pareto-Verteilung : ...




</doc>
<doc id="36267" url="https://de.wikipedia.org/wiki?curid=36267" title="Herbert Kroemer">
Herbert Kroemer

Herbert Kroemer (Herbert Krömer) (* 25. August 1928 in Weimar) ist ein deutscher Physiker und Nobelpreisträger.

Herbert Kroemer begann nach dem Abitur am Friedrich-Schiller-Gymnasium Weimar 1947 mit dem Physikstudium an der Universität Jena und besuchte unter anderem Vorlesungen bei Friedrich Hund. Als er während der Berliner Luftbrücke zu einem Praktikum in Berlin war, nutzte er die Gelegenheit für die Flucht in den Westen und setzte sein Studium an der Georg-August-Universität Göttingen fort. Er promovierte 1952 bei Richard Becker auf dem Gebiet der Theoretischen Physik über Effekte von heißen Elektronen in Transistoren und arbeitete anschließend als „angewandter Theoretiker“, wie er es selbst nannte, im FTZ der Deutschen Bundespost, 1954 ging er in die USA und arbeitete bei verschiedenen Forschungseinrichtungen in Princeton und Palo Alto. Er lehrte von 1968 bis 1976 als Professor für Physik an der University of Colorado at Boulder und wechselte anschließend an die University of California, Santa Barbara.

Herbert Kroemer arbeitete nie in Bereichen, die gerade „aktuell“ waren, sondern bevorzugte Bereiche, deren Bedeutung erst Jahre später deutlich wurden. So veröffentlichte er in den 1950ern Arbeiten über das Konzept eines Heterojunction bipolar transistors, der Frequenzen im Gigahertzbereich ermöglicht. 1963 entwickelt er das Konzept von Doppelheterostrukturlasern, das Grundkonzept von Halbleiterlasern. Beide Konzepte waren ihrer Zeit weit voraus und konnten erst, nach der Entwicklung der Epitaxie, in den 1980ern verwirklicht werden.

In seiner Zeit in Santa Barbara verschoben sich seine Hauptaktivitäten von der Theorie in den experimentellen Bereich. So war er Ende der 1970er Jahre maßgeblich an der Entwicklung der Molekularstrahlepitaxie beteiligt, wobei er neue Materialkombinationen wie Galliumphosphid und Galliumarsenid auf Siliciumsubstraten herstellte und untersuchte. Nach 1985 verschob sich sein Interesse auf die Materialkombinationen Indiumarsenid, Galliumantimonid und Aluminiumantimonid.

Er wurde 2000 zusammen mit Schores Iwanowitsch Alfjorow mit der Hälfte des Nobelpreises für Physik für die Entwicklung von Halbleiterheterostrukturen für Hochgeschwindigkeits- und Optoelektronik ausgezeichnet, die andere Hälfte ging an Jack Kilby für die Entwicklung Integrierter Schaltkreise.

Von ihm und Charles Kittel stammt ein Lehrbuch über Thermodynamik.



</doc>
<doc id="37470" url="https://de.wikipedia.org/wiki?curid=37470" title="Walther Bothe">
Walther Bothe

Walther Wilhelm Georg Bothe (* 8. Januar 1891 in Oranienburg; † 8. Februar 1957 in Heidelberg) war ein deutscher Physiker. Seine Arbeiten waren ein wichtiger Beitrag zur Begründung der modernen Kernphysik.

Für die Entwicklung der Koinzidenzmethode und der damit gemachten Entdeckungen erhielt er im Jahr 1954 den Nobelpreis für Physik.

Bothe wurde im Haus an der Berliner Straße 2 in Oranienburg als Sohn des Uhrmachermeisters Friedrich Bothe und der Schneiderin Charlotte, geborene Hartung, geboren.
Er verbrachte seine Kindheit und einen Großteil seiner Jugend in seiner Heimatstadt. Ab 1892 wohnte er in der Bernauer Straße 7. Beide Häuser wurden im Zweiten Weltkrieg vollständig durch Bomben zerstört.
Bothe zeichnete sich durch einen scharfen Verstand und unerbittlich logisches Denken aus, besaß eine Begabung für Musik und Malerei und einen zielstrebigen Willen.

Ostern 1908 legte er an der Oberrealschule in Berlin das Abitur ab und studierte zwischen 1908 und 1913 Physik, Mathematik, Chemie und Musikwissenschaften an der Universität Berlin. Sein Studium wurde durch Privatunterricht, Gelegenheitsarbeiten und Stipendien weitestgehend von ihm selbst finanziert.

Nach der 1913 bestandenen Lehramtsprüfung arbeitete Bothe kurzzeitig als Assistent an der Landwirtschaftlichen Hochschule in Berlin, doch bald wurde er wissenschaftlicher Hilfsarbeiter an der Physikalisch-Technischen Reichsanstalt (PTR) im ein Jahr zuvor gegründeten Radioaktiven Laboratorium von Hans Geiger.
Als Schüler von Max Planck promovierte er 1914 zum Dr. phil. mit der theoretischen Arbeit „Zur Molekulartheorie der Brechung, Reflexion, Zerstreuung und Extinktion“. Im Ersten Weltkrieg geriet Bothe 1915 in russische Kriegsgefangenschaft, aus der er erst 1920 zurückkehrte. In der Gefangenschaft in Sibirien lernte er Russisch und vertiefte seine mathematischen Kenntnisse. Während dieser Zeit der Gefangenschaft baute er mit primitivsten Mitteln eine Zündholz- und eine Sodafabrik mit auf. Außerdem verfolgte er weiter mathematische Probleme und widmete seine Kraft dem Studium der russischen Sprache.

Am 8. Juli 1920 heiratete Bothe in Moskau Barbara (Warwara) Belowa. Er hatte sie vor dem Krieg in Berlin kennengelernt und stand mit ihr in ständigem Briefwechsel. Aus der Ehe gingen zwei Töchter hervor.

Nach Bothes Rückkehr aus der Kriegsgefangenschaft arbeitete er bis 1925 unter der Leitung von Hans Geiger an der Physikalisch-Technischen Reichsanstalt und wurde 1925 Geigers Nachfolger als Laboratoriumsvorsteher. Diesen Posten behielt er bis 1930. Von Hans Geiger lernte er, mit dem Phänomen der Radioaktivität experimentell umzugehen und entwickelte sich so zu einem theoretisch wie experimentell besonders gut ausgebildeten Kernphysiker. 1924 begannen er und Hans Geiger mit Versuchen zur Untersuchung des Compton-Effekts (die Rückstoßelektronen von Stößen mit Röntgenstrahlen sah Bothe schon einige Monate vor Entdeckung des Comptoneffekts in Wilson-Kammern) und sie entwickelten die Koinzidenzmethode.

Einen gewissen Wendepunkt auf dem Wege zur Klärung der quantentheoretischen Grundlagen stellte die von Niels Bohr gemeinsam mit Hendrik Anthony Kramers und John Clarke Slater 1924 verfasste Arbeit „The Quantum Theory of Radiation“ dar, in der angenommen wurde, dass die Sätze von der Impuls- und Energieerhaltung auf atomarer Ebene nur statistische Gültigkeit besäßen. Dass diese Annahme nicht zu halten war, zeigten sehr bald die Experimente von Hans Geiger und Walter Bothe sowie von Arthur Holly Compton und Alfred W. Simon, die nachwiesen, dass die Erhaltungssätze auch den einzelnen Elementarprozess beherrschen. Damit brach die Kopenhagener Begründung der verwendeten „dispersionstheoretischen Methode“, die Strahlentheorie von Bohr, Kramers und John Slater und die darin geforderte nur statistische Erhaltung von Energie und Impuls in atomaren Prozessen, zusammen. Die Widerlegung der Theorie von Bohr, Kramers und Slater, bei der Geiger und Bothe die Koinzidenzmethode anwandten, verschaffte beiden damals große Aufmerksamkeit.

1925 habilitierte Bothe sich bei Max Planck an der Universität Berlin „Über den Elementarprozess der photoelektrischen Elektronenauslösung“ und war damit der letzte der sieben Habilitanden Plancks.
Aus der Berliner Zeit wird folgender Zwischenfall berichtet: Wenn Otto Frisch, der Neffe von Lise Meitner, den Flur in der Nähe von Walther Bothes Laboratorium entlang ging, pfiff er gerne seine Interpretation der Brandenburgischen Konzerte von Bach. Bothe ließ sich dadurch regelmäßig beim Auszählen von Alphateilchen ablenken, was ihn viel Zeit für die Wiederholung der Versuche kostete.

Bothe wird von Arnold Sommerfeld in einem Brief an Tübingen von 11. Juni 1929 charakterisiert:

1929 wurde Bothe Privatdozent und außerordentlicher Professor an der Universität Gießen, 1930 ordentlicher Professor an der gleichen Universität und Direktor des Physikalischen Instituts. Er war der Erste, der die Quantenmechanik in seine Vorlesungen aufnahm.
Die Gießener Universität wurde im Jahre 1607 durch Landgraf Ludwig gegründet und zählt damit zu den ältesten deutschen Universitäten. Eine Reihe großer Physiker hatte einen Lehrstuhl in Gießen inne. Zu ihnen gehörten außer Walther Bothe auch Wilhelm Conrad Röntgen, Wilhelm Wien, Christian Gerthsen und Wilhelm Hanle.
1930 gelang Walter Bothe in Gießen die Entdeckung des angeregten Atomkerns. Die Situation der Experimentalphysik in Gießen wurde durch Bothe trotz seiner nur zweijährigen Tätigkeit völlig verändert. Gießen war zu einer Forschungsstätte größter Aktualität geworden.

Das Heidelberger Kaiser-Wilhelm-Institut (KWI) für medizinische Forschung war im Mai 1930 unter der Leitung des Internisten Ludolf von Krehl eingeweiht worden. Krehl strebte für seine Kreislaufforschungen die Zusammenarbeit mit anderen naturwissenschaftlichen Disziplinen an, und so wurden in diesem Institut vier Fachrichtungen in selbständigen Teilinstituten gleichberechtigt vereinigt: Pathologie, Physiologie, Physik und Chemie. Die historischen Umstände bedingten, dass sich der Arbeitsschwerpunkt des Heidelberger KWI gegen Ende der dreißiger Jahre stark in Richtung Chemie und Physik verschob, die von Richard Kuhn und Walther Bothe vertreten wurden.

1932 ging Bothe an die Universität Heidelberg und wurde Nachfolger von Philipp Lenard. Infolge der 1933 eintretenden politischen Veränderungen nach der Machtübernahme durch die Nationalsozialisten trat er jedoch vom Ordinariat und von der Institutsleitung zurück. 1934 wurde er zum Leiter des Instituts für Physik des Kaiser-Wilhelm-Instituts für medizinische Forschung – später ging aus einem Teil das Max-Planck-Institut für Kernphysik hervor – ernannt und amtierte bis 1957, gleichzeitig von 1934 bis 1945 als Honorarprofessor.

Bothes Interessen lagen nicht so sehr in biologischer Richtung. Deshalb wurde 1943 auf Vorschlag von Bothe der bereits das am Pariser Zyklotron arbeitende Personal ärztlich überwachende Gerhard Schubert zu biologischen Versuchen, insbesondere Tierversuchen mit künstlichen radioaktiven Stoffen, hinzugezogen.

Bothe pflegte einen barschen Umgangston, der Doktoranden und jüngeren Assistenten gegenüber oft dem eines Rekrutenfeldwebels nahekam. Auch Kollegen gegenüber äußerte er sich manchmal wenig verbindlich. Das hatte seinen Ursprung wohl einmal in dem militärischen Ton, der in seinen Jugendjahren in Teilen der kaiserlichen Physikalisch-Technischen Reichsanstalt üblich war. Zum anderen entsprang er der Haltung der Planckschen Schule. Lise Meitner hat dazu festgestellt, "„dass er nie etwas getan oder nicht getan hat, weil es ihm nützlich oder schädlich hätte sein können. Was er für richtig erkannt hat, hat er durchgeführt ohne Rücksicht auf seine eigene Person.“" Diese Devise war für die Arbeit im Institut und die Position des Instituts unter den politischen Umständen der dreißiger und vierziger Jahre nicht unbedingt förderlich. Sein Mitarbeiter Wolfgang Gentner wirkte hier ausgleichend. Er wurde von Bothe voll respektiert und konnte zum Wohl des Instituts und insbesondere der jüngeren Mitarbeiter die großzügige weltoffene Atmosphäre der Frankfurter und Pariser Laboratorien, die er in seinen Jugendjahren kennengelernt hatte, zur Geltung bringen.

Nach 1942 kehrte Bothe allmählich zu seiner ursprünglichen Grundlagenforschung zurück. So arbeitete er auch an der kontrollierten Kernspaltungs-Kettenreaktion. In diese Zeit fällt der Bau des ersten deutschen Zyklotrons, ein Teilchenbeschleuniger, den Bothe zusammen mit seinem Assistenten Wolfgang Gentner konstruierte.
Damit fand dann die fast zehnjährige Zusammenarbeit Gentners mit Walther Bothe ihren Abschluss, die sich als so fruchtbar erwiesen hat, weil Gentner mit seinem Blick für das Wesentliche, mit seiner Großzügigkeit und seiner auf solider Gesundheit gegründeten Arbeitskraft Bothe in glücklicher Weise ergänzte.

Bothe zog sich 1953 mit 61 Jahren auf den Direktorsposten des Instituts für Physik im Max-Planck-Institut für medizinische Forschung an der Heidelberger Jahnstraße zurück mit der Absicht, hier nur mit wenigen hochqualifizierten Assistenten und Studenten zu arbeiten. Drei wichtige wissenschaftliche Projekte fallen in diese Zeit: der Wiederaufbau des Zyklotrons, die Weiterentwicklung der Kernspektroskopie sowie die Fortsetzung der Untersuchungen von kosmischer Strahlung. In den 1950er und 1960er Jahren fanden die Arbeiten von Bothe und seinen Mitarbeitern zunehmend internationale Anerkennung.

Bothe gehörte neben weiteren Nobelpreisträgern zu den Unterzeichnern eines Appells vom 15. Juli 1955 an die Staatsmänner der Welt, auf die Gewalt als Mittel der Politik zu verzichten.

Anerkennung fand die Rolle seiner früheren Mitarbeiter Wolfgang Gentner und Heinz Maier-Leibnitz in deutschen und europäischen Wissenschaftsprojekten, z. B. der Gründung des Europäischen Zentrums für Kernforschung (CERN) und des Institut Laue-Langevin (ILL).

Mit wachsendem Alter mehrten sich bei Bothe die Krankheiten. Fortschreitende Gefäßverengungen hatten die Amputation eines Beines notwendig gemacht. Von diesem Eingriff hat er sich nicht mehr richtig erholen können. Ein Jahr nach seinem Tod im Jahr 1958 bekam das Institut für Physik einen eigenständigen Status als Max-Planck-Institut für Kernphysik unter der Leitung Wolfgang Gentners.

Walther Bothe hat als Pionier der modernen Kern- und Elementarteilchenphysik mit einer Fülle herausragender wissenschaftlicher Leistungen eine bleibende Spur in der Physikgeschichte des 20. Jahrhunderts hinterlassen.

Zu seinen wichtigsten Leistungen gehört, wie oben geschildert, die Entwicklung der Koinzidenzmethode.

Nachdem Victor Franz Hess 1912 bei Ballonfahrten die Höhenstrahlung entdeckt hatte, war es sein Zeitgenosse Walter Bothe zusammen mit Werner Kolhörster, die 1929 in Koinzidenzmessungen den Beweis durchdringender extraterrestrischer Strahlung, der Kosmischen Strahlung, erbrachten. Sie bewiesen damit auch, dass es sich um Teilchenstrahlung und nicht wie damals vielfach angenommen (insbesondere nach einer Theorie von Robert Millikan) um Gammastrahlung handelte.

Im Jahre 1929 entwickelten Bothe und Kolhörster eine spezielle Methode, um die Entladung von zwei oder mehreren getrennten Geiger-Müller-Zählrohren nur dann anzeigen zu lassen, wenn die Messung in einem vorbestimmten Zeitintervall erfolgte. Diese neue „Koinzidenzzählung“ ermöglichte es, die Bahn eines geladenen Teilchens durch die Zählrohre hindurch zu verfolgen.

1928 und in den folgenden Jahren untersuchte er mit Hans Fränz den Beschuss von Bor und dann auch anderen Atomkernen mit Alphateilchen, wobei Gruppen von Protonen als Streuprodukte entstanden, die definierte Energiedifferenzen hatten. Das war ein deutlicher Hinweis auf Kernanregungen und Bothe suchte zur Bestätigung seiner Vermutung nach Gammastrahlung mit gleicher Energie, die er auch 1930 fand (bei Bor mit einer Energie von 3 MeV). Auch dafür entwickelte er ein Koinzidenzverfahren.

Walther Bothe und sein Student Herbert Becker waren die ersten, die sich mit der Entdeckung des Neutrons beschäftigten. Sie beschrieben im Jahr 1930 einen ungewöhnlichen Typ von „Gammastrahlung“, der entstand, wenn sie Beryllium mit Polonium-Alphateilchen beschossen mit dem Ziel, die Theorie Ernest Rutherfords zu bestätigen und herauszufinden, ob bei diesem Vorgang sehr energiereiche Strahlen emittiert werden. Er erkannte allerdings nicht, dass es sich um ein neues Teilchen handelte. Für die Entdeckung des Neutrons erhielt später James Chadwick den Nobelpreis.

Bothe beschäftigte sich mit den fundamentalen Eigenschaften und der Struktur des Atoms. Er hatte kaum Interesse an medizinischer Forschung – das Angebot in Heidelberg war offensichtlich der Versuch, einen der führenden Experimentalphysiker Deutschlands davon abzuhalten, das Land zu verlassen.
In den 30er Jahren gehörten er und seine Mitarbeiter zu den ersten Wissenschaftlern, die den „nuklearen Photoeffekt“ beobachteten, kernspektroskopische Untersuchungen vornahmen und künstliche Isotope herstellten. Der Kernphotoeffekt ist eine Reaktionen eines Photons mit einem Atomkern.

Ende 1935, nach Ablauf Pariser Stipendiums Wolfgang Gentners, führten ihn seine Arbeitsthemen zu Walter Bothe nach Heidelberg. Bothe war zusammen mit Horn bei seinen Untersuchungen zum Durchgang harter Gammastrahlung durch Materie zu ähnlichen Ergebnissen wie Gentner gekommen und untersuchte ebenfalls Neutronen aus Kernreaktionen.
Gentner setzte seine Pariser Arbeiten in Heidelberg einerseits mit Bothe, andererseits mit Rudolf Fleischmann nahtlos fort. Bei dem Versuch, die Energieabhängigkeit des Kernphotoeffektes am Beryllium zu bestimmen und bei Überlegungen über die Fortsetzung dieser Arbeiten wurde klar, dass die Energie der Gammastrahlung relativ zur Bindungsenergie der Neutronen im Kern zu klein ist und dass Gammastrahlungsquellen mit deutlich höherer Energie und mit deutlich größerer Intensität benötigt werden. Bothe und Gentner beschlossen daraufhin, einen Bandgenerator nach Van de Graaff zu bauen. Dieses mit den wesentlichen Merkmalen und Instrumenten moderner elektrostatischer Beschleuniger ausgestattete Gerät wurde von Gentner unglaublich schnell aufgebaut. Schon im November 1936 war die Anregungsfunktion für

bis 500 keV Energie gemessen und im Sommer 1937 lagen umfangreiche Daten über den Kernphotoeffekt der 17 MeV Li (p, gamma) Strahlung an vielen mittelschweren und schweren Kernen vor. Der Wirkungsquerschnitt, ein Maß für die Wahrscheinlichkeit, dass infolge einer Wechselwirkung zwischen einem einfallenden Teilchen und einem anderen Teilchen eine Reaktion stattfindet, ergab sich um zwei Zehnerpotenzen größer als von Hans Bethe und Placzek berechnet. Mit Wolfgang Gentner gelang es 1937, künstliche Radioaktivität zu erzeugen. Dazu verwendeten sie eine Hochspannungsanlage mit einer Million Volt. Gentner und Bothe entdeckten damit die Möglichkeit, eine Vielzahl künstlich radioaktiver Nuklide zu erzeugen. Diese Entdeckung des Kernphotoeffektes an mittelschweren und schweren Kernen war der bedeutendste Erfolg des Bothe’schen Instituts in diesen Jahren. Dieser Erfolg verschaffte Gentner in gewisser Weise eine Sonderstellung.

Nach dem Waffenstillstand zwischen Deutschland und Frankreich im Sommer 1940 erhielten Bothe und Gentner den Auftrag, das Pariser Zyklotron, dessen Bau Joliot in Angriff genommen hatte, zu inspizieren. 1940 erschienen Walter Bothe und Wolfgang Gentner mit Mitarbeitern des Heereswaffenamtes im Pariser Institut. Joliot war abwesend, und sie stellten fest, dass das Zyklotron wegen Mängeln in der Hochfrequenzanlage noch nicht lief. Bothe bekam den Auftrag, in Heidelberg ein Zyklotron zu bauen, und schon im Laufe des Jahres 1941 war es ihm gelungen, beinahe alles hierfür Notwendige zu arrangieren. Im März 1943 traf schließlich der Magnet ein, und im Herbst desselben Jahres kam das Zyklotron bereits zum Einsatz. Gegenüber Albert Speer erklärte Bothe, die Maschine werde nur für die medizinische und biologische Forschung nützlich sein.

Walter Bothe gehörte in den 20er bis 50er Jahren zu den führenden deutschen Experimentalphysikern. Bothes Beweggründe, dem Uranprojekt beizutreten, waren vielschichtig. Er war gegen das nationalsozialistische Regime eingestellt, besonders nach seiner Entfernung aus der Universität im Jahre 1933. Doch obwohl er auch wusste, dass die Gestapo ihn jahrelang überwacht hatte, meldete er sich aus patriotischen Gründen freiwillig zur Kriegsforschung. Dafür fand er nach dem Krieg weder entschuldigende noch erklärende Worte, wie dies andere Mitglieder des Uranvereins taten.
Politisch patriotisch bis nationalistisch eingestellt, begann er in Heidelberg ab Juni 1940 für das Heereswaffenamt (HWA), Messungen am Neutronenquerschnitt des Kohlenstoffs durchzuführen. Dabei maß er im Januar 1941 einen völlig falschen Wert für die Diffusionslänge von Neutronen in Graphit, da er verunreinigtes Graphit verwendete, das er fälschlich für rein hielt. So kam es zum folgenschweren Ausschluss von Graphit als Moderator im deutschen Uranprojekt, im Gegensatz zu den USA, wo Enrico Fermi den ersten Reaktor in Chicago mit Graphit als Moderator betrieb. Eine abweichende Meinung von Georg Joos in Göttingen, der die Notwendigkeit hochreinen Graphits erkannte, setzte sich nicht durch und Paul Harteck in Hamburg wurde von weiteren Experimenten entmutigt. Der Fehler wurde erst 1945 bei Versuchen in Haigerloch erkannt, wo man Graphit als Reflektor verwendete.

Im Rahmen der Alsos-Mission erreichten amerikanische Sonderbeauftragte Mitte 1945 Heidelberg, da sich dort, im damaligen Kaiser-Wilhelm-Institut für medizinische Forschung, das einzige deutsche Zyklotron befand. Die Übernahme des Instituts erfolgte ohne Zwischenfälle. Bothe, der das Institut leitete, wurde verhört und seine Arbeiten konfisziert. Er teilte Goudsmit jedoch mit, dass er entsprechend den Anweisungen der Regierung alle seine geheimen Berichte verbrannt habe. Bis zur deutschen Kapitulation verweigerte Bothe die Aussage, er wurde jedoch nicht wie die anderen Mitglieder des Uranvereins in England interniert. Schließlich übergab Bothe sämtliche verbliebene Dokumente an Alsos, wollte sich jedoch zu geheimen Forschungen an seinem Institut nicht äußern.

In der Zeit der Besatzung fertigte Bothe im Rahmen der „Field Information Allied Technical (FIAT) reports“ zusammen mit Siegfried Flügge einen Band über Kernphysik und kosmische Strahlen an, der sich mit der Arbeit des Uranprojekts befasste.

In der zweiten Hälfte der 40er Jahre war die Hauptsorge Essen und ein Dach über dem Kopf zu finden, weshalb Bothe Mühe hatte, seine Arbeitsgruppe aufrechtzuerhalten und so etwas wie ernsthafte Forschung zu betreiben. Obwohl er nicht auf seinem ursprünglichen Gebiet der Kernphysik arbeiten durfte, wurde Bothe wieder als Direktor des Instituts für Physik der Universität Heidelberg eingesetzt. Er nutzte diese Position, um seine alte Arbeitsgruppe zu erhalten und das Institut zu modernisieren und auf feste Füße zu stellen.

Walther Bothe kehrte 1945 ins I. Physikalische Institut der Universität zurück. Wolfgang Gentner aber entschied sich 1946 für Freiburg, wo das Physikalische Institut vollständig zerstört war.

Schon während des Krieges hatten Bothe und Gentner Pläne für ein neues Kaiser-Wilhelm-Institut mit größeren Teilchenbeschleunigern ausgearbeitet. Gentner griff diese Ideen wieder auf. In engem Kontakt mit den Kernphysikern der Universität, insbesondere mit Otto Haxel und J. Hans D. Jensen, nahm Gentner als ersten Schritt die Aufstellung eines Tandem-Beschleunigers mit einer Maximalspannung von 6 MV in Angriff. Außerdem wurde ein besonderes Gebäude für Kosmophysik vorgesehen, um mit radioaktiven Methoden Altersbestimmungen an Meteoriten durchzuführen. Zu beiden Arbeitsgebieten kamen jüngere Physiker aus Freiburg mit Gentner nach Heidelberg.

Seit Mai 1946 bis zu seinem Tode leitete Bothe das Physikalische Institut beim Max-Planck-Institut (dem Nachfolger des Kaiser-Wilhelm-Instituts) der Universität Heidelberg.

1947 versuchte Walther Bothe, der nach dem Zweiten Weltkrieg in Heidelberg alleiniger Lehrstuhlinhaber für Physik war und außerdem der kernphysikalischen Abteilung des dortigen Kaiser- Wilhelm- Instituts für medizinische Forschung vorstand, Hans Jensen für Heidelberg zu gewinnen. Im Wintersemester 1948/49 folgte Jensen diesem Ruf. Abgesehen von einer vertretungsweisen Lehrveranstaltung durch Walter Wessel, der jedoch bald in die Vereinigten Staaten ging, hatte es nach Kriegsende in Heidelberg keine Vorlesung über theoretische Physik gegeben. Das erste theoretisch-physikalische Seminar wurde vom Assistenten Jensens, Helmut Steinwedel, der schon einige Monate vor Jensen nach Heidelberg gekommen war, durchgeführt. Dazu kamen Michael Danos, den Jensen ebenfalls von Hannover her kannte, und etwas später Heinz Koppe und Arnold Schoch. Der Aufbau des Instituts für Theoretische Physik hatte begonnen.

Bothe nahm bereits vor 1948 das einzige existierende Zyklotron in Heidelberg wieder in Betrieb. Er führte mit seinen Studenten kernphysikalische Experimente durch und stellte radioaktive Präparate für die benachbarte Klinik her.
Zum Umbau des Zyklotrons holte Bothe Christoph Schmelzer aus Jena, der sich 1949 mit einer Arbeit über das dielektrische Verhalten polar aufgebauter Materie habilitierte. Bothe veranlasste auch, dass Hans Jensen aus Hamburg 1949 nach Heidelberg berufen wurde, ebenso wie Otto Haxel aus Göttingen.

Interesse an den Heidelberger Forschungsarbeiten stellte sich ein. Wolfgang Pauli, der Deutschland nach dem Kriege zunächst fernblieb, konnte von Jensen bewogen werden, nach Heidelberg zu kommen. Eine Gelegenheit dazu bot sich beim 60. Geburtstag von Walther Bothe. Auch Hans Bethe, George Gamow, Maria Goeppert-Mayer, Lothar Nordheim, Isidor Isaac Rabi, Victor Weisskopf, Eugene Wigner und viele andere hervorragende Persönlichkeiten kamen bald zu Besuchen nach Heidelberg.

Nach dem Krieg entstanden Physikalische Gesellschaften zunächst als getrennte Vereine für die Britische Zone, für Württemberg, Baden, Pfalz, für Hessen, für Bayern für Berlin. Im Südwesten versammelten sich 160 Mitglieder unter dem Vorsitz von Bothe.

Am 29. Februar 1952 formierte sich eine Kommission für Atomphysik der Deutschen Forschungsgemeinschaft DFG, der unter Heisenbergs Vorsitz auch Walther Bothe angehören sollte.

In den Folgejahren widmete sich Bothe den Forschungen auf dem Gebiet der Kernphysik und der Anwendung künstlich erzeugter radioaktiver Elemente. Fortschreitende Krankheit zwang ihn dann jedoch, sich schrittweise aus dem Forscherleben zurückzuziehen.

Nach Bothes Tod im Februar 1957 stand die Zukunft seines Heidelberger Instituts längere Zeit zur Diskussion. Stimmen, die vorher für Schließung plädiert hatten, traten schließlich in den Hintergrund, weil die Physiker der Heidelberger Universität die wissenschaftliche Leistungsfähigkeit des Instituts zur Geltung brachten. Das umgebaute Zyklotron war seit 1956 in Betrieb und es liefen weltweit anerkannte Arbeiten über die Nichterhaltung der Parität in der schwachen Wechselwirkung.
Er malte (Öl und Aquarell), hatte eine Vorliebe für Impressionisten in der Malerei, und spielte sehr gut Klavier (mit einer Vorliebe für Bach vor Beethoven).

1952 wurde Bothe in den Orden Pour le mérite für Wissenschaften und Künste aufgenommen und war dort neben Max von Laue der einzige Physiker.

1953 wurde Bothe die Max-Planck-Medaille verliehen. Die Max-Planck-Medaille ist eine Auszeichnung, die seit 1929 jährlich von der Deutschen Physikalischen Gesellschaft (DPG) für besondere Leistungen auf dem Gebiet der Theoretischen Physik verliehen wird. Diese Auszeichnung gilt als die bedeutendste in diesem Fach in Deutschland. Sie besteht aus einer Urkunde und einer goldenen Medaille mit dem Porträt Max Plancks.

Den Nobelpreis für Physik erhielt Walther Bothe am 10. Dezember 1954 zusammen mit dem deutschen, aber schon 1929 nach England emigrierten Forscher Max Born. Gewürdigt wurde die von Bothe entwickelte Koinzidenzmethode und die damit gemachten Entdeckungen.
Das Verfahren aus der Elementarteilchen- und Kernphysik zieht Rückschlüsse aus dem gleichzeitigen oder mit definiertem zeitlichen Abstand erfolgenden Eintreffen von kernphysikalischen Messungen auf unterschiedliche Merkmale von Elementarteilchen. So lassen sich durch die Verwendung verschiedener Nachweisgeräte Flugbahnen, Geschwindigkeiten und Reichweiten einzelner Teilchen bestimmen. Die Erkenntnisse sind von grundlegender Bedeutung für das Verständnis vom Aufbau der Materie und von unterschiedlichen Strahlungen.
Bothe war aus gesundheitlichen Gründen nicht in der Lage, nach Stockholm zu reisen. Auf seinen Wunsch nahm seine Tochter Dr. Elena Riedel die Auszeichnung in Empfang. Die deutsche Fassung seines Nobelvortrages befindet sich im Archiv der Max-Planck-Gesellschaft (MPG). Vgl. Peter Brix, „Hans Geiger, Ein Wegbereiter der modernen Naturwissenschaft“, Heidelberger Jahrbücher XXVII 1983, S. 110.

1955 erhielt Bothe das Große Verdienstkreuz der Bundesrepublik Deutschland, 1956 die Ehrendoktorwürde der Universität Gießen.

In Oranienburg wurde 1993 die Ernst-Thälmann-Straße in Walther-Bothe-Straße umbenannt.

Bothe war Mitglied der Preußischen, der Sächsischen, der Göttinger (seit 1933) und der Heidelberger Akademie der Wissenschaften.

Der Asteroid (19178) Walterbothe wurde ihm zu Ehren benannt.

Bothe verfasste mehr als 200 wissenschaftliche Publikationen in den Bereichen von der Optik bis zur kosmischen Ultrastrahlung, u. a.:


Weiter wurden für den Artikel verwendet:



</doc>
<doc id="73153" url="https://de.wikipedia.org/wiki?curid=73153" title="Thermische Zustandsgleichung idealer Gase">
Thermische Zustandsgleichung idealer Gase

Die thermische Zustandsgleichung idealer Gase, oft auch als allgemeine Gasgleichung bezeichnet, beschreibt den Zusammenhang zwischen den thermischen Zustandsgrößen eines idealen Gases. Sie vereint die experimentellen Einzelergebnisse und die hieraus abgeleiteten Gasgesetze zu einer allgemeingültigen Zustandsgleichung.

Die Gleichung beschreibt den Zustand des idealen Gases bezüglich der Zustandsgrößen Druck formula_1, Volumen formula_2, Temperatur formula_3 und Stoffmenge formula_4 bzw. Teilchenzahl formula_5 bzw. Masse formula_6. Sie kann in verschiedenen zueinander äquivalenten Formen dargestellt werden, wobei alle diese Formen den Zustand des betrachteten Systems in gleicher Weise und eindeutig beschreiben. Ihre erste Formulierung stammt von Émile Clapeyron im Jahr 1834.

Extensive Formen:

Intensive Formen:

Hierbei stehen die einzelnen Formelzeichen für folgende Größen:


Die Gleichung stellt den Grenzfall aller thermischen Zustandsgleichungen für verschwindende Dichte formula_27 dar, das heißt für verschwindenden Druck bei genügend hoher Temperatur. In diesem Fall kann man das Eigenvolumen der Gasmoleküle und die Kohäsion – die anziehende Kraft zwischen den Molekülen – vernachlässigen. Die Gleichung ist für viele Gase wie zum Beispiel wasserdampfungesättigte Luft auch bei Normalbedingungen eine gute Näherung.

1873 erweiterte Johannes Diderik van der Waals das Gasgesetz zur Van-der-Waals-Gleichung, die das Eigenvolumen der Gasteilchen und die Anziehung zwischen ihnen im Gegensatz zur allgemeinen Gasgleichung mit berücksichtigt und somit auch als Näherung auf deutlich reale Gase angewendet werden kann. Eine andere Näherungslösung für reale Gase stellt die Reihenentwicklung der Virialgleichungen dar, wobei die allgemeine Gasgleichung identisch mit einem Abbruch der Reihenentwicklung nach dem ersten Glied ist. Generell gilt, dass die allgemeine Gasgleichung als Näherungslösung für schwach reale Gase bei geringen intermolekularen Wechselwirkungen, kleinen Drücken und hohen Temperaturen (großen Molvolumina) geeignet ist. Insbesondere weisen ideale Gase hierbei keinen Joule-Thomson-Effekt auf.

Es gibt verschiedene Spezialfälle des allgemeinen Gasgesetzes, die einen Zusammenhang zwischen zwei Größen herstellen, während alle anderen Größen konstant gehalten werden. Erklärt und nicht nur empirisch abgeleitet werden diese Zusammenhänge zwischen den Zustandsgrößen eines Gases durch dessen Teilchencharakter, also durch die kinetische Gastheorie.

Das Gesetz von Boyle-Mariotte, auch Boyle-Mariottesches Gesetz oder Boyle-Mariotte-Gesetz und oft mit Boyle’sches Gesetz abgekürzt, sagt aus, dass der Druck idealer Gase bei gleichbleibender Temperatur (isotherme Zustandsänderung) und gleichbleibender Stoffmenge umgekehrt proportional zum Volumen ist. Erhöht man den Druck auf ein Gaspaket, wird durch den erhöhten Druck das Volumen verkleinert. Verringert man den Druck, so dehnt es sich aus. Dieses Gesetz wurde unabhängig von zwei Physikern entdeckt, dem Iren Robert Boyle (1662) und dem Franzosen Edme Mariotte (1676).

Für formula_28 und formula_29 gilt:

Das erste Gesetz von Gay-Lussac, auch Gay-Lussacsches Gesetz, Gesetz von Charles oder Charlessches Gesetz, besagt, dass das Volumen idealer Gase bei gleichbleibendem Druck (isobare Zustandsänderung) und gleichbleibender Stoffmenge direkt proportional zur Temperatur ist. Ein Gas dehnt sich also bei einer Erwärmung aus und zieht sich bei einer Abkühlung zusammen. Dieser Zusammenhang wurde 1787 von Jacques Charles und 1802 von Joseph Louis Gay-Lussac erkannt.

Für formula_31 und formula_32 gilt:

Das eigentliche Gesetz von Gay-Lussac (obiges ist nur der Teil, den man meist als das "Gesetz von Charles" bezeichnet) lautet:

Hierbei ist formula_35 die Temperatur am Nullpunkt der Celsiusskala, also 273,15 K oder 0 °C. formula_36 ist das Volumen bei formula_35 und formula_38 der Volumenausdehnungskoeffizient bei formula_35, wobei für ideale Gase allgemein formula_40 gilt. Dagegen ist formula_3 die Temperatur, für die das Volumen formula_2 gesucht wird.

Aus dieser Gleichung kann man folgern, dass es einen absoluten Temperaturnullpunkt geben muss, da die Gleichung für diesen ein Volumen von Null voraussagt und das Volumen nicht negativ werden kann. Ihre empirische Basis ist daher auch Grundlage für die absolute Temperaturskala Kelvins, da hierüber durch Extrapolation der Temperaturnullpunkt bestimmt werden konnte.

Das Gesetz von Amontons, oft auch 2. Gesetz von Gay-Lussac, sagt aus, dass der Druck idealer Gase bei gleichbleibendem Volumen (isochore Zustandsänderung) und gleichbleibender Stoffmenge direkt proportional zur Temperatur ist. Bei einer Erwärmung des Gases erhöht sich also der Druck und bei einer Abkühlung wird er geringer. Dieser Zusammenhang wurde von Guillaume Amontons entdeckt.

Für formula_43 und formula_32 gilt:

Analog zum "Gesetz von Gay-Lussac" gilt hierbei auch:

Das Gesetz der Homogenität sagt aus, dass ein ideales Gas durch und durch homogen, das heißt gleichförmig, ist, dass es also überall dieselbe Dichte hat. Wenn in einem großen Behälter mit einem homogenen Stoff, zum Beispiel mit einem Gas, an einer Stelle eine Teilmenge formula_47 eingeschlossen wird, so enthält diese dieselbe Stoffmenge wie eine Teilmenge mit demselben Volumen formula_47 an anderer Stelle. Teilt man die gesamte Stoffmenge auf zwei gleich große Volumina auf, so enthalten sie die gleiche Stoffmenge, nämlich die Hälfte der ursprünglichen. Daraus folgt:

Das Volumen ist bei gleichbleibendem Druck und gleichbleibender Temperatur proportional zur Stoffmenge.

Für formula_49 und formula_31 gilt:

Diese Gesetze gelten für alle homogenen Stoffe, solange Temperatur und Druck unverändert bleiben, und eben auch für ideale Gase.

Das Gesetz von Avogadro sagt aus, dass zwei gleich große Gasvolumina, die unter demselben Druck stehen und dieselbe Temperatur haben, auch die gleiche Teilchenzahl einschließen. Dies gilt sogar dann, wenn die Volumina verschiedene Gase enthalten. Selbstverständlich gilt es auch für den Fall, dass die Zusammensetzung in den beiden Volumina gleich ist; deswegen folgt auch aus dem Gesetz von Avogadro die Beziehung formula_52 für formula_49 und formula_31. Darüber hinaus bedeutet es aber auch, dass ein Gaspaket in einem bestimmten Volumen auch eine bestimmte Anzahl von Teilchen hat, die unabhängig von der Stoffart ist. Allerdings gibt es gewisse Ausnahmen, wenn zum Beispiel weniger oder zu viele Teilchen in einem Gaspaket sind.

Das Gesetz von Avogadro wurde 1811 durch Amedeo Avogadro entdeckt.

Es kann auch so formuliert werden: Das molare Volumen ist bei einer bestimmten Temperatur und bei einem bestimmten Druck für alle idealen Gase identisch. Messungen haben ergeben, dass ein Mol eines idealen Gases bei 0 °C = 273,15 K und 1013,25 hPa Druck ein Volumen von rund 22,4 dm³ einnimmt.

Eine bedeutende Folge des Gesetzes ist: Die Gaskonstante ist für alle idealen Gase identisch.

Die kinetische Gastheorie besagt, dass sich Gase aus vielen einzelnen Atomen bzw. Molekülen zusammensetzen, die jedes für sich eine Masse formula_6 und eine Geschwindigkeit formula_25 haben. Die mittlere kinetische Energie aller Teilchen ist der Temperatur des Gases proportional. Es gilt

wobei formula_58 das mittlere Geschwindigkeitsquadrat der Teilchen ist. Man sieht, dass sich die Moleküle bei höherer Temperatur des Gases mit höheren Geschwindigkeiten bewegen. Dabei besitzen nicht alle Teilchen die gleiche Geschwindigkeit, sondern es tritt eine statistische Verteilung der Geschwindigkeiten auf (Maxwell-Boltzmann-Verteilung).

Ist das Gas in einem Behälter mit dem Volumen formula_2 eingeschlossen, so stoßen immer wieder Gasmoleküle gegen die Wand des Behältnisses und werden reflektiert. Dadurch übertragen die Teilchen pro Zeiteinheit und pro Wandfläche einen bestimmten Impuls auf die Wand. Es wirkt mit den Teilchenstößen auf jeden Teil der Wand eine Kraft, die wir als den Gasdruck formula_1 begreifen.

Dieser Druck formula_1 ist umso größer, je schneller die Teilchen sind. Zum einen steigt bei hohen Teilchengeschwindigkeiten die Rate, mit der die Gasmoleküle auf die Wand treffen, da sie den Behälterraum schneller durchqueren. Zum anderen werden die Stöße gegen die Wand "heftiger" und es wächst der dabei übertragene Impuls. Wird die Teilchendichte formula_62 erhöht, so wächst die Wahrscheinlichkeit, mit der Moleküle an die Wand stoßen. Aus solchen Überlegungen kann man diese Gleichung für den Druck herleiten:

Drückt man die mittlere kinetische Energie der Gasteilchen durch die Temperatur aus, so ergibt sich daraus die thermische Zustandsgleichung idealer Gase:

Diese Gleichung gilt jedoch nur bei Gasen mit geringer Teilchendichte und bei genügend hoher Temperatur. Bei dieser Herleitung wird nämlich vernachlässigt, dass Anziehungskräfte zwischen den Teilchen wirken, die den Teilchendruck gegen die Wand abschwächen. Außerdem besitzen die Moleküle selbst ein Volumen und das Gas kann nicht beliebig komprimiert werden, weil die Teilchen sich gegenseitig verdrängen. Die Beschreibung eines solchen realen Gases bewältigt die Van-der-Waals-Gleichung.

Die Gesetze von Amontons und Gay-Lussac, die beide zeitlich "vor" der Gasgleichung gefunden wurden, lassen sich beispielsweise durch das Gedankenexperiment einer zweistufigen Zustandsänderung zusammenfassen, wobei man hierbei generell von einer gleichbleibenden Stoffmenge ausgeht.

Zunächst betrachtet man eine isochore Zustandsänderung nach dem "Gesetz von Amontons". Der Ausgangspunkt ist hierbei der Zustand 1 mit formula_65 und formula_66. Endpunkt ist Zustand 2 mit formula_67 und formula_68.

Es folgt eine isobare Zustandsänderung nach dem "Gesetz von Gay-Lussac" von Zustand 2 nach Zustand 3 mit formula_70 und formula_71.

Setzt man nun den Ausdruck für formula_68 aus obiger Gleichung in den Ausdruck für formula_68 aus unterer Gleichung ein und stellt um, wobei man formula_75 und formula_76 berücksichtigen muss, so erhält man als Resultat die Beziehung:

und daher

Als letzten Schritt muss man die Konstante im rechten Term des obigen Ausdrucks ermitteln. Geht man davon aus, dass ein Mol bei 273,15 Kelvin und 101,325 Kilopascal genau 22,414 Liter einnimmt, also das Gesetz von Avogadro gültig ist, so kann man auch davon ausgehen, dass formula_4 Mol eines idealen Gases genau formula_80 22,414 Liter einnehmen. Wenn man dies in obige Gleichung einsetzt, erhält man:

Multipliziert man die Gleichung mit formula_71, so erhält man:

Der Bruch auf der linken Seite der Gleichung ist eine konstante Größe, man bezeichnet diese als "universelle Gaskonstante formula_84" (auch formula_20 für "molare Gaskonstante") und berechnet sie so:

Streicht man nun die Indizes, so erhält man die gesuchte allgemeine Gasgleichung:

Grundlage für die Herleitung ist die thermische Zustandsgleichung:

Hierbei stehen die einzelnen Formelzeichen für folgende Größen:


Bei einem idealen Gas gilt speziell:

Hierdurch wird die thermische Zustandsgleichung zu folgender Form vereinfacht:

Man kann die Zustandsgleichung nun von einem Zustand 1 bis zu einem Zustand 2 integrieren (bestimmtes Integral) und erhält dadurch:

Gleichen sich zwei der Zustandsgrößen, haben sie sich also vom Zustand 1 zum Zustand 2 nicht geändert, so können diese gekürzt werden und man erhält dadurch die jeweiligen Spezialfälle. Die universelle Gaskonstante formula_84 muss hierbei experimentell bestimmt werden und leitet sich nicht aus der Integrationskonstanten ab. Einen beispielhaften Versuchsaufbau kann man im Artikel „Gaskonstante“ nachlesen.



</doc>
<doc id="88085" url="https://de.wikipedia.org/wiki?curid=88085" title="Energiedosis">
Energiedosis

Die Energiedosis formula_1 ist eine physikalische Größe und gibt die mittlere, von ionisierender Strahlung an einen spezifischen Absorber mit der Dichte formula_2, abgegebene Energie formula_3 bezogen auf die Masse formula_4 des bestrahlten Volumenelements formula_5 an.

formula_6

Das Absorbermaterial muss daher bei Bestimmung und Angabe von Energiedosen beachtet werden.
Dies hat den Grund darin, dass die Ionisierungsenergien für unterschiedliche Atome beziehungsweise Moleküle variieren.

Die Energiedosis wird aus praktischen Gründen nicht direkt gemessen, sondern über die Größen Ionendosis oder Kerma bestimmt.

Aus der gemessenen Ionendosis kann man leicht die entsprechende Energiedosis in Luft ableiten. Da zur Bildung eines Ionenpaares in Luft im Mittel die Energie 34 eV nötig ist, muss in Luft für die Erzeugung der Ladungsmenge 1 Coulomb in Form von freien Ionen die Energie (33,97 ± 0,05) J aufgebracht werden. Damit entspricht einer Ionendosis von formula_7 ungefähr eine Energiedosis von formula_8. Es gilt die Beziehung:

mit "f" := Korrekturfaktor, zur Bestimmung von Energiedosen in unterschiedlichen Absorptionsmaterialien.

Die SI-Einheit der Energiedosis ist das Gray. 

formula_10

Veraltet ist die Einheit Rad ("rd").

formula_11

Die SI-Einheit ist Watt/kg.

Der physikalisch korrekte Ausdruck "Leistungsdosis" ist nicht gebräuchlich.



</doc>
<doc id="92540" url="https://de.wikipedia.org/wiki?curid=92540" title="Fokus">
Fokus

Der Fokus (von für „Feuerstätte“, „Herd“) oder Brennpunkt ist jener besondere Punkt eines abbildenden optischen Geräts, in dem sich die Strahlen schneiden, die parallel zur optischen Achse einfallen. Er befindet sich im Abstand der Brennweite auf der optischen Achse. Die im Brennpunkt zur Achse senkrechte Ebene heißt Brennebene (manchmal auch "Fourierebene", "Fokalfläche" oder "Fokalebene"). Für weit entfernte Objekte, wie die Sonne, ist die Brennebene zugleich Bildebene – vom heißen Abbild der Sonne, in dem sich ihre Strahlungsenergie konzentriert, hat der Brennpunkt seinen Namen. Hier und in der Praxis wird er deshalb auch Brennfleck genannt. Je schwächer die Optik "fokussiert", desto weiter ist der Fokus entfernt.

Bei Zerstreuungslinsen oder Wölbspiegeln sind es nicht die gebrochenen bzw. reflektierten Strahlen selbst, sondern ihre rückwärtigen Verlängerungen, die sich im Fokus schneiden. Auch diese Art Fokus wird Brennpunkt genannt, obwohl sich hier keine Strahlungsenergie konzentriert.

Eine Linse oder Linsengruppe besitzt für die Strahlen, die aus der entgegengesetzten Richtung einfallen, einen zweiten Brennpunkt. Ein Parabolspiegel dagegen hat für beide Richtungen denselben Brennpunkt.

Die mathematischen Brennpunkte einer Ellipse, Parabel oder Hyperbel sind zwar keine Fokusse im Sinne dieses Artikels, ihr Name leitet sich jedoch vom physikalischen Brennpunkt ab, da auch reelle Ellipsoide und Paraboloide Lichtstrahlen im (mathematischen) Brennpunkt fokussieren können, z. B. im Flüstergewölbe, beim Parabolspiegel oder der Parabolantenne.



</doc>
<doc id="106672" url="https://de.wikipedia.org/wiki?curid=106672" title="Urmeter">
Urmeter

Der Urmeter ( ‚Archivmeter‘) ist die bis 1960 gültige Maßverkörperung der Längeneinheit Meter. Bei der Einführung des Metrischen Einheitensystems wurde der Meter zunächst als zehn-millionster Teil des Viertels desjenigen Erdumfangs festgelegt, der Paris und den Nordpol berührt. Der Erdumfang wurde geodätisch vermessen und das Ergebnis auf den Abstand zweier Markierungen auf einem Metallstab übertragen. Dieser Metallstab wurde in der Folge zur Eichung von Messgeräten und Maßstäben für den täglichen Gebrauch und die Landesvermessung verwendet.

Der Begriff des Urmeters bezieht sich insbesondere auf das zweite Spezimen (Muster) von 1799, jenes Endmaß aus Platiniridium, das bis 1889 die Längeneinheit verkörperte. Vielfach wird aber auch der "internationale Meterprototyp" von 1889 als "Urmeter" bezeichnet.

Am 26. März 1791 beschloss die verfassunggebende Versammlung in Paris auf Vorschlag der Académie des sciences (Akademie der Wissenschaften) die Einführung einer universellen Längeneinheit. Das neue, noch nicht „Meter“ genannte Längenmaß solle der zehnmillionste Teil des Erdmeridianquadranten (Strecke vom Pol zum Äquator) sein. Dazu sollte der Meridianbogen von Dünkirchen bis Barcelona von zwei französischen Astronomen, Jean-Baptiste Joseph Delambre und Pierre Méchain, neu vermessen werden, was sich allerdings in den Wirren der französischen Revolution bis 1798 hinzog. 1791 wurde ein anderer Definitionsvorschlag Talleyrands und Jeffersons, basierend auf einem Sekundenpendel, verworfen, da diese Methode von lokalen Unterschieden der Erdbeschleunigung beeinflusst worden wäre.

Am 1. August 1793, unter der Terrorherrschaft, wurde dieses Längenmaß – auf Vorschlag Bordas Meter genannt – im Nationalkonvent gesetzlich eingeführt, allerdings mit einem provisorischen Wert von 443,440 Pariser Linien, was knapp 1000,325 Millimetern entspricht. Auf Grundlage dieses Wertes wurde 1795 ein erster Messing-Prototyp hergestellt.

Nach Abschluss der Triangulationen zwischen Dünkirchen und Barcelona auf der Meridianexpedition wurde 1799 ein zweites sogenanntes definitives Urmeter als Endmaß aus Platin hergestellt und am 22. Juni 1799 im französischen Nationalarchiv in einem Stahlschrank verschlossen. Heute wird es in einem Tresor des Internationalen Büros für Maß und Gewicht (BIPM) in Sèvres bei Paris aufbewahrt. Seine Endflächen waren jedoch spätestens Anfang des 20. Jahrhunderts beschädigt.

Allerdings ist dessen Genauigkeit – bezüglich des angestrebten zehnmillionsten Teils der Distanz vom Äquator zum Pol – mit definierten 443,296 Pariser Linien (entsprechend 1000,0001606 mm) – noch geringer als die des sogenannten „provisorischen Meters“, da diese Distanz nach dem WGS84 etwa 10001,966 km beträgt.  Die Übernahme dieses Urmeters als Maßeinheit wurde am 20. Mai 1875 in der „internationalen Meterkonvention“ von siebzehn Staaten beschlossen.

Bisher hatte man "Endmaße" benutzt, u. a. weil sich diese leichter und genauer mit anderen Maßstäben abgleichen lassen. Dem steht der gravierende Nachteil gegenüber, dass die Endflächen beim Kopieren durch die Berührung mit Fühlhebeln oder dergleichen beschädigt werden können (wie es beim zweiten Urmeter geschehen war). Als am 26. September 1889 das Urmeter von der Generalkonferenz für Maß und Gewicht durch einen Meterprototypen aus einer Legierung aus 90 % Platin und 10 % Iridium ersetzt wurde, ging man daher zu einem "Strichmaß" über. Auf diesem 102 cm langen Normal mit X-förmigem Querschnitt (20 mm × 20 mm) repräsentierten Strichgruppen die Länge von einem Meter. Definiert wurde er über den Abstand der Mittelstriche dieser Strichgruppen – aufgrund der Wärmeausdehnung des Materials bei einer Temperatur von 0 °C. Diese Längendefinition besaß eine Genauigkeit von 10 und war damit um drei Größenordnungen genauer als das Urmeter von 1799. Kopien dieses Meterprototyps wurden an die Eichinstitute in vielen Ländern vergeben.

Gleichzeitig mit dem Pariser Urmeter von 1889, sozusagen der Nummer Null, wurden noch dreißig nummerierte Kopien des dritten Urmeters hergestellt. Die Kopien, in Deutschland despektierlich auch „Knüppel“ genannt, wurden an die Mitgliedsstaaten verlost. Deutschland erhielt die Kopie Nr. 18. Da das Königreich Bayern 1870, noch vor der Reichsgründung, als eigenständiges Mitglied beigetreten war, nahm es auch an der Verlosung teil und erhielt die Kopie Nr. 7. Während des Dritten Reiches musste Bayern sein Exemplar an die Physikalisch-Technische Reichsanstalt (PTR) Berlin abgeben. Weil die meisten PTR-Laboratorien 1943 von Berlin vor allem nach Weida in Thüringen verlagert worden waren, blieben auch beide Spezimen zwischen 1949 und 1990 im Besitz der DDR. Da Westdeutschland nun ohne Prototyp dastand, erwarb es 1954 die Kopie Nr. 23 von Belgien, das auch zwei Kopien (für Flandern und Wallonien) erhalten hatte. Zwei der drei Exemplare (die Kopien Nr. 18 und 23) befinden sich heute in der Physikalisch-Technischen Bundesanstalt in Braunschweig. Dort befand sich bis 2001 auch die Kopie Nr. 7, bevor sie an den Freistaat Bayern zurückgegeben wurde und seitdem im Bayerischen Landesamt für Maß und Gewicht in München aufbewahrt wird.

Mit dem Fortschritt der Messtechnik traten die Nachteile des Urmeters immer deutlicher zutage: Ein körperlicher Gegenstand ist nie ganz stabil, sondern verliert an Substanz. Messungen können den Gegenstand beschädigen und können auch nur dort stattfinden, wo er sich befindet. 

Als unveränderliches, universell verfügbares Längennormal schlug Albert A. Michelson zu Beginn des 20. Jahrhunderts die Wellenlänge von Licht vor. In den 1950er Jahren wurde mit der Krypton-86-Lampe eine Lichtquelle von ausreichender Stabilität und Präzision verfügbar. Mit Bezug auf deren Wellenlänge wurde 1960 das Meter neu definiert. Eine nochmalige Änderung gab es 1983, als die Lichtgeschwindigkeit zur Referenzgröße wurde. Die neuen Definitionen wurden so gewählt, dass sie im Rahmen der Messgenauigkeit möglichst gut mit dem Urmeter übereinstimmten. Das Urmeter selbst hat seit 1960 keine Funktion als Normal mehr und ist nur noch von historischem Interesse.



</doc>
<doc id="149644" url="https://de.wikipedia.org/wiki?curid=149644" title="Elektronenspektrometer">
Elektronenspektrometer

Ein Elektronenspektrometer ist ein Spektrometer zur Energie- und Richtungsanalyse von Elektronen. Es besteht in der Regel aus einem Analysator, der Elektronen einer bestimmten Energie (englisch "pass energy") und einer bestimmten Laufrichtung durchlässt, sowie einem Detektor.

Der Analysator besitzt in der Regel einen Eintrittsspalt, durch den die Elektronen eintreten, und einen Austrittsspalt, durch den sie auf den Detektor gelangen. Bei der Verwendung ortsauflösender Detektoren kann oft auf einen Austrittsspalt verzichtet werden. Zur Filterung der Elektronenenergie wird die Ablenkung von Elektronen in einem elektrischen oder magnetischen Feld ausgenutzt. Nur die Elektronen einer bestimmten kinetischen Energie (der sog. Passenergie), welche unter einem bestimmten Winkelbereich auf den Eintrittsspalt treffen, können dann sowohl den Eintritts- als auch den Austrittsspalt dieses Monochromators passieren. Die Passenergie des Monochromators wird durch Verändern der elektrischen Spannung am Kondensator geregelt, sodass Elektronen einer anderen kinetischen Energie den Detektor erreichen. Mit dieser Anordnung wird für verschiedene Passenergien die Elektronenzählrate ermittelt und als Spektrum aufgetragen. Mögliche Geometrien für einen elektrostatischen Analysator sind z. B. Plattenkondensator, Zylinderkondensator und Kugelkondensator, jeweils mit Eintritts- und Austrittsöffnung.

Als Detektor kommt häufig ein Sekundärelektronenvervielfacher zum Einsatz, dieser ist oft als Kanalelektronenvervielfacher (englisch channeltron) ausgeführt. Die Verstärkung reicht hier aus, um einzelne Elektronen zu zählen. In modernen Elektronenspektrometern werden meist ortsauflösende Detektoren verwendet. Diese bestehen aus einer Mikrokanalplatte (englisch micro-channel-plate MCP) und einem Fluoreszenzschirm. Das so entstandene Bild des Spektrums wird dann mit einem CCD aufgenommen und zu einem Spektrum verarbeitet.


</doc>
<doc id="167150" url="https://de.wikipedia.org/wiki?curid=167150" title="Bragg-Gleichung">
Bragg-Gleichung

Die Bragg-Gleichung, auch Bragg-Bedingung genannt, wurde 1912 von William Lawrence Bragg entwickelt. Sie beschreibt, wann es zu konstruktiver Interferenz von Wellen bei Streuung an einem dreidimensionalen Gitter kommt. Sie erklärt die Muster, die bei der Beugung von Röntgen- oder Neutronenstrahlung an kristallinen Festkörpern entstehen, aus der Periodizität von Gitterebenen. Das Analogon zur Bragg-Bedingung im reziproken Raum ist die Laue-Bedingung.

Trifft Röntgenstrahlung auf einen Kristall, so wird dieser von einem Großteil der Strahlung ungehindert durchdrungen. Es wird allerdings auch beobachtet, dass ein kleiner Teil durch den Kristall abgelenkt wird – ein Phänomen, das man als Röntgenbeugung bezeichnet. Montiert man hinter dem Kristall einen geeigneten Detektor, zum Beispiel eine Fotoplatte, um die abgelenkten Strahlungsanteile sichtbar zu machen, entstehen darauf charakteristische Muster.

Ursache für die Beugung ist die Streuung der Röntgenstrahlung an den einzelnen Atomen des Gitters. Dies kann man auch als eine schwache Reflexion der Röntgenstrahlung an den einzelnen Gitterebenen des Kristalls betrachten, wobei die Strahlung nur in solche Richtungen nennenswert reflektiert wird, in denen die einzelnen Reflexionen sich konstruktiv überlagern. Diese Bedingung beschreibt die Bragg-Gleichung:

Die Bragg-Gleichung verknüpft:

Jede Schar paralleler Gitterebenen hat einen charakteristischen Gitterebenenabstand "d" und damit, so die Bragg-Gleichung, auch einen charakteristischen Braggwinkel "formula_3". Für verschiedene Orientierungen, unter denen Strahlung auf den Kristall trifft, erhält man auf dem Detektor hinter dem Kristall fast immer auch verschiedene Bilder, weil sich immer andere Scharen paralleler Gitterebenen (mit anderen Braggwinkeln und mit anderen Orientierungen im Kristall) in Reflexionsstellung zum einfallenden Strahl befinden.

Die Gitterebenen werden üblicherweise mit Laue-Indizes "hkl" gekennzeichnet, so dass sich für den Abstand zum Beispiel im kubischen System ergibt:

mit der Gitterkonstanten formula_6. Die Bragg-Bedingung lautet damit im kubischen System:

Für "n = 1" sind die Laue-Indizes mit den Miller-Indizes identisch. Für höhere Ordnungen "n" steht "hkl" für die Laue-Indizes, die mit der Beugungsordnung "n" multiplizierten Miller-Indizes der Gitterebene.

Tatsächlich handelt es sich um ein Beugungsphänomen. Im elektromagnetischen Feld der einfallenden Strahlung werden die Elektronen der Atome zu erzwungenen Schwingungen angeregt und beginnen selbst Strahlung in Form von kugelförmigen Wellen abzustrahlen.
Da die Wellen der einzelnen Elektronen sich in erster Näherung zu Wellen der zugehörigen Atome aufsummieren, und weiterhin die Abstände im Kristallgitter und die Wellenlänge der Röntgenstrahlung von ähnlicher Größenordnung sind, treten Interferenzerscheinungen auf.

Ist die Bragg-Gleichung bei gegebener Wellenlänge formula_2 für eine Schar von parallelen Gitterebenen erfüllt, das heißt, trifft die Röntgenstrahlung unter dem richtigen Winkel auf den Kristall, kommt es zu konstruktiver Interferenz der bei der Beugung an den Elektronenhüllen entstehenden Kugelwellen. Makroskopisch entsteht der Eindruck einer Reflexion der Strahlung am Kristall.

Die blauen Linien in der nebenstehenden Grafik (Schema zur Beugungsgeometrie) entsprechen Strahlen, die auf parallele Gitterebenen treffen und dabei mit dem Lot den Winkel formula_9 einschließen. Der Komplementärwinkel formula_10 heißt "Braggwinkel" oder "Glanzwinkel". "d" ist der Gitterebenenabstand, die schwarzen Punkte sind Atome auf den Gitterebenen.

Aufgrund der großen Anzahl von Atomen in einem Kristall gibt es für den Fall nicht ausschließlich konstruktiver Interferenz statistisch zu jedem Atom immer ein zweites, das die gebeugte Welle des ersten genau auslöscht, so dass keine Reflexion mehr beobachtet werden kann. Dies ist auch die Situation in nicht-kristallinem Material, unabhängig von der Einstrahlrichtung.


Die Drehkristallanordnung ist eine mögliche Durchführung des Versuchs. Da frühere Röntgenapparate sehr schwer und somit nicht drehbar waren, wurde der Röntgenstrahl auf einen drehbaren Kristall gelenkt. Durch Drehung des Kristalls und des Empfängers konnte der Kristall dann unter verschiedenen Winkeln untersucht werden.
Eine zweite Möglichkeit stellt das Debye-Scherrer-Verfahren dar, bei dem der Kristall pulverisiert wird, so dass jede „Drehrichtung“ statistisch verteilt gleichzeitig vorhanden ist.




</doc>
<doc id="204661" url="https://de.wikipedia.org/wiki?curid=204661" title="Lambdapunkt">
Lambdapunkt

Der Lambdapunkt bezeichnet eine druckabhängige Temperatur, deren Unterschreiten eine Flüssigkeit in den suprafluiden Zustand versetzt.

Der Lambdapunkt hat seinen Namen von dem Graphen, der entsteht, wenn man die spezifische Wärmekapazität als Funktion der Temperatur aufträgt. Die Form des Graphen ähnelt dem griechischen Buchstaben Lambda „λ“, wobei die spezifische Wärmekapazität am Lambdapunkt den Wert "unendlich" annimmt.

Das häufigste Helium-Isotop, He ("im suprafluiden Zustand auch Helium II genannt"), wird bei einem Druck von 5036 Pa unterhalb einer Temperatur von 2,1768 K suprafluid. Darüber hinausgehend konnte bisher lediglich bei He und Li Suprafluidität bei Unterschreitung des Lambdapunktes beobachtet werden.



</doc>
<doc id="252467" url="https://de.wikipedia.org/wiki?curid=252467" title="Akustischer Kurzschluss">
Akustischer Kurzschluss

Akustischer Kurzschluss ist die Verminderung der Schallabstrahlung von schwingenden Flächen (Lautsprechermembran) durch direkten Druckausgleich zwischen gegenphasig schwingenden Bereichen. Bei Verminderung der Schallaufnahme durch eine schwingende Membran (Mikrofon) durch diesen Effekt, spricht man ebenfalls von akustischem Kurzschluss.

Eine erste Art des Kurzschlusses ist die gegenseitige teilweise Auslöschung der von beiden Membranseiten abgestrahlten Schallwellen. Beim Betrieb werden aufgrund der Schwingungen der Lautsprechermembran Schallwellen vor und hinter der Membran erzeugt. Die entstehenden Druckschwankungen sind dabei vor und hinter der Membran fast genau gegenphasig (also verpolt), was deutlich stärker für tiefe Frequenzen zutrifft. 

Anschaulich gesprochen: Die gleiche Luftmenge, die an der Vorderseite weggedrückt wird, saugt die Rückseite ein. Diese Luftmenge muss eigentlich nur „um die Ecke laufen“.

Wenn ein Lautsprecher ohne große Schalltrennwand oder ohne Gehäuse betrieben wird, können die von beiden Membranseiten abgestrahlten Schallwellen sich so überlagern, dass sich Druck und Unterdruck gegenseitig fast auslöschen. Dadurch fällt der Schalldruck stark ab. Dieser akustische Kurzschluss ist umso ausgeprägter, je kleiner die Dipol-Membran im Vergleich zur Wellenlänge ist: also bei tiefen Tönen.
Die einfachste Abhilfe stellt eine "Schallwand" dar, die den Schallwellen einen Umweg aufzwingt, so dass kein direkter Kurzschluss mehr stattfinden kann. Der Durchmesser der Schallwand muss dazu der Größenordnung der tiefsten wiederzugebenden Wellenlänge des Schalls entsprechen.

Eine weitere Art des akustischen Kurzschlusses kann nicht durch eine ausreichend ausgedehnte Schallwand verhindert werden: Es handelt sich um den Kurzschluss zwischen benachbarten Flächenstücken "auf der gleichen Seite" der Membran, die – bei ausreichend hohen Frequenzen – gegenphasig schwingen. Es dauert einige Mikrosekunden, bis sich der Antriebsimpuls der Tauchspule bis zum Rand ausgebreitet hat. Daher kann es vorkommen, dass sich der Außenbereich der Membran noch nach hinten bewegt, während der Zentralbereich bereits wieder nach vorn schiebt. Auf diese Weise bilden sich Partialschwingungen, die bei manchen Frequenzen besonders ausgeprägt sind. In diesem Fall muss die Luft nicht „um die Ecke laufen“, sondern nur einige Zentimeter weiter. Dann wird kaum Schall abgestrahlt und man hat bei dieser Frequenz einen Einbruch des Wirkungsgrades.

Im Prinzip kann der akustische Kurzschluss nicht nur bei Lautsprechern ohne Trennwand auftreten, sondern auch bei Druckgradientenmikrofonen. Hier ist er für die schwache Aufnahme der tiefen Töne verantwortlich. Eine lange Wellenlänge, also ein tiefer Ton, trifft beinahe mit gleichem Druck auf die Vorder- und auf die Rückseite der Mikrofonmembran, die sich deshalb kaum bewegt und deshalb tiefe Töne nur schwach aufzeichnet.




</doc>
<doc id="258761" url="https://de.wikipedia.org/wiki?curid=258761" title="Anregungsenergie">
Anregungsenergie

Unter Anregungsenergie versteht man in der Physik den energetischen Abstand eines angeregten Zustands von seinem Grundzustand in einem physikalischen System mit diskreten oder kontinuierlichen Anregungszuständen. Zum Beispiel in einem Atomelektronensystem, Molekül, Atomkern, Potential-Feld oder Festkörper-Elektronen- oder Phononen-System.

In einem quantenmechanischen "kleinen" Vielteilchensystemen wie z. B. der Elektronenhülle eines Atoms oder den Bausteinen des Atomkerns sind nur wenige bestimmte, diskrete Energieniveaus möglich — was auch eine begrenzte Anzahl möglicher Energiesprünge beim Übergang zwischen zwei Niveaus bedingt. Und auch in den quasi-kontinuierlichen Zuständen "großer" Systeme mit Energiebändern gibt es einen Zustand minimaler Energie, beziehungsweise quasi-separierbare Freiheitsgrade der Anregung mit Zuständen jeweils minimaler Energie.

Der Zustand mit der kleinsten Energie heißt Grundzustand. Alle anderen Zustände besitzen gegenüber diesem eine höhere Energie, nämlich die Anregungsenergie. Meist wird die Energie des Grundzustands willkürlich gleich null gesetzt, so dass die Anregungsenergien die Energien bezogen auf den Grundzustand sind.


</doc>
<doc id="274804" url="https://de.wikipedia.org/wiki?curid=274804" title="Verschiebungsstrom">
Verschiebungsstrom

Der Verschiebungsstrom ist der Teil des elektrischen Stromes, der durch die zeitliche Änderung der elektrischen Flussdichte gegeben ist. Er wurde von James Clerk Maxwell als nötiger Zusatzterm im ampèreschen Gesetz erkannt.

Der elektrische Strom setzt sich aus zwei additiven Komponenten zusammen:


Mathematisch lässt sich der totale Strom formula_3 als Summe aus beiden Komponenten ausdrücken als:

Dadurch wird eine begriffliche Erweiterung des ampèreschen Durchflutungsgesetzes nötig, die den gesamten elektrischen Strom in der Form

ausdrückt. Dabei ist der erste Summand der Leitungsstrom, der von der elektrischen Feldstärke formula_6 ausgelöst wird. Die dabei auftretende Konstante formula_7 ist die elektrische Leitfähigkeit des Mediums (Leiters), in dem der Leitungsstrom fließt. 

Der zweite Summand ist der Verschiebungsstrom mit der zeitlichen Änderungsrate der Feldstärke und der Permittivität formula_8. Die Permittivität ist das Maß der im Medium möglichen Polarisation. Verschiebungsstrom ist wichtig in Materialien mit hoher Permittivität und geringer Leitfähigkeit, also Nichtleitern (Isolatoren). Ein Sonderfall mit nicht vorhandener Leitfähigkeit, aber schwach vorhandener Permittivität formula_9 ist der leere Raum (Vakuum): In ihm fließt (abgesehen von freien Ladungsträgern infolge eventueller hoher Feldstärken) nur Verschiebungsstrom.

Die beiden Stoffkonstanten Leitfähigkeit und Permittivität sind im Allgemeinen Tensoren 2. Stufe und beschreiben auch nichtlineare und nichtisotrope Abhängigkeiten des Gesamtstroms von der Feldstärke. Für die meisten Materialien können diese Konstanten jedoch als Skalare betrachtet werden.

Die Einteilung, ab wann in einem Medium der Leitungsstrom vorherrscht und dieses daher als elektrischer Leiter bezeichnet werden kann, und ab wann der Verschiebungsstrom vorherrscht, ergibt sich folglich aus den Werten der beiden Stoffkonstanten und – weil beim Verschiebungsstromes die zeitliche Ableitung der Feldstärke auftritt – der Kreisfrequenz formula_10 des Feldes. Allgemein gilt:

Typische Leiter wie Kupfer oder typische Isolatoren wie manche Kunststoffe (PVC) weisen von der Frequenz unabhängige Stoffkonstanten auf. Bei Leitern wie Kupfer überwiegt bis zu sehr hohen Frequenzen (im Röntgenbereich, siehe Plasmaoszillation) der Leitungsstrom gegenüber dem Verschiebungsstrom. Hingegen sind bei bestimmten Stoffen wie Ionenleitern (Salzwasser) die Stoffkonstanten stark frequenzabhängig. Dann hängt es von der Frequenz (zeitliche Änderungsrate des elektrischen Feldes) ab, ob der Stoff als Leiter oder Nichtleiter anzusehen ist.

Bei zeitlichen harmonischen (sinusförmigen) Änderungen ist im gleichen Medium der Verschiebungsstrom gegenüber dem Leitungsstrom immer um 90° (π/2) phasenverschoben. Hingegen sind in einem Stromkreis, der durch einen Isolator unterbrochen ist, der im Isolator dominierende Verschiebungsstrom und der im elektrischen Leiter dominierende Leitungsstrom miteinander in Phase, und die beiden Ströme sind betragsmäßig praktisch gleich. Dieser technisch wichtige Fall tritt beim Kondensator im sinusförmigen Wechselstromkreis in Erscheinung: Der Strom in den Zuleitungsdrähten und den Kondensatorplatten (elektrischer Leiter) wird durch den Leitungsstrom getragen, der Strom durch das Dielektrikum (Isolator) zwischen den Kondensatorplatten primär durch den Verschiebungsstrom. Ohne Verschiebungsstrom wäre keine Stromleitung durch den Kondensator möglich – wenngleich diese Stromleitung durch den Verschiebungsstrom wegen der nötigen zeitlichen Änderungsrate beim elektrischen Fluss immer auf Wechselströme (zeitliche Änderung) limitiert ist.

Als Maxwell die bis dahin von anderen Physikern wie Ampère und Faraday zusammengetragenen Erkenntnisse über elektromagnetische Phänomene in den Maxwellschen Gleichungen zu vereinen suchte, wurde ihm klar, dass das Ampèresche Gesetz über die Erzeugung von Magnetfeldern durch Ströme nicht vollständig sein konnte.

Diese Tatsache wird durch ein einfaches Gedankenexperiment klar. Ein Strom "I" fließe durch einen langen Draht, in dem ein Kondensator liegt.
Das Ampèresche Gesetz

besagt nun, dass das Wegintegral des Magnetfelds entlang eines beliebigen Weges um den Draht proportional zu dem Strom ist, der durch eine von diesem Weg aufgespannte Fläche fließt. Auch die differentielle Form

verlangt, dass die Wahl dieser aufgespannten Fläche beliebig ist.
Nun habe der Integrationsweg die einfachste mögliche Form, ein Kreis um die Längsachse des Drahts (in der Grafik mit ∂S bezeichnet). Die natürlichste Wahl der durch diesen Kreis aufgespannten Fläche ist offenbar die Kreisfläche S. Wie erwartet schneidet diese Kreisfläche den Draht, somit ist der Strom durch die Fläche I. Aus der Symmetrie des Drahtes ergibt sich entsprechend für das Magnetfeld des langen Drahtes, dass dessen Feldlinien Kreisbahnen um die Längsachse sind.

Auch wenn man die Fläche beliebig „ausbeult“ oder „aufbläst“, fließt durch sie immer noch der gleiche Strom – es sei denn, man dehnt sie soweit aus, dass sie zwischen den beiden Kondensatorplatten verläuft. Durch diese Fläche S fließt scheinbar kein Strom. Maxwell ging davon aus, dass das Ampèresche Gesetz nicht falsch, sondern nur unvollständig ist.

Durch den Kondensator fließt kein Strom, aber das elektrische Feld und damit der elektrische Fluss ändert sich beim Aufladen des Kondensators (es ist das elektrische Feld "D" ohne Einflüsse durch dielektrische Materie gemeint; in der Grafik mit "E" bezeichnet). Maxwell definierte einen Verschiebungsstrom nun als die Änderung des elektrischen Flusses durch die gegebene Oberfläche. Der Verschiebungsstrom ist daher kein Strom, bei dem Ladung transportiert wird. Vielmehr ist es eine anschauliche Bezeichnung für ebendiese Änderung des elektrischen Flusses, da sie offenbar die gleiche Wirkung hat wie ein richtiger Strom.

Der Verschiebungsstrom, die Änderung des elektrischen Flusses durch eine Oberfläche formula_15, ist definiert durch

wobei der elektrische Fluss definiert ist durch

Der Vorfaktor, bestehend aus den beiden Dielektrizitätskonstanten, eliminiert hierbei dielektrische Effekte, da für die elektrische Flussdichte, die von diesen unberührt bleibt und nur von Ladungen ausgeht, gilt

mit der Dielektrizitätskonstante des Vakuums und der Konstante der entsprechenden Materie.

Analog gilt für das von dia- und paramagnetischen Effekten unberührte magnetische Feld

Außerdem kann bekanntlich der (tatsächliche) Strom "I" durch einen Leiter als Oberflächenintegral einer Stromdichte "j" dargestellt werden:

Mit dieser Vorbereitung erhält man

Dieser Verschiebungsstrom muss nun in das im ersten Abschnitt zitierte Ampère'sche Gesetz eingefügt werden:

womit die integrale Form der vierten Maxwellschen Gleichung erreicht ist.

Für die differentielle Formulierung fehlt nur noch die Definition einer Verschiebungsstromdichte für den Verschiebungsstrom analog zum Betrag der Stromdichte "J" des tatsächlichen Stromes "I":

Man erhält

die differentielle Form der vierten Maxwellschen Gleichung.


</doc>
<doc id="335744" url="https://de.wikipedia.org/wiki?curid=335744" title="Cauchy-Zahl">
Cauchy-Zahl

Die Cauchy-Zahl formula_1 (nach Augustin Louis Cauchy) gehört zu den dimensionslosen Kennzahlen der Physik. Sie gibt das Verhältnis der Trägheitskräfte zu den elastischen Kräften in festen Körpern an:

wobei 

Die Cauchy-Zahl wird bei der Untersuchung elastischer Schwingungsvorgänge benutzt. Weiterhin findet sie Einsatz in der Ähnlichkeitstheorie: zwei Vorgänge, die hauptsächlich unter Trägheits- und elastischen Kräften stattfinden, sind mechanisch ähnlich, wenn ihre Cauchy-Zahlen übereinstimmen.



</doc>
<doc id="407054" url="https://de.wikipedia.org/wiki?curid=407054" title="Stromlinienform">
Stromlinienform

Stromlinienform heißt die Form eines Körpers, die sich durch einen geringen Strömungswiderstand gegenüber dem umströmenden Medium, zumeist Luft oder Wasser, auszeichnet. Ein quantitatives Maß für die Stromlinienförmigkeit ist der Strömungswiderstandskoeffizient formula_1; dieser ist in erster Näherung nur von der Form, nicht der Größe des umströmten Körpers abhängig. Je niedriger sein formula_1-Wert, umso stromlinienförmiger der Körper. Qualitativ ablesbar ist Stromlinienform an einem glatten, weitestgehend wirbelfreien Verlauf der Stromlinien, die in numerischen Simulationen oder Windkanalexperimenten sichtbar gemacht werden können.

Fahr- und Flugzeuge, Schiffe und U-Boote werden meist nach den Regeln der Fluiddynamik entworfen, um möglichst stromlinienförmig zu sein.

Auch die Natur zeigt bei einigen Tieren eine besondere Stromlinienform. So sind speziell Fische, Wale und Pinguine besonders stromlinienförmig. Vögel haben eine besonders gute Anpassung an die Regeln der Aerodynamik.

Ziel ist es sowohl in der Technik wie in der Natur, sich mit möglichst wenig Energieaufwand durch ein Medium zu bewegen.

Der Begriff Stromlinienform wurde Anfang des 20. Jahrhunderts im Zuge der immer höheren Geschwindigkeiten motorisierter Luft- und Landfahrzeuge und der aufkommenden Problematik des Luftwiderstandes geprägt. Nicht zuletzt Luftschiffe, allen voran Zeppeline, wurden zu einem Synonym für die Stromlinienform und des technischen Fortschritts, obwohl der Zeppelin-Konkurrent Schütte-Lanz diese Form bei Luftschiffen eingeführt hatte. Viele wissenschaftliche aerodynamische Grundlagen stammen aus dieser Zeit.

Berühmt sind z. B. die Stromlinienformen älterer Autos, die sich durch besonders geschwungene Linien darstellten. Da sich die Stromlinienform aber meist auf Andeutungen beschränkte und kaum im Windkanal geforscht wurde, war der tatsächliche Widerstandsbeiwert sehr schlecht. Strömungsgünstige Fahrzeuge wie der Tropfenwagen von Edmund Rumpler vom Herbst 1921 mit einem formula_1 von nur 0,28 und der Chrysler Airflow von 1934 scheiterten am Markt. In Europa in den dreißiger Jahren am erfolgreichsten waren der Tatra 77 und seine Nachfolger. Forschungen von Paul Jaray und Wunibald Kamm wurden wenig beachtet und später vom Zweiten Weltkrieg unterbrochen.

Erwähnenswert sind auch die Stromlinienlokomotiven. Beispielhaft dafür ist z. B. die deutsche DR-Baureihe 03.10 oder die DR-Baureihe 05 (Weltrekord 1935 mit 200,4 km/h), die Stromlinienausführungen der US-amerikanischen Hudson-Lokomotiven und die englische Lokomotive "Mallard" (bis heute bestehender Weltrekord für Dampflokomotiven mit 201,2 km/h). Parallel dazu wurden ganze Stromlinienzüge entwickelt, beginnend mit dem dieselelektrischen Fliegenden Hamburger und dem dampfbetriebenen Henschel-Wegmann-Zug. Seit den 1930er Jahren stellten diese Züge den Premium-Service des Eisenbahnverkehrs. Sie bildeten auch die Grundlage für die heutigen Hochgeschwindigkeitszüge.

Bedeutenden Pioniere des auch nur kurz als "Streamlining" bezeichneten Stromliniendesigns waren Raymond Loewy, Norman Bel Geddes, Henry Dreyfuss und Otto Kuhler.



</doc>
<doc id="523942" url="https://de.wikipedia.org/wiki?curid=523942" title="Gibbs-Duhem-Gleichung">
Gibbs-Duhem-Gleichung

Die Gibbs-Duhem-Gleichung (nach Josiah Willard Gibbs und Pierre Duhem) beschreibt in einem thermodynamischen System den Zusammenhang zwischen den Änderungen der chemischen Potentiale der Komponenten.

Hierbei bezeichnet

Oft wird die Gibbs-Duhem-Gleichung bei gleichzeitig isothermer und isobarer Prozessführung verwendet. Dann folgt:

Bei einem solchen Prozess verschwindet also die Summe der Produkte aus der Stoffmenge formula_2 der einzelnen Komponente und der Änderung ihres chemischen Potentials formula_8

Die Gibbs-Duhem-Gleichung ist von großem Interesse für die Thermodynamik, da sie aufzeigt, dass in einem thermodynamischen System nicht alle intensiven Variablen (Variablen wie Temperatur, Druck, chemisches Potential, die "nicht" von der Menge einer Substanz abhängen) unabhängig voneinander veränderlich sind.

Nimmt man z. B. die Temperatur und den Druck als veränderlich an, so können nur noch formula_9 der formula_5 Komponenten voneinander unabhängige chemische Potentiale aufweisen. Hieraus folgt die Gibbs'sche Phasenregel, die die Anzahl der möglichen Freiheitsgrade für dieses System angibt.

Die Gibbs-Energie ist eine positiv homogene Funktion vom Grade formula_11 in den formula_12 Stoffmengen formula_13; das heißt für jedes formula_14 und formula_15 gilt:
formula_16. Daher gilt für die Gibbs-Energie die Eulersche Homogenitätsrelation:

Somit gilt für das totale Differential

Andrerseits gilt wegen der Definition von formula_19

Aus dem Vergleich der beiden Ausdrücke folgt die Gibbs-Duhem-Gleichung:



</doc>
<doc id="801336" url="https://de.wikipedia.org/wiki?curid=801336" title="International Linear Collider">
International Linear Collider

Der International Linear Collider (ILC) ist ein geplanter Linearbeschleuniger für Elektronen und Positronen mit einer Schwerpunktsenergie von 500 GeV und einer Gesamtlänge von 34 km. Als möglicher Standort wird die Präfektur Iwate im Norden Japans diskutiert.
Der ILC wäre ein Nachfolgeprojekt für den Large Electron-Positron Collider (LEP), an dem die bisher höchste Energie für Elektron-Positron-Kollisionen von 209 GeV erreicht wurde.

Dezember 2018 sprach sich ein Komitee von japanischen Naturwissenschaftlern, die das Projekt begutachtete, gegen den Bau aus. Die Kosten von geschätzt 32 Milliarden Euro wären zu hoch im Vergleich zum erwarteten Erkenntnisgewinn. Der Bau des ILC steht damit in Japan vor dem Aus. Eine Rolle spielte dabei auch dass am LHC bis auf die Entdeckung des Higgs-Teilchens keine großen Neuentdeckungen gemacht wurden.

Der ILC ist ein Projektvorschlag für einen Elektron-Positron-Beschleuniger, dessen Schwerpunktsenergie mit mindestens 500 GeV weit über der bisher erreichten Energie für Elektron-Positron-Kollisionen von 209 GeV liegt. Damit wäre es erstmals möglich, die Eigenschaften des Higgs-Bosons und des Top-Quarks an einem Elektron-Positron-Beschleuniger zu untersuchen, was andere und genauere Messungen erlaubt als an einem Proton-Beschleuniger wie dem Tevatron oder dem Large Hadron Collider (LHC), an denen diese Teilchen zuerst nachgewiesen wurden. Ein weiterer Forschungsgegenstand wird die Suche nach neuen, unbekannten Elementarteilchen sein.

Im Unterschied zu dem bisher höchstenergetischen Elektron-Positron-Beschleuniger LEP ist der ILC kein Kreisbeschleuniger, sondern ein Linearbeschleuniger. Damit wird die Begrenzung der Strahlenergie bei Kreisbeschleunigern überwunden, die aus dem zunehmenden Energieverlust durch Synchrotronstrahlung resultiert.

Um Energieverluste zu minimieren ist geplant, supraleitende Beschleunigermodule aus Niob einzusetzen, die bei einer Temperatur von 2,0 K (−271 °C) betrieben und mit flüssigem Helium gekühlt werden.

Der Beschleuniger besteht aus zwei, jeweils ca. 17 km langen Armen. Im Hauptbeschleuniger des einen Arms werden Elektronen, im anderen Positronen auf eine Energie von 250 GeV beschleunigt. Diese Strahlen werden in einem Strahlfokussierungssystem gebündelt und im Wechselwirkungspunkt zur Kollision gebracht. Es ist geplant, zwei Detektoren zu bauen, die im Wechsel in die Wechselwirkungszone geschoben werden können, um Daten zu nehmen.

Anders als bei Ringbeschleunigern wie dem LHC lassen sich beschleunigte Teilchen nur einmal verwenden, es müssen also ständig neue Teilchen beschleunigt werden. Dazu werden zunächst alle 200 ms insgesamt 1312 Gruppen von Elektronen („Bunches“) aus einer Photokathode freigesetzt. Diese werden auf 5 GeV beschleunigt und gelangen in einen Speicherring („Damping Ring“), in dem sie innerhalb von 200 ms komprimiert werden. Dies ist nötig, um die geplanten hohen Kollisionsraten zu erreichen. Anschließend werden die Elektronen an ein Ende des langen Beschleunigertunnels geleitet und von dort aus in Richtung Kollisionspunkt beschleunigt.

Nach der Beschleunigungsstrecke werden die Elektronen durch einen Undulator geleitet und setzen dabei Gammastrahlung frei. Diese wird auf eine Titanplatte geleitet, wo über Paarerzeugung Positronen und Elektronen erzeugt werden. Die Positronen werden ebenfalls in einen Speicherring geleitet und innerhalb von 200 ms verdichtet. Danach werden sie zum anderen Ende des Beschleunigertunnels geführt und von dort aus beschleunigt. Sie erreichen den Kollisionspunkt 200 ms nach den Elektronen, mit denen sie erzeugt wurden – sie treffen also auf die Elektronen des nächsten Zyklus.

Zwischen den Beschleunigungsstrecken und dem Kollisionspunkt werden für Elektronen und Positronen jeweils ein 2,2 km langes „beam delivery system“ gebaut, das die Teilchenpakete auf eine Länge von 0,3 mm, eine Breite von 700 nm und eine Höhe von 6 nm komprimiert.

Die Beschleunigertunnel, der Hauptteil des ILCs, soll mit einer Länge von bis etwa 31 km mehr als zehnmal so lang wie die des Linearbeschleunigers SLAC in Kalifornien sein. Mit der Fertigstellung ist nicht vor 2019 zu rechnen. Die supraleitende Technologie für den Beschleuniger wird bereits an dem Freie-Elektronen-Laser FLASH am DESY in Hamburg erprobt und wird auch in dem europäischen Röntgenlaserprojekt XFEL Verwendung finden.

Geplant ist, den ILC mit zwei Detektoren auszurüsten. Da die Teilchenstrahlen nur an einem Punkt kollidieren, werden die Detektoren seitlich verschiebbar sein und können sich somit mit den Messungen abwechseln.

Der ILC wird Elektronen und Positronen mit Schwerpunktsenergien zwischen 200 und 500 GeV kollidieren, ein Ausbau auf 1000 GeV (1 TeV) ist möglich.

Die zentralen Forschungsziele sind

Das Higgs-Boson wurde 2012 am Beschleuniger LHC des CERN in Genf entdeckt. Das Higgs-Boson nimmt im Standardmodell der Elementarteilchenphysik eine Sonderstellung ein. Es ist das einzige Teilchen ohne Spin (Eigendrehimpuls). Die Existenz des Higgs-Bosons ist eine Konsequenz des Higgs-Mechanismus, der erklärt, weshalb elementare Teilchen wie Elektronen, Quarks sowie die Träger der schwachen Wechselwirkung Masse besitzen. Das erklärt insbesondere, warum die schwache Wechselwirkung, die z. B. für den radioaktiven Beta-Zerfall verantwortlich ist, so schwach und kurzreichweitig ist: Die Kopplung an das Higgs-Feld führt dazu, dass die W- und Z-Bosonen, die die schwache Wechselwirkung vermitteln, eine Masse von 80 bzw. 91 GeV haben und deshalb nur über extrem kurze Distanzen von ca. 10 m (1/100 Protonradius) ausgetauscht werden können.

Im Standardmodell der Teilchenphysik existiert das sogenannte Hierarchieproblem: Quantenkorrekturen führen dazu, dass die Masse des Higgs-Bosons stark sensitiv auf die Energieskala ist, an der das Standardmodell seine Gültigkeit verliert. Viele Lösungsansätze für das Hierarchieproblem basieren auf der Annahme neuer Elementarteilchen (z. B. supersymmetrische Partner), oder neuer Wechselwirkungen. In solchen Fällen erwartet man etwas andere Eigenschaften des Higgs-Bosons, zum Beispiel andere Verzweigungsverhältnisse in verschiedene Zerfallskanäle als sie das Standardmodell vorhersagt. Deshalb ist eine genaue Messung dieser Verzweigungsverhältnisse von fundamentaler Bedeutung.

Das Top-Quark ist mit einer Masse von 173 GeV (was in etwa der Masse eines Gold-Atoms entspricht) das bei weitem schwerste Quark, und das schwerste bekannte Elementarteilchen überhaupt. Aufgrund der großen Masse koppelt das Top-Quark stärker als alle anderen Teilchen an das Higgs-Boson, und es trägt besonders stark zu Quantenkorrekturen von Eigenschaften anderer Elementarteilchen bei, zum Beispiel zur Masse des W-Bosons.

Top-Quarks können an einem Elektron-Positron-Beschleuniger paarweise produziert werden (als Paar eines Top-Quarks und eines Top-Antiquarks), wenn die Schwerpunktsenergie oberhalb der sogenannten Top-Schwelle bei der doppelten Top-Masse von 346 GeV liegt.
Bei früheren Elektron-Positron-Beschleunigern reichte die Energie zur Top-Paarerzeugung nicht aus, erst der ILC würde diese Messung ermöglichen.

Eine Messung der Erzeugungsrate in Abhängigkeit von der Schwerpunktsenergie zeigt im Bereich um die Top-Schwelle einen steilen Anstieg, dessen Position und Höhe sehr genau durch die Theorie vorhergesagt werden und dadurch eine hochgenaue Messung der Masse und der Zerfallsbreite des Top-Quarks ermöglichen.

Messungen der Verteilung der Flugrichtungen der Top-Quarks (der Erzeugungswinkel) geben Aufschluss über die verschiedenen Kopplungen von links- und rechtshändigen Top-Quarks an Z-Bosonen. Auch diese Messung ist ein empfindlicher Test der Vorhersagen des Standardmodells, und Abweichungen würden Rückschlüsse auf Physik jenseits des Standardmodells geben.

Der ILC wäre mit einer Schwerpunktsenergie von 500 GeV in der Lage, Teilchen-Antiteilchen-Paare neuer, unbekannter Teilchen mit einer Masse bis zu 250 GeV (das entspricht etwa der Masse eines Uranatoms) zu produzieren, nach einem Ausbau auf 1 TeV Schwerpunktsenergie würde sich dieser Bereich verdoppeln. Daher wird die Suche nach neuen Teilchen ein Schwerpunkt der Forschung am ILC sein, wie an jedem Teilchenbeschleuniger, der eine höhere Energie erreicht als frühere Anlagen.

Um ein neues Teilchen entdecken zu können, muss dieses hinreichend oft erzeugt werden, und Ereignisse mit dem neuen Teilchen müssen sich hinreichend sicher von anderen Ereignissen unterscheiden (qualitativ oder quantitativ). Daher ist die verfügbare Schwerpunktsenergie nur ein Parameter, der die Aussicht beeinflusst, neue Teilchen zu finden. Andere Parameter sind der Typ der Strahlteilchen (Elektronen und Positronen oder Quarks bzw. Gluonen aus Protonen) und die Rate, mit der andere Ereignisse (sogenannter Untergrund) erzeugt werden. Obwohl der LHC-Beschleuniger am CERN schon jetzt Teilchen mit Massen oberhalb von 250 GeV oder 500 GeV erzeugen kann, sagen einige Theorien Teilchen mit geringeren Massen voraus, die voraussichtlich erst am ILC aufgrund der geringeren Untergrundrate entdeckt oder dort genauer untersucht werden könnten.

In vielen Modellen sind Daten von einem Elektron-Positron-Beschleuniger unbedingt notwendig, um die Existenz neuer Teilchen in bestimmten Massenbereichen sicher nachweisen oder ausschließen zu können. Dies gilt auch für viele Modelle im Bereich der Supersymmetrie, in denen je nach den Werten einiger Parameter auch vergleichsweise leichte Teilchen unter Umständen in Proton-Proton-Kollisionen zu selten erzeugt werden oder zu unauffällige Signale erzeugen, um sicher nachgewiesen zu können. Ein Elektron-Positron-Beschleuniger mit möglichst großer Schwerpunktsenergie wie der ILC wäre damit komplementär zum LHC.

Der Large Hadron Collider (LHC) ist seit 2008 im Betrieb und erreicht bei der Kollision von Protonen mit Protonen eine Schwerpunktsenergie von 13 TeV, womit potentiell auch Teilchen erzeugt werden können, deren Masse zu groß ist, um sie beim ILC direkt erzeugen zu können. Verglichen mit einem Proton-Proton-Beschleuniger hat ein Elektron-Positron-Beschleuniger mehrere Eigenschaften, die seinen Einsatz trotz der im Regelfall geringeren Schwerpunktsenergie attraktiv machen:

Insgesamt sind Elektron-Positron-Beschleuniger wie der ILC und Proton-Proton-Beschleuniger wie der LHC komplementäre Forschungsgeräte; Elektron-Positron-Beschleuniger haben Vorteile in der Genauigkeit und bei der Untersuchung seltener Ereignisse, insbesondere wenn diese durch die elektroschwache Wechselwirkung vermittelt werden, während Proton-(Anti)Proton-Beschleuniger wie der LHC höhere Energien erreichen und insbesondere bei der Untersuchung stark wechselwirkender Teilchen wie schweren Quarks (oder deren hypothetische Superpartner, die Squarks) Vorteile bieten.



</doc>
<doc id="812549" url="https://de.wikipedia.org/wiki?curid=812549" title="Schallreflexion">
Schallreflexion

Die Schallreflexion ist mit der Reflexion in der Optik vergleichbar, wenn die Abmessungen des Reflektors mindestens die fünffache Wellenlänge haben. Der einfachste Fall ist eine Reflexion an einer ebenen Fläche. Hierfür gilt die Regel Einfallswinkel gleich Reflexionswinkel auch für gekrümmte Flächen. Als Reflexionsfläche legt man die Tangente durch den Reflexionspunkt. An der Reflexionsfläche kann sich die Schallquelle spiegeln und auf der anderen Seite der Fläche abbilden. Diese neue Schallquelle wird als Spiegelschallquelle bezeichnet.
Bei beidohrigem (binauralem) Hören tritt bei einer Verzögerung von 0 bis 1 ms zwischen Original- und Spiegelschallquelle Summenlokalisation auf. Zwischen 1 und 50 ms (Sprache) bzw. 80 ms (Musik) wirkt der Präzedenzeffekt. Darüber wird das zweite Schallereignis als Echo wahrgenommen. 1 ms entspricht 34,3 cm Wegdifferenz (bei 20 °C). Bei Aufnahme mit einem Mikrofon (zum Teil auch bei einohrigem (monauralem) Hören) ist eine Klangverfärbung wahrnehmbar, die durch eine kammfilterartige Überlagerung der zwei Schallsignale hervorgerufen wird.
Am Immissionsort treffen neben der direkten Schallwelle reflektierte Wellen auf. Auch wenn keine reflektierenden Objekte – wie Mauern oder Gebäude – vorhanden sind, tritt im Regelfall immer eine Schallreflexion als Schalldruck am Boden ein. Die direkte und die reflektierte Welle überlagern sich am Immissionsort, und dieses führt zur konstruktiven bzw. destruktiven Interferenz, was zu Pegelerhöhungen bzw. zu Pegelabnahmen führt.
Folgende Wörter gehören zur gestörten Schallausbreitung: Schallreflexion, Schallabsorption, Schalltransmission und Schalldissipation.
Zusammenhänge: Der Schallreflexionsgrad "ρ" ist ein Maß für die reflektierte Schallintensität. Der Schallabsorptionsgrad "α" ist ein Maß für die absorbierte Schallintensität. Der Schalltransmissionsgrad "τ" ist ein Maß für die durchgelassenen Schallintensität der Schalldissipationsgrad "δ" ist ein Maß für die „verlorengegangene“ Schallintensität.
Zusammenhänge:
Aus der letzten Gleichung ist zu erkennen, dass sich der Anteil der absorbierten Schallintensität, also die Schallabsorption, aus der Summe der Anteile von durchgelassener (transmittierter) und „verlorengegangener“ (dissipierter) Schallintensität aus Schalltransmission und Schalldissipation zusammensetzt. Die erste Gleichung zeigt, dass die Summe von reflektierter und absorbierter Schallintensität, das sind Schallreflexion und Schallabsorption, also der gesamten Schallintensität entspricht.



</doc>
<doc id="849635" url="https://de.wikipedia.org/wiki?curid=849635" title="Schwingungsspektroskopie">
Schwingungsspektroskopie

Die Schwingungsspektroskopie ist eine Gruppe von analytischen Messmethoden, die auf der Anregung der Normalschwingungen von Molekülen basiert. Die Schwingungsspektroskopie kann somit als ein Teilbereich der Molekülspektroskopie betrachtet werden. Typische Methoden sind die Infrarotspektroskopie (IR) und die Ramanspektroskopie sowie die HREEL-Spektroskopie.

Moleküle bestehen vereinfacht gesehen aus Atomen gleicher oder unterschiedlicher Masse, die durch „elastische“ Bindungen miteinander verbunden sind. Ähnlich wie ein vergleichbares Gebilde aus Metallkugeln, die durch Federn verbunden sind, ist jedes Molekül zu Schwingungen fähig, bei denen die Atome sich gegeneinander periodisch verschieben (vgl. Normalschwingung). Der Frequenzbereich dieser Schwingungen erstreckt sich vom ferninfraroten (meist anorganische Verbindungen) über den mittelinfraroten (in der Regel für organische Verbindungen) bis in nahinfraroten Bereich (Kombinations- und Oberschwingungen der von Schwingungen aus den anderen beiden Bereichen, vgl. Nahinfrarotspektroskopie).

Durch Masse der beteiligten Atome und der Stärke, Länge und Winkel der Bindung sind die Schwingungen und die zugehörige Schwingungsfrequenzen in einem Molekül charakteristisch für das jeweilige Molekül bzw. für eine Molekülgruppe (Gruppenfrequenz) wie einer Hydroxygruppe. Daher ermöglicht die Untersuchung von Absorptions- und Emissionsspektren die Bestimmung von Substanzen in einer Probe sowie die Identifikation von Reaktionsvorgängen und die Strukturaufklären (auch wenn es hier weiter analytische Methoden gibt, die diese Aufgaben besser erfüllen). Die Bestimmung einer Substanz erfolgt in der Regel durch den Vergleich eines Spektrum mit dem Spektrum einer Referenzprobe, kann aber durch eine Vergleich mit sich selbst (beispielsweise einem Differenzspektrum vor einer Reaktion) oder mit einem berechneten Spektrum erfolgen.

Die Schwingungsspektroskopie findet breite Anwendung in Forschung und Industrie. Die jeweiligen Untermethoden haben jeweils für sich spezifische Einschränkungen, so dass sie teilweise nur auf sehr spezielle Probleme angewandt werden können. Industrielle Einsatzbereiche finden sich vor allem im Bereich der Qualitäts- und Prozesskontrolle.

Eine der beiden meisteingesetzten Methoden der Schwingungsspektroskopie ist die Infrarotspektroskopie. Sie basiert auf der Wechselwirkung der Molekülschwingungen mit Infrarotstrahlung, beispielsweise Absorption und Emission. Das ist immer dann der Fall, wenn das Molekül entweder ein veränderbares oder ein induzierbares Dipolmoment aufweist (IR-aktiv). Typische Einsatzbereiche sind sowohl die qualitative, als auch die quantitative Analyse von meist organischen Verbindungen, beispielsweise für die Identifikation von Festkörpern und Flüssigkeiten oder die Qualitätskontrolle von Ausgangsstoffen. Für komplexere Aufgaben, wie die Analyse von Proteinkonformationen, ist die Infrarotspektroskopie weniger gut geeignet, da die Kopplungen einzelner Molekülgruppen mit benachbarten Molekülgruppen schnell zu sehr komplexen Spektren führt. Die Messung erfolgt heutzutage meist mittels Fourier-Transform-Spektrometern, die gegenüber dispersiven Spektrometern schneller, empfindlicher und mit höherer Genauigkeit messen können. Die kurze Messzeit ermöglicht zudem die die laterale Rasterung einer Probe um Materialverteilungen mittels FTIR-Mikroskopie zu analysieren.

Bei Schwingungen symmetrisch zum Symmetriezentrum tritt keine Dipolmomentänderung auf, und daher mit der infrarotspektroskopie nur schwer bis nicht nachweisbar. Sie werden auch als „IR-inaktiv“ bezeichnet. Solche „verbotenen“ Schwingungen sind allerdings oft ramanaktiv, das heißt mittels Ramanspektroskopie nachweisbar.
Anders als bei der Infrarotspektroskopie wird bei der Ramanspektroskopie nicht Infrarotstrahlung, sondern sichtbares Laserlicht genutzt. Das Licht wird auf eine Probe geleitet und das von der Probe indirekt gestreute Licht (Raman-Effekt) detektiert. Durch die indirekte Streuung werden neben der eingestrahlten Frequenz noch weitere Frequenzen beobachtet, deren Frequenzunterschiede zum eingestrahlten Licht den für das Material charakteristischen Energien von Rotations- und Schwingungs-Prozessen entsprechen. Die erhaltenen Spektren ermöglichen Rückschlüsse auf die untersuchte Substanz. Typische Probenmaterialien sind anorganische Substanzen, da sie sich meist leichter als organische Materialien analysieren lassen. Ein Vorteil gegenüber der Infrarotspektroskopie, der vor allem im Bereich der Prozesskontrolle zum Vorschein kommt, ist die Möglichkeit Lichtleitfasern einsetzen zu können. Auch bei der Ramanspektroskopie kommen zunehmend Fourier-Transform-Spektrometer zum Einsatz.

Weitere Methoden, die aufgrund ihrer Komplexität oder einem eng begrenzten Anwendungsbereichs meist nur in der Forschung eingesetzt werden sind beispielsweise die HREEL- und Summenfrequenzspektroskopie.



</doc>
<doc id="973238" url="https://de.wikipedia.org/wiki?curid=973238" title="Spannungsoptik">
Spannungsoptik

Als Spannungsoptik wird ein Teilgebiet der Optik beziehungsweise der Konstruktionslehre bezeichnet, in dem durch die Verwendung von polarisiertem Licht die Spannungsverteilung in lichtdurchlässigen Körpern untersucht wird. An transparenten, zweidimensionalen Werkstückmodellen werden bei mechanischer Belastung Stellen besonders hoher Beanspruchung sichtbar. Grundlage bildet die Eigenschaft vieler optisch isotroper Materialien, bei mechanischen Spannungen doppelbrechend zu werden. Dadurch wird die Polarisationsebene einfallenden Lichts gedreht beziehungsweise es entsteht elliptisch oder zirkular polarisiertes Licht. Das kann mit einem Polarimeter sichtbar gemacht werden.

Durch Verwendung von monochromatischem Licht entsteht ein System aus dunklen und hellen Streifen, deren Anordnung zuverlässige Rückschlüsse auf Verteilung und Größe der mechanischen Spannung an allen Stellen des Körpers erlaubt.

Im Bild rechts entstehen an einem durch Kräfte belasteten Prüfkörper zwei Arten von dunklen Streifen: die "Isochromaten" sind Linien mit konstanter Hauptspannungs-Differenz, an den "Isoklinen" fällt die Richtung einer Hauptspannung mit der Polarisationsrichtung des einfallenden Lichts zusammen, sie repräsentieren somit die Spannungstrajektorien des Körpers unter der gegebenen Belastung.

Zur Unterscheidung zwischen Isochromaten und Isoklinen kann die belastete Probe (oder die Polarisationsrichtung des Lichts) gedreht werden – im spannungsoptischen Bild verändern sich dadurch die Isoklinen, nicht aber die Isochromaten. Eine andere Möglichkeit ist die Verwendung von zirkular-polarisiertem Licht – in diesem Fall sind keine Isoklinen sichtbar (untere zwei Bilder).

Wird weißes Licht verwendet, entstehen für jede Farbe unterschiedliche Hell-Dunkel-Muster – dies ergibt die Farbmuster, wie sie links im Bild gezeigt sind. Einzig bei der 0ten Ordnung der Streifen fallen alle Farben zusammen. Dies ergibt schwarze Streifen an den unbelasteten Stellen in der Probe (untere Ecke des Winkels).

Bei der quantitativen Auswertung geht man von einem ebenen Spannungszustand aus. Die beiden Hauptspannungen in der Probenebene beeinflussen den Brechungsindex des Materials, die dritte Hauptspannung in Dickenrichtung hat keine Wirkung.

Dieses Verfahren wird noch vereinzelt in der Werkstoffprüfung angewandt, indem durchsichtige Kunstharzmodelle (mit optischer Aktivität) auf Belastungen, die in der praktischen Anwendung auftreten können, untersucht werden. Die Spannungsverteilung im Modell stimmt mit derjenigen im realen Bauteil überein, auch wenn die Deformation sich unterscheidet.

Durch die verbesserten Rechenverfahren mit der Finite-Elemente-Methode werden diese Untersuchungen heute vorwiegend am Computer durchgeführt.


</doc>
<doc id="1079328" url="https://de.wikipedia.org/wiki?curid=1079328" title="Kautsky-Effekt">
Kautsky-Effekt

Der Kautsky-Effekt ist ein nach Hans Kautsky benannter Effekt, der im Zusammenhang mit der Chlorophyllfluoreszenz auftritt.

Ein intaktes Pflanzenblatt gibt – aus dem Dunklen schnell ans Licht geholt – über einen kurzen Zeitraum von rund drei Minuten ein hohes Maß an Fluoreszenz ab. Während dieser Zeitspanne nimmt die Fluoreszenz allerdings permanent ab, bis sie einen gewissen Wert erreicht hat.

Grund für diese Beobachtung ist, dass das komplizierte Photosynthesesystem Zeit benötigt, um aktiv zu werden. Während der drei Minuten müssen Enzyme und Stoffwechselwege in Gang gesetzt werden, so dass die durch den plötzlichen Lichteinfall gelieferte Energie an der Elektronentransportkette nicht sofort abgenommen bzw. weitergeleitet werden kann, wobei es zu einem Stau kommt. Das Chlorophyll strahlt daher die Überschussenergie als Fluoreszenz ab.


</doc>
<doc id="1123672" url="https://de.wikipedia.org/wiki?curid=1123672" title="Wechselwirkungsfreie Quantenmessung">
Wechselwirkungsfreie Quantenmessung

In der makroskopischen und „traditionellen“ mikroskopischen Welt verursacht jede Messung eine Störung des beobachteten Zustands. Jedoch erlauben Quanteneffekte in der mikroskopischen Welt der Quanten, Objekte erkennen zu können, ohne diese auch nur einem einzigen Lichtquant aussetzen zu müssen. Dadurch wird das zu messende Objekt nicht verändert. Dieser Vorgang wird als wechselwirkungsfreie Quantenmessung bezeichnet. 

Sie wurde zuerst 1993 in einem Gedankenexperiment von Avshalom Elitzur und Lev Vaidman vorgeschlagen und experimentell 1994 von einer Gruppe um Anton Zeilinger erstmals demonstriert.
Dennis Gábor, der Entdecker der Holografie, hat 1962 die Behauptung aufgestellt, dass sich ein Gegenstand nur beobachten lässt, wenn dieser Gegenstand von mindestens einem Photon, d. h. einem Lichtquant, getroffen wird. 

In der klassischen Vorstellung von Photonen als Teilchen ist dies auch durchaus richtig und dient auch heute noch als das gängige Erklärungsmodell. In der Quantenphysik lässt sich dieses Problem jedoch teilweise umgehen.

Ein bekanntes Gedankenexperiment findet sich in der griechischen Mythologie. Hier stand Perseus vor der Aufgabe Medusa zu töten. Das Problem dabei war, dass jeder, der Medusa ansah, zu Stein erstarrte. Mit geschlossenen Augen war es einem Angreifer kaum möglich Medusa zu lokalisieren. Perseus löste dieses Problem, indem er Medusa seinen glänzenden Schild vorhielt, sodass Medusa sich selbst sah und erstarrte. 

Als weiteres Beispiel sei das Hütchenspiel erwähnt. Hierbei ist ein kleiner Gegenstand (z. B. eine Murmel) unter einem von mehreren Hütchen versteckt und kann von einem Beobachter nicht erkannt werden. Im Gedankenexperiment soll dieser Gegenstand zu Staub zerfallen, sobald man das Hütchen abnimmt und der Gegenstand ins Licht kommt.

Die wechselwirkungsfreie Quantenmessung kann diese Probleme lösen. Mit der wechselwirkungsfreien Quantenmessung könnte man beim Hütchenspiel immer gewinnen. Perseus wäre es möglich gewesen, Medusa zu lokalisieren ohne sie zu betrachten.

Das Prinzip der wechselwirkungsfreien Quantenmessung nutzt die Welleneigenschaft des Lichts aus. Wenn sich zwei solcher Wellen überschneiden, entsteht eine Interferenz, wie dies aus dem Doppelspaltexperiment ersichtlich ist. Im Teilchenmodell ist ein heller Streifen auf dem Schirm hinter dem Spalt eine Stelle, an der ein Photon mit hoher Wahrscheinlichkeit auftritt; ein dunkler, wenig belichteter Streifen auf dem Schirm hingegen ist eine Stelle, an der ein Photon mit niedriger Wahrscheinlichkeit auftritt.

Nach den Prinzipien der Quantenmechanik tritt eine Interferenz bei einem einzelnen Lichtstrahler wie im Doppelspaltexperiment nur auf, wenn es mehr als einen Weg gibt, den ein Photon nehmen kann, um an einen bestimmten Ort zu gelangen.

Beim Doppelspaltexperiment können die Lichtquanten entweder durch den einen oder den anderen Spalt fliegen. Wenn man feststellen kann, welchen Weg ein Lichtquant nimmt, so entsteht kein Streifenmuster, sondern abwechselnd eine Beleuchtung von der einen und dann der anderen Seite.

Die Physiker A. C. Elitzur und L. Vaidman von der Universität Tel Aviv in Israel haben sich in ihrem Gedankenexperiment eine Bombe vorgestellt, die beim Auftreffen eines Quants explodiert. Es gelang ihnen, eine Methode zu entwickeln, die in der Hälfte aller Messungen wechselwirkungsfrei ist. Die Versuchsanordnung wird auch als Knallertest oder Bombentest bezeichnet.

Bei dem von Vaidman und Elitzur erdachten Verfahren durchläuft das Photon ein Mach-Zehnder-Interferometer. Dabei wird der Photonenstrahl eines Lasers mit Hilfe eines halbdurchlässigen Spiegels in zwei Strahlen aufgeteilt. Über zwei Umlenkspiegel treffen die beiden Strahlen anschließend wieder in einem weiteren halbdurchlässigen Spiegel zusammen und können damit interferieren. Das Mach-Zehnder-Interferometer ähnelt in dieser Hinsicht also dem Doppelspaltexperiment.

Die obere Abbildung zeigt den Strahlenverlauf ohne Objekt im Strahlengang. Der im zweiten Strahlteiler reflektierte Teil des unteren Strahls vereint sich mit dem von links passierenden Teil des oberen Strahls und trifft auf den rechten Detektor. Der im zweiten Strahlteiler passierende Teil des unteren Strahls vereint sich aber auch mit dem nach oben reflektierten Teil des oberen Strahls und trifft auf den oberen Detektor. Bei geeigneter Dimensionierung des Interferometers kommt es zur Interferenz der zusammentreffenden Strahlen. Dabei verstärken sich die Strahlen in Richtung des rechten Detektors etwa bis zur Intensität der Quelle, in Richtung des oberen Detektors hingegen löschen sie sich aus. Hervorgerufen wird dies durch eine unterschiedliche Anzahl von reflexionsbedingten Phasenverschiebungen der Strahlengänge. Die Reflexionen an den Halbspiegeln verschieben die Phase des Lichts nur dann um 180°, wenn das Licht direkt auf die Grenzfläche zwischen Vakuum und Silberbeschichtung fällt. Trifft das Licht erst nach Durchqueren des Glasträgers auf die Silberbeschichtung, so findet keine Phasenverschiebung statt. Bei entsprechender Ausrichtung der Halbspiegel trifft im oberen Detektor um 360° phasenverschobenes Licht auf um 180° phasenverschobenes Licht; die Phasenverschiebung der zwei Strahlen zueinander beträgt demnach 180° und sie interferieren destruktiv.

Klassisch betrachtet tritt hier der Wellencharakter des Lichts in den Vordergrund. Quantenmechanisch gesehen ist es objektiv unbestimmt, welchen Weg ein Photon nimmt, solange dies nicht durch eine Messung festgestellt wird.

In der unteren Abbildung wird ein lichtundurchlässiges Objekt in den oberen Lichtstrahl eingefügt. Dadurch wird verhindert, dass der obere Lichtstrahl beim zweiten halbdurchlässigen Spiegel eintrifft. Somit entsteht keine Wechselwirkung zwischen den beiden Lichtstrahlen. Somit erreicht nur der untere Lichtstrahl den zweiten Strahlteiler und wird dort zu gleichen Teilen aufgeteilt. Damit sprechen beide Detektoren mit gleicher Intensität an.

Alternativ kann das Photon auch als Teilchen betrachtet werden. Im ersten Strahlteiler nimmt das Photon dann mit einer Wahrscheinlichkeit von 50 % entweder den unteren Weg oder den oberen Weg. Wenn das Photon den oberen Weg nimmt, trifft es auf das Messobjekt und wird absorbiert. Das Photon erreicht dann keinen der beiden Detektoren. Nimmt das Photon den unteren Weg, kann es wieder auf beide Detektoren treffen, da das Photon ab dem oberen halbdurchlässigen Spiegel beide Wege nehmen kann.

Das Anschlagen des rechten Detektors entspricht nun dem Ergebnis ohne Objekt im oberen Strahlengang und lässt somit keinen Rückschluss auf das Vorhandensein des Objektes zu. Schlägt hingegen der obere Detektor an, hat das Photon zwar den unteren Weg ohne Wechselwirkung gewählt, konnte jedoch den zweiten Halbspiegel geradeaus durchqueren. Diese Möglichkeit konnte vorher durch die Interferenz der Strahlengänge nicht realisiert werden.

Das einzelne Photon ist also nicht mit dem Objekt in Wechselwirkung getreten. Es ist nicht einmal in dessen Nähe gekommen. Dennoch weist das Photon durch das geänderte Verhalten am zweiten Strahlteiler die Existenz des Objekts nach.

Durch Anpassen der Reflektivitäten der Halbspiegel kann die Trefferquote von 25 % überschritten werden.


</doc>
<doc id="1155685" url="https://de.wikipedia.org/wiki?curid=1155685" title="Karl-Otto Kiepenheuer">
Karl-Otto Kiepenheuer

Karl-Otto Kiepenheuer (* 10. November 1910 in Weimar; † 23. Mai 1975 in Ensenada, Mexiko), war ein deutscher Astronom und Astrophysiker.

Karl-Otto Kiepenheuer wurde als Sohn des Verlegers Gustav Kiepenheuer in Weimar geboren. Er war zunächst Assistent an der Sternwarte Göttingen und war dort mit Fragen der praktischen und theoretischen Sonnenforschung beschäftigt. 1939 wurde er ziviler Mitarbeiter der Luftwaffe an der Erprobungsstelle Rechlin, um am Projekt der Vorhersage der optimalen Frequenzbänder für das Militär durch Beobachtung der Sonnenaktivität mitzuarbeiten. 

Sein damaliger Chef war der Hochfrequenzexperte und Staatsrat Johannes Plendl. Nachdem Plendl in der Hierarchie des Dritten Reiches weiter aufstieg und Leiter der Reichsstelle für Hochfrequenzforschung wurde, sorgte er für höchste Dringlichkeit und die erforderlichen Geldmittel, mit denen ein rapider Aufbau einer Kette von Sonnenobservatorien möglich wurde. 

1942 erhielt Kiepenheuer den Auftrag zur Gründung des Fraunhofer-Instituts (heute: Kiepenheuer-Institut für Sonnenphysik) mit Sitz in Freiburg im Breisgau. Dieses Institut wurde von ihm laufend erweitert und auch bis zu seinem Tode von ihm geführt.

Während des Zweiten Weltkriegs arbeitete Kiepenheuer zusammen mit Erich Regener, der schon 1934 mit einem Ballon-Gespann die ersten Spektrogramme der Sonne außerhalb des Hauptteils der Erdatmosphäre aufgenommen hatte. Beide wollten nun mit einem UV-Spektrographen an der Spitze einer V-2 umfassendere Spektrogramme aufnehmen. Dieses Projekt wurde wegen des weiteren Kriegsverlaufs nicht mehr realisiert.

Bedeutsam waren seine Entwicklungsarbeiten für das Projekt „Spektrostratoskop“. Das mit einem Stratosphärenballon getragene Gerät zur Erforschung der Sonne mit einem Lyot-Filter und einem Spektrographen hatte sechs Tage vor seinem Tode den ersten erfolgreichen Flug in Palestine, Texas, USA, bestanden. Grundlage für diese Arbeiten dürfte der 1949–1951 von ihm und Georg Heinrich Thiessen entwickelte Magnetograph zur Messung solarer Magnetfelder gewesen sein. 

Im Rahmen der Sonnenbeobachtung gibt es die von ihm entwickelte „Modifizierte Kiepenheuer Skala“, die dazu genutzt wird, um Sonnenaufnahmen besser klassifizieren zu können. Die Hauptpunkte dieser Skala beziehen sich auf die Luftruhe und auf die Bildschärfe.






</doc>
<doc id="1301103" url="https://de.wikipedia.org/wiki?curid=1301103" title="Alternativverbot">
Alternativverbot

Das Alternativverbot ist ein Begriff aus der Schwingungsspektroskopie. 
Es besagt, dass eine Normalschwingung eines Moleküls mit Inversionszentrum entweder nur Infrarot-aktiv oder nur Raman-aktiv ist.

Bei einer zum Inversionszentrum symmetrischen Schwingung ist die Schwingung im IR-Spektrum verboten (d. h., sie wird nicht beobachtet), bei einer anti-symmetrischen ist die Schwingung im Raman-Spektrum verboten. Das folgt daraus, dass sich bei einer IR-aktiven Schwingung das Dipolmoment des Moleküls ändern muss, was es bei einer inversions-symmetrischen Schwingung nicht tut. Damit eine Schwingung Raman-aktiv ist, muss sich dagegen die Polarisierbarkeit der detektierten Spezies ändern.
In der Charaktertafel des Moleküls zeigt sich eine symmetrische Schwingung als „+1“ und eine unsymmetrische als „–1“.

Veranschaulichen kann man sich das beim Molekül formula_1, bei dem asymmetrische Streckschwingung und Biegeschwingung nur IR-aktiv sind, während die symmetrische Streckschwingung nur Raman-aktiv ist.



</doc>
<doc id="1685520" url="https://de.wikipedia.org/wiki?curid=1685520" title="Macedonio Melloni">
Macedonio Melloni

Macedonio Melloni (* 11. April 1798 in Parma; † 11. August 1854 in Portici bei Neapel) war ein italienischer Physiker.

Melloni arbeitete vor allem über Wärmestrahlung, außerdem Magnetismus der Gesteine, elektrostatische Induktion und Photographie.

Er wurde 1834 mit der Rumford Medal of the Royal Society ausgezeichnet. Ab 1835 war er korrespondierendes Mitglied der Académie des Sciences, ab 1839 auswärtiges Mitglied der Royal Society. Melloni war erster Direktor des 1841 eröffneten Vesuv-Observatoriums ("Osservatorio Vesuviano"), des ersten vulkanologischen Instituts der Welt. 1836 wurde er korrespondierendes Mitglied der Preußischen Akademie der Wissenschaften. 1842 erhielt er den preußischen Orden Pour le Mérite. 1849 wurde er in die American Academy of Arts and Sciences gewählt.

Melloni starb 1854 an Cholera.




</doc>
<doc id="3361570" url="https://de.wikipedia.org/wiki?curid=3361570" title="Sickerlinie">
Sickerlinie

Eine Sickerlinie ist die Grenze zwischen trockenem und feuchtem Material in einem Staudamm oder Deich, der auf einer Seite mit Wasser eingestaut ist und auf der anderen Seite nicht.

Die Sickerlinie verbindet den Wasserspiegel auf der eingestauten Seite (Wasserseite) mit dem Wasserspiegel, Grundwasserspiegel (wenn vorhanden) oder der Geländeoberkante auf der nicht eingestauten Seite, der Luftseite. Die Sickerlinie kann am luftseitigen Dammfuß austreten und somit in Form von Sickerwasser sichtbar werden. Bei einem homogenen Damm tritt die Sickerlinie an der Luftseite ungefähr auf einem Drittel der Dammhöhe aus, wie auf dem Bild dargestellt ist (nach der Theorie von Arthur Casagrande). Unterhalb dieser Stelle kann Dammbaumaterial ausgespült werden, was die Stabilität des Dammes oder Deiches gefährden kann.

Steigt der Wasserspiegel an, so steigt auch die Sickerlinie im Damm an. Fällt er wieder, sinkt auch die Sickerlinie. Steigen und Fallen der Sickerlinie geschehen meist nicht genauso schnell wie der Wasserspiegel steigt bzw. fällt, sondern allmählich, das heißt, diese Vorgänge sind instationär. Bleibt der Wasserspiegel gleich, so stellt sich nach einiger Zeit auch eine gleich bleibende Sickerlinie ein. Je undurchlässiger der Damm ist, desto länger dauert es, bis auch die Sickerlinie konstant geworden ist, weil das Wasser erst durch das Material sickern muss.
Je höher die Sickerlinie steigt, desto geringer wird die Standsicherheit. Das geschieht zum Beispiel bei Deichen, wenn Hochwasser ist.

Beim homogenen Damm ist die Sickerlinie nicht von der Durchlässigkeit des verwendeten Materials, sondern allein von der Geometrie abhängig. Die Durchlässigkeit des Dammes bestimmt hier lediglich den Durchfluss.

Im inhomogenen Fall spielt die Durchlässigkeit des Dammes und seiner unterschiedlichen Bereiche jedoch eine Rolle. Bei einem Damm, der an der Wasserseite dichter (undurchlässiger) als im übrigen Teil ist, wird die Sickerlinie in dem dichteren Bereich nach unten gezogen und tritt dann nicht mehr an der Luftseite aus, was die Standsicherheit erheblich vergrößert. Deswegen ist es sinnvoll, Dämme und Deiche an der Wasserseite abzudichten und an der Luftseite durch durchlässigere Filterschichten zu entwässern.





</doc>
<doc id="3835868" url="https://de.wikipedia.org/wiki?curid=3835868" title="Mandelstam-Variable">
Mandelstam-Variable

Bei den Mandelstam-Variablen "s", "t" und "u" (nach Stanley Mandelstam, der sie 1958 einführte) handelt es sich um Kurzschreibweisen für Terme, die in der Teilchenphysik bei der Berechnung von Streuprozessen mit zwei einlaufenden und zwei auslaufenden Teilchen häufig auftauchen.

Sind die Viererimpulse der beiden einlaufenden Teilchen mit formula_1 und die der auslaufenden Teilchen mit formula_2 bezeichnet, so sind die Mandelstam-Variablen gegeben durch:

Das in diesen Definitionen auftauchende Quadrat von Viererimpulsen ist dabei – wie in der relativistischen Physik üblich – definiert als formula_6 (siehe Vierervektor). Die Mandelstam-Variablen sind damit lorentzinvariante Skalare ebenso wie die Streuamplitude selbst, die durch sie in relativistisch invarianter Weise ausgedrückt werden soll.

Die drei Mandelstamvariablen sind nicht voneinander unabhängig: ihre Summe ist gleich der Summe der Massenquadrate der beteiligten Teilchen:

wobei wie in der Teilchenphysik üblich der dimensionslose Wert c=1 für die Lichtgeschwindigkeit angenommen ist (natürliche Einheiten).

Allgemein sollte die Streuamplitude, da sie eine relativistische Invariante ist, von den relativistischen Invarianten formula_8 (i = 1, 2, 3, 4) und den sechs möglichen unabhängigen (relativistischen) Skalarprodukten formula_9 abhängen – auch die Mandelstamvariablen s, t, u sind aus diesen als Linearkombination zusammengesetzt. Die formula_8 sind keine Variablen wegen formula_11 (die äußeren Beine der Feynmandiagramme sind "on shell"). Wegen der Erhaltung formula_12 der Viererimpulse (was vier Gleichungen ergibt, da die formula_13 je vier Komponenten haben) sind von den sechs Skalarprodukten formula_14 nur zwei unabhängig. Also sind auch nur zwei der Mandelstamvariablen unabhängig, die dritte ergibt sich aus der o. g. Summe.

Die Beiträge zum Streuprozess, in denen die jeweiligen Mandelstam-Variablen bei ihrer Berechnung auftauchen, werden als s-, t- und u-Kanal bezeichnet. Die zugehörigen Feynman-Diagramme sind in den folgenden Abbildung gezeigt.

Die Darstellung folgt der Konvention, dass die einlaufenden Teilchen als von links kommende Linien und die Streuprodukte als nach rechts auslaufende Linien dargestellt sind. Die gestrichelte Zwischenlinie stellt ein virtuelles Teilchen dar; das Quadrat seines Viererimpulses ist dem jeweiligen Diagramm entsprechend gleich "s", "t" oder "u".

Beispielsweise gibt das Diagramm für den s-Kanal eine Elektron-Positron-Paarvernichtung auf der linken Seite unter Bildung eines virtuellen Photons und einer Elektron-Positron-Paarerzeugung auf der rechten Seite wieder. Auch die Bildung eines instabilen Zwischenzustands (Resonanz) bei der Wechselwirkung zweier Teilchen wird so wiedergegeben. Eine gewöhnliche Elektron-Elektron-Streuung wird durch das Diagramm des t-Kanals wiedergegeben (wobei der u-Kanal mit betrachtet werden muss, bei dem die äußeren Beine 3,4 des Diagramms vertauscht sind).

In der Streuamplitude werden den Regeln der Quantenmechanik entsprechend alle möglichen Prozesse mit virtuellen Teilchen vom Typ s, t, u aufsummiert, da nur die Anfangszustände 1,2 und Endzustände 3,4, aber nicht die virtuellen Teilchen der Zwischenzustände beobachtet werden.


</doc>
<doc id="3881746" url="https://de.wikipedia.org/wiki?curid=3881746" title="Institut für Quantenoptik und Quanteninformation">
Institut für Quantenoptik und Quanteninformation

Das Institut für Quantenoptik und Quanteninformation (IQOQI) ist eine Einrichtung der Österreichischen Akademie der Wissenschaften (ÖAW) und wurde im November 2003 gegründet. Das Institut mit Standorten in Innsbruck und Wien, jeweils mit ungefähr 80 Mitarbeitern, widmet sich der Grundlagenforschung, theoretisch wie experimentell, auf dem Gebiet der Quantenoptik und Quanteninformation.

Das Institut besteht aus einer Abteilung in Innsbruck mit sechs Arbeitsgruppen um die wissenschaftlichen Direktoren Rainer Blatt, Francesca Ferlaino, Rudolf Grimm, Gerhard Kirchmair, Oriol Romero-Isart und Peter Zoller sowie einer Abteilung in Wien (IQOQI-Vienna) mit acht Arbeitsgruppen um Markus Aspelmeyer (IQOQI-Viennass derzeitiger Direktor), Časlav Brukner, Miguel Navascués, Marcus Huber, Markus Müller, Yelena Guryanova, Rupert Ursin und Anton Zeilinger. Die Einrichtung wurde als unabhängiges Forschungsinstitut mit starken Verbindungen zu den Universitäten Innsbruck und Wien eingerichtet. Dadurch soll ein enger Austausch von Studierenden und Postdoktoranden angeregt und die Integration der Mitglieder des Akademie-Instituts in den Lehrbetrieb der Universitäten gewährleistet werden.

Arbeitsgruppen am IQOQI Innsbruck:

Arbeitsgruppen am IQOQI Wien:



</doc>
<doc id="3939658" url="https://de.wikipedia.org/wiki?curid=3939658" title="Giorgio Bellettini">
Giorgio Bellettini

Giorgio Bellettini (* 5. Mai 1934 in Bologna) ist ein italienischer experimenteller Elementarteilchen-Physiker. Er war an der Entdeckung des Top-Quarks beteiligt.

Bellettini erhielt 1957 sein Diplom (Laurea) "cum laude" in Physik an der Universität Pisa. Er ist seit 1979 Professor in Pisa und gleichzeitig seit 1980 am Fermilab (Collider Detector Gruppe, CDF). 1980 bis 2000 war er Sprecher der italienischen Gruppen am CDF.

1974 bis 1976 war er Direktor des LNF Labors am Frascati-Beschleuniger der italienischen Gesellschaft für Kernphysik (INFN). Er war 1980 bis 1985 Vorsitzender (Chairman) im ISR (Intersecting Storage Ring) Komitee des CERN und war 1992 bis 1998 im wissenschaftlichen Rat (Scientific Council) des CERN. 1997 bis 2001 war er auch im wissenschaftlichen Beirat des Fermilab.

Ende der 1950er Jahre untersuchte er Wechselwirkungen von Antiprotonen. 1960 bis 1963 war er Sprecher der Gruppe, die die Photoproduktion von Mesonen am frühen Elektronensynchrotron in Frascati erforschte. 1968 bis 1974 war er Sprecher der Pisa-Stony Brook-Kollaboration am ISR des CERN. Dort gelang ihrer Gruppe 1972 der Nachweis des Anstiegs der totalen Proton-Proton Wirkungsquerschnitts bei hohen Energien. Weiter untersuchte er dort in den 1970er Jahren Myonen-Paarproduktion in Proton-Proton-Kollisionen.

Bellettini war der Entwickler des Magnetischen Spektrometers ALA am 2 GeV Elektron-Positron-Speicherring ALA in Frascati.

1994/95 war er an der Entdeckung des Top-Quarks in der CDF Kollaboration des Fermilab-Tevatrons beteiligt. Das Top-Quark wurde aufgrund der immensen Energie, die zur Erzeugung benötigt wird, erst 18 Jahre nach dem Bottom-Quark experimentell belegt, obwohl es schon im Jahr 1977 mit der Entdeckung des Bottom-Quarks theoretisch postuliert wurde.

Bellettini erhielt 2006 die Matteucci-Medaille und 1999 den ersten Preis des Kernforschungszentrums Dubna. 1999 wurde er Fellow der American Physical Society. 2000 wurde er Commendadore der italienischen Republik und 1998 erhielt er den Physikpreis des italienischen Wissenschaftsministeriums.



</doc>
<doc id="4418858" url="https://de.wikipedia.org/wiki?curid=4418858" title="Max Jammer">
Max Jammer

Max Jammer ("Moshe Jammer"; * 13. April 1915 in Berlin; † 18. Dezember 2010 in Jerusalem) war ein deutschstämmiger israelischer Physiker, Wissenschaftshistoriker und Philosoph.

Jammer studierte nach dem Abitur 1932 Physik, Mathematik und Philosophie an der Universität Wien und ab 1935 an der Hebrew University in Jerusalem, wo er 1936 sein Diplom erhielt und 1942 mit einer experimentalphysikalischen Arbeit promoviert wurde. Im Zweiten Weltkrieg war er in der Britischen Armee und war dann Dozent für Wissenschaftsgeschichte und Wissenschaftsphilosophie an der Hebrew University. 1952 wurde er Dozent an der Harvard University (in dieser Zeit hatte er auch engen Kontakt zu Albert Einstein in Princeton) und später an der University of Oklahoma und der Boston University. 1959 wurde er Professor für Physik und Vorsitzender der Physik-Fakultät an der neu gegründeten Bar-Ilan-Universität in Israel, wo er 1962 Rektor und von 1967 bis 1968 Präsident war. Außerdem war er Mitgründer des Instituts für Wissenschaftsphilosophie der Universität Tel Aviv. Er war unter anderem Gastprofessor an der ETH Zürich, der Universität Göttingen, dem Institut Henri Poincaré in Paris, der Catholic University of America in Washington, D.C.

Seit seinem ersten Buch „Konzepte des Raumes“ (Concepts of Space) von 1954 ist Jammer für grundlegende Untersuchungen wichtiger Konzepte der Physik (Raum, Energie und Kraft, Masse, Grundlagen der Quantenmechanik, Gleichzeitigkeit) sowohl unter physikalischen, als auch unter historischen und philosophischen Aspekten bekannt. Er schrieb darüber jeweils viel beachtete Monographien, geschätzt sowohl von Philosophen als auch von Wissenschaftshistorikern. Dazu führte er Gespräche mit vielen wichtigen Pionieren der modernen Physik (wie David Bohm, Paul Dirac, Max Born, Werner Heisenberg, Louis de Broglie und Pascual Jordan).

2007 erhielt er den Abraham-Pais-Preis für Physik-Historiker. Außerdem erhielt er 1984 den Israel-Preis und 2003 den israelischen EMET-Preis. Für sein Buch „Einstein and Religion“ erhielt er einen Buchpreis der Templeton Foundation. Er erhielt den Preis für Monographien der American Academy of Arts and Sciences.




</doc>
<doc id="4583502" url="https://de.wikipedia.org/wiki?curid=4583502" title="Luftelektrizität">
Luftelektrizität

Luftelektrizität, auch "atmosphärische Elektrizität", ist eine alte Bezeichnung für die Gesamtheit elektrischer Erscheinungen in der Erdatmosphäre, besonders das allgemeine luftelektrische Feld mit den in der Atmosphäre fließenden Strömen und die Wolken- und Gewitter<nowiki></nowiki>elektrizität.

Michael Faraday wies 1833 nach, dass die bis dahin als „verschiedene Elektrizität“ aufgefasste „statische“ (oder „gewöhnliche“), die „atmosphärische“, die „physiologische“, die „Volta’sche“ (oder „Berührungselektrizität“) und die „Thermoelektrizität“ in Wahrheit nur verschiedene Aspekte des einen – von ihm „Magnetelektrizität“ bezeichneten – physikalischen Prinzips darstellten. Die Erforschung der Luftelektrizität spielte um die Wende vom 19. zum 20. Jahrhundert eine bedeutende Rolle. Durch seine Arbeiten auf dem Gebiet der Luftelektrizität weckte Franz-Serafin Exner das Interesse seines Schülers, des späteren Nobelpreisträgers Victor Franz Hess, für dieses Thema.




</doc>
<doc id="5619854" url="https://de.wikipedia.org/wiki?curid=5619854" title="Heinrich Majus">
Heinrich Majus

Heinrich Majus (auch: "Henricus May"; * 7. Februar 1632 in Cassel; † 31. Dezember 1696 in Rinteln) war ein deutscher Mediziner und Physiker.

Der Sohn des Hofdiakons in Kassel und späteren Archidiakons der Freiheitsgemeinde Mag. Johann Majus (* 6. Dezember 1599; † 15. März 1640 in Kassel) und dessen Ehefrau Catharina († 7. Januar 1685 in St. Goar), die Tochter des Kammerschreibers Daniel Schild, hatte bereits in Kinderjahren seinen Vater verloren. Da seine Mutter am 16. September 1647 mit dem Professor und Dr. Johannes Crocius eine erneute Ehe einging, hatte er gute Voraussetzungen gehabt sich zu entwickeln.

Seine ersten philosophischen und medizinischen Studien begann er an der Universität Kassel, setzte diese in Berlin, an der Universität Frankfurt (Oder) und in Hamburg fort. Anschließend absolvierte er eine Kavaliersreise nach Holland, wobei er die Universität Leiden, die Universität Franeker und die Universität Groningen frequentierte. In Groningen promovierte er im Dezember 1657 zum Doktor der Medizin, wurde 1658 Stadtphysikus in Kassel, 1665 Professor der Medizin an der Universität Rinteln und Ostern 1669 Professor der Physik und Medizin an der Universität Marburg.

1672 ernannte man ihn zum fürstlich hessischen Leibarzt. 1682 legte er kurzzeitig seine Marburger Professur nieder, begab sich nach Kassel und wurde am 5. März 1658 wieder Professor der Medizin an der Universität Rinteln. Er hatte sich auch an den organisatorischen Aufgaben der Hochschulen beteiligt. So war er in den Jahren 1667 1688, 1689 und 1696 Rektor der Alma Mater in Rinteln sowie in den Jahren 1671 und 1681 Rektor der Hochschule in Marburg.

Heinrich Majus starb am 31. Dezember 1696 in Rinteln. Sein Leichnam wurde am 8. Januar in Rintelns reformierter Kirche beigesetzt.

May war zwei Mal verheiratet. Seine erste Ehe ging er am 19. August 1662 mit Dorothea ein (* 30. November 1630; † 12. Februar 1670), der Tochter des hessischen Oberleutnants Heinrich Gleim. Aus der Ehe stammen drei Kinder. Danach schloss er am 2. Oktober 1672 mit Magdalene, der Tochter des hessen kasselischen Rats David Hartmann, seine zweite Ehe. Aus der Ehe gingen acht Kinder hervor. Von den Kindern sind bekannt:

Majus hatte während seiner Hochschulzeit Disputationen bei zahlreichen Responenten agiert. Zudem erscheint er als eigenständiger Autor bei den Werken:




</doc>
<doc id="5760839" url="https://de.wikipedia.org/wiki?curid=5760839" title="Liste der Ionisationsmethoden in der Massenspektrometrie">
Liste der Ionisationsmethoden in der Massenspektrometrie

Diese Liste der Ionisationsmethoden in der Massenspektrometrie listet alle gebräuchlichen Methoden der Ionisation von Molekülen in der Massenspektrometrie auf. Gebräuchlich sind hier die in Klammer angegebenen Abkürzungen zu benutzen. Dabei wird diese häufig mit dem dazugehörigen Gerätetyp kombiniert (z. B. MALDI-TOF).

Atmosphärendruck-Methoden: 

Niederdruck-Methoden: 


</doc>
<doc id="6006078" url="https://de.wikipedia.org/wiki?curid=6006078" title="Der große Entwurf">
Der große Entwurf

Der große Entwurf: Eine neue Erklärung des Universums (Originaltitel: "The Grand Design") ist ein populärwissenschaftliches Buch der Astrophysiker Stephen Hawking und Leonard Mlodinow über die Möglichkeit einer Weltformel, mit der alle physikalischen Phänomene des Universums sowie dessen Entstehung vollständig erklärt werden können. Die Hauptthese des Buches lautet, dass nach der M-Theorie das Universum ohne den Einfluss eines Schöpfergottes spontan entstanden sein kann und nicht nur eines, sondern eine Vielzahl unterschiedlicher Universen wahrscheinlich ist. Als Schlüssel zum Nachweis dieser Theorie wird die Verbindung von Einsteins Allgemeiner Relativitätstheorie und der Quantenphysik dargestellt. Die deutsche Ausgabe des Buches erschien im September 2010 als Hardcover.

Ausgangspunkt des Buches ist das Konzept eines modellabhängigen Realismus. Demnach akzeptieren wir ein Erklärungsmodell dann, wenn es Ereignisse treffend vorhersagt. Gleichzeitig ist es aber möglich, dass ein anderes Erklärungsmodell mit fundamental verschiedenen Grundlagen dieselben Ereignisse ebenso treffend vorhersagt. In so einem Fall lässt sich nicht behaupten, eines der Modelle sei realer als das andere, so Hawking und Mlodinow. Diesen Ansatz nutzen die Autoren, um in ihre Einführung einer "Theorie von Allem" sowohl Theorien der klassischen Physik als auch der Quantenphysik integrieren zu können, obwohl unsere Alltagserfahrungen nur den Erkenntnissen der klassischen Physik entsprechen, der Quantenphysik aber widersprechen.

Grundlage des quantentheoretischen Teils des Buches ist das so genannte Doppelspaltexperiment. Es zeigt, dass kleinste Lichtteilchen auf dem Weg von einem Punkt A nach Punkt B keinem vorhersagbaren Pfad folgen. Der Physiker Richard Feynman folgerte als Erster, dass dies nicht bedeute, die Lichtteilchen des Experiments würden überhaupt keinem Pfad folgen. Nach Feynmans Theorie folgen sie vielmehr allen überhaupt möglichen Pfaden. Hawking und Mlodinow übertragen diesen Gedanken unter Berufung auf den modellabhängigen Realismus aus dem Quantenbereich auf die Kosmologie des gesamten Universums. Demnach hat sich auch das Universum nicht auf einem einzigen determinierten Weg vom Urknall aus entwickelt, sondern gleichzeitig auf allen möglichen Wegen zu einer Vielzahl möglicher Universen. Für die Lösung der Frage, wie alle diese Welten zeitlich und räumlich parallel existieren können, verweisen die Autoren auf die Modelle der Stringtheorie. Die Allgemeine Relativitätstheorie wird zur Auflösung des Problems der Entstehung des Universums herangezogen. Hawking und Mlodinow erklären, dass nach Einsteins Theorie die Zeit in der Phase des Urknalls als räumliche Dimension verstanden werden muss, die nichts mehr mit unserem alltäglichen Zeitverständnis zu tun hat. Die so genannte Raumzeit wird an dieser Stelle analog wie eine geschlossene Oberfläche ohne Rand beschrieben. Die Frage nach dem Beginn der Zeit (wie nach einem Rand) ist in der Argumentation damit hinfällig, weil dieser Beginn genau so wenig bestimmbar ist wie ein Punkt südlich des Südpols.

Das Buch ist in acht Kapitel gegliedert:

Das Buch erreichte im September 2010 den vierten Platz auf der Spiegel-Bestsellerliste für Hardcover-Bücher. Kritikpunkte aus wissenschaftlicher Sicht betrafen bei der Veröffentlichung unter anderem die Aktualität der im Buch verwendeten Theorien. Hinsichtlich der Verständlichkeit für Laien wurde einerseits der klare und bildreiche Stil hervorgehoben, andererseits dessen Fehlen bei komplexeren Sachverhalten.
Aus philosophisch-metaphysischer Sicht bestehen Kritikpunkte an den weltanschaulichen Grundannahmen des Werkes, insbesondere dem Materialismus. Auch wird der Standpunkt des Buches zur Gottesfrage kritisiert und bietet so eine Diskussionsgrundlage zur Gottesfrage.



</doc>
<doc id="6105389" url="https://de.wikipedia.org/wiki?curid=6105389" title="Aufbauprinzip">
Aufbauprinzip

Das Aufbauprinzip ist ein von Niels Bohr 1921 entwickeltes Konzept, um das periodische Auftreten der chemischen Eigenschaften im Periodensystem der Elemente mithilfe der Eigenschaften der Atomhülle erklären zu können. Es wird verwendet, um die Anordnung der Elektronen in Atomen, Molekülen oder Ionen im energetisch niedrigsten Zustand zu ermitteln. Dem Prinzip liegt ein Prozess zu Grunde, der das sukzessive Auffüllen der Atomhülle mit Elektronen beschreibt. Elektrostatisch angezogen durch die Protonen im Atomkern sucht jedes neu hinzugefügte Elektron für sich den Zustand geringster Energie. Dieser befindet sich in dem Atomorbital, das die niedrigste Energie hat und noch nicht voll besetzt ist, wobei die maximale Elektronenanzahl in jedem Orbital durch das Pauli-Prinzip gegeben ist. In diesem Orbital verbleibt das Elektron auch, wenn weitere Elektronen hinzu kommen, denn die energetische Reihenfolge der Orbitale bleibt bei steigender Elektronenzahl fast immer erhalten. Daher sind die Atomhüllen aller Atome in ihrem Innern gleich aufgebaut, nur dass mit steigender Kernladung die Orbitale fester gebunden werden und sich enger um den Kern konzentrieren.

Nach dem Prinzip füllen die Elektronen die Orbitale immer so auf, dass die Hülle den Zustand einnimmt, der im Rahmen des Modells unabhängiger Teilchen die geringstmögliche Energie hat. Wenn für ein Elektron mehrere Orbitale mit gleicher Energie zur Auswahl stehen, wird laut den Hundschen Regeln aufgrund von Mehrteilcheneffekten ein unbesetztes Orbital bevorzugt.

Das Aufbauprinzip kann analog auch herangezogen werden, um die Anordnung von Protonen und Neutronen im Atomkern zu beschreiben.

Die Reihenfolge, nach der die Elektronen die Orbitale besetzen, wird durch die formula_1-Regel beschrieben (auch bekannt als die Madelung-Regel nach Erwin Madelung oder die Klechkowski-Regel in manchen, meist französischsprachigen Ländern oder die Moeller-Regel in manchen meist spanischsprachigen Ländern):


Dieses Verhalten der Elektronen wurde experimentell durch die spektroskopischen Eigenschaften der Elemente herausgefunden. Die quantenmechanische Erklärung der Regel basiert auf der Gesamtanzahl der Knoten in einem Orbital, was den Energiezustand widerspiegelt, sowie auf der bei größerem formula_4 stärkeren Abschirmung des anziehenden Kernpotentials durch die anderen Elektronen.

Konkret werden die Elektronen der Reihe nach in folgende Orbitale eingegliedert (vgl. Abb. rechts):
1s ⇒ 2s ⇒ 2p ⇒ 3s ⇒ 3p ⇒ 4s ⇒ 3d ⇒ 4p ⇒ 5s ⇒ 4d ⇒ 5p ⇒ 6s ⇒ 4f ⇒ 5d ⇒ 6p ⇒ 7s ⇒ 5f ⇒ 6d ⇒ 7p

"Hinweis: Das Madelung-Energieschema kann nur auf neutrale Atome in ihrem Grundzustand angewendet werden. (Siehe Ausnahmen)"

Erläuterung zum rechten Bild:

Die Orbitale der Atomhülle werden in der Reihenfolge des Pfeils mit Elektronen besetzt. Von links nach rechts sind die Orbitale der Atomhülle aufgezählt (steigende Nebenquantenzahl formula_4) und von oben nach unten die Schalen (steigende Hauptquantenzahl formula_3), jeweils mit Buchstaben als entsprechendes Kürzel. Die hochgestellten Zahlen geben jeweils an, wie viele Elektronen sich maximal im Orbital oder in der Schale aufhalten können. Der für die Besetzung entscheidende Wert formula_1 nimmt in dieser Darstellung diagonal "nach rechts unten" zu. Daher haben alle Orbitale auf Linien senkrecht zu dieser Richtung jeweils den gleichen Wert formula_1. Gemäß der Regel werden in diesem Fall zunächst die Orbitale mit den kleineren formula_3-Werten besetzt, d. h. die einzelnen Diagonalen werden von rechts oben nach links unten durchlaufen. Der blass dargestellte Bereich ist theoretischer Natur, da noch keine Atome mit so vielen Elektronen und den damit erforderlichen großen Kernen entdeckt oder erzeugt werden konnten.

Nicht bei allen Atomen folgt die Besetzung der Schalen der obigen einfachen Aufbauregel. Grund sind relativistische Effekte und Effekte aufgrund der Korrelationen mehrerer Elektronen untereinander, die bei größerer Ordnungszahl eine immer größere Rolle spielen, aber innerhalb dieser Aufbauregeln noch nicht berücksichtigt sind. Beispiele von Elementen, die sich anders verhalten:



Die Elektronenkonfigurationen jenseits des Elementes Rutherfordium (Ordnungszahl: 104) sind noch nicht eindeutig bestätigt bzw. bewiesen.
Das folgende Periodensystem und die folgende Liste geben einen Überblick über die Ausnahmen, dabei wurde die auffälligsten Gemeinsamkeiten zusammengefasst.




</doc>
<doc id="6139047" url="https://de.wikipedia.org/wiki?curid=6139047" title="Keiichi Kodaira">
Keiichi Kodaira

Keiichi Kodaira (jap. , "Kodaira Keiichi"; * 20. Februar 1937 in der Präfektur Tokio) ist ein japanischer Astrophysiker.

Er studierte zunächst Physik an der Universität Tokio. Im Anschluss studierte er von 1959 bis 1961 Astronomie und erhielt ein Stipendium, das ihm ermöglichte, an der Universität Kiel die Forschung für seinen Ph.D. zu beenden. 1964 wurde er schließlich für seine Forschungsarbeit am Institut für Theoretische Physik der Universität Kiel promoviert. Kodaira arbeitete anschließend am National Astronomical Observatory of Japan und erhielt für seine spektroskopischen und photometrischen Untersuchungen von Sternen den Doktortitel der Universität Tokio. Weitere Forschung auf diesem Gebiet betrieb er von 1967 bis 1969 am California Institute of Technology.

Seit 1971 gibt Kodaira ebenfalls Lehrveranstaltungen und lehrte von 1972 bis 1973 an der Universität Heidelberg als Gastprofessor. Anschließend untersuchte er die chemische Zusammensetzung von Sternen und war zunächst Vizepräsident der Internationalen Astronomischen Union (IAU). Von 1982 bis 1985 war er schließlich Vorsitzender der Kommission Nr. 36 ("Theorie der Sternatmosphären") der IAU.

Kodairas Interesse weitete sich auf Galaxien aus und er untersuchte diese im ultravioletten und infraroten Bereich des elektromagnetischen Spektrums.
Da Kodaira ab 1982 Professor am National Astronomical Observatory of Japan war, förderte er das "Japan National Large Telescope Project". Aus diesem ging später das Subaru-Teleskop hervor, mit welchem unter anderem eine der frühesten Galaxien in einer Entfernung von etwa 12,88 Milliarden Lichtjahren entdeckt wurde.
Nachdem er das Subaru-Projekt fertiggestellt hatte, förderte er die Entwicklung von ALMA. 
Karl-Schwarzschild-Medaille, 2001.

Kodaira ist derzeit der Direktor des "JSPS Bonn Office", der Vertretung der Japan Society for the Promotion of Science in Deutschland.


</doc>
<doc id="6420529" url="https://de.wikipedia.org/wiki?curid=6420529" title="Velocity overshoot">
Velocity overshoot



</doc>
<doc id="6568215" url="https://de.wikipedia.org/wiki?curid=6568215" title="Antitelefon">
Antitelefon

Ein Antitelefon (auch Tachyon-Antitelefon) ist ein hypothetisches Gerät, mit dem Signale in die eigene Vergangenheit gesendet werden könnten. Es beruht auf einem Gedankenexperiment Albert Einsteins (1907),
wonach sich mit Überlichtgeschwindigkeit ausbreitende Signale gemäß der speziellen Relativitätstheorie rückwärts durch die Zeit bewegen. Einstein (1910) bezeichnete das in einem Gespräch mit Arnold Sommerfeld als ein Mittel, um „in die Vergangenheit zu telegraphieren“.
Ein ähnliches Gedankenexperiment wurde von Richard Chace Tolman (1917) besprochen, weswegen es auch als Tolmans Paradoxon bekannt ist.
Ein hypothetisches Gerät, mit dem das bewerkstelligt werden könnte, wurde von Gregory Benford später als „Tachyon-Antitelefon“ bezeichnet. Bei Tachyonen handelt es sich um hypothetische, überlichtschnelle Teilchen, denen unübliche Eigenschaften, wie z. B. eine imaginärwertige Masse zugesprochen werden.

Ein solches „Telegraphieren in die Vergangenheit“ mittels überlichtschneller Kommunikation würde allerdings zu Kausalitätsverletzungen führen, deswegen wurde es von Einstein, Tolman und von der überwiegenden Mehrheit der Physiker als praktisch nicht möglich eingestuft. Auch die damit zusammenhängenden Tachyon-Theorien sind umstritten.

Basierend auf Einsteins Gedankenexperiment betrachtete Tolman folgende Situation: Es sei eine Strecke L mit den Endpunkten A und B gegeben. Ein Signal wird nun bei A zum Zeitpunkt formula_1 mit Geschwindigkeit formula_2 in Richtung B gesendet und kommt zum Zeitpunkt formula_3 dort an. Die Flugzeit des Signals ergibt sich mit

Hier findet also die Ursache (Ereignis bei A) vor der Folge (Ereignis bei B) statt. Jedoch in einem Inertialsystem, das sich mit der Geschwindigkeit formula_5 relativ dazu bewegt, ergibt sich gemäß der Lorentz-Transformation die Flugzeit des Signals bis zur Ankunft bei B mit:

Es ist zu sehen, dass man im Fall formula_7 durch geeignete Wahl von "v" ein negatives formula_8 erreicht. Mit anderen Worten, die Wirkung bei B ereignet sich in diesem System vor der Ursache bei A. Einstein und Tolman verwiesen darauf, dass dieses Resultat keinen logischen Widerspruch enthält, jedoch widerspricht es der Gesamtheit der Erfahrung, sodass die Unmöglichkeit von Überlichtgeschwindigkeiten ihrer Ansicht nach ausreichend bewiesen ist.

Komplizierter werden die Zusammenhänge, wenn folgendes Beispiel diskutiert wird: Es sei ein Inertialsystem S' gegeben, in dem Beobachter A ruht, und ein Inertialsystem S, in dem B ruht. B bewegt sich in negativer x-Richtung aus Sicht von A. Ebenso wird vorausgesetzt, dass A und B über gleichartige Gerät verfügen, mit denen in ihren jeweiligen Inertialsystemen überlichtschnelle Signale gesendet werden können.

A sendet zum Zeitpunkt formula_9 ein mit Überlichtgeschwindigkeit formula_10 (gemessen in S') sich ausbreitendes Signal zu B, das dort zum Zeitpunkt

ankommt. Hier ist formula_12 die Strecke, die vom Signal durchlaufen wird, bis es das in negativer x-Richtung davonfliegende B erreicht hat. Bei B wird nun unmittelbar darauf der eigene Sender aktiviert und ein mit Überlichtgeschwindigkeit formula_2 (gemessen in S) sich ausbreitendes Signal zu A gesendet. Dabei muss berücksichtigt werden, dass die Strecke längenkontrahiert ist, und dass A dem Signal in positiver Richtung davonläuft. Das ergibt eine Ankunftszeit von

In S' hingegen ergibt sich (vgl. Formel für formula_8 im Einwegbeispiel)

Insgesamt ergibt sich also die Gesamtzeit formula_17 bis zur Rückkehr zu A

Auch hier ergibt sich bei formula_7, dass bei geeigneter Wahl von formula_5 ein negatives formula_17 entsteht, das heißt, A erhält die Antwort zurück, bevor das ursprüngliche Signal überhaupt abgesendet wurde. Benford u. a. schrieben über solche Situationen:

Daraus schlossen sie, dass überlichtschnelle Teilchen wie Tachyonen keine Signale übertragen dürften.



</doc>
<doc id="6738609" url="https://de.wikipedia.org/wiki?curid=6738609" title="Isidor Fröhlich">
Isidor Fröhlich

Isidor Fröhlich (* 23. Dezember 1852 in Pest; † 24. Januar 1931) war ein ungarischer Physiker.

Isidor Fröhlich studierte an den Universitäten Pest und Berlin. An der Universität Budapest wurde er 1876 Privatdozent, 1879 wurde er außerordentlicher Professor und 1885 ordentlicher Professor für theoretische Physik. Seine Arbeiten zur Optik aus 1883 wurden mit einem Preis ausgezeichnet. 1891 wurde er Mitglied der Ungarischen Akademie der Wissenschaften, 1920 Direktionsmitglied. 1905 ernannte man ihn zum Hofrat.


</doc>
<doc id="7468683" url="https://de.wikipedia.org/wiki?curid=7468683" title="Henry G. Blosser">
Henry G. Blosser

Henry Gabriel Blosser (* 16. März 1928 in Harrisonburg, Virginia, Vereinigte Staaten; † 20. März 2013 in Lansing) war ein US-amerikanischer experimenteller Kernphysiker und Beschleunigerphysiker.

Blosser studierte an der University of Virginia Physik mit dem Bachelor Abschluss 1951, dem Master-Abschluss 1952 und der Promotion 1954. Als Post-Doktorand war er in der Zyklotron-Kernphysik-Gruppe des Oak Ridge National Laboratory, ab 1956 als Gruppenleiter (Cyclotron Analogue One). 1958 wurde er Associate Professor und 1961 Professor an der Michigan State University. Seit 1990 war er Distinguished University Professor. 1958 bis 1966 und 1969 bis 1980 war er dort Leiter des Zyklotron Labors und 1985 bis 1989 Ko-Direktor des National Superconducting Cyclotron Laboratory.

Er wurde durch Entwicklungen in der Beschleunigerphysik bekannt, worauf er fünf Patente erhielt. Unter anderem war er an der Entwicklung von Zyklotronen mit supraleitenden Magneten ("supraleitende Zyklotrone") beteiligt einschließlich kompakter supraleitender Zyklotrone für die Neutronentherapie.

Seit 1984 war er auch Adjunct Professor in der Abteilung Strahlungs-Onkologie der Wayne State University.

Seit 1968 war Blosser Fellow der American Physical Society (APS) und von 1973 bis 1974 Guggenheim Fellow. 1992 erhielt er den Tom-W.-Bonner-Preis für Kernphysik der APS.




</doc>
<doc id="8097157" url="https://de.wikipedia.org/wiki?curid=8097157" title="Angelo Marcello Anile">
Angelo Marcello Anile

Angelo Marcello Anile (* 3. Januar 1948 in Santa Maria di Licodia; † 16. November 2007 in Catania) war ein italienischer theoretischer Physiker und Angewandter Mathematiker. Er war Professor für mathematische Physik an der Universität Catania.

Anile erwarb 1970 seinen Laurea-Abschluss in Pisa (Universität und Scuola Normale Superiore). Neben Mathematik hatte er auch Astrophysik bei Bruno Bertotti studiert und wurde 1974 in Oxford bei Dennis Sciama über Kosmologie promoviert (Geometrical Optics and Radiative Transfer in Irregular Universes). 1980 gewann er einen Lehrstuhl-Wettbewerb in mathematischer Physik und wurde Professor in Catania.

Er befasste sich mit relativistischer Hydrodynamik und Magnetohydrodynamik mit Anwendungen in der Astrophysik, Allgemeiner Relativitätstheorie und Kosmologie, geometrischer Optik, Ladungstransport in Halbleitern und mathematische Modellierung von Halbleiter-Schaltungen und Fuzzy Logik.

1981 erhielt er den Bartolozzi-Preis.

Das Europäische Konsortium für Industriemathematik (ECMI) vergibt ihm zu Ehren einen Preis.




</doc>
<doc id="8948908" url="https://de.wikipedia.org/wiki?curid=8948908" title="Natalia Toro">
Natalia Toro

Natalia Toro (* 1985) ist eine US-amerikanische theoretische Physikerin.

Toro war 1999 mit 14 Jahren die jüngste Gewinnerin des Ersten Preises in der Intel Science Talent Search (damals war sie an der Fairview High School in Boulder (Colorado)), wobei sie unabhängig die Super-Kamiokande-Daten zur Neutrinooszillation untersuchte. 1999 gewann sie eine Silbermedaille in der Internationalen Physik-Olympiade. Sie studierte Physik und Mathematik am Massachusetts Institute of Technology und an der Harvard University, an der sie promoviert wurde. Als Post-Doktorandin war sie 2007 bis 2010 an der Stanford University und 2011 am Institute for Advanced Study. Sie forscht und lehrt am Perimeter Institute.

Sie entwickelte vereinfachte Modelle für eine Suche nach Physik jenseits des Standardmodells zum Beispiel beim Large Hadron Collider. Sie sucht nach nicht im Standardmodell beschriebenen schwachen Kräften und Dunkler Materie, zum Beispiel in geplanten Experimenten am Elektronenbeschleuniger des Thomas Jefferson National Accelerator Facility.

Außerdem untersucht sie die Möglichkeit masseloser Teilchen mit kontinuierlichem Spin (CSP), die Kandidaten für neuartige langreichweitige Wechselwirkungen wären.

2015 erhielt sie mit Philip C. Schuster den New Horizons in Physics Prize "für vereinfachte Modelle zur Suche nach neuer Physik am Large Hadron Collider und für experimentelle Suche nach dunklen Sektoren mittels hochintensiver Elektronenstrahlen".

Sie ist mit dem Physiker Philip C. Schuster verheiratet.




</doc>
<doc id="9145384" url="https://de.wikipedia.org/wiki?curid=9145384" title="Società Italiana di Fisica">
Società Italiana di Fisica

Die Società Italiana di Fisica ist die Fachgesellschaft und Interessensvertretung der Physiker Italiens. Die 1897 gegründete „Italienische Gesellschaft für Physik“ hat ihren Sitz in Bologna.

Seit ihrer Gründung gibt die Gesellschaft die Fachzeitschrift "Il Nuovo Cimento" heraus, aber auch die "Quaderni di Storia della Fisica" und das "Giornale di Fisica". Teile des "Nuovo Cimento" gingen 1986 in den damaligen "Europhysics Letters" und 1999 im "European Physical Journal" auf. Von 1956 bis 1984 gab die Gesellschaft das "Bollettino della Società Italiana di Fisica" heraus, das dann zu "Il Nuovo Saggiatore" wurde und 1991 die Zeitschrift "Fisica e Tecnologia" übernahm.

Für besondere wissenschaftliche Leistungen verleiht die Gesellschaft eine Reihe von Preisen, darunter den "Premio Enrico Fermi" und zusammen mit dem "Institute of Physics" den "Premio Giuseppe Occhialini".




</doc>
<doc id="10170649" url="https://de.wikipedia.org/wiki?curid=10170649" title="Goldstone-Boson-Äquivalenztheorem">
Goldstone-Boson-Äquivalenztheorem

Das Goldstone-Boson-Äquivalenztheorem ist ein Theorem in der Quantenfeldtheorie. Es besagt, bei hohen Energien können zur Berechnung von Elementen der S-Matrix longitudinal polarisierte Eichbosonen durch die korrespondierenden Goldstone-Bosonen ersetzt werden. Es wurde 1985 von Michael Chanowitz und Mary Gaillard bewiesen.

In einer ungebrochenen Quantenfeldtheorie sind Eichbosonen immer masselos und können daher nur transversale Spinpolarisation innehaben. Aufgrund des Goldstone-Theorems entsteht bei der Brechung der Symmetrie, die zur Masse der Eichbosonen führt, für jedes massive Eichboson ein korrespondierendes Goldstone-Boson.

Bei hohen Energien ist die konstante Ruheenergie eines Teilchens, aufgrund der Äquivalenz von Masse und Energie also seine Masse, vernachlässigbar im Vergleich zur kinetischen Energie, und alle Teilchen können näherungsweise als masselos betrachtet werden. Das Goldstone-Boson-Äquivalenztheorem beschreibt daher, wie in diesem Fall mit der longitudinalen Polarisation verfahren werden kann.

Bezeichnet formula_1 eine Streuamplitude mit formula_2 longitudinalen Eichbosonen
und formula_3 die entsprechende Streuamplitude, in der die Eichbosonen durch Goldstone-Bosonen mit identischem Impuls formula_4 ersetzt wurden,
dann gilt:


</doc>
<doc id="10359944" url="https://de.wikipedia.org/wiki?curid=10359944" title="Ludwig Meinunger">
Ludwig Meinunger

Ludwig Meinunger (* 11. Mai 1936 in Steinach (Landkreis Sonneberg); † 21. Mai 2018 in Kronach) war ein deutscher Astrophysiker, Astronom und Botaniker.

Meinunger arbeitete von 1960 bis 1991 als Astronom an der Sternwarte Sonneberg in Thüringen. Als Botaniker war er ein Spezialist für Moose und Flechten.

Zusammen mit seiner langjährigen Lebenspartnerin, der Bryologin Wiebke Schröder (1934–2018), erforschte er über einen Zeitraum von insgesamt 12 Jahren, jeweils sieben volle Monate im Jahr, die Verbreitung von Moosen in ganz Deutschland und legte die Grundlage zur Herausgabe des dreibändigen Werkes "Verbreitungsatlas der Moose Deutschlands", das im Jahr 2007 erschien.

Im Jahr 2010 wurde er zusammen mit Wiebke Schröder mit dem Akademie-Preis der Bayerischen Akademie der Wissenschaften geehrt.




</doc>
<doc id="10840647" url="https://de.wikipedia.org/wiki?curid=10840647" title="Michael Nauenberg">
Michael Nauenberg

Michael Nauenberg (* 19. Dezember 1934 in Berlin; † 22. Juli 2019) war ein US-amerikanischer theoretischer Physiker und Physikhistoriker.

Nauenberg studierte am Massachusetts Institute of Technology und promovierte 1960 an der Cornell University bei Hans Bethe mit einer Arbeit zur Teilchenphysik. Anschließend war er Visiting Fellow am Institute for Advanced Study. Ab 1961 war er Assistant Professor für Physik an der Columbia University sowie am Stanford Linear Accelerator Center und der Stanford University tätig. 1966 wurde er Professor für Physik an der University of California, Santa Cruz (UCSC). Außerdem war er von 1988 bis 1994 Direktor des Institute für Nonlinear Science der UCSC. Nach seiner Emeritierung 1994 wurde er Research Professor of Physics an der UCSC. Er war Gastprofessor an verschiedenen Forschungseinrichtungen und Universitäten in Europa.

Nauenberg arbeitete auf dem Gebiet der Teilchen- und Kernphysik sowie zur theoretischen Festkörperphysik, Astrophysik und Nichtlinearen Dynamik. Seine meistzitierte Veröffentlichung, entstanden in Zusammenarbeit mit dem Nobelpreisträger Tsung-Dao Lee, ist die zum Kinoshita-Lee-Nauenberg-Theorem (KLN-Theorem). Seit den 1990er Jahren publizierte er zahlreiche Arbeiten zur Geschichte der Naturwissenschaften, vor allem über Physiker aus dem 17. Jahrhundert. Darunter sind Arbeiten über das Wirken von Isaac Newton, Robert Hooke und Christiaan Huygens. Darüber hinaus veröffentlichte er Beiträge zu Physikern des 20. Jahrhunderts, so über Edmund Clifton Stoner und Subrahmanyan Chandrasekhar.

Von 1963 bis 1964 hatte er ein Guggenheim-Stipendium, und von 1964 bis 1966 war er Sloan Research Fellow. 1989 bis 1990 war er Stipendiat der Alexander-von-Humboldt-Stiftung. 




</doc>
<doc id="228" url="https://de.wikipedia.org/wiki?curid=228" title="André-Marie Ampère">
André-Marie Ampère

André-Marie Ampère (* 20. Januar 1775 in Lyon, Frankreich; † 10. Juni 1836 in Marseille) war ein französischer Physiker und Mathematiker. Er war der herausragende Experimentator und Theoretiker der frühen Elektrodynamik. Nach ihm ist die internationale Einheit der Stromstärke Ampere benannt.

Ampère war der Sohn von Jean-Jacques Ampère und dessen Ehefrau Jeanne-Antoinette de Sarcey. Er fiel schon früh als wissbegieriger Knabe und durch sein gutes Gedächtnis auf. Sein Vater war ein Verehrer von Jean-Jacques Rousseau und erzog Ampère nach dessen "Emile", seine Mutter sorgte für seine religiöse Verwurzelung im Katholizismus. Ampère las als Jugendlicher Buffons Naturgeschichte und systematisch die 35 Bände der Enzyklopädie von Denis Diderot und Jean d'Alembert und lernte Griechisch, Latein und Italienisch. Sein Vater wurde 1793 nach dem Fall von Lyon (während der Französischen Revolution) als Girondist hingerichtet (als Friedensrichter hatte er zuvor einen führenden Jakobiner in Lyon, Joseph Chalier, verhaften und hinrichten lassen), was bei Ampère eine tiefe Krise auslöste. Als Achtzehnjähriger befasste er sich mit den Lehrbüchern des Schweizer Mathematikers Leonhard Euler und der klassischen Mechanik von Joseph-Louis Lagrange. Im gleichen Alter entwickelte er eine Plansprache, die er als friedensförderndes Werkzeug ansah. Er wandte sich ebenfalls der Botanik, der Metaphysik und der Psychologie zu, ehe er Mathematik und Physik studierte. Nachdem das elterliche Vermögen zusammengeschmolzen war, gab er Privatunterricht besonders in Mathematik. Seine Kontakte zur Außenwelt waren aber gering.

Im Jahre 1796 lernte er Julie Carron kennen, die er 1799 heiratete. Sie war etwas älter und stammte aus einer angesehenen bürgerlichen Familie in einem Nachbarort von Ampère. Obwohl sie aus ähnlichem sozialem Hintergrund kamen, war Ampère keine gute Partie und er warb lange und hartnäckig um sie, was in seinem Tagebuch dokumentiert ist. 1800 wurde ihr Sohn Jean-Jacques Ampère geboren, der ein bekannter Historiker, Philologe und Schriftsteller wurde. 1802 wurde er Lehrer für Physik und Chemie an der École centrale in Bourg-en-Bresse. Im selben Jahr verfasste Ampère ein mathematisches Werk zu einem wahrscheinlichkeitstheoretischen Aspekt von Glücksspielen, und zwar der Frage der Wahrscheinlichkeit des Ruins des Spielers bei stetigem Einsatz eines festen Bruchteils seines Kapitals. Die Arbeit machte ihn unter Wissenschaftlern in Paris bekannt. Bald darauf verfasste er eine Arbeit zur theoretischen Mechanik und eine Abhandlung über partielle Differentialgleichungen, die ihm 1814 die Mitgliedschaft in der französischen Akademie der Wissenschaften (damals Institut Impèrial) einbrachte.

Die vier Jahre seiner ersten Ehe waren die glücklichsten seines Lebens. Im Jahr 1803 starb nach vierjähriger Ehe seine Frau, die sich von der Geburt des Sohnes nie völlig erholt hatte. Ampère war tief getroffen und zog Jahr 1804 nach Paris. Sein Interesse für Mathematik erlahmte, und er befasste sich zunehmend mit den Schriften von Kant, allgemeiner Wissenschaftstheorie und mit der Chemie. Ampère war Repetitor für Mathematik an der Pariser École polytechnique, was ihn aber bald langweilte. Im Jahre 1808 wurde er Generalinspektor der Universitäten, was er bis auf ein paar Jahre in den 1820er Jahren bis zu seinem Tod blieb. Ab 1819 lehrte er außerdem Philosophie an der Historisch-Philosophischen Fakultät der Sorbonne und 1820 wurde er Assistenzprofessor in Astronomie. 1824 erhielt er den Lehrstuhl für Experimentalphysik am Collège de France.

Im August 1806 heiratete er in Paris Jeanne-Françoise Potot (1778–1866), die Ehe war aber unglücklich und wurde bald geschieden. Aus dieser Ehe stammt die Tochter Albine (1807–1842). Er musste nun allein für die zwei Kinder aus den beiden Ehen sorgen. Beide bereiteten ihm später Sorgen, seine Tochter war mit einem jähzornigen und oft betrunkenen Armeeoffizier verheiratet und sein Sohn verfiel dem Einfluss von Madame Recamier.

1836 starb Ampère in Marseille auf einer Inspektionstour im Alter von 61 Jahren an einer Lungenentzündung. Er ist in Paris auf dem Cimetière de Montmartre beigesetzt.

Ampère stellte drei Jahre nach Amedeo Avogadro unabhängig von diesem das Avogadrosche Gesetz auf. Er war auch offen für die Arbeiten von Humphry Davy, die die Grundfesten der französischen Schule der Chemie (Antoine Laurent de Lavoisier) erschütterten: für Lavoisier war Sauerstoff der Träger des Säureprinzips, nach Davys Entdeckung von Natrium und Kalium fand sich dieser aber in starken Basen. Damit löste sich auch das Rätsel des grünen Gases (Chlorgas) bei der Zersetzung von Salzsäure; Ampère wie Davy vermuteten, dass es ein neues Element (Chlor) sein könnte (während man nach der Lavoisier-Theorie Sauerstoff als Bestandteil vermutete). Da Ampère aber weder Zeit noch Mittel hatte, dem weiter nachzugehen, gilt Davy als dessen Entdecker. Später (1813) erkannte Ampère die Verwandtschaft des gerade in Seetang entdeckten Jods mit Chlor, in der öffentlichen Anerkennung als Entdeckung eines neuen Elements kamen ihm aber wieder andere zuvor. Er versuchte die chemische Affinität von Molekülen, die aus punktförmigen Atomen bestehen, aus der Geometrie von geometrischen Körpern (zum Beispiel Tetraeder, Oktaeder oder Würfel) abzuleiten. Beispielsweise bildeten bei Sauerstoff, Stickstoff und Wasserstoff vier Moleküle ein Tetraeder, bei Chlor acht Moleküle ein Oktaeder (nach Ampère); Verbindungen aus Elementen konnten nur bestehen, falls sie reguläre Polyeder bildeten (was bei Tetraeder und Oktaeder nicht möglich war, wohl aber zwei Tetraeder mit einem Oktaeder zu einem Dodekaeder). Ampères spekulativere Arbeiten zur Chemie fanden jedoch bei anderen Gelehrten seiner Zeit kaum Interesse. 

Seine bedeutendsten Arbeiten entstanden ab 1820 und machten ihn zum Begründer der Elektrodynamik. Im Jahr 1827 verschlechterte sich Ampères Gesundheitszustand und er wandte sich von der Elektrodynamik anderen Gebieten zu (Philosophie, Logik, Anatomie, Kristalloptik, Botanik). In der Philosophie war er von Kant beeinflusst und war sogar einer der Ersten in Frankreich, die dessen Werk ernsthaft rezipierten. Für Ampère war dies eine Alternative zu der damals in Frankreich vorherrschenden sensualistischen Erkenntnistheorie von Étienne Bonnot de Condillac. Ampère lehnte aber gleichzeitig die Lehre von Raum und Zeit als A-priori-Anschauungen nach Kant ab, behielt aber dessen Unterscheidung von Phänomenen und Noumenon. Er folgte teilweise der Lehre seines Freundes Maine de Biran im Nachweis der Existenz einer unabhängigen materiellen Welt, von Gott und Seele. Ampère vertrat ein hypothetisch-deduktives Verfahren des wissenschaftlichen Erkenntnisgewinns: Der Naturforscher stellt eine Hypothese auf und fragt sich, welche Experimente unternommen werden müssen, um die Theorie zu stützen oder zu falsifizieren. Dabei ging er pragmatisch vor: Hypothesen konnten frei eingeführt werden, wichtig war nur, wie erfolgreich sie in der Naturerklärung waren. Später beschäftigte er sich mit der Naturphilosophie und der prästabilierten Harmonie von Gottfried Wilhelm Leibniz. Da das Denken des Menschen ein Bild des Denkens Gottes sei und Gott das Universum geschaffen habe, sollte nach Leibniz des Menschen Geist imstande sein, das Universum in reinen Denkakten zu verstehen: Sein und Denkgesetze sollten also einander entsprechen. Einheit der Wissenschaft sollte die Widerspiegelung des göttlichen Geistes sein. Ampère strebte danach, alle Wissenschaften zu klassifizieren, und veröffentlichte darüber 1834 ein Buch. Unter den 64 Disziplinen waren auch einige neu von ihm eingeführt worden, wie die technische Kinematik und Kybernetik.

In der Mathematik ist die Monge-Ampèresche Gleichung nach ihm benannt, eine nichtlineare partielle Differentialgleichung zweiter Ordnung, die in der Differentialgeometrie und bei Transportproblemen Anwendung findet und mit der sich Ampère um 1820 befasste (und davor Gaspard Monge).

Im Frühherbst 1820 wurde Ampère, der nun schon 45 Jahre alt war und dessen bisherige wissenschaftliche Arbeiten höchstens als Fußnoten in Lehrbüchern erschienen wären, durch François Arago auf die Versuche Hans Christian Ørsteds zur Ablenkung einer Magnetnadel durch den elektrischen Strom aufmerksam. Ampère wiederholte den Versuch und erkannte, dass Ørsted die Ablenkung des Magneten durch das Erdmagnetfeld nicht beachtet hatte. Mit einer verbesserten Versuchsanordnung konnte Ampère nun feststellen, dass sich die Magnetnadel immer senkrecht zum stromdurchflossenen Leiter stellte. Ampère nahm nun als Modellhypothese an, dass jeder Magnetismus seine Ursache in elektrischen Strömen habe und Ströme Magnetfelder erzeugen. Er überprüfte seine Hypothese – hypothetisch-deduktiv – zwischen dem 18. September und dem 2. November 1820 und konnte in aufeinanderfolgenden Versuchen nachweisen, dass zwei stromdurchflossene Leiter eine Anziehungskraft aufeinander ausüben, wenn in beiden Leitern die Elektrische Stromrichtung gleich ist, und dass sie eine Abstoßungskraft aufeinander ausüben, wenn die Stromrichtung entgegengesetzt ist. Ampère konstruierte ein Gerät zur Messung des Stroms, das er Galvanometer nannte (unabhängig von Ampère tat dies Johann Schweigger in Deutschland). Ampère verfeinerte seine Hypothese, indem er annahm, dass jeder Magnet viele Moleküle enthält, die jeweils einen kleinen Kreisstrom erzeugen (sog. Ampèresche Molekularströme zur Erklärung des Magnetismus). Er erkannte, dass die fließende Elektrizität die eigentliche Ursache des Magnetismus ist.

Im Jahr 1822 beschäftigte sich Ampère mit der Kraft zwischen zwei nahe beieinander liegenden stromdurchflossenen Leitern. Er konnte zeigen, dass diese Kraft zu dem Kehrwert des Abstands proportional ist. Bei der mathematischen Behandlung dieser Phänomene nahm er sich das Gravitationsgesetz (als Punkt-Kraft-Gesetz) von Isaac Newton zum Vorbild. Da der Strom jedoch als gerichtete Größe behandelt werden muss und die Stromstärke die Zeit als neue Größe enthält, hat das ampèresche Modell nur eine beschränkte Gültigkeit.

Ampère erklärte den Begriff der elektrischen Spannung und des elektrischen Stromes und setzte die Stromrichtung fest.

Neben der Begründung der Elektrodynamik erkannte Ampère das Prinzip der elektrischen Telegrafie (Vorschlag eines elektromagnetischen Telegraphen mit Jacques Babinet 1822), der aber über größere Entfernungen wenig praktikabel war. Erstmals realisiert wurde ein elektrischer Telegraph 1833 von Carl Friedrich Gauß und Wilhelm Eduard Weber in Göttingen.

Ampère glaubte, dass das Erdmagnetfeld durch starke elektrische Ströme ausgelöst wird, die in der Erdrinde von Osten nach Westen fließen.

James Clerk Maxwell zählte Ampères Arbeiten über Elektrodynamik in seinem "Treatise on electricity and magnetism" zu den "glänzendsten Taten, die je in der Wissenschaft vollbracht worden sind". "Seine Schrift ist in der Form vollendet, in der Präcision des Ausdrucks unerreichbar und ihre Bilance besteht aus einer Formel, aus der man alle Phänomene, die die Electricität bietet, abzuleiten vermag, und die in allen Zeiten als Cardinal-Formel der Electrodynamik bestehen bleiben wird." Gleichzeitig vermutete Maxwell, dass der "Newton der Electricität", wie ihn Maxwell nennt, seine Theorie nicht allein aus induktiven Schlussfolgerungen (aus dem Experiment) erhalten habe, sondern einem anderen Weg folgte und dann "vom Gerüst, das ihm zur Aufrichtung seines Gebäudes diente, alle Spuren entfernt hat".

Ampères Charakter war von großer Liebenswürdigkeit und Sensibilität geprägt. Er neigte aber auch zu Überschwang und zur Melancholie, verstärkt durch mehrere Schicksalsschläge, zur Unentschlossenheit und einer gewissen Hilflosigkeit in Alltagsdingen und seine Zerstreutheit war sprichwörtlich. In seiner wissenschaftlichen Arbeit war er von großer Beharrlichkeit, folgte aber im Allgemeinen keinem systematischen Plan, sondern folgte einem Geistesblitz fieberhaft bis zu dessen Ausarbeitung. Ampère hatte eine Neigung zu metaphysischen Spekulationen und war tief religiös.

Zu Ehren Ampères ist die SI-Einheit des elektrischen Stromes „Ampere“ (Einheitenzeichen A) benannt worden. Er wurde durch Namensnennung auf dem Eiffelturm geehrt. Nach ihm ist seit 1935 ein Mondberg, der Mons Ampère, benannt. 1814 wurde er Mitglied der Académie des sciences in Paris und 1822 Fellow der Royal Society of Edinburgh. Ab 1827 war er korrespondierendes Mitglied der Preußischen Akademie der Wissenschaften sowie auswärtiges Mitglied der Royal Society. Im Dezember 1830 wurde er zum Ehrenmitglied der Russischen Akademie der Wissenschaften in St. Petersburg gewählt.

Nach ihm ist die Pflanzengattung "Amperea" aus der Familie der Wolfsmilchgewächse (Euphorbiaceae) benannt. 


Briefe:




</doc>
<doc id="23727" url="https://de.wikipedia.org/wiki?curid=23727" title="Pion">
Pion

Pionen oder formula_1-Mesonen (früher auch als Yukawa-Teilchen bezeichnet, da von Hideki Yukawa vorhergesagt) sind die leichtesten Mesonen. Sie enthalten nach dem Standardmodell der Teilchenphysik zwei Valenzquarks und werden daher heute meist nicht mehr als Elementarteilchen angesehen. Wie alle Mesonen sind sie Bosonen, haben also einen ganzzahligen Spin. Ihre Parität ist negativ.

Es gibt ein neutrales Pion formula_2 und zwei geladene Pionen: formula_3 und sein Antiteilchen formula_4. Alle drei sind instabil und zerfallen durch schwache oder elektromagnetische Wechselwirkung.

Das formula_3 ist eine Kombination aus einem up-Quark formula_6 und einem Anti-down-Quark formula_7 (Antiquarks werden durch einen Überstrich gekennzeichnet):
sein Antiteilchen formula_4 eine Kombination aus einem down-Quark formula_10 und einem Anti-up-Quark formula_11:

Beide haben eine Masse von 139,6 MeV/c². Die derzeit genauesten Messungen seiner Masse basieren auf Röntgenübergängen in exotischen Atomen, die statt eines Elektrons ein formula_4 besitzen. Die Lebensdauer des formula_14 beträgt 2,6 · 10 s.

Das formula_2 ist ein quantenmechanischer Überlagerungszustand einer formula_16- und einer formula_17-Kombination, d. h. zweier Quarkonia. Es gilt:

während der dazu orthogonale Zustand, formula_19, mit formula_20 zu den Eta-Mesonen mischt.

Seine Masse ist mit 135,0 MeV/c² nur geringfügig kleiner als die der geladenen Pionen.
Da es über die wesentlich stärkere elektromagnetische Wechselwirkung zerfällt, ist seine Lebensdauer mit 8,5 · 10 s etwa 10 Größenordnungen geringer.

Aufgrund einer frei wählbaren Phase können die drei Wellenfunktionen auch in der seltener verwendeten Form formula_8, formula_22 und formula_23 geschrieben werden. Dies entspricht dann der Condon-Shortley-Konvention.

Die unterschiedlichen Lebensdauern sind durch die unterschiedlichen Zerfallskanäle begründet:

die geladenen Pionen zerfallen zu 99,98770(4) % durch die Schwache Wechselwirkung in ein Myon und ein Myon-Neutrino:
Der eigentlich energetisch günstigere Zerfall in ein Elektron und das dazugehörige Elektron-Neutrino ist aus Helizitätsgründen stark unterdrückt.

Dagegen findet der Zerfall des neutralen Pions mittels der stärkeren und damit schnelleren elektromagnetischen Wechselwirkung statt. Endprodukte sind hier in der Regel zwei Photonen formula_26

mit einer Wahrscheinlichkeit von 98,823(32) % oder ein Positron e, ein Elektron e und ein Photon

mit einer Wahrscheinlichkeit von 1,174(35) %.

Wegen seiner kurzen Lebensdauer von 8,5 · 10 s wird das neutrale Pion in Experimenten durch Beobachtung der beiden Zerfallsphotonen in Koinzidenz nachgewiesen.

Das Pion wurde schon 1934/35 als Austauschteilchen der Kernkraft von Hideki Yukawa in Japan vorhergesagt, der dafür 1949 mit dem Nobelpreis ausgezeichnet wurde. Das erste ‚Meson‘, zunächst für das Yukawa-Teilchen gehalten und später als Myon bezeichnet, fanden Carl D. Anderson und Seth Neddermeyer 1936 in der Höhenstrahlung („Meson“ war damals die Bezeichnung für jedes geladene Teilchen schwerer als ein Elektron, aber leichter als ein Proton). Die Abgrenzung zum Pion schälte sich erst in den 1940er Jahren heraus (zuerst postuliert von Y. Tanikawa und Shoichi Sakata in Japan 1942). Cecil Powell, Giuseppe Occhialini und César Lattes am H. H. Wills Physical Laboratory in Bristol entdeckten 1947 in der Höhenstrahlung neben Myonen Pionen und untersuchten ihre Eigenschaften. Powell erhielt dafür 1950 den Nobelpreis für Physik. Das formula_4 war allerdings, wie erst später bekannt wurde, 1947 schon etwas früher von Donald H. Perkins in der Höhenstrahlung entdeckt worden. 1948 wurden Pionen erstmals künstlich in Beschleunigern nachgewiesen (Lattes).

Beim Vergleich der Massen der Pionen, die jeweils aus zwei Quarks bestehen (Mesonen), mit den Massen des Protons und des Neutrons (der Nukleonen), die beide aus jeweils drei Quarks bestehen (Baryonen), fällt auf, dass Proton und Neutron jeweils weit über 50 % schwerer sind als die Pionen; die Protonenmasse ist fast siebenmal so groß wie die Pionenmasse. Die Masse eines Protons oder eines Neutrons ergibt sich also "nicht" durch bloßes Addieren der Massen ihrer drei Stromquarks, sondern zusätzlich durch die Anwesenheit der für die Bindung der Quarks zuständigen Gluonen und der sogenannten Seequarks. Diese virtuellen Quark-Antiquark-Paare entstehen und vergehen im Nukleon in den Grenzen der Heisenbergschen Unschärferelation und tragen zur beobachteten Konstituentenquarkmasse bei.

Eine Erklärung für die weitaus geringere Masse liefert das Goldstone-Theorem: Die Pionen sind die Quasi-Goldstone-Bosonen der spontan (und darüber hinaus explizit) gebrochenen chiralen Symmetrie in der Quantenchromodynamik.

Die Pionen können die Rolle der Austauschteilchen übernehmen in einer effektiven Theorie der Starken Wechselwirkung (Sigma-Modell), die die Bindung der Nukleonen im Atomkern beschreibt. (Dies ist analog zu den Van-der-Waals-Kräften, die zwischen neutralen Molekülen wirken, jedoch selbst auch keine elementare Kraft sind; vielmehr liegt ihnen die elektromagnetische Wechselwirkung zu Grunde.)

Diese zuerst von Hideki Yukawa und Ernst Stueckelberg vorgeschlagene Theorie ist zwar nur innerhalb eines begrenzten Energiebereiches gültig, erlaubt darin aber einfachere Berechnungen und anschaulichere Darstellungen. Beispielsweise kann man die von den Pionen vermittelten Kernkräfte durch das Yukawa-Potential kompakt darstellen: dieses Potential hat bei kleinen Abständen abstoßenden Charakter (hauptsächlich über ω-Mesonen vermittelt), bei mittleren Abständen wirkt es stark anziehend (aufgrund von 2-Mesonen-Austausch, analog zum 2-Photonen-Austausch der Van-der-Waals-Kräfte), und bei großen Abständen zeigt es exponentiell abklingenden Charakter (Austausch einzelner Mesonen).

In diesem Austauschmodell folgt die endliche Reichweite der Wechselwirkung zwischen den Nukleonen aus der von Null verschiedenen Masse der Pionen. Die maximale Reichweite formula_30 der Wechselwirkung kann abgeschätzt werden über
Sie liegt in der Größenordnung der Compton-Wellenlänge des Austauschteilens. Im Fall der Pionen kommt man auf Werte von wenigen Fermi (10 m). Diese im Vergleich zur Kernausdehnung kurze Reichweite spiegelt sich in der konstanten Bindungsenergie pro Nukleon wider, die wiederum Grundlage für das Tröpfchenmodell darstellt.

Als Beispiel soll der Austausch eines geladenes Pions zwischen einem Proton und einem Neutron beschrieben werden:




</doc>
<doc id="31733" url="https://de.wikipedia.org/wiki?curid=31733" title="UV/VIS-Spektroskopie">
UV/VIS-Spektroskopie

Die UV/VIS-Spektroskopie ist ein zur optischen Molekülspektroskopie gehörendes spektroskopisches Verfahren, das elektromagnetische Wellen des ultravioletten (UV) und des sichtbaren (, VIS) Lichts nutzt. Die Methode ist auch unter UV/VIS-Spektralphotometrie oder als Elektronenabsorptionsspektroskopie bekannt. Im Alltag werden die verwendeten Geräte häufig ungenau als Photometer bezeichnet.

Lichtabsorption im Bereich der sichtbaren und ultravioletten Strahlung wird durch Elektronenübergänge zwischen verschiedenen Zuständen im Molekül verursacht. Bei diesen Übergängen werden Valenzelektronen (beispielsweise die der p- und d-Orbitalen der äußeren Schalen) angeregt, das heißt, in ein höheres Energieniveau angehoben.

Um ein Elektron beispielsweise von einem besetzten (HOMO) auf ein unbesetztes, höheres Orbital (LUMO) anzuheben, muss die Energie des absorbierten Photons genau der Energiedifferenz zwischen den beiden Energieniveaus entsprechen. Über den Zusammenhang

kann die Wellenlänge des absorbierten Lichts für die aufzuwendende Energie berechnet werden, wobei formula_2 die Energie, formula_3 das plancksche Wirkungsquantum, formula_4 die Lichtgeschwindigkeit, formula_5 die Frequenz und formula_6 die Wellenlänge der elektromagnetischen Welle sind. Dieser Zusammenhang wird manchmal auch als Einstein-Bohr-Gleichung bezeichnet.
Näherungsweise lässt sich dieser Zusammenhang in Form einer zugeschnittenen Größengleichung vereinfacht darstellen:

Stoffe, die nur im UV-Bereich (formula_8 < 400 nm) absorbieren, erscheinen dem menschlichen Auge farblos. Einen Stoff nennt man dann farbig, wenn er Strahlung im Bereich des sichtbaren Spektrums absorbiert. Dies ist sowohl bei Verbindungen zu erwarten, die über niedrige Anregungsenergien verfügen (π-zu-π-Übergänge, konjugierten π-Elektronensystemen wie beispielsweise bei den Polyenen), als auch bei anorganischen Ionenkomplexen mit unvollständig aufgefüllten Elektronenniveaus (Beispiel: Cu-Verbindungen (meist bläulich - grünlich) gegenüber farblosen Cu-Verbindungen). Ebenfalls erscheinen Verbindungen farbig, wenn stark polarisierende Wechselwirkungen zwischen benachbarten Teilchen bestehen, wie es z. B. beim gelben AgI der Fall ist. Bei nur einem Absorptionsgebiet nimmt das Auge die zur absorbierten Strahlung komplementäre Farbe wahr.

Grundsätzlich wertet man überwiegend Erscheinungen der Strahlungsabsorption im Rahmen der UV/VIS-Spektroskopie aus. Im Grundaufbau strahlt eine Lichtquelle elektromagnetische Strahlung aus, die über einen Strahlengang mit Spiegeln und weiteren Bauelementen (Details weiter unten) durch die Probe / den Analyten geleitet wird und dann auf einen Detektor trifft. Durch Anregung von Elektronen in der Probe ist die Intensität der Strahlung gegenüber dem originalen Primärstrahl in entsprechenden Bereichen geschwächt. Diese Differenz in der Strahlungsintensität wird gegen die jeweilige Wellenlänge, bei der gemessen wurde, aufgetragen und als Spektrum ausgegeben.

Die Lichtquelle strahlt ultraviolettes, sichtbares und nahinfrarotes Licht im Wellenlängenbereich von etwa 200 nm bis 1100 nm aus. Im Monochromator wird zunächst die zur Messung ausgewählte Wellenlänge selektiert, worauf der Lichtstrahl auf den Sektorspiegel fällt. Der Sektorspiegel lässt das Licht abwechselnd durch die Messlösung und durch die Vergleichslösung fallen. Beide Lösungen befinden sich in sogenannten Küvetten. Die zwei Lichtstrahlen werden im Detektor empfangen und die Intensitäten im Verstärker verglichen. Der Verstärker passt dann die Intensität des Lichtstrahls aus der Vergleichslösung durch Einfahren der Kammblende der Intensität des Lichtstrahls aus der Messlösung an. Diese Bewegung wird auf einen Schreiber übertragen oder die Messwerte an eine Datenverarbeitung weitergegeben.

Zunehmend werden küvettenfreie UV/VIS-Spektrometer zur Konzentrationsbestimmung kleiner Probevolumina (unter 2 Mikroliter) von Proben höherer Konzentrationen eingesetzt. Sogenannte Nanophotometer arbeiten mit Schichtdicken in Bereichen von 0,04 mm bis 2 mm. Sie benötigen keine Küvetten, keine Verdünnungen und können Proben mit einem Volumen von nur 0,3 µl analysieren (bei kleinster Schichtdicke), besitzen jedoch aufgrund der geringen Schichtdicke eine höhere Nachweisgrenze. Eine bewährte Technik basiert auf einer Kompression der Probe, welche somit unabhängig von der Oberflächenspannung und Verdunstung der Probe ist. Diese Methode findet Verwendung bei der Analyse von Nukleinsäuren (DNA, RNA, Oligonucleotide) und Proteinen (UV-Absorption bei 280 nm). Nach dem Lambert-Beer’schen Gesetz besteht ein Zusammenhang zwischen Absorption und Schichtdicke. Die Absorptionswerte bei den verschiedenen Schichtdicken (0,04 mm bis 2 mm) können somit berechnet werden. Geringe Schichtdicken wirken wie eine virtuelle Verdünnung der Probe, können jedoch nur bei entsprechend höheren Konzentrationen eingesetzt werden. Oftmals kann daher auf eine manuelle Verdünnung der Probe ganz verzichtet werden.

Eine weitere Technologie ist die Diodenarray-Technologie. Die Probe in der Küvette wird mit einem Lichtstrahl bestrahlt, bestehend aus dem kontinuierlichen Wellenlängenbereich der Lichtquelle (z. B. Xenonblitzlichtlampe, 190 nm bis 1100 nm). Die Probe absorbiert bei einer Messung unterschiedliche Wellenlängen der Lichtquelle. Nicht absorbiertes Licht gelangt durch den Eintrittsspalt und wird an einem Beugungsgitter nach seiner Wellenlänge aufgespalten. Das Spektrum wird mithilfe eines CCD-Sensors detektiert und anschließend ausgewertet. Bei nicht automatisierten Geräten muss die Referenzprobe zusätzlich gemessen werden. Vorteile der Technologie sind kurze Messzeiten, da das gesamte UV/VIS Spektrum mit einer Messung aufgenommen werden kann, ein niedriger Wartungsaufwand, da keine beweglichen Bauteile im Spektrometer vorhanden sind und dass die Geräte kompakt konstruiert werden können.

Nützlich sind die π-zu-π-Übergänge bei ungesättigten Kohlenwasserstoffen (beispielsweise Alkenen).
Sie erfolgen über längerwelliges UV-Licht und sind einfach zu messen.
Man erhält Informationen über die absorbierende Wellenlänge des Moleküls, die Struktur und Farbe. Dabei erfolgt die Lichtabsorption im umso längerwelligen Bereich, je größer die Anzahl der konjugierten Doppelbindungen ist. Liegt die Energie der π-zu-π-Übergänge im Bereich des sichtbaren Lichts, so erscheint das Molekül farbig. Dabei nimmt es immer die Komplementärfarbe des absorbierten Lichts an.

Bei den betrachteten Elektronenübergängen sind stets die folgenden Auswahlregeln zu beachten (u. a. Laporte-Regel):


Bei dem Laporte-Verbot müssen nach der Ligandenfeldtheorie zwei Abfragen getätigt werden.
Bsp.:

Achtung: Verboten heißt nicht, dass diese Übergänge nicht vorkommen! Die schwache Farbe von Komplexen entsteht durch Schwingungen der Liganden relativ zum Metallzentrum. Dadurch wird die beim Laporte-Verbot wichtige Inversionssymmetrie kurzzeitig aufgehoben und ein Übergang kann stattfinden.




</doc>
<doc id="78922" url="https://de.wikipedia.org/wiki?curid=78922" title="Seifenblase">
Seifenblase

Eine Seifenblase ist ein dünner Film Seifenwasser, der eine hohle Kugel mit schillernder Oberfläche formt. Sie bleibt gewöhnlich nur für wenige Momente stabil und reagiert empfindlich auf die Berührung mit festen Objekten.

Wegen dieser leichten Vergänglichkeit wurde ,Seifenblase‘ zu einer Metapher für etwas, das zwar anziehend, aber dennoch inhalts- und gehaltlos ist. Dies spiegelt sich zum Beispiel in der Redewendung „Der Traum zerplatzte wie eine Seifenblase“ oder im Synonym ,Seifenblasenwirtschaft‘ für "Bubble Economy".

In der Kunst wird spätestens seit dem Barock die Seifenblase durchgängig ikonographisch als ein Vanitassymbol benutzt und spiegelt sowohl die Schönheit als auch die Flüchtigkeit des menschlichen Lebens wider.

Seifenblasen lösen auf physikalische Weise komplexe räumliche Probleme in der Mathematik, da sie im Gleichgewichtszustand die kleinste Oberfläche zwischen Punkten und Kanten bilden.

Seifenblasen bestehen aus einem dünnen (dipolaren) Wasserfilm, an dem sich innen und außen Seifenmoleküle anlagern mit einer dem Wasser zugewandten polaren, hydrophilen Carboxylat-Gruppe und einem dem Wasser abgewandten unpolaren, hydrophoben Alkylrest.

Der Aufbau ähnelt dem von Biomembranen, jedoch befindet sich bei Seifenblasen das Wasser innerhalb der Membran, nicht außerhalb.

Eine Seifenblase entsteht, wenn sich ein dünner Wasserfilm mit Seifenmolekülen vermischt. Beim Aufblasen entsteht eine Kugelform. Infolge des gravitationsbedingten Auslaufens ("Drainage") der zwischen den Seifenfilmoberflächen befindlichen Flüssigkeit dünnt eine Seifenblase in ihrem oberen Teil zunehmend aus. Man kann das beobachten, wenn man einen Seifenfilm auf eine Tassenöffnung zieht und dann senkrecht hält. Zudem erfolgt im Laufe des Auslaufprozesses eine Anreicherung von Seifenfilm-stabilisierenden Tensidmolekülen im unteren Bereich der Seifenblase, sodass deren obere Region infolge des relativen Mangels von an die Oberfläche adsorbierten Tensidmolekülen zusätzlich destabilisiert wird. Tatsächlich platzen die meisten Seifenblasen im oberen Teil. Das Verdunsten kann man behindern, indem man die Seifenblase oder einen Seifenfilm in ein Einmachglas „sperrt“. Dadurch verlängert sich die Lebensdauer der Blase erheblich.

Die Schichtdicke der Seifenblase lässt sich beobachten: Spiegelt die Oberfläche in bunten Interferenzfarben, ist die Schichtdicke vergleichbar mit der Wellenlänge des Lichts. Bei abnehmender Schichtdicke wird die Seifenhaut zunächst farblos und zum Schluss dunkel.

In der Schwerelosigkeit überleben Seifenblasen mit etwa einer Minute doppelt so lang wie auf der Erde. Die Blasenwand ist dicker und gleichmäßiger und übersteht auch einen Nadelstich.

Die Erzeugung von Seifenblasen ist möglich, da die Oberfläche einer Flüssigkeit – in diesem Falle des Wassers – eine Oberflächenspannung besitzt, die zu einem elastischen Verhalten der Oberfläche führt. Häufig wird angenommen, dass die Seife nötig ist, um die Oberflächenspannung des Wassers zu vergrößern. Das Gegenteil ist jedoch der Fall: Die Oberflächenspannung des Seifenwassers ist nur etwa ein Drittel so groß wie die des Wassers. Seifenblasen mit reinem Wasser zu machen ist so schwierig, weil die Oberflächenspannung zu hoch ist, wodurch die Blase sofort zerplatzt. Zusätzlich verlangsamt die Seife die Verdunstung, so dass die Blasen länger halten. Der Gasdruck in einer Seifenblase ist höher als der Druck außerhalb, siehe dazu unter Young-Laplace-Gleichung.

Die Oberflächenspannung ist ebenfalls der Grund für die kugelförmige Gestalt der Seifenblasen. Durch Minimierung der Oberfläche zwingt sie die Blase in diese Form, da von allen möglichen Formen zu einem gegebenen Volumen die Kugel die kleinste Oberfläche aufweist. Ohne äußere Kräfte (insbesondere Schwerkraft in Kombination mit Luftreibung) würden alle Blasen ideale Kugelform besitzen. Aufgrund ihres geringen Eigengewichts kommen Seifenblasen diesem Ideal in der Realität sehr nahe.

Wenn zwei Seifenblasen aufeinander treffen, wirken dieselben Prinzipien weiterhin, und die Blasen nehmen die Form mit der kleinsten Oberfläche an. Ihre gemeinsame Wand wölbt sich in die größere Blase hinein, da eine kleinere Seifenblase einen höheren Innendruck besitzt. Wenn beide Seifenblasen gleich groß sind, entsteht keine Wölbung, und die Trennwand ist flach.

Plateaus Regeln besagen, dass beim Zusammentreffen mehrerer Seifenblasen alle Winkel gleich groß sind. In einem Schaum mit vielen Blasen treffen immer jeweils drei Flächen in einem Winkel von 120° zusammen. Hierbei ist die Oberfläche gleichfalls minimal. Durch die gleiche Oberflächenspannung entsteht ein Kräftegleichgewicht. Jeweils vier Kanten treffen sich unter einem Winkel von etwa 109° 28′ 16″ in einem Knoten, auch als Vertex bezeichnet. Diese Regeln wurden im neunzehnten Jahrhundert aufgrund von experimentellen Untersuchungen vom belgischen Physiker Joseph Plateau aufgestellt.

Die schillernden Farben entstehen durch Interferenz von Lichtwellen an der dünnen Seifenhaut. Die Interferenz führt innerhalb eines bestimmten Betrachtungswinkels zur Auslöschung eines Teils des Farbspektrums. Der verbleibende Teil wird farbig wahrgenommen, da nur das komplette Farbspektrum weißes Licht ergibt.

Da die Wand einer Seifenblase eine gewisse Dicke hat, wird einfallendes Licht zweimal reflektiert – einmal an jeder Seite der Wand (siehe rechts).
Die leicht unterschiedlichen Weglängen der beiden Lichtstrahlen (und besondere Effekte an der äußeren Wand, s. u.) führen zu einem Gangunterschied zwischen ihnen. Wenn der Gangunterschied genau die Hälfte einer Wellenlänge beträgt, fallen die Wellentäler des einen Strahls mit den Wellenbergen des anderen zusammen (s. zweites Bild). In der Summe ergibt sich Null, also eine Auslöschung der entsprechenden Farbe. Dies nennt man destruktive Interferenz, im Gegensatz zur konstruktiven Interferenz, bei der sich die beiden Strahlen durch einen anderen Gangunterschied positiv überlagern (drittes Bild).

Die tatsächliche Farbe der Seifenblase (d. h. die Wellenlänge des ausgelöschten Lichtes, beziehungsweise die Länge des Gangunterschiedes), ist abhängig von der Dicke der Seifenhaut und des Beleuchtungswinkels der Oberfläche. Die Abhängigkeit von der Schichtdicke kann beobachtet werden, wenn die Seifenblase durch Verdunstung ausdünnt. Mit abnehmender Dicke werden jeweils andere Farben ausgelöscht. Letztlich, wenn die Dicke der Wand kleiner ist als die Hälfte der kleinsten Wellenlänge sichtbaren Lichts, löschen sich keine sichtbaren Lichtwellen gegenseitig aus und es können keine Komplementärfarben mehr beobachtet werden. In diesem Zustand ist die Seifenblasenwand dünner als zwei Zehntausendstel eines Millimeters. Bei noch kleinerer Schichtdicke kann man aufgrund anderer Effekte (s. u.) dunkle Flecken beobachten – sie wird wahrscheinlich im nächsten Moment zerplatzen.

Die Voraussetzung für Interferenzerscheinungen, die Kohärenz der Wellenzüge, ist wegen der Dünne der Schicht erfüllt. Zusätzlich zur unterschiedlichen geometrischen Weglänge trägt hier noch ein anderer Effekt zum Gangunterschied bei:

Die direkt an der Grenzfläche Luft-Seifenhaut (Punkt X im zweiten Bild) reflektierte Welle erfährt einen Phasensprung um formula_1 bzw. formula_2 während die Phase der transmittierten Welle auch nach der Reflexion an der Grenzfläche Seifenhaut-Luft (Punkt O im Schaubild) unverändert ist. Hier findet kein Phasensprung statt. Der gesamte Gangunterschied setzt sich aus den unterschiedlichen Weglängen und dem Phasensprung bei der Reflexion an der äußeren Grenzfläche zusammen.

Dies erklärt auch die Verdunkelung der Blase im unmittelbaren Moment vor dem Zerplatzen, wenn die Dicke der Seifenhaut auf einen sehr kleinen Wert gesunken ist: Dies liegt darin begründet, dass die transmittierte Welle, die zuvor den längeren Weg durch die Seifenhaut nahm, nun praktisch keine längere Distanz zurücklegt als die direkte reflektierte Welle und sich deshalb ihre Phase relativ zu dieser nicht ändert. Die reflektierte Welle hat allerdings den oben erwähnten Phasensprung erfahren was zur destruktiven Interferenz (Auslöschung) aller Wellen führt.

Hätte eine Seifenblase überall die gleiche Wandstärke, so würde der Gangunterschied nur durch den Beleuchtungswinkel definiert, und sie würde einen gleichmäßigen Farbverlauf zeigen. Da der Flüssigkeitsfilm in einer Seifenblase, die sich durch eine Luftströmung bewegt, jedoch durch Luftreibung verwirbelt wird, ist die Wandstärke nicht homogen. Unter günstigen Bedingungen kann man diese Verwirbelungen mit bloßem Auge sehen. Schwebt die Seifenblase aber relativ ruhig, treten nur wenige Verwirbelungen auf: Man kann einzelne relativ gleichmäßige Farbbänder beobachten. Die meistens vorhandenen Dickeschwankungen aufgrund der Gravitationskraft sind relativ gleichförmig und stören den gleichmäßigen Farbverlauf nicht prinzipiell.

In einem ebenen Seifenfilm sind diese Farben einfacher sichtbar zu machen. Solch ein ebener Film kann z. B. in einem rechteckigen oder kreisrunden Rahmen aus dünnen Polymer-Fasern oder dünnem Draht geformt werden. Optimale Bedingungen für die Sichtbarkeit der Interferenzfarben sind hier eine indirekte Beleuchtung (z. B. ein Blatt weißes Papier, das von einer Halogenlampe angestrahlt wird) mit 45 Grad Einfallswinkel und Beobachtung in Reflexion bei 45 Grad Ausfallswinkel. Der Hintergrund hinter dem Seifenfilm sollte dunkel sein.

An den Rändern bildet der Film einen Meniskus entweder mit dem Rahmen oder mit einem Flüssigkeits-Reservoir am unteren Ende des Films. In letzterem Fall ist eine Kombination aus Gravitation und Kapillarkraft die treibende Kraft, die eine inhomogene Filmdicke bewirkt.

Verwirbelungen und ästhetische bewegte Muster im Bereich des Meniskus und an den Rändern mit dem Rahmen kommen durch hydrodynamische Instabilitäten zustande, bei denen höchstwahrscheinlich der Marangoni-Effekt eine wichtige Rolle spielt.

Die Membran einer Seifenblase kann bei tiefen Temperaturen gefrieren, ohne zu zerplatzen. Das geschieht mit fliegenden Seifenblasen bei Temperaturen unter −10 °C im Freien oder mit anhaftenden Seifenblasen in der Gefriertruhe. Sie sind bis zu 10 Minuten stabil. Manchmal überstehen gefrorene Seifenblasen eine Landung auf hartem und kaltem Untergrund.

Das Gefrieren einer auf Schnee liegenden Seifenblase erfolgt typisch durch Wachsen von fiederartigen Kristallen ab dem als Keim wirkenden anliegenden Schnee und kann etwa 2 Sekunden dauern.

Seifenblasenshows verbinden Unterhaltung mit künstlerischer Leistung. Hohe Kunstfertigkeit ist dafür ebenso vonnöten wie perfekte Seifenblasenlösungen.

Beispiele üblicher Darbietungen:


Ein Seifenfilm formt eine natürliche Minimalfläche. Minimalflächen stehen schon seit dem 19. Jahrhundert im Blickpunkt mathematischer Forschung. Ein wesentlicher Beitrag dazu waren die Experimente des belgischen Physikers Joseph Plateau (vgl. Plateau-Problem).

Ein Beispiel: Schon 1884 wurde von Herrmann Amandus Schwarz bewiesen, dass eine kugelförmige Seifenblase die kleinstmögliche Oberfläche eines bestimmten Luftvolumens besitzt. Jedoch erst in den letzten Jahrzehnten wurde mit Hilfe der geometrischen Maßtheorie eine angemessene Sprache für solche Probleme gefunden. Im Jahr 2000 gelang Michael Hutchings, Frank Morgan, Manuel Ritoré und Antonio Ros der Beweis, dass zwei verbundene Seifenblasen (eine sogenannte "Doppelblase") zwei verschieden große Luftvolumina mit der kleinstmöglichen Oberfläche umschließen (auch "Doppelblasen-Theorem"; ).

Lange Zeit waren Seifenblasen das einzige Mittel zur zuverlässigen Bestimmung der optimalen Neigung von nicht-trivialen Dachkonstruktionen auf Basis von Seilsystemen und Tragbögen. Dazu wurde die Konstruktion als Rahmen aus Draht geformt und dann in Seifenwasser getaucht. Beim vorsichtigen Herausziehen ergaben sich Kurvenverläufe, die als das experimentell gefundene Optimum der Form zu gelten hatten. Durch Fotografie und andere Methoden wurde das Ergebnis fixiert und auf die zugehörigen Konstruktionszeichnungen übertragen. Die jeweilige Statik für die vorgegebene Form ließ sich dann mit anderen Methoden bestimmen. Ein Beispiel dieser Methodik ist das Olympiagelände München.

Eine der frühesten künstlerischen Darstellungen von Seifenblasen als Kinderspielzeug findet sich in Pieter Bruegels Gemälde "Die Kinderspiele" von 1560, woraus sich schließen lässt, dass Seifenblasen bereits seit mindestens 500 Jahren von Kindern zum Zwecke der Unterhaltung verwendet werden. Die Massenproduktion von Seife begann im 19. Jahrhundert, wobei der Seifenhersteller "Pears" zur Vermarktung insbesondere auch John Everett Millais’ Gemälde "Seifenblasen" („Bubbles“) nutzte, das dessen Enkel beim Spiel mit Seifenblasen zeigt.

1948 entwickelte der Chemiker Rolf Hein eine neue Formel für ein Waschmittel, das allerdings den Nachteil hatte, zu sehr zu schäumen. Er ließ die flüssige Seife in Flaschen abfüllen, fügte eine Blasring aus einer zum Ring gebogenen feinen Federdrahtwendel mit Stiel hinzu und verkaufte das Produkt unter dem Markennamen "Pustefix" gezielt als Kinderspielzeug. Seitdem sind zur Herstellung von Seifenblasen vorwiegend Kombinationen von mit Lauge gefüllten Plastikröhrchen und Pusterohr im Gebrauch.




</doc>
<doc id="80202" url="https://de.wikipedia.org/wiki?curid=80202" title="Angewandte Physik">
Angewandte Physik

Die angewandte Physik steht in (unscharfer) Abgrenzung zur Experimentalphysik, teilweise auch zur theoretischen Physik. Ihr wesentliches Kennzeichen ist, dass sie ein gegebenes physikalisches Phänomen nicht um seiner selbst willen erforscht, sondern vor dem Hintergrund eines technischen oder interdisziplinären Problems. In diesem Sinne wird die angewandte Physik zwischen einer modernen Technologie auf der einen und der reinen Physik auf der anderen Seite angesehen.

Obwohl es an den meisten Universitäten innerhalb der Fachbereiche Physik ein oder mehrere "Institute für Angewandte Physik" gibt, existieren keine entsprechenden wissenschaftlichen Fachgruppen oder Vereinigungen. Die angewandte Physik ist in diesem Sinne keine physikalische Disziplin, sondern umfasst eine Vielzahl von Disziplinen innerhalb der Physik.

Foren für die angewandte Physik sind wissenschaftliche Journale wie das "Journal of Applied Physics" und die "Applied Physics Letters" des American Institute of Physics, das "Japanese Journal of Applied Physics" oder die "Applied Physics A" bzw. "B". Die dort behandelten Themen bieten einen Überblick über das Themenspektrum. 

Aufgeführte technologische Gebiete der angewandten Physik sind Halbleiterelemente, Photonik, Supraleitfähigkeit, magnetische Datenspeicherung, Plasmaphysik, Teilchenbeschleuniger, Nanotechnologie, angewandte Biowissenschaften, Materialwissenschaft und -bearbeitung sowie Laser und Optik.



</doc>
<doc id="109787" url="https://de.wikipedia.org/wiki?curid=109787" title="Wirbelstrom">
Wirbelstrom

Wirbelstrom nennt man einen Strom, der in einem ausgedehnten elektrischen Leiter in einem sich zeitlich ändernden Magnetfeld oder in einem bewegten Leiter in einem zeitlich konstanten, dafür räumlich inhomogenen Magnetfeld induziert wird. Der Name wurde gewählt, weil die Induktionsstromlinien wie Wirbel in sich geschlossen sind.

Wirbelströme erzeugen ihrerseits ein Magnetfeld, das gemäß der Lenzschen Regel der Änderung des Feldes entgegenwirkt. Dadurch wird bei hohen Frequenzen und großen Querschnitten der Strom aus der Mitte des Leiters verdrängt (Skin-Effekt).

Hat der Leiter einen endlichen elektrischen Widerstand, erwärmt er sich. Diesen Wirbelstromverlusten entspricht im Fall des sich zeitlich ändernden Magnetfeldes eine von 90° abweichende Phasenverschiebung zwischen Strom und Spannung in der anregenden Spule bzw. im Fall der Relativbewegung zwischen Feld und Leiter eine bremsende Kraft. Die Kraft skaliert linear, die Verlustleistung quadratisch mit der Frequenz bzw. der Geschwindigkeit, soweit der Skineffekt vernachlässigbar ist. Insbesondere verschwindet die Kraft, wenn die Relativgeschwindigkeit zwischen Feld und Leiter Null wird.

Eine genaue Berechnung der Stromverteilung und der wirkenden Kräfte erfordert das Lösen der Maxwellschen Gleichungen, für anwendungsnahe Geometrien durch numerische Verfahren.

Die Wirbelstromprüfung dient der zerstörungsfreien Materialprüfung und Materialcharakterisierung und beruht auf der Messung der Amplitude und Phase von Wirbelströmen.

Die Schirmwirkung nichtferromagnetischer metallischer Gehäuse gegen magnetische Wechselfelder beruht auf Wirbelströmen im Gehäuse, deren Magnetfeld die auftreffenden Felder teilweise kompensiert. Die Quantität der Schirmwirkung wird mit der Größe Schirmdämpfung erfasst. Sie ist gut, wenn die Wirbelströme wenig durch elektrischen Widerstand gedämpft werden.

Dämpfung der Ströme führt zu einer Phasenverschiebung gegenüber dem erzeugenden Feld. Beim Spaltpolmotor erzeugt eine Kurzschlusswindung um den Spaltpol ein verzögertes Feld quer zum Hauptfeld und mit diesem zusammen ein Drehfeld. Bei mit Wechselstrom betriebenen Schützen und Zugmagneten verhindert eine Kurzschlusswindung um einen Teil des Ankers, dass die Zugkraft periodisch Null wird. Strenggenommen ist der Strom in der Kurzschlusswindung kein Wirbelstrom, da ihm sein Weg durch die Form des Leiters vorgegeben ist.

Ein schnell zunehmendes inhomogenes Magnetfeld stößt geschlossene gute Leiter ab. Das wird zum Magnetumformen und dem Elektromagnetischen Pulsschweißen und beim Gaußgewehr benutzt.

Die bremsende Kraft zwischen einem bewegten geschlossenen Leiter und einem Magnetfeld wird ausgenutzt bei der Wirbelstrombremse von Bahnfahrzeugen, bei Frei-Fall-Türmen, Fahrrad-Ergometern und zur Dämpfung mechanischer Schwingungen beim Rastertunnelmikroskop und in Drehspulmesswerken.

Auch wenn sich das Magnetfeld gegenüber dem Leiter bewegt, entsteht eine Kraft und der Leiter wird ggf. bewegt. Auch hier ist die Kraft proportional zur Differenzgeschwindigkeit. In Asynchronmotoren, die auch als Linearmotor ausgeführt sein können, wird das Dreh- bzw. Wanderfeld elektromagnetisch erzeugt. In mechanischen Tachometern und Drehzahlmessern lenkt ein rotierender Permanentmagnet eine Aluminiumscheibe gegen eine Federkraft aus. Ähnlich arbeiten Wirbelstromabscheider, mit denen z. B. Aludosen oder -folien aus Abfällen abgetrennt werden.

In Ferraris-Stromzählern wirken Wirbelströme sowohl antreibend als auch dämpfend: Ein aus den zu messenden Größen Strom und Spannung gebildetes magnetisches Wanderfeld treibt eine Aluminiumscheibe an, deren Drehung von einem Permanentmagneten stark gedämpft wird.

Induktives Erwärmen von Metall (z. B. in Schmelzöfen, Kochgeschirr auf Induktionskochfeldern, Werkstücke zum induktiven Härten oder Thermischem Aufschrumpfen) nutzt ein magnetisches Wechselfeld. Auch das Getter von Elektronenröhren wird, in einem kleinen Blechnapf, induktiv durch den Glaskolben hindurch erhitzt, um es zu verdampfen. Notwendig ist, dass der zu erwärmende Metallteil zumindest flächig ausgedehnt ist und im zu erhitzenden Bereich ausreichend homogen elektrisch leitfähig ist. 

Oft sind die mit der Erwärmung verbundenen Verluste eine unerwünschte Begleiterscheinung beim Einsatz magnetischer Wechselfelder. Als Maßnahme gegen die Wirbelstromverluste (andere siehe unter Eisenverluste) werden Eisenkerne von Transformatoren und Elektromotoren nicht massiv ausgeführt, sondern „geblecht“. Die Pakete aus meist lackisolierten Elektroblechen werden dabei parallel zu den magnetischen Feldlinien orientiert, sodass die möglichen großen Wirbelstrombahnen unterbrochen sind (siehe Abbildung); lediglich kleinere Wirbelströme in den einzelnen Blechen können sich ausbilden, deren Verlustleistung aber gering ist. Bei hohen Frequenzen werden für Transformatoren und Drosseln Ferrite oder Pulverkerne eingesetzt. 

Leiter für Öl-Transformatoren hoher Leistung werden als Drillleiter (Röbeldraht) ausgeführt, im Querschnitt aufgebaut aus zwei nebeneinander liegenden Stapeln isolierter Flachdrähte (reduziert die Wirbelstromverluste im Kupfer), die laufend umgestapelt, transponiert, also sukzessive verdrillt werden, was die Flexibilität zum Wickeln schafft. Die Magnetwicklungen in Fehlerstrom- und Leitungsschutzschaltern sind mitunter ebenfalls aus Bündeln isolierter Einzelleiter aufgebaut.

Leiter für höherfrequente Wechselströme bestehen zur Vermeidung der Stromverdrängung (Skin-Effekt) aus verflochtenen, isolierten, parallel geschalteten Einzelleitern (HF-Litze).




</doc>
<doc id="135838" url="https://de.wikipedia.org/wiki?curid=135838" title="Isenthalpe Zustandsänderung">
Isenthalpe Zustandsänderung

In der Thermodynamik wird eine Zustandsänderung als isenthalp bezeichnet, bei der sich die Enthalpie formula_1 nicht ändert:

mit

Ein Beispiel für einen isenthalpen Prozess ist die Expansion eines Gases durch ein Drosselventil, siehe Joule-Thomson-Effekt.

Nach der Zustandsgleichung eines idealen Gases gilt

Für ideale Gase sind die isenthalphen Prozesse also gerade die Isothermen.

Ein isothermer reversibler Prozess ist immer auch isenthalp, die Umkehrung gilt aber nicht.

Auch die Linien gleicher Enthalpie im Zustandsdiagramm werden Isenthalpen (oder Drosselkurven) genannt.


</doc>
<doc id="195617" url="https://de.wikipedia.org/wiki?curid=195617" title="Population (Astronomie)">
Population (Astronomie)

In der Astronomie wird mit Population oder Sternpopulation () eine Untermenge von Sternen in einer Galaxie bezeichnet, die eine ähnlich große Metallizität (ein ähnliches Alter) aufweisen.

Die Klassifikation geht auf Walter Baade (1944) zurück. Sie ist nützlich für die Beschreibung von Spiralgalaxien wie der Milchstraße, auch wenn das heutige Bild dieser Objekte wesentlich komplexer ist. Obschon der Zeitpunkt der Entstehung ein Charakteristikum einer Population ist, korrespondiert die sie bezeichnende römische Zahl (I, II oder III) "nicht" mit der Reihenfolge ihrer Entstehung bzw. ist genau umgekehrt.


Heute unterscheidet man aufgrund genauerer Messergebnisse im Wesentlichen fünf Populationen.

Das Alter dieser Sterne liegt unter 100 Mio. Jahren. Solche jungen Sterne befinden sich häufig in Spiralarmen und irregulären Galaxien und dort in diffusen Nebeln, Reflexionsnebeln, offenen Sternhaufen und Sternassoziationen.

Sterne dieser Population, zu der auch die Sonne gehört, haben bis auf ihr fortgeschritteneres Alter ähnliche Charakteristika wie die der extremen Population I.

Sterne dieser größten Population haben ein mittleres Alter. Die meisten Sterne unserer Milchstraße gehören ihr an, denn sie stellen den Großteil der galaktischen Scheibe und des galaktischen Zentrums dar.

Dies ist eine kleinere Gruppe, die vor allem im galaktischen Zentrum vorherrscht. Zu dieser Gruppe zählen vor allem die Schnellläufer, die sich mit Geschwindigkeiten von über dreißig Kilometern pro Sekunde senkrecht zur galaktischen Ebene bewegen.

Dies sind Sterne mit einem Alter von über sechs Milliarden Jahren. Sie befinden sich überwiegend in Kugelsternhaufen und elliptischen Galaxien. Sehr wichtige Mitglieder sind die Unterzwerge und die RR-Lyrae-Sterne.

HD 140283 ist ein Stern im Sternbild Libra. Er ist einer der nächstgelegenen Sterne der Population II. Mithilfe des Hubble-Weltraumteleskops wurde die Entfernung zu ihm auf 190 Lichtjahre bestimmt und das Alter – mit einer Genauigkeit von ± 800 Millionen Jahren – auf etwa 14,5 Milliarden Jahre abgeschätzt.

Davor war der älteste Stern der 2014 entdeckte (Sky Mapper Telescope, Siding-Spring-Observatorium, Australian National University, und danach Magellan), SMSS J031300.36-670839.3 (SM0313) mit einem Alter von 13,6 Milliarden Jahren, entstanden also nur rund 100 bis 200 Millionen Jahren nach dem Urknall. Er befindet sich nur 6000 Lichtjahre entfernt in unserer Galaxie. Er gehört zur Population II, sein Alter ist daran erkennbar, dass er kaum Eisen enthält (weniger als ein Zehnmillionstel der Konzentration in der Sonne) und nur sehr wenig andere schwere Elemente, nur Wasserstoff, Helium, etwas Kohlenstoff, Lithium, Magnesium und Calcium.

Da er aus Supernovaüberresten der ersten Sterngeneration entstand (die bisher nicht beobachtete Population III), hatten Astronomen eigentlich einen höheren Eisenanteil erwartet und werten das als Hinweis darauf, dass die ersten Supernovaexplosionen relativ wenig Energie freisetzten und der überwiegende Teil der schweren Elemente in den dabei entstandenen schwarzen Löchern verschwand. Aus dem relativ hohen Zinkanteil in HE 1327-2326 folgerten Anna Frebel und Kollegen 2019 jedoch, dass die ersten Sterne in sehr asymmetrischen Supernovaexplosionen ihr Ende fanden und auf diese Weise doch relativ große Mengen an schwereren Elementen verteilt worden sein konnten.



</doc>
<doc id="224429" url="https://de.wikipedia.org/wiki?curid=224429" title="DEMO">
DEMO

DEMO (DEMOnstration Power Plant) könnte das Nachfolgeprojekt zum im Bau befindlichen experimentellen Kernfusionsreaktor ITER werden. DEMO würde der Entwicklung und dem Test von Technologien, physikalischen Betriebsbereichen und Steuerungsalgorithmen dienen und soll einen sogenannten „geschlossenen Brennstoffkreislauf“, d. h. das Erbrüten des laufend benötigten Tritiums im Blanket des Reaktors, demonstrieren. Als Demonstrationskraftwerk würde DEMO alle Komponenten zur Stromerzeugung enthalten und bei 2 bis 4 Gigawatt Fusionsleistung frühestens ab 2040 oder 2050 1 bis 1,5 Gigawatt ins Netz einspeisen. Aufgrund der großen Verzögerungen bei ITER (Zündung eines DT-Plasmas nun erst 2035 geplant.) dürfte sich der Zeitplan allerdings entsprechend nach hinten schieben. Entwicklungsziele für DEMO sind auch genügende Verfügbarkeit und möglichst kompakte Abmessungen. DEMO wird als Forschungsprojekt noch nicht wirtschaftlich Strom produzieren, soll aber eine Abschätzung der Kosten kommerzieller Kraftwerke ermöglichen. Um wirtschaftlich zu arbeiten, müssten kommerzielle Fusionskraftwerke im letzten Viertel dieses Jahrhunderts voraussichtlich größer als DEMO sein.

Nach einem Ansatz würde sich die konkrete Planung und Konstruktion von DEMO auf etwa zehn Jahre Plasmaforschung mit ITER stützen, die 2025 beginnen soll. Ein anderer Ansatz (K-DEMO) ging zugunsten der Auslegung schon nach zwei Jahren ITER-Betrieb von dann noch unbestätigten, aber konservativeren Annahmen für die erreichbaren Parameter aus.

Auch ist noch nicht sicher, ob mit dem Tokamak-Prinzip ein dauerhaft brennendes Plasma wirtschaftlich möglich ist; DEMO könnte auch ein Nachfolger des Stellarators Wendelstein 7-X werden.

PROTO ist ein vorgeschlagener Kernfusionsreaktor, der aber nicht vor 2050 in Bau gehen kann und ein Nachfolgeprojekt des DEMO-Projekts wäre. Es gibt aber auch Überlegungen PROTO und DEMO zu kombinieren.



</doc>
<doc id="279026" url="https://de.wikipedia.org/wiki?curid=279026" title="Tonne (Einheit)">
Tonne (Einheit)

Mit Tonne werden unter anderem verschiedene Maßeinheiten bezeichnet, insbesondere für Massen. Während im deutschsprachigen Bereich mit der Masseneinheit „Tonne“ stets die „metrische Tonne“, also 1000 Kilogramm, gemeint ist, ist in Ländern vor allem des angelsächsischen Sprachraums die Unterscheidung von "metrischer Tonne", "Long ton" und "Short ton" wichtig.

Die Tonne (von ‚das Fass‘) oder auch metrische Tonne mit dem Einheitenzeichen „t“ ist eine Maßeinheit der Masse. Nach dem internationalen Einheitensystem entspricht eine Tonne 1000 Kilogramm (oder einer Million Gramm, also einem Megagramm). In Ländern, in denen das metrische System als Standard eingerichtet ist, wird gewöhnlicherweise einfach „Tonne“ gesprochen. In allen anderen Ländern, vor allem im angelsächsischen Sprachraum, spricht man von der „metrischen Tonne“ ("metric ton," abgekürzt "mT", nicht zu verwechseln mit Metertonne), um sie von der "long ton" und der "short ton" zu unterscheiden.

In manchen wissenschaftlichen Büchern und Zeitschriften, wie auch vereinzelt in amtlichen Bekanntmachungen, wird der Schritt von der Bezeichnung „Tonne“ hin zur Bezeichnung „Megagramm“ (Mg) vollzogen. Vor allem auf internationaler Ebene sollen damit Verwirrungen vermieden werden. Entsprechend wird in englischsprachiger Literatur die „Gigatonne“ (Gt) inzwischen manchmal auch durch „Petagramm“ (Pg) ersetzt.

Die Tonne gehört zwar nicht zum Internationalen Einheitensystem (SI), ist zum Gebrauch mit dem SI aber zugelassen. Dadurch ist sie eine gesetzliche Einheit.

Natürlich sind auch alle weiteren Vorsätze für Maßeinheiten denkbar, manche werden auch verwendet.

In der deutschen Landwirtschaft wurden früher häufig die Einheiten Zentner (Einheitenzeichen: ztr; entspricht 50 kg) und der daraus resultierende Doppelzentner (Einheitenzeichen: dz; entspricht 100 kg) verwendet. Der Doppelzentner ist in Deutschland durch die gesetzliche Einheit „Dezitonne“ (Einheitenzeichen: dt) abgelöst worden. Somit ist die alte Einheit dz massengleich der neuen Einheit dt (1 dz = 1 dt = 100 kg). Landwirtschaftliche Erträge sind in deutschen Büchern oft in „Dezitonnen pro Hektar“ (dt/ha) angegeben, international werden sie aber in der zehnmal so großen Einheit „Tonnen pro Hektar“ (t/ha) angegeben; diese wiederum wird in der jüngsten Literatur zunehmend durch die Bezeichnung „Megagramm pro Hektar“ (Mg/ha) ersetzt.

Es gilt: formula_1

Die sogenannte "Long ton" – von englisch ', wörtlich "lange Tonne"; auch „Britische Tonne“ genannt – ist eine Masseeinheit im Avoirdupois-Maßsystem. Das Einheitenzeichen der Long ton ist tn. l. (mit Leerzeichen). Die Long ton entspricht 20 ' und damit 2240 Angloamerikanischen Pfund (englisch "").

Im militärischen Bereich wird die Einheit häufig mit dem Einheitenzeichen "ts", von englisch "", verwendet. Besonders für die 1922 auf der Flottenkonferenz in Washington festgelegte Standardverdrängung eines Schiffs wird die long ton genutzt. Die festgelegten Größen der Standardverdrängung ergeben in tons runde Zahlen.

Der Ausdruck "long ton" wurde gebildet zur Unterscheidung von der "short ton".
Die sogenannte "Short ton" – von englisch ', wörtlich "kurze Tonne"; auch „Amerikanische Tonne“ genannt – ist eine Masseeinheit, die vor allem in den USA gebräuchlich ist. Das Einheitenzeichen ist: tn. sh. (mit Leerzeichen). Eine Short ton entspricht 20 ' und damit 2000 Angloamerikanischen Pfund (Pound).
Im Bergbau wird mit der sogenannten ' (kurz ', amerikanisch-englisch wörtlich für „Trocken-Metrische-Tonnen-Einheit“) gearbeitet. 1 DMTU entspricht einer metrischen Tonne Erz ohne Wasseranteil mit einem Metallgehalt von einem Prozent, was einem Metallgewicht von 10 Kilogramm entspricht. Wird beispielsweise der Preis eines 48-prozentigen Manganerzes mit 2,30 USD/DMTU angegeben, so wird das Erz mit 48 × 2,30 USD/Tonne gehandelt.

Bei der Angabe von Fertigungskapazitäten von Massenware, wie beispielsweise Stahl oder Chemikalien, wird anstelle von „Tonnen pro Jahr“ oft – in Anlehnung zum „Stundenkilometer“ – die Umschreibung „Jahrestonne(n)“, kurz „Jato“ oder „jato“ genutzt; das Jahr ist in Deutschland jedoch keine gesetzliche Einheit. Im Englischen ist die Abkürzung ‚‘ (für ‚‘ bzw. ‚‘) üblich.

Die Preußische Tonne war ein Volumen-, Raum- oder Hohlmaß für Schüttgüter wie Salz, Kalk, Gips, Steinkohlen, Holzkohle, Asche und ähnlich grobe Güter.
Beispiele:
In Riga wurde die Tonne ebenso für Schüttgüter genutzt, damit jedoch folgendermaßen gerechnet:

Die "Lüneburger Salztonne" war ein Gewichtsmaß gleichermaßen seit dem 13. Jahrhundert und durch kaiserlichen Erlass geschützt gewesen.

Eine Lüneburger Salztonne als Maß- und Normgefäß:

In Königsberg und Danzig war die Tonne ein Stückmaß im Handel mit Heringen.


Eine Besonderheit für das Maß Tonne ist aus Braunschweig, Hamburg und Bremen bekannt. Geführt wird diese Tonne unter zählbaren Maßen. Hier erhielt das Maß bei Butter den Zusatz kleines bzw. schmal (Kleinband) oder großes Band bzw. bucket (Großband), also:

In Schleswig-Holstein, Lübeck, Dänemark, Norwegen und Island existierten auch Tonnenbezeichnungen als Flächenmaß.

Die Registertonne ist ein Volumenmaß aus der Schiffsvermessung.


</doc>
<doc id="340037" url="https://de.wikipedia.org/wiki?curid=340037" title="Erste Quantisierung">
Erste Quantisierung

Die erste Quantisierung, auch kanonische Quantisierung genannt, ist ein "schematisches Vorgehen" zum Aufstellen einer quantenmechanischen Bewegungsgleichung für ein physikalisches System. Sie wurde erstmals – in zwei verschiedenen Formen – 1925 von Werner Heisenberg und 1926 von Erwin Schrödinger vorgestellt, die damit die moderne Quantenmechanik begründeten.

Die erste Quantisierung lässt sich in konkreten Fällen plausibel machen, indem man die Bewegung von Wellenpaketen für den klassischen Grenzfall formula_1 untersucht (formula_2: reduziertes Plancksches Wirkungsquantum).

Die Bezeichnung "erste Quantisierung" ist in ihrem Verhältnis zur "zweiten Quantisierung" begründet. Historisch war sie nicht der erste Versuch der Quantisierung in der modernen Physik (s. Quantisierung (Physik)).

Heisenberg und Schrödinger gehen davon aus, dass zunächst wie in der klassischen Physik die Hamiltonfunktion des Systems aufgestellt wird.

Nach Schrödinger werden dann Energie und Impulse durch Operatoren ersetzt, die auf einem Hilbertraum definiert sind:

Es ergibt sich eine Differentialgleichung für einen zeitveränderlichen Zustandsvektor, in dieser Darstellung eine Wellengleichung für die Wellenfunktion. Die stationären Lösungen der Differentialgleichung, die man für konstante Randbedingungen erhält, haben diskrete Eigenwerte für die Energie und einige weitere mechanische Größen.

Aus der klassischen Hamiltonfunktion formula_4 entsteht so die Schrödinger-Gleichung, aus einer relativistischen Hamiltonfunktion die Klein-Gordon-Gleichung für Bosonen oder die Dirac-Gleichung für Fermionen.

Vielleicht noch weniger anschaulich, mathematisch aber äquivalent, ist das von Heisenberg eingeführte Vorgehen, die klassischen Größen Ort "x" und Impuls "p" als Matrizen (formula_5) aufzufassen, die bestimmte Vertauschungsrelationen erfüllen müssen:


</doc>
<doc id="446034" url="https://de.wikipedia.org/wiki?curid=446034" title="Benedetta Ciardi">
Benedetta Ciardi

Benedetta Ciardi (* 7. Oktober 1971 in Florenz, Italien) ist eine italienische Astrophysikerin. Seit 2006 leitet sie eine Forschungsgruppe am Max-Planck-Institut für Astrophysik in Garching.

Am 7. November 2004 wurde ihr der Marie-Curie-Preis der Europäischen Kommission verliehen.

Sie untersuchte unter anderem den Effekt der Strahlung der ersten Sterne in der Frühphase des Universums (Reionisierungsphase) auf die Gaswolken bei der Galaxienentstehung. Dabei führte sie auch umfangreiche Computersimulationen durch.



</doc>
<doc id="456529" url="https://de.wikipedia.org/wiki?curid=456529" title="Diffusor">
Diffusor

Ein Diffusor ist ein Bauteil im Maschinen-, Elektrizitätswerks-, Ventilator-, Fahrzeug-, Flugzeug- und Schiffbau, das Gas-/Flüssigkeitsströmungen verlangsamt und den Gas-/Flüssigkeitsdruck erhöht. Es stellt im Prinzip die Umkehrung einer Düse dar. Er dient weiterhin zur „Rückgewinnung“ von kinetischer Energie in der Rohrhydraulik. So werden Diffusoren technisch genutzt, um kinetische Energie in Druckenergie zu wandeln. Dazu muss die Strömung verzögert werden. Man erreicht dies in der Regel durch eine stetige oder unstetige Erweiterung des Strömungsquerschnitts, die geometrisch auf verschiedene Weisen realisiert werden kann.

Ein Diffusor stellt im Unterschallbereich immer eine Vergrößerung des Durchflussquerschnittes in Fließrichtung des strömenden Mediums dar.

In der Aerodynamik wird der Diffusor zum Beispiel bei Überschallflugzeugen dazu verwendet, die Luft im Triebwerkseinlauf einer Turbine auf Unterschallgeschwindigkeit abzubremsen, da die Luft die Schaufeln der Rotoren und Statoren nur im Unterschallbereich umströmen darf. Denn kommt es zu Überschallgeschwindigkeiten an den Schaufeln der Rotoren und Statoren in einem Jetantrieb, reißt die Strömung ab, die Brennkammer erstickt, der Antrieb fällt aus. Außerdem laufen Schockwellen durch das Fließmedium und die Rotor-/Statorflügel, welche das Triebwerk zerstören können.

In der sich in Fließrichtung verjüngenden Düse, die dem Triebwerk folgt, beschleunigt die Luft dann wieder auf Überschall-Geschwindigkeit.

Befindet sich das Fließmedium selbst in Überschallgeschwindigkeit und soll es auch in Überschallgeschwindigkeit verbleiben (z. B. im Lufteinlass eines Pulsertriebwerkes), dann muss sich die Düse in Fließrichtung weiten, nicht verjüngen. Im Artikel Düse wird dieses paradoxe Phänomen erklärt.

In der Hydrodynamik zur Beeinflussung des Phänomens der Kavitation angewandt, sowie in der Aerodynamik im Bereich des Überschalls, ist der Diffusor ein sehr komplex zu berechnendes Bauteil.

Bei Diffusoren mit einem Öffnungswinkel von formula_1 (überkritischer Diffusor) entsteht eine Dissipation durch Ablösen der Strömung von der Diffusorwand, dadurch kommt es zu starken Verwirbelungen in den Übergangsgebieten zu den Toträumen. Bei einer plötzlichen Querschnittserweiterung (formula_2) spricht man auch von einem „Carnotschen Stoßverlust“, den entsprechenden Diffusor nennt man "Sprungdiffusor". In einem solchen Diffusor kommt die Strömung nach einer Distanz von etwa dem Acht- bis Zehnfachen des großen Durchmessers wieder zum Anliegen.

Die Qualität eines Diffusors wird mit dem „Diffusorwirkungsgrad“ formula_3 oder der „Druckrückgewinnziffer“ beschrieben.

Relativ überschaubar ist die Wirkung eines Diffusors jedoch im Fall von nicht-turbulenten und nicht viskosen Strömungen (das heißt, es kommt nicht aufgrund von plötzlichen Querschnittsänderungen oder Ähnlichem zu Wirbeln und die Reibungsverluste des Mediums an den Wänden kann vernachlässigt werden). Dann gilt die vereinfachte Bernoulli-Gleichung

Dabei ist formula_5 der sogenannte statische Druck, der auf die Außenwände des Diffusors wirkt, formula_6 die Dichte des Mediums und formula_7 seine Fließgeschwindigkeit. (Anmerkung: Die Bernoulli-Gleichung ist dahingehend vereinfacht, dass Höhenunterschiede nicht berücksichtigt sind.) Wie ersichtlich ist, muss der statische Druck formula_5 abnehmen, wenn sich die Fließgeschwindigkeit formula_7 erhöht.

Da bei einem Rohr mit veränderlichem Querschnitt an jeder Stelle das gleiche Volumen pro Zeiteinheit durchströmen muss, ist ersichtlich, dass sich die Strömungsgeschwindigkeit formula_10 bei einem Querschnitt formula_11 zu der Strömungsgeschwindigkeit formula_12 bei dem Querschnitt formula_13 umgekehrt proportional zu dem Verhältnis des Querschnitte verhält, es also gelten muss:

Außerdem gilt wegen der obenstehenden (vereinfachten) Bernoulli-Gleichung:

Beides zusammen ergibt:

bzw. umgeformt:

D. h. mit wachsendem Querschnitt (Diffusor: formula_19) steigt der Druck (und sinkt die Strömungsgeschwindigkeit) und mit sinkendem Querschnitt (Düse: formula_20) sinkt der Druck (und steigt die Strömungsgeschwindigkeit).

Bei sehr engen Querschnitten oder sehr zähflüssigen Medien müssen zusätzlich die Reibungsverluste berücksichtigt werden, ebenso wie bei sich plötzlich ändernden Querschnitten die auftretenden Turbulenzen berücksichtigt werden müssen (siehe nächster Abschnitt).

Bei einer plötzlichen Querschnittsänderung gilt für die, in der Strömungslehre definierte, Verlustziffer formula_21:

Mit der Verlustziffer und dem erweiterten Bernoullisatz der Rohrhydraulik (Berücksichtigung von Dissipation) folgt:

Energiesatz: formula_23

Massenerhaltungssatz: formula_24 für formula_25

Diffusoren werden bei schnellfliegenden, insbesondere überschallschnellen Flugzeugen verwendet, um im unmittelbaren Ansaugbereich der Triebwerke einen definierten Gasdruck zu erzielen. Um dabei die optimalen Verhältnisse einstellen zu können, wird der Diffusor meist beweglich ausgelegt. Flugzeuge mit besonders großem Geschwindigkeitsbereich, oder bei denen es auf hohe Reichweite ankommt, haben komplexe Diffusoren mit verstellbaren Klappen und mehrfach veränderlichen Querschnitten.

Diffusoren finden Anwendung im Motorsport, häufig auch bei Supersportwagen und gelegentlich bei Sportwagen. Hierbei wird Unterdruck unter dem Wagenboden erzeugt, der das Fahrzeug an den Boden presst und damit höhere Kurvengeschwindigkeiten zulässt und auch das Fahrverhalten bei hohen Geschwindigkeiten verbessert. Der Diffusor hat dabei die Aufgabe, den Unterdruck unter dem Fahrzeug wieder auf den hinter dem Fahrzeug herrschenden Umgebungsdruck zu erhöhen. In vielen Fällen handelt es sich bei einem solchen Diffusor um einen bodennahen Flügel.

In der Hydrodynamik findet man den Diffusor z. B. in Pumpen und Wasserstrahl-/Strahltriebwerken sowie bei Ventilatoren in der Aerodynamik.

Ebenso finden sich Diffusoren im Bereich der Aquaristik. Dort dienen sie dem Zweck, für das Leben und Wachstum der Fauna und Flora im Aquarium zusätzlich benötigte Gase wie Kohlendioxid oder Sauerstoff in das Wasser einzubringen. Dies geschieht durch den „Saugrohreffekt“, indem durch das Fließwasser innerhalb der Ausströmleitung mittels eines Frischluft- oder einer Gaszuleitung durch den Unterdruck angesaugt, mit dem Wasser vermischt und anschließend in das Becken geleitet wird. Insbesondere dienen diese Diffusoren während der Verwendung von Algenbekämpfungsmitteln, Medikamenten und/oder bei einem Missverhältnis zwischen Pflanzen- und Fischbesatz der zusätzlichen und ausgleichenden Belüftung des künstlichen Lebensraumes. Strittig ist, ob ein Diffusor darüber hinaus für normale Beckengrößen sinnvoll ist.

Mit Diffusoren kann die Ausströmmenge aus einem Gefäß erhöht werden, man spricht vom "Saugrohreffekt". Dieser war offenbar schon im antiken Rom bekannt, denn dort war es nach Sextus Iulius Frontinus verboten, ein Rohr mit zunehmendem Durchmesser direkt an ein Mundstück der Wasserleitung anzuschließen.

Im Bereich kleinerer Windkraftanlagen gibt es immer wieder Versuche, mit Hilfe von Diffusoren ihre Effektivität zu steigern.




</doc>
<doc id="561107" url="https://de.wikipedia.org/wiki?curid=561107" title="Delisle-Skala">
Delisle-Skala

Die Delisle-Skala ist eine Temperaturskala, die 1732 von dem französischen Astronomen Joseph Nicolas Delisle (1688–1768) eingeführt wurde. Die Einheit der Delisle-Skala ist Grad Delisle, wobei diese keine SI-Einheit darstellt.

1732 entwarf Delisle ein Thermometer, das mit flüssigem Quecksilber arbeitete. Als Bezugspunkt nahm er den Siedepunkt von Wasser (0 °De) bei einem normalen atmosphärischen Druck (1013,25 mbar) und maß dann die Volumenänderung des Quecksilbers.
Das Delisle-Thermometer hatte ursprünglich 2400 Gradeinteilungen. Im Winter 1738 rekalibrierte Josias Weitbrecht (1702–1747) das Thermometer, als er feststellte, dass die Volumenänderung von Quecksilber über diese Temperaturdifferenz näherungsweise einem Verhältnis von 150 zu 10.000 entspricht. Als Bezugspunkte dienten nun der Siedepunkt von Wasser (0 °De) und der Schmelzpunkt von Eis (150 °De).
Die Delisle-Skala verläuft somit wie die ursprünglich von Celsius entworfene Skala (Siedetemperatur von Wasser formula_1 und Schmelztemperatur formula_2) und damit entgegen der heutigen Celsiusskala, die vom Schmelzpunkt des Eises (0 °C) zum Siedepunkt von Wasser (100 °C) verläuft.

Das Delisle-Thermometer wurde über 100 Jahre lang in Russland genutzt.


</doc>
<doc id="730529" url="https://de.wikipedia.org/wiki?curid=730529" title="Robert Jemison Van de Graaff">
Robert Jemison Van de Graaff

Robert Jemison Van de Graaff (* 20. Dezember 1901 in Tuscaloosa, Alabama; † 16. Januar 1967 in Boston) war ein US-amerikanischer Physiker. 

Nach einem Maschinenbaustudium an der University of Alabama (Master Abschluss 1923) studierte Van de Graaff in Paris bei Marie Curie und an der Universität Oxford Physik. 1926 erwarb er den Bachelor-Abschluss in Oxford, wo er 1928 promovierte. 1929 bis 1931 war er als Fellow des National Research Council an der Princeton University und 1931 bis 1934 als Research Associate am Massachusetts Institute of Technology (MIT). 

1934 wurde er am MIT Associate Professor of Physics. 1935 wurde er in die American Academy of Arts and Sciences gewählt.

Van de Graaff war von 1946 bis 1967 Chefwissenschaftler der High Voltage Engineering Corporation.

1929 entwickelte er den Van-de-Graaff-Generator und auf dessen Grundlage den Van-de-Graaff-Beschleuniger. 

1966 erhielt er den Tom-W.-Bonner-Preis für Kernphysik der American Physical Society, deren Fellow er seit 1934 war.



</doc>
<doc id="975158" url="https://de.wikipedia.org/wiki?curid=975158" title="Schwingungsebene">
Schwingungsebene

Die Schwingungsebene ist in der Schwingungslehre die Ebene einer Schwingung.


Ganz allgemein ist die Schwingungsebene einer Transversalwelle die Ebene, die senkrecht auf der Ausbreitungsrichtung steht.

Polarisation einer Welle kann nur in der Schwingungsebene geschehen, weshalb nur Transversalwellen polarisierbar sind. Longitudinalwellen, etwa Schall, hingegen besitzen keine Schwingungsebene und sind daher nicht polarisierbar.


</doc>
<doc id="1079638" url="https://de.wikipedia.org/wiki?curid=1079638" title="Stern-Gerlach-Medaille">
Stern-Gerlach-Medaille

Die Stern-Gerlach-Medaille ist eine Auszeichnung, die seit 1993 jährlich von der Deutschen Physikalischen Gesellschaft (DPG) für besondere Leistungen auf dem Gebiet der experimentellen Physik verliehen wird.

Diese Auszeichnung gilt als die bedeutendste in diesem Fach in Deutschland. Sie besteht aus einer handgeschriebenen Urkunde aus Pergament und einer goldenen Medaille mit den Porträts von Otto Stern und Walther Gerlach, nach denen auch der Stern-Gerlach-Versuch benannt ist. Die Stern-Gerlach-Medaille ging 1993 aus dem Stern-Gerlach-Preis hervor, dessen Aufwertung durch eine Medaille 1992 beschlossen wurde.

Die entsprechende höchste Auszeichnung der DPG für Leistungen auf dem Gebiet der theoretischen Physik ist die Max-Planck-Medaille.




</doc>
<doc id="1288174" url="https://de.wikipedia.org/wiki?curid=1288174" title="Tangentialbeschleunigung">
Tangentialbeschleunigung

Die Tangentialbeschleunigung (auch Bahnbeschleunigung genannt) bezeichnet die Geschwindigkeitsänderung pro Zeit, die ein Massepunkt auf einer gekrümmten Bahn, tangential zu dieser, erfährt. Sie ist das Produkt aus der Winkelbeschleunigung und dem Krümmungsradius am betreffenden Bahnpunkt. Wir betrachten hier als Beispiel eine Kreisbahn.

Betrachtet man nur den Betrag der Tangentialbeschleunigung, so gilt:

formula_1

Dabei ist formula_2 der Betrag der Tangentialbeschleunigung, v die Bahngeschwindigkeit, formula_3 die Winkelgeschwindigkeit, r der Radius der Kreisbahn und formula_4 die Winkelbeschleunigung.

Die Tangentialbeschleunigung wirkt senkrecht zur Zentripetalbeschleunigung, welche zum Kreismittelpunkt hin gerichtet ist. Die Gesamtbeschleunigung ist die Summe der Vektoren von Tangentialbeschleunigung und Zentripetalbeschleunigung.

Die Möglichkeit zur Aufteilung des Beschleunigungsvektors in Tangential- und Normalbeschleunigung entdeckte erstmals Huygens.

Ein Karussell fängt an, sich zu drehen. Es erfährt also eine Beschleunigung. Bei gleicher Winkelbeschleunigung erfährt eine Person, die nahe an der Drehachse steht, eine geringere Tangentialbeschleunigung (kleiner Abstand zur Drehachse), als eine Person, die am äußeren Rand des Karussells steht (großer Abstand zur Drehachse).
Die Tangentialbeschleunigung formula_5 verhält sich also proportional zum Radius formula_6 des Karussells:


</doc>
<doc id="1303487" url="https://de.wikipedia.org/wiki?curid=1303487" title="Kayser (Einheit)">
Kayser (Einheit)

</math>
Das Kayser (Einheitenzeichen: kayser) ist eine nach dem deutschen Physiker Heinrich Kayser benannte veraltete Einheit der Wellenzahl. Sie wurde vor 1952 aufgrund eines Vorschlags von William Frederick Meggers von der Joint Commission for Spectroscopy empfohlen, wurde in das Internationale Einheitensystem allerdings nicht aufgenommen. Dennoch wurde die Einheit in der Spektroskopie lange benutzt.

Ursprünglich sollte die Einheit, vorgeschlagen von Albert Christopher Candler, den Namen Rydberg (Einheitenzeichen: R), nach dem schwedischen Physiker Johannes Rydberg tragen. Ein weiterer 1951 vorgeschlagener Name war Balmer, nach dem Schweizer Physiker Johann Jakob Balmer. Nach teilweise heftiger Diskussion, wurde schließlich keine eigene Einheit eingeführt, sondern weiter cm verwendet. 

1 kayser = 1 cm = 100 m


</doc>
<doc id="1537366" url="https://de.wikipedia.org/wiki?curid=1537366" title="Paraxiale Optik">
Paraxiale Optik

Die paraxiale Optik, auch gaußsche Optik oder Optik erster Ordnung, ist eine Vereinfachung der geometrischen Optik, bei der nur Lichtstrahlen betrachtet werden, die mit der optischen Achse kleine Winkel bilden und kleine Abstände von ihr haben (sog. paraxiale Strahlen).

Durch den Grenzübergang zu unendlich kleinen Achsabständen und Winkeln ergeben sich lineare Formeln für die Berechnung der durch das System gehenden Lichtstrahlen und der Abbildungen. Paraxiale Strahlen verursachen außer der chromatischen Aberration keine Abbildungsfehler; bei Verwendung monochromatischen Lichts (d. h. von Licht mit nur einer Wellenlänge) scheidet auch dieser Fehler aus.

Dann gilt: Strahlen, die von demselben Objektpunkt ausgehen, sind im Bildraum (nach Durchgang durch das System) entweder parallel oder schneiden sich alle in demselben Bildpunkt; Ebenen werden auf Ebenen abgebildet und Geraden auf Geraden, auch dann, wenn sie "nicht" senkrecht zur optischen Achse sind (Scheimpflugsche Regel).

Die paraxiale Optik kann man auf drei Weisen beschreiben und benutzen:

Für die wichtigen Größen, die das Abbildungsverhalten eines optischen Systems bestimmen, liegen in der paraxialen Optik Definitionen vor, u. a.:


</doc>
<doc id="1600985" url="https://de.wikipedia.org/wiki?curid=1600985" title="Tapered Fiber">
Tapered Fiber

Eine Tapered Fiber ist eine Glasfaser mit einem extrem kleinen Durchmesser. Zu ihrer Herstellung nimmt man eine normale Glasfaser und erhitzt sie lokal, etwa mit einem Brenner oder mit einem Kohlendioxidlaser. Dabei wird die Faser an ihren Enden auseinandergezogen, so dass sie dünner und länger wird. Hat die ursprüngliche Faser noch einen Durchmesser von typischerweise 125 μm, liegt der Durchmesser der gezogenen Faser nur noch im Bereich von 0,5 bis 5 μm. Typische gezogene Fasern gehen dabei herstellungsbedingt über einen Übergangsbereich in die normale Faser über. Diese ermöglicht die einfache Einkopplung von Laserlicht in solche Fasern.

Bei einer gezogenen Faser gilt es die einzelnen Bereiche der Faser zu unterscheiden. Eine normale Faser besteht aus einem Kern mit hohem Brechungsindex, der von einem Mantel mit geringerem Brechungsindex umgeben ist. Der Kern mit einem Durchmesser von typischerweise 8 μm führt dabei das Licht in seinem Inneren als Lichtwellenleiter. In der gezogenen Faser ist hingegen der Durchmesser wesentlich kleiner als die Wellenlänge des Lichts. Deshalb wird das Licht nicht im Kern, sondern im Mantel geleitet.

Solche Fasern sind in der letzten Zeit vermehrt Gegenstand der Forschung geworden, da sie die Erzeugung von Weißlicht (Superkontinuum) ermöglichen. Koppelt man nämlich einen Lichtimpuls geringer Dauer, etwa aus einem Femtosekundenlaser, in eine solche Faser ein, findet man eine hohe Leistung auf einer kleinen Fläche. Durch die dadurch entstehenden hohen Intensitäten können nichtlineare Prozesse angeregt werden, die zu einer Verbreiterung des Spektrums führen. Dieses Weißlicht kann für Frequenzkammgeneratoren, die optische Kohärenztomographie, Lidar und die Multiphotonenmikroskopie eingesetzt werden.




</doc>
<doc id="1694498" url="https://de.wikipedia.org/wiki?curid=1694498" title="Magsat">
Magsat

Magsat („Magnetic Field Satellite“, auch als "AEM-3" oder "Explorer-61" bekannt) war ein Wissenschaftssatellit im Rahmen des Explorer-Programms der NASA.

Der Satellit wurde speziell dafür entwickelt, das nahe Magnetfeld der Erde zu untersuchen. Zusätzlich konnte er Daten zur geologischen Struktur und Zusammensetzung der Erde sammeln.

Magsat hatte eine Masse von 181 Kilogramm. Ausgerüstet war er mit zwei Magnetometern: einem drei-Achsen-Fluxgate-Magnetometer zur Detektierung der Stärke und Richtung des magnetischen Feldes und ein Ionen-Dampf/Vector-Magnetometer zum Messen des durch das Magnetometer selbst erzeugten Feldes. Um das Magnetfeld unbeeinflusst vom Eigenmagnetfeld des Satelliten messen zu können, wurden die Magnetometer an einem langen Ausleger montiert, der im Orbit ausgefahren wurde.

Magsat wurde am 30. Oktober 1979 mit der Trägerrakete Scout G von der Vandenberg Air Force Base aus gestartet und blieb bis zum Frühjahr 1980 in Betrieb.

Nach dem Start wurde der Satellit in eine Umlaufbahn mit 96.8° Bahnneigung gebracht und zur Sonne ausgerichtet. Die Höhe der Bahn schwankte zwischen 352 und 561 Kilometer. Der Satellit blieb in einem erdnahen Orbit, damit die Magnetometer das Erdmagnetfeld besser erfassen konnten. Die gesammelten Daten erlaubten eine noch nie vorher gesehene dreidimensionale Darstellung der magnetischen Feldverteilung der Erde. Zusammen mit seinem Nachfolger Ørsted war er eine wichtige Komponente zur Erklärung des sich verringernden Erdmagnetfeldes.



</doc>
<doc id="1701858" url="https://de.wikipedia.org/wiki?curid=1701858" title="Rudolph Koenig">
Rudolph Koenig

Rudolph Koenig (* 26. November 1832 in Königsberg; † 2. Oktober 1901 in Paris) war ein deutscher Akustiker.
Koenig besuchte zwischen 1840 und 1851 das Kneiphöfische Stadtgymnasium in seiner Heimatstadt, ging Ende 1851 nach Paris, wo er beim berühmten Fabrikanten von Saiteninstrumenten Jean-Baptiste Vuillaume in die Lehre ging und bald eine besondere Vorliebe für die Akustik entwickelte. So errichtete er 1858 eine Werkstätte für die Konstruktion akustischer Apparate. Er begann mit Geräten für den Unterricht und errang auf mehreren Ausstellungen Medaillen.

Wissenschaftlichen Wert hatten seine Arbeiten über die Anwendung der graphischen Methode auf die Akustik, wozu ihm ein von ihm konstruierter Phonautograph behilflich war. Auch beschäftigte er sich mit der Messung der Schallgeschwindigkeit, mit den Klangfiguren, der Tonveränderung bewegter Schallquellen (also dem akustischen Dopplereffekt), mit manometrischen Flammen, akustische „Stöße“ (Stoßwellen), Normalstimmgabeln und über die Klangfarbe, zu deren Untersuchung er eine selbstkonstruierte Wellensirene einsetzte.




</doc>
<doc id="1858358" url="https://de.wikipedia.org/wiki?curid=1858358" title="Pyrofusion">
Pyrofusion

Unter Pyrofusion versteht man die Möglichkeit, Kernfusion mit Hilfe eines pyroelektrischen Kristalls zu erreichen. 

Seth Putterman von der Universität von Kalifornien (UCLA) und seine Mitarbeiter Brian Naranjo und Jim Gimzewski veröffentlichten in Nature im Jahr 2005 einen Artikel über pyroelektrisch induzierte Kernverschmelzungen. Die Arbeitsgruppe stellt darin eine einfach zu handhabende, kleine Apparatur vor, die Verschmelzungen von Deuteriumkernen ermöglicht.

Um Deuteriumatome zu ionisieren und anschließend auf die für die Fusion benötigte Geschwindigkeit zu beschleunigen, benutzten die Forscher einen pyroelektrischen Kristall als Spannungsquelle. Pyroelektrische Kristalle besitzen elektrische Dipolmomente, die sich bei Änderungen der Temperatur umorientieren und so eine elektrische Spannung zwischen den beiden Grundflächen des Kristalls aufbauen. Dass mit einem solchen Kristall eine Art Minibeschleuniger für Elektronen realisiert werden kann, ist bereits bekannt. 

Putterman und seine Mitarbeiter wandten das Beschleunigungsprinzip nun auf Deuterium an. Der im Experiment genutzte, zentimetergroße Kristall aus Lithiumtantalat (LiTaO) erreichte beim Erwärmen Spannungen von mehr als 100 kV. An der auf der positiven Seite des Kristalls angebrachten, winzigen Wolframspitze konzentrierte sich dann eine elektrische Feldstärke von über 25 GV/m. Befindet sich die Spitze in einem mit Deuteriumgas gefüllten Behälter, werden die vor der Spitze befindlichen Atome ionisiert (Feldionisation). Die Deuterium-Ionen werden dann von der Wolframspitze abgestoßen und zu einem 10 cm entfernten, deuteriumhaltigen Target (Erbiumdeuterid ErD) hin beschleunigt. Beim Aufprall kommt es zu Kernverschmelzungen. Es lassen sich Neutronen mit einer Energie von 2,45 MeV sowie Röntgenstrahlung nachweisen.

Mit einer Ausbeute von knapp 1.000 Neutronen pro Sekunde und einer Energieausbeute von nur etwa formula_1 Joule pro Erhitzungszyklus kann die Apparatur zwar nicht zur Energieerzeugung genutzt werden, als handliche Neutronenquelle beispielsweise für Sicherheits- oder Materialuntersuchungen ist das Gerät jedoch geeignet. Eine wesentlich höhere Ausbeute an Neutronen mit einer Energie von 14 MeV wäre beim Beschuss von Tritium zu erwarten. 



</doc>
<doc id="2293224" url="https://de.wikipedia.org/wiki?curid=2293224" title="Dan Shechtman">
Dan Shechtman

Daniel „Dan“ Shechtman (auch "Schechtman"; ; * 24. Januar 1941 in Tel Aviv, damals Völkerbundsmandat für Palästina) ist ein israelischer Physiker. 2011 wurde ihm der Nobelpreis für Chemie für die Entdeckung der Quasikristalle verliehen.

Dan Shechtman wurde 1941 in Tel Aviv geboren und wuchs zunächst in Ramat Gan, dann in Petach Tikwa, einige Kilometer östlich von Tel Aviv auf.

Seine Großeltern waren während der Zweiten Alija (1904–1914) nach Palästina eingewandert und hatten dort eine Druckerei gegründet. Der Wirkung eines Buches ist es zu verdanken, dass Dan Shechtman sich der Wissenschaft zuwandte und nicht dem Druckgewerbe. Als Kind verschlang er Jules Vernes Werk „Die geheimnisvolle Insel“ ungezählte Male und sein Kindheitstraum war Ingenieur zu werden wie Cyrus Smith, der Held des Romans.

"„I thought that was the best thing a person could do. The engineer in the book knows mechanics and physics, and he creates a whole way of life on the island out of nothing. I wanted to be like that.“" ("„Ich glaubte, das wäre das Beste, was jemand überhaupt tun konnte. Der Ingenieur in diesem Buch kennt sich aus mit Mechanik und Physik und er schafft aus dem Nichts auf der Insel einen vollständigen Lebensraum. So wie er wollte [auch] ich sein.“")

Er nahm am Technion (Israel Institute of Technology – Israelisches Institut für Technologie) in Haifa ein Studium der Ingenieur- und Materialwissenschaften auf.
Am Technion machte Shechtman seinen B.Sc (Bachelor of Science) und M.Sc (Master of Science) und wurde im Jahre 1972 dort promoviert. Unmittelbar nach seiner Promotion ging er mit Frau und seinen drei Töchtern als NRC-Fellow in die USA, um in den Forschungslaboren der Wright-Patterson Air Force Base in Ohio für die US-Luftwaffe zu forschen.

Während seines dreijährigen Forschungsaufenthaltes beschäftigte er sich mit der Mikrostruktur und den physikalisch-metallurgischen Eigenschaften von Titan-Aluminiden. Ab 1975 arbeitete er am Institut für Materialwissenschaften der Technischen Universität Technion in Haifa.

Dan Shechtman ist Professor am Technion und forscht am Louis Edelstein Center und am Wolfson Centre, dem er vorsteht. Seit vielen Jahren fördert er Unternehmensgründungen von Absolventen des Technion, indem er Unternehmer zu Vorträgen dort einlädt.

Von 1981 bis 1983 arbeitete er im Rahmen eines Sabbaticals an der Johns Hopkins University mit sich rasch verfestigenden Aluminium-Metall-Legierungen und entdeckte dort die sogenannte ikosaedrische Phase, die das neue Forschungsgebiet der quasiperiodischen Kristalle eröffnete.

Diese Entdeckung wurde lange kritisiert: „Es gibt keine Quasikristalle, nur Quasi-Wissenschaftler“ sagte der 1994 verstorbene Chemie-Nobelpreisträger Linus Carl Pauling. Der Leiter von Shechtmans Forschungsgruppe empfahl ihm, noch einmal die Lehrbücher zu lesen, und drängte ihn daraufhin, die Gruppe zu verlassen, um sie nicht zu blamieren. Später wurden Quasikristalle auch von anderen Forschern gefunden. Im Jahr 2011 erhielt Shechtman den mit zehn Millionen Schwedische Kronen (ca. 1,1 Millionen Euro) dotierten Chemie-Nobelpreis.

Dan Shechtman ist verheiratet mit (T)zipora Shechtman, Leiterin des Department of Counseling and Human Development an der Universität Haifa und Autorin mehrerer Bücher und zahlreicher Artikel in Fachzeitschriften zum Thema Psychotherapie. Das Paar hat einen Sohn und drei Töchter.





</doc>
<doc id="2745624" url="https://de.wikipedia.org/wiki?curid=2745624" title="Ericsson-Kreisprozess">
Ericsson-Kreisprozess

Der Ericsson-Kreisprozess (nach John Ericsson), auch Ackeret-Keller-Kreisprozess (nach Jakob Ackeret), ist ein thermodynamischer Kreisprozess. Der Prozess dient als Vergleichsprozess für eine Gasturbinenanlage (interne oder externe Erwärmung) mit interner Wärmeübertragung aus dem Abgas der Turbine an das verdichtete Gas (z. B. Luft). Der ideale Prozess hat den Wirkungsgrad des Carnot-Prozesses.

Der Ericsson-Prozess besteht aus zwei Isobaren und zwei Isothermen. Er ist in den Diagrammen im Bild rechts orangefarben dargestellt und verläuft zwischen den Eckpunkten 1-2E-3E-4E-1.

Die 4 Zustandsänderungen sind:


Die Isobaren verlaufen im T-s-Diagramm äquidistant (in horizontaler Richtung gesehen!), so dass die der übertragenen Wärme entsprechende Fläche unter dem Verlauf von 2E bis 3E gleich der Fläche unter dem Verlauf von 4E bis 1 ist. Für die Wärmeübertragung wird ein idealer Gegenstrom-Wärmeübertrager benötigt. Die mittlere Temperatur der Wärmezufuhr von außen ist die höchste Prozesstemperatur und die mittlere Temperatur der Wärmeabfuhr die niedrigste, so dass der Wirkungsgrad gleich dem des Carnot-Prozesses ist (im T-s-Diagramm lässt sich die vom Kreisprozess eingeschlossene Fläche in ein flächengleiches Rechteck mit der Entropiedifferenz zwischen 1 und 2E und der Temperaturdifferenz zwischen 3E und 2E verformen).

Isotherme Turbinen und Verdichter sind nicht realisierbar. Zur Wärmeübertragung über die Gehäusewand reicht die Verweilzeit nicht aus bzw. ist die Fläche zu klein. Deshalb ist die Annäherung an den idealen Prozess nur durch Aufteilen in verschiedene Verdichter- und Turbinenstufen mit jeweiliger Zwischenkühlung bzw. Zwischenerwärmung möglich. In den Diagrammen und im Schaltbild (rechts) ist eine Unterteilung in jeweils zwei Stufen dargestellt. Der interne Wärmeaustausch beschränkt sich jetzt auf die im T-s-Diagramm für die Wärmeaufnahme grün und die Wärmeabgabe rot gekennzeichneten gleich großen Flächen (auch als ideal angenommen, d. h. ohne die zur Wärmeübertragung erforderliche Temperaturdifferenz). Man erkennt daraus, dass die Annäherung an den Ericsson-Prozess umso besser ist, je mehr Stufen gewählt werden (für den Idealfall würden unendlich viele Stufen benötigt). Der bauliche Aufwand für mehrere Stufen ist hoch.





</doc>
<doc id="2886624" url="https://de.wikipedia.org/wiki?curid=2886624" title="Viking (Satellit)">
Viking (Satellit)

Viking ist ein schwedischer geophysikalischer Satellit. Er ist zugleich Schwedens erster Satellit. Er wurde am 22. Februar 1986 um 01:44:35 Uhr UTC von Kourou als Huckepack-Nutzlast beim Start einer Ariane-1-Rakete zusammen mit dem französischen Satelliten SPOT-1 ins All gebracht. 

Er sollte die Wechselwirkungen zwischen Sonnenwind und dem elektromagnetischen Feld der Erde erforschen. Dazu wurde er auf eine sehr exzentrische Bahn mit etwa 814 km Perigäum und etwa 13.530 km Apogäum gebracht, so dass er sich bei einer Umlaufzeit von 261 min mehr als 215 min in einer Höhe von mehr als 4000 km befindet. Die Inklination der Bahn beträgt 98,70°. Der Satellit mit der Form einer achteckigen flachen Scheibe (190 cm Durchmesser, 50 cm hoch) wurde spinstabilisiert und seine Solarzellen von insgesamt 2,2 m² Fläche lieferten eine Leistung von 85 Watt (sie wurden unterstützt von NiCd Akkumulatoren mit 12 Ah Kapazität). 

Viking hatte eine Startmasse von etwa 530 kg und eine Trockenmasse von 286 kg, wobei nur 40 kg auf die wissenschaftliche Ausrüstung entfallen. Diese bestand aus einem UV-Lichtmesser für Polarlichtuntersuchungen, Plasmawellendetektor im Hoch- und Niederfrequenzbereich, einem Vektor-Elektrometer, Plasmaspektrometer und Magnetometer. Die einzelnen Experimente wurden von verschiedenen Wissenschaftlergruppen aus verschiedenen Ländern (Schweden, Kanada, Dänemark, Frankreich, Norwegen, USA und Deutschland) beigesteuert. Um die elektrischen Felder in allen drei Raumrichtungen zu messen, entfaltete er im Orbit zwei 4 m lange Stangen in axialer Richtung und entrollte vier Seile von je 40 m Länge mit jeweils einem Sensorsystem am Ende.

Er lieferte bis zum 12. Mai 1987 (mehr als 200 Tage länger als geplant) sehr wertvolle Daten (über S-Band) an die Bodenstation in Esrange (Schweden).



</doc>
<doc id="3746250" url="https://de.wikipedia.org/wiki?curid=3746250" title="Stochastische Kühlung">
Stochastische Kühlung

Stochastische Kühlung ist ein Verfahren, um einen Teilchenstrahl in einem Hochenergiephysik-Experiment zu kühlen, d. h. die Größe des Teilchenpaketes im Phasenraum zu verkleinern. Der Name Kühlung rührt daher, dass die Bewegung der Teilchen relativ zueinander dadurch abnimmt. Das Verfahren wurde unter Führung von Simon van der Meer an den Intersecting Storage Rings am Forschungszentrum CERN entwickelt. Er erhielt hierfür im Jahre 1984 eine Hälfte des Nobelpreises für Physik, als das Verfahren genutzt wurde, um einen Antiprotonenstrahl in den Speicherring Super Proton Synchrotron am CERN einzuspeisen und so die W- und Z-Bosonen direkt nachzuweisen.

Die stochastische Kühlung versucht gezielt, mit einem Sensor die Phasenraumposition von Teilchen festzustellen und diese dann in die Mitte des Volumens zu bewegen. Der Satz von Liouville, der besagt, dass Phasenraumvolumina von konservativen Kräften nicht verändert werden können, wird hierbei nicht verletzt, da es sich bei der gezielten, kurzzeitigen Anwendung elektromagnetischer Felder eben nicht um konservative Kräfte handelt.
Der erste Sensor, der sogenannte "Pick-Up", misst dazu beispielsweise die Abweichung des Teilchens von der Kreisbahn im Speicherring und gibt diese Information an den "Kicker", welcher an einem anderen Ort im Speicherring steht, weiter. Dieser stößt dann das Teilchen in Richtung Kreisbahn. Durch die hohe Zahl von Teilchen im Strahl überlagern sich dabei "Kicker"-Signale verschiedener Teilchen beziehungsweise Strahlteile. Da sich dieser Effekt aber im Mittel aufhebt, hat dies erst in zweiter Ordnung Auswirkung auf die restlichen Teilchen.

Zurzeit wird die stochastische Kühlung am Antiproton Decelerator im CERN verwendet. Des Weiteren wird die Technologie am RHIC, CERN, GSI sowie am COSY verwendet. Auch an der Antiprotonenquelle des Tevatron am Fermi National Accelerator Laboratory wurde die stochastische Kühlung eingesetzt.



</doc>
<doc id="3887267" url="https://de.wikipedia.org/wiki?curid=3887267" title="Edmund Hoppe">
Edmund Hoppe

Edmund Hoppe (* 25. Februar 1854 in Burgdorf; † 12. August 1928 in Göttingen) war ein deutscher Historiker der Mathematik und der Naturwissenschaften.

Edmund Hoppe war der Sohn eines lutherischen Pfarrers und studierte 1873 bis 1877 Naturwissenschaften in Leipzig und Göttingen, wo er 1877 promoviert wurde und dann zunächst Assistent von Eduard Riecke am physikalischen Institut war. 1877 bis 1896 war er Gymnasiallehrer für Physik und Mathematik an der Gelehrtenschule des Johanneums in Hamburg, ab 1894 als Professor. 1896 bis 1919 war er am Hamburger Wilhelm-Gymnasium. Danach zog er nach Göttingen, wo er ab 1919 Dozent für Geschichte der exakten Wissenschaften war.

Hoppe befasste sich mit der Geschichte der Naturwissenschaften und Mathematik. Unter anderem arbeitete er über Heron von Alexandria und die Geschichte der Optik. Seine Physikgeschichte gilt neben der Physikgeschichte von Ferdinand Rosenberger (1845–1899) zu den älteren deutschen Standardwerken. Er verfasste auch Bücher über die Beziehung der Naturwissenschaften zur Religion.

Er ist nicht mit dem Mathematiker und Herausgeber des Grunert-Archivs Ernst Reinhold Eduard Hoppe (1816–1900) zu verwechseln.




</doc>
<doc id="3899121" url="https://de.wikipedia.org/wiki?curid=3899121" title="Arthur McDonald">
Arthur McDonald

Arthur Bruce McDonald (* 29. August 1943 in Sydney, Nova Scotia) ist ein kanadischer Physiker und Direktor des Sudbury Neutrino Observatory Institute (SNO). Er hat auch den "Gordon and Patricia Gray Chair" (Lehrstuhl) für Teilchenastrophysik an der Queen’s University in Kingston, Ontario inne. 2015 wurde ihm gemeinsam mit Takaaki Kajita der Nobelpreis für Physik zugesprochen.

McDonald erhielt den B.Sc. in Physik 1964 und den M.Sc. in Physik 1965 an der Dalhousie University in Nova Scotia. Den Ph. D. in Physik erhielt er vom California Institute of Technology.

McDonald arbeitete von 1970 bis 1982 als Wissenschaftler an den Chalk River Laboratories nordwestlich von Ottawa. Von 1982 bis 1989 war er Professor für Physik an der Princeton University, wo er einer der leitenden Wissenschaftler am Princeton Cyclotron war. Er verließ Princeton 1989, um Professor an der Queen’s University in Kingston (Ontario) zu werden. Gleichzeitig wurde er 1989 Direktor des SNO. Ab 2002 hat er an der Queen’s University den "University Research Chair" und ab 2006 den "Gordon and Patricia Gray Chair in Particle Astrophysics" inne.

Er war unter anderem Gastwissenschaftler am CERN (2004), Los Alamos National Laboratory (1981), Oxford, der Universität von Hawaii, der University of Washington in Seattle.

Er ist verheiratet und hat vier Kinder.

Im August 2001 entdeckte eine Forschungsgruppe um McDonald, dass Neutrinos von der Sonne wirklich in Myon-Neutrino (formula_1) und Tau- oder Tauon-Neutrino (formula_2) oszillieren. Dieser Bericht wurde in den "Physical Review Letters" veröffentlicht und weithin als bedeutend angesehen. Er gilt als überzeugender Nachweis der Erklärung des Rätsels der solaren Neutrinos durch Neutrinooszillationen (MSW-Effekt).

McDonald und Yōji Totsuka wurden im Jahre 2007 die Benjamin-Franklin-Medaille in Physik verliehen. McDonald ist Fellow der Royal Society of Canada (1997) und der American Physical Society (1983). Er ist mehrfacher Ehrendoktor (University College Cape Breton, Royal Military College (D. Sc.), Dalhousie University, University of Chicago (D. Sc.)).

2015 wurde ihm gemeinsam mit Takaaki Kajita „für die Entdeckung von Neutrinooszillationen, die zeigen, dass Neutrinos eine Masse haben“ der Nobelpreis für Physik zuerkannt.

1969/70 war er Rutherford Fellow und 1998 Killam Research Fellow. 2003 erhielt er den Tom-W.-Bonner-Preis für Kernphysik für seine führende Rolle bei der Aufklärung des Rätsels der Solaren Neutrinos im Sudbury Neutrino-Observatorium. 2003 erhielt er die Lifetime Achievement Medal der Canadian Association of Physicists und im selben Jahr die kanadische Gerhard-Herzberg-Goldmedaille. 2004 erhielt er den Bruno-Pontecorvo-Preis und 2016 den Breakthrough Prize in Fundamental Physics. 2007 wurde er Officer und 2015 Companion of the Order of Canada, 2016 wurde er in die National Academy of Sciences gewählt.




</doc>
<doc id="5095417" url="https://de.wikipedia.org/wiki?curid=5095417" title="Schawlow-Townes-Limit">
Schawlow-Townes-Limit

Das Schawlow-Townes-Limit (selten auch Schawlow-Townes-Linienbreite) beschreibt in der Physik eines Lasers die minimale spektrale Linienbreite eines Laserstrahls, die nicht unterschritten werden kann. Es ist benannt nach den beiden Physik-Nobelpreisträgern Arthur L. Schawlow und Charles H. Townes, die diesen Grenzwert im Jahr 1958 und somit bereits vor dem Bau des ersten Lasers im Jahr 1960 vorhersagten. Ein Laser kann keine „unendlich schmale“ Linienbreite, also eine "einzige" und "exakt" definierte Lichtfrequenz, haben. Die Ursachen für diese Grenze sind quantenmechanische (besonders die spontane Emission, die im Gegensatz zur stimulierten Emission phasenverschoben stattfindet, und die Heisenbergsche Unschärferelation) und optische Effekte (Wechselwirkung des Lichts mit den Komponenten des Lasers, „Rauschen“).

Die Linienbreite ist gegeben durch die Formel

wobei formula_2 das Plancksche Wirkungsquantum, formula_3 die Bandbreite des Laserresonators und formula_4 die Ausgangsleistung des Lasers sind. In manchen Quellen fehlt in dieser Formel der Faktor 2, dies hängt davon ab, ob die Herleitung der Formel mit der vollen oder nur der halben Halbwertsbreite der Normalverteilung durchgeführt wird. Dies ändert jedoch nichts am Ergebnis, da dann die Bandbreiten formula_5 und formula_3 zur Berechnung ebenfalls halbiert werden müssen.

Es existieren Arbeiten, die diese Grenze unterbieten. Dies bedeutet jedoch weiterhin nicht, dass unendlich schmale Linienformen möglich wären, sondern lediglich, dass die minimale Breite von Schawlow und Townes zu „pessimistisch“ geschätzt wurde.


</doc>
<doc id="5207634" url="https://de.wikipedia.org/wiki?curid=5207634" title="Julius-Edgar-Lilienfeld-Preis">
Julius-Edgar-Lilienfeld-Preis

Der Julius-Edgar-Lilienfeld-Preis ist nach dem österreichisch-ungarischen Physiker Julius Edgar Lilienfeld benannt und wird seit 1989 von der American Physical Society für herausragende Leistungen in der Physik vergeben. Das Preisgeld von 10.000 US-$ wird aus einer Stiftung von Lilienfelds Frau, Beatrice Lilienfeld, finanziert.



</doc>
<doc id="5453155" url="https://de.wikipedia.org/wiki?curid=5453155" title="W. David Arnett">
W. David Arnett

William David Arnett (* 1940), genannt David Arnett, ist ein US-amerikanischer theoretischer Astrophysiker.

Arnett studierte Physik an der University of Kentucky (Bachelor 1961) und promovierte 1965 bei Alastair G. W. Cameron an der Yale University. 1965 war er am Goddard Institute of Space Studies und 1967 bis 1969 war er bei William A. Fowler am Caltech bevor er 1969 Assistant Professor an der Rice University wurde. 1971 wurde er Associate Professor an der University of Texas at Austin und 1975 Professor an der University of Illinois at Urbana-Champaign. 1977 wurde er Professor am Enrico Fermi Institut der University of Chicago, wo er bis 1989 blieb. Ab 1988 war er Regents Professor am Steward Observatory der University of Arizona. Ab 2007 war er zusätzlich Chandrasekhar Professor am International Center for Relativistic Astrophysics (ICRA) in Rom–Pescara–Nizza.

Arnett war ein Pionier in der hydrodynamischen Simulation (mit Strahlungskopplung und -transport) des Gravitationskollapses zum Beispiel bei der Bildung von Neutronensternen und Schwarzen Löchern und bei Supernovae. Dabei machte er Vorhersagen über die dabei entstehenden Elementehäufigkeiten und die Lichtkurvenverläufe, wobei er 1977 neue Typen vorhersagte (Ib und Ic). Er untersuchte auch Sternmodelle und Computer-Modellierung von turbulenten Flüssigkeiten und damit zusammenhängend Konvektion in Sternen.

Er ist Mitglied der American Academy of Arts and Sciences, der National Academy of Sciences und Fellow der American Physical Society, der American Association for the Advancement of Science sowie der American Astronomical Society. 1970 wurde er Sloan Research Fellow. 1980 erhielt er mit J. W. Truran den Distinguished Graduate Award der Yale University und 1981 den US Senior Scientist Humboldt Award. 2008 war er Chandrasekhar Lecturer am Bose Center in Kolkata. 2009 erhielt er den Hans-A.-Bethe-Preis. 2012 wurde er mit der Henry Norris Russell Lectureship ausgezeichnet und mit dem Marcel Grossmann Award.




</doc>
<doc id="5870103" url="https://de.wikipedia.org/wiki?curid=5870103" title="CGh">
CGh

cGh ist ein popularisierendes Kürzel der modernen Physik, mit dem die angestrebte Vereinigung physikalischer Theorien angedeutet wird: Der Relativitätstheorie, der Gravitation und der Quantenmechanik. Dabei steht


In einem 3D-Koordinatensystem mit diesen Einheiten würden die Ecken eines „c-G-h-Würfels“ auf der Grundfläche die drei Basistheorien darstellen und darüber die zwei- bzw. dreifach vereinigten Theorien.

Unter anderem befassten sich die Physiker Matwei Petrowitsch Bronstein und George Gamow mit dieser Thematik. Letzterer gab deshalb auch seiner populärwissenschaftlichen Romanfigur Mr. Tompkins die Initialen "C. G. H. Tompkins".



</doc>
<doc id="6133246" url="https://de.wikipedia.org/wiki?curid=6133246" title="Ralph Ambrose Kekwick">
Ralph Ambrose Kekwick

Ralph Ambrose Kekwick (* 11. November 1908 in Leytonstone, Essex; † 17. Januar 2000 in Woodford) war ein britischer Biophysiker. Er wirkte ab 1952 als Reader sowie von 1966 bis 1971 als Professor an der University of London, und beschäftigte sich insbesondere mit Studien zu den physiko-chemischen Eigenschaften von Proteinen, mit der Präparation und Untersuchung von Serum und Plasma für transfusionsmedizinische Anwendungen sowie mit der Gewinnung von einzelnen Plasmabestandteilen wie Fibrinogen und Blutgerinnungsfaktor VIII für therapeutische Zwecke. Zu seinen Verdiensten, für die er unter anderem 1966 in die Royal Society aufgenommen wurde, zählt insbesondere die Entwicklung eines Verfahrens zur Herstellung des ersten wirksamen Faktor-VIII-Präparats zur Behandlung von Hämophilie.

Ralph Kekwick wurde 1908 in Leytonstone geboren und absolvierte ab 1925 ein Studium der Chemie am University College London (UCL). Er erlangte drei Jahre später einen Bachelor-Abschluss und blieb anschließend als wissenschaftlicher Mitarbeiter am UCL. Im Sommer 1931 ging er mit einem "Commonwealth Fund Fellowship" an die New York University, wo er auch seine Frau kennenlernte. Er kehrte im September 1933 in sein Heimatland zurück und erhielt am UCL eine Anstellung als Lecturer. In den Jahren 1935/1936 war er mit einem Stipendium der Rockefeller-Stiftung an der Universität Uppsala beim Nobelpreisträger The Svedberg tätig. Hier lernte er die Techniken der Ultrazentrifugation und der Elektrophorese kennen, die sein weiteres wissenschaftliches Wirken prägten.

Nachdem das Lister Institute of Preventive Medicine in London eine Ultrazentrifuge erworben hatte, war Ralph Kekwick ab 1937 mit Unterstützung durch das Medical Research Council für das Institut tätig. Von der University of London erhielt er 1941 einen Doktortitel für seine Veröffentlichungen zu den physiko-chemischen Eigenschaften verschiedener Proteine. 1952 wurde Ralph Kekwick zum Reader für Biophysik an der University of London ernannt, an der er 1966 einen ordentlichen Lehrstuhl erhielt. Fünf Jahre später ging er im Alter von 62 Jahren aufgrund gesundheitlicher Probleme seiner Frau in den vorzeitigen Ruhestand. Nachdem seine Frau rund 18 Monate später starb, heiratete er 1974 erneut; nach dem Tod seiner zweiten Frau im Jahr 1982 lebte er die letzten 17 Jahre seines Lebens allein. Er starb im Januar 2000 im Alter von 91 Jahren in Woodford.

Ralph Kekwick, der rund 50 wissenschaftliche Publikationen veröffentlichte, widmete sich am Lister Institute zunächst der Untersuchung von pathologisch veränderten Seren und von Immunseren mittels Ultrazentrifugation und elektrophoretischer Techniken und später der Präparation und Untersuchung von Serum und Blutplasma für Transfusionszwecke. Ab 1944 beschäftigte er sich mit der Gewinnung einzelner therapeutisch nutzbarer Bestandteile aus Plasma sowie der Aufreinigung von Fibrinogen und Blutgerinnungsfaktor VIII.

Im Rahmen dieser Arbeiten gelang ihm die Herstellung des ersten wirksamen Faktor-VIII-Präparats zur Behandlung von Hämophilie. Die von ihm entwickelte Methode zur Gewinnung von Faktor VIII wurde vom Lister Institute bis in die 1970er Jahre zur Produktion entsprechender Medikamente genutzt. Weitere seiner Arbeiten betrafen die physiko-chemischen Eigenschaften der AB0-Antigene und die Analyse von Proteinen im Urin bei Nierenfunktionsstörungen.

Ralph Kekwick erhielt 1957 den vom Royal College of Pathologists verliehenen "Oliver Memorial Award" für seine wissenschaftlichen Beiträge zur Bluttransfusion. Darüber hinaus wurde er 1966 in die Royal Society aufgenommen.



</doc>
<doc id="6537245" url="https://de.wikipedia.org/wiki?curid=6537245" title="Toroidale-Poloidale Zerlegung">
Toroidale-Poloidale Zerlegung

Die Toroidale-Poloidale Zerlegung ist eine Zerlegung eines dreidimensionalen, divergenzfreien Vektorfeldes, z. B. des Erdmagnetfeldes, in zwei Anteile, die jeweils nur von einem eindeutigen Skalarfeld abhängen.

Die Zerlegung geht von einem Vektorpotential formula_1 des dreidimensionalen divergenzfreien Vektorfeldes formula_2 aus. Divergenzfrei bedeutet, dass formula_3 ist, bzw. anschaulich, dass das Feld keine Quellen und Senken hat. Ein Beispiel für ein divergenzfreies Feld ist die magnetische Flussdichte. Das Vektorpotential des Vektorfeldes ist so definiert, dass die Rotation des Potentials das Vektorfeld ergibt:
Das Potential lässt sich in eine radiale formula_5 und eine tangentiale Komponente formula_6 aufteilen.
Dabei stellt formula_8 einen für die Geometrie des Problems geeigneten Einheitsvektor dar. Für sphärische Geometrie bietet sich der Einheitsvektor in radialer Richtung an.
Durch geeignete Wahl des poloidalen Vektorpotentials formula_9 lässt es sich aus einem Divergenzfeld ableiten, ohne dass das Magnetfeld geändert wird.
Das Vektorfeld formula_2 erhält damit die Form
und kann somit durch die beiden skalaren Potentiale formula_13 und formula_14 beschrieben werden.

Das "toroidale Feld" formula_15 ergibt sich aus der Rotation des Vektorpotentials formula_5:

Durch Ausmultiplizieren der Rotation in Kugelkoordinaten sieht man, dass das Feld keine radialen Anteile hat:

Das Feld ist auf der Kugeloberfläche divergenzfrei. Es gibt keine radiale Feldkomponente. Toroidale Magnetfelder können von poloidalen Strömen angetrieben werden und umgekehrt. Das bedeutet in der Geophysik, dass die von in der Erde und in Ozeanen fließenden Wirbelströmen erzeugten toroidalen Magnetfelder an der Erdoberfläche null sind.

Der Begriff "toroidal" leitet sich aus der Torusform dieser Felder in rotationssymmetrischen Systemen ab – die Feldlinien verlaufen in Kreisen. Für die Beschreibung allgemeiner toroidaler Felder ist er daher missverständlich.

Das "poloidale Feld" formula_19 entsteht aus der Rotation des Vektorpotentials formula_6.

Es hat sowohl radiale als auch tangentiale Komponenten. Der Begriff leitet sich aus der Dipol-Form beim Erdmagnetfeld ab. Da das toroidale Magnetfeld der Erde nur "in" ihr auftritt, beschreibt das poloidale Feld das Erdmagnetfeld oberhalb der Erdoberfläche vollständig.

Das Feld eines magnetischen Dipols mit Dipolmoment formula_22 im Koordinatenursprung hat ein Vektorpotential
welches sofort als poloidales Feld erkennbar ist. Dabei ist formula_24 die magnetische Feldkonstante. Das damit verbundene Potential formula_14 ergibt sich aus der Multipolentwicklung zu

Falls sich Dipole außerhalb des Koordinatenursprungs befinden, so enthält das Feld auch Multipolmomente anderer Ordnungen.

Wenn der zuvor beschriebene radiale Dipol entlang des magnetischen Moments verschoben wird (d. h. das magnetische Moment liegt in Radialrichtung), so ändert sich das Vektorpotential in

Das zugehörige Potential formula_14 ist

Das bedeutet, dass aus radialen Dipolmomenten ausschließlich ein poloidales Feld erzeugt werden kann. Für die Erzeugung toroidaler Komponenten müssen tangentiale magnetische Momente beteiligt sein.

Der erste Summand stellt den poloidalen Teil des Feldes dar, der zweite den toroidalen Anteil.

Legt man das magnetische Moment auf die "z"-Achse und den Dipol selbst auf die "x"-Achse, so erhält man

Radiale und tangentiale Dipole können als Basis zum Aufbau des Magnetfeldes dienen. D. h. zusammen mit räumlichen Drehungen der Basiselemente lässt sich jede magnetische Konfiguration erstellen. Wenn man also für beide Dipole Potentiale formula_33 und formula_14 bestimmt, kann man für jede Konfiguration die Gesamtpotentiale errechnen.




</doc>
<doc id="6608551" url="https://de.wikipedia.org/wiki?curid=6608551" title="Sternatmosphäre">
Sternatmosphäre

Als Sternatmosphäre bezeichnet man die sichtbaren äußeren Bereiche eines Sterns. Physikalisch exakter umfasst der Begriff die für Licht durchlässigen Schichten von Sternen verschiedener Art.

Geleitet von Beobachtungen der Sonne, des bestuntersuchten Sterns, unterscheidet man von innen nach außen 3–4 heiße Gasschichten:
Die Photosphäre (griech. "Lichthülle") ist die unterste, dichteste und kühlste Schicht der Sternatmosphäre. Durch sie dringt die vom Sonneninnern kommende Strahlungshitze als sichtbares Licht nach außen.

Sie dominiert das sichtbare Sternenlicht, das die höheren Schichten weitgehend unbeeinflusst durchquert. Die Photosphäre der Sonne hat eine effektive Temperatur von etwa 5800 K. Sie zeigt Phänomene wie Sonnenflecken, helle Sonnenfackeln und die Granulation, eine durch Konvektion verursachte körnige Struktur der Sonnenoberfläche. Das Photosphärische Spektrum wird bestimmt von Eigenschaften wie Temperatur (wesentlich für die Spektralklasse), Schwerebeschleunigung (bestimmend für die Leuchtkraftklasse), und Gehalt an schweren Elementen im Vergleich zu Wasserstoff und Helium (Metallizität). Genaue physikalische Modelle von Sternatmosphären und ihren Spektren sind deshalb ein wichtiges Werkzeug der Astrophysik.

Die Chromosphäre (griech. "Farbhülle") ist die nach oben anschließende Gasschicht. Sie hat ihren Namen vom roten Licht, das bei einer totalen Sonnenfinsternis kurz sichtbar wird.

Sie wird normalerweise von der Photosphäre völlig überstrahlt. Ihre Temperatur steigt nach einem Minimum am Oberrand der Photosphäre wieder an, ihr Spektrum besteht aus schmalen Emissionslinien, insbesondere H-alpha bei der Wellenlänge von 656,3 nm, was einem tiefroten Licht entspricht.

Oberhalb der Chromosphäre wird (bei der Sonne) manchmal eine Übergangsschicht zur Korona definiert.

Die Korona ist der bei Sonnenfinsternissen sichtbare "Strahlenkranz". Er besteht aus sehr dünnem, aber über eine Million Kelvin heißem Gas. Dieses wird durch verschiedene Mechanismen erhitzt und kann mehrere Sonnenradien in den Raum hinausreichen. Die Struktur der "Strahlen" hängt von der momentanen Sonnenaktivität ab. Im Sonnenfleckenminimum hat die Korona einen eher runden Umriss, während sie im Fleckenmaximum in Äquatorrichtung länglich ausgedehnt erscheint. Dies hängt mit dem Verlauf der magnetischen Feldlinien zusammen, die das ionisierte Gas stark beeinflussen.

In den unteren Teil der Sonnenkorona werden von der Chromosphäre und von eruptiven Flares andauernd kleine Spikulen und weit aufsteigende Protuberanzen emporgeschleudert, die oft erst nach mehreren Tagen wieder zur Photosphäre herunter sinken.

Während diese Atmosphärenschichten bei der Sonne bestens erforscht sind, kann man bei anderen Sternen aufgrund ihrer großen Entfernung meist nur die Photosphäre genauer erforschen. Im Regelfall ist nur ihr Spektrum hell genug dafür; jenes der darüber befindlichen Schichten wird fast völlig überstrahlt.

Die Existenz von Stern-Chromosphären und Koronae folgt überwiegend aus theoretischen Modellen des Sternaufbaus -- siehe dort. Bei einigen Sterntypen hat man jedoch ähnliche Phänomene beobachtet, was die aktuelle Theorie des Sternaufbaus stützt. So wurden auf nahen Riesensternen aus Helligkeitsschwankungen sogenannte Sternflecken postuliert, deren Natur unseren Sonnenflecken entsprechen dürfte. 

Einige Veränderliche zeigen Materieausbrüche, die als Korona-Phänomene oder äußerst heftiger Sonnenwind interpretiert werden, und Junge Sterne stoßen regelmäßig Gaswolken aus, die mit dem Mechanismus von Spikulen verglichen werden können.



</doc>
<doc id="7656631" url="https://de.wikipedia.org/wiki?curid=7656631" title="Proteincharakterisierung">
Proteincharakterisierung

Die Proteincharakterisierung umfasst biochemische und biophysikalische Methoden zur Bestimmung der Eigenschaften eines Proteins oder zur Darstellung eines Proteoms.

Proteine werden zur Bestimmung ihrer Eigenschaften meistens durch einen Zellaufschluss freigesetzt und mittels einer Proteinreinigung von den anderen Bestandteilen des Ausgangsmaterials getrennt.

Die Aminosäuresequenz kann durch einen Edman-Abbau, einen Schlack-Kumpf-Abbau oder massenspektrometrisch durch eine De-Novo-Sequenzierung nach In-Gel-Verdau ermittelt werden. Posttranslationale Modifikationen können per Western Blot oder massenspektrometrisch bestimmt werden.

Die Identifikation eines Proteins kann auch mit spezifischen Antikörpern per Immunmarkierung in einem Western Blot oder durch eine Affinitätschromatographie erfolgen. Ebenso kann ein Nachweis auch durch Messung seiner Funktion erbracht werden, bei Enzymen per Enzymkinetik. Eindeutig identifizierbare Teile eines Proteins können durch Massenspektrometrie (MALDI-TOF, ESI-MS/MS, per LC-MS/MS), über einen Peptidmassenfingerprint oder eine De-Novo-Sequenzierung nach In-Gel-Verdau nachgewiesen werden, die anschließend über Datenbanken wie Mascot identifiziert werden.

Das Transposon Tagging, das TILLING oder eine ungezielte Mutagenese erlaubt die Identifizierung der proteincodierenden Gene anhand der Veränderung des Phänotyps mit anschließender Bestimmung der DNA-Sequenz der den Phänotyp verändernden Mutation.

Eine Bestimmung der Molmasse eines Proteins kann z. B. durch Gelpermeationschromatographie, durch SDS-PAGE, durch isopyknische Zentrifugation, durch Feld-Fluss-Fraktionierung oder massenspektrometrisch erfolgen.

Die Proteinfaltung wird z. B. durch eine Röntgenstrukturanalyse (XRD), per Elektronenmikroskop oder per NMR bestimmt. Änderungen in der Proteinfaltung können über FTIR-Spektroskopie oder durch Circulardichroismusmessung verfolgt werden.

Die Proteinfaltung kann im Zuge eines Protection Assays durch eine limitierte Proteolyse oder eine thermische Denaturierung mit Proteinfarbstoffen teilweise eingegrenzt werden. Die Oberfläche eines Proteins kann durch einen Deuterium-Austausch oberflächennaher Wasserstoffatome bestimmt werden.

Proteine können andere Moleküle mit der jeweils entsprechenden Affinität binden, bei Enzymen kommt zusätzlich eine katalytische Aktivität vor.

Daneben werden Protein-Protein-Wechselwirkungen und Wechselwirkungen mit anderen Molekülen z. B. durch Affinitätschromatographie, Ko-Immunpräzipitation, SPINE und andere Pulldown-Assays, Molekulares Display, Hefe-Zwei-Hybrid-System, chemische Quervernetzung und SDS-PAGE oder Massenspektrometrie, Native Gelelektrophorese, Far-Western-Blot, Ligandenbindungstests (Radioligand, Reporterenzyme, fluoreszente Liganden, elektrische Ableitung), Label-Transfer, Proximity Ligation Assay, Affinitätselektrophorese, Alanin-Scan, Microscale Thermophoresis, Isotherme Titrationskalorimetrie, Oberflächenplasmonenresonanzspektroskopie, Bio-Layer-Interferometrie, Fluoreszenzkorrelationsspektroskopie, Förster-Resonanzenergietransfer, Biolumineszenz-Resonanzenergietransfer, Bimolekulare Fluoreszenzkomplementation, Proteinfragment-Komplementation, Thermal Shift Assay, Dichtegradientenzentrifugation oder Gel-Permeations-Chromatographie bestimmt. Daneben kann eine Strukturaufklärung per Röntgenstrukturanalyse oder per NMR-Spektroskopie erfolgen.

Ebenso kann ein Gen-Knockout zu zellulären und phänotypischen Effekten führen, die einen Hinweis auf die Funktion eines Proteins liefern. Durch die Wiederherstellung einer Funktion durch Gen-Knockin oder durch eine Verstärkung der Funktion durch eine Überexpression kann die Wirkung bei gegenteiliger Genexpression nachgewiesen werden. Durch den Vergleich der Aminosäuresequenz mit Datenbanken wie BLAST und Pfam können sequenzverwandte Proteine mit bekannten Funktionen einen Hinweis auf die möglichen Funktionen des zu untersuchenden Proteins geben.

Methoden zur Bestimmung von Protein-DNA-Wechselwirkungen (DNA-Sequenz, DNA-bindende Proteine) sind z. B. der Electrophoretic Mobility Shift Assay, die ChIP, die ChIP-on-Chip, die ChIP-Seq, der DNase Footprinting Assay, das Yeast-One-Hybrid-System (Y1H), das Bacterial-One-Hybrid-System (B1H) und das DamID.

Protein-RNA-Interaktionen können z. B. durch RNA-Seq, RIP-Chip, ICLIP, CLIP-Seq oder per PAR-CLIP nachgewiesen werden.

Methoden zur Untersuchung von Protein-Lipid-Interaktionen sind z. B. die ESR-Spektroskopie, die duale Polarisationsinterferometrie, die Fluoreszenzmikroskopie mit fluoreszenzmarkierten Lipiden oder Proteinen oder mit Lipid-Farbstoffen wie Laurdan oder Filipin.

Bei unbekannter Proteinfunktion werden enzymatische Eigenschaften wie eine Katalyse durch Ermittlung der Enzymkinetik untersucht. Hierbei kommen oftmals Inhibitoren verschiedener möglicherweise an der Funktion beteiligter Proteine zum Einsatz. Ein Alanin-Scan kann zur Bestimmung des aktiven Zentrums eingesetzt werden.

Eine Bestimmung der Menge (Quantifizierung) eines Proteins erfolgt z. B. durch Photometrie, ELISA, SDS-PAGE und Western Blot, 2D-PAGE oder massenspektrometrisch (iTRAQ, SILAC, ICAT, die Isobarenmarkierung, Tandem Mass Tags).

Durch Photometrie kann die Proteinkonzentration z. B. per Bradford-Test, per Lowry-Test, per Biuretreaktion, per BCA-Test oder bei höheren Proteinkonzentrationen auch durch die Absorption der Peptidbindung ermittelt werden, wobei jede Methode eigene Störsubstanzen besitzt, die bei Vorhandensein eine Verwendung dieser Methode ausschließen.
Verschiedene Farbstoffe binden bevorzugt Proteine, z. B. Coomassie-Brillant-Blau, Fast Green FCF, Amidoschwarz oder Ponceau S. Als Fluoreszenzfarbstoff werden z. B. Nilrot, Brilliant Sulfaflavin, 8-Anilinonaphthalin-1-sulfonsäure (8-ANS), Scopoletin, Iridium-Komplexe, SYPRO Orange, SYPRO Red, SYPRO Ruby, SYPRO Tangerine, Flamingo, Krypton, Coomassie Fluor Orange, Lucy 506, Lucy 565, Lucy 569 oder Epicocconon ("Lightning Fast", "Deep Purple Stain") verwendet.

Durch die Peptidbindung absorbieren Proteine ultraviolettes Licht bei einer Wellenlänge von etwa 205 nm (190 nm bis 230 nm), daneben absorbieren auch Phenylalanin, Tyrosin und Tryptophan UV-Licht bei Wellenlängen von 280 nm bis 288 nm. Diese Absorption kann zur photometrischen Quantifizierung und zur Bestimmung der Aufreinigungsfaktoren verwendet werden.

Im Gegensatz zu Kohlenhydraten und Nukleinsäuren kommen manche Strukturmotive durch die verschiedenen enthaltenen Aminosäuren nur bei Proteinen vor, z. B. Sulfhydryl-enthaltende Cysteine oder Phenolreste in Tyrosinen. Diese können mit entsprechenden Reagenzien selektiv markiert werden. Einige Proteaseinhibitoren führen zu einer selektiven Modifikation von Proteinen. Durch eine Quervernetzung werden meistens zwei reaktive Gruppen in einem Protein miteinander verbunden. Aminogruppen in Proteinen wie bei Lysinen oder unmodifizierten "N"-Termini können selektiv mit N-Hydroxysuccinimidestern, Sulfosuccinimidestern [z. B. Bis(sulfosuccinimidyl)suberat], Imidoestern (z. B. Dimethyladipimidat), Isothiocyanaten (z. B. PITC, FITC), 1-Fluor-2,4-dinitrobenzol (DNFB), Dansylchlorid oder Aldehyden (z. B. Glutaraldehyd) modifiziert werden. Cysteine können mit Maleimiden (z. B. "N"-Ethylmaleinimid), Disulfiden und Sulfiden (z. B. 2-Mercaptoethanol, Dithiothreitol, Dithioerythritol, Tris(2-carboxyethyl)phosphin, Ellmans Reagenz) oder Iodacetamiden (z. B. IAEDANS) reagieren. Tyrosine können per Oxidation mit Jod markiert werden. Fixierungsmittel führen meist zu einer Quervernetzung verschiedener Aminosäuren.

Durch Bromcyan kann ein Protein chemisch an enthaltenen Methioninresten gespalten werden, aus dem Methionin entsteht das Homoserin-Lacton. Mit einigen Isothiocyanaten wie PITC kann die N-terminale Aminosäure abgespalten werden, was im Zuge des Edman-Abbaus auch wiederholt angewendet werden kann. Selektive Spaltungen können enzymatisch durch Endopeptidasen bewirkt werden, die im Vergleich zu nichtenzymatischen Spaltungen längere Erkennungssequenzen benötigen.

Die unterschiedliche Proteinexpression in einem Proteom kann z. B. durch Gelelektrophorese (2D-PAGE), Proteinarrays, Difference Gel Electrophoresis, MeCAT oder ICAT dargestellt werden.

Anhand der Aminosäuresequenz eines Proteins können verschiedene Eigenschaften eines Proteins erkannt werden. Durch einen Ramachandran-Plot oder einen Janin-Plot können bestimmte Sekundärstrukturen identifiziert werden. Die Lokalisation in einem zellulären Kompartiment kann anhand der Signalsequenzen und der Transmembrandomänen ermittelt werden. Der Hydropathische Index beschreibt dabei die Neigung bestimmter Sequenzen zur Einlagerung in eine Biomembran. Der Instabilitätsindex liefert einen Hinweis auf die biologische Halbwertszeit eines Proteins. Anhand der Sequenz können mögliche Stellen für posttranslationale Modifikationen bestimmt werden.



</doc>
<doc id="8095090" url="https://de.wikipedia.org/wiki?curid=8095090" title="Rudolf Allmann (Kristallograph)">
Rudolf Allmann (Kristallograph)

Rudolf Allmann (* 19. Februar 1931 in Kötzschau) ist ein deutscher Kristallograph.

Allmann wuchs in Wettelrode auf und legte in Sangerhausen 1949 das Abitur ab. 1949 bis 1952 studierte er in Halle (Saale) Chemie und Mathematik auf Lehramt und unterrichtete danach. 1955 bis 1959 studierte er in Marburg. Dort kam er mit der Kristallographie in Berührung und begann mit einer Dissertation bei Erwin Hellner. 1961 folgte er Hellner an das Mineralogische Institut der Universität Kiel, wurde aber von der Universität Marburg promoviert mit der Kristallstrukturanalyse einer Jodadditionsverbindung. 1964 folgte er Hellner zurück an den Lehrstuhl für Mineralogie an der Universität Marburg. Nach einem Aufenthalt an der Johns Hopkins University in Baltimore 1966–1967 habilitierte er sich in Marburg 1968. Die Habilitationsschrift beschäftigte sich mit Doppelschichtstrukturen mit brucitähnlichen Schichten. 1970 wurde er zum Professor ernannt und arbeitete bis zu seinem Ruhestand 1996 am Institut für Mineralogie, Petrologie und Kristallographie der Universität Marburg.

Allmann bestimmte über 100 Kristallstrukturen verschiedener Substanzen und beschäftigte sich auch mit den Grundlagen der Kristallstrukturbestimmung und der Messtechnik sowie den Grundlagen von Strukturen. Auch veröffentlichte er ein Buch über Röntgenpulverdiffraktometrie.

Nach seinem Ausscheiden aus dem aktiven Dienst widmete er sich in jahrelanger Arbeit der kritischen Revision und Erweiterung der Inorganic Crystal Structure Database (ICSD). Insbesondere für diese Arbeit verlieh ihm die Deutsche Gesellschaft für Kristallographie 2009 die Will-Kleber-Gedenkmünze.



</doc>
<doc id="8515295" url="https://de.wikipedia.org/wiki?curid=8515295" title="Luca Turin">
Luca Turin

Luca Turin (* 20. November 1953 in Beirut) ist ein italienischer Biophysiker und Autor, der sich mit dem Geruchssinn, Parfumerie und der Riechstoffbranche befasst.

Turin ist das Kind argentinisch-italienischer Eltern, er wuchs in Frankreich, Italien und der Schweiz auf. Er studierte am University College London (UCL) Physiologie und Biophysik und wurde dort promoviert. 1988 begann Turin bei Henri Korn am Institut Pasteur zu arbeiten. Er verlor seine Stellung, nachdem er Korn vorgeworfen hatte, dass einige seiner Forschungsergebnisse auf Fälschungen beruhten. Später arbeitete er an den National Institutes of Health in North Carolina. Danach zog er nach London und unterrichtete Biophysik am University College London. 2001 wurde er als Technischer Leiter bei der eben gegründeten Firma Flexitral in Chantilly (Virginia) angestellt. 2009 bis 2011 forschte er am MIT, von 2011 bis 2013 am Alexander Fleming Institut in Athen, 2014 war er Gastprofessor am Institut für Theoretische Physik der Universität Ulm.

Turin griff eine Theorie von Malcolm Dyson (1938) auf, dass der Geruchssinn nicht durch Formerkennung mittels Rezeptoren funktioniere, sondern wie ein Spektrometer, d. h. dass Gerüche aufgrund der unterschiedlichen Schwingungsfrequenzen von Molekülen unterschieden werden. Die Theorie ist jedoch umstritten. Eine wichtige Folge dieser These wäre der Isotopeneffekt, dass z. B. die gewöhnliche und die deuterierte Variante einer Verbindung aufgrund der unterschiedlichen Schwingungsfrequenz auch unterschiedlich riechen müssten – trotz der gleichen Form.

Haffenden u. a. zeigten 2001 in einer Studie, dass Menschen den Geruch von gewöhnlichem und von deuteriertem Benzaldehyd unterscheiden können 2011 veröffentlichten Turin und seine Kollegen einen Artikel in PNAS, dass Fruchtfliegen zwischen Geruchsstoffen und deren deuterierten Varianten unterscheiden können. 2013 veröffentlichten Turin und seine Kollegen eine Studie in PLoS ONE, dass Menschen gewöhnliches und deuteriertes Moschus leicht am Geruch unterscheiden können. Dabei gibt es einen Schwellenwert, erst ab einem Anteil von 14 Deuterium-Atomen, d. h. einer 50%igen Deuterierung, können Menschen den Duft unterscheiden. Nach Turin ist der Geruchssinn eine biologische Methode der Spektroskopie und wäre daher ein Beispiel für Quantenbiologie.






</doc>
<doc id="8903812" url="https://de.wikipedia.org/wiki?curid=8903812" title="Gottlieb Aenetius">
Gottlieb Aenetius

Gottlieb Aenetius eigentlich: "Theophilus Ænetius" auch: "Löblich" (* 20. April 1574 in Königsfeld (Sachsen); † 15. September 1631 in Jena) war ein deutscher Physiker.

Gottliebs Eltern waren der damalige Königsfelder Pfarrer Jacob Aenetius und dessen aus Rochlitz stammenden Frau Anna (Margaretha) Burs. Nach dem Tod seines Vaters zog seine Mutter wieder nach Rochlitz, wo er an der dortigen Schule seine Bildungsgrundlagen legte. Anfang Juni 1589 setzte er seine Ausbildung am Gymnasium in Halle (Saale) fort, welches unter der Leitung des Rektors Christoph Caesar stand. 1591 begann er bei seinem Verwandten Michael Burs in Wien eine Ausbildung zum Kaufmann. Nach drei Jahren entschloss er sich 1594 die Universität Leipzig zu beziehen, um ein Studium der philosophischen und theologischen Wissenschaften zu absolvieren. 

In Leipzig erwarb er am 28. Juni 1596 das Bakkalaurat und am 26. Januar 1598 den akademischen Grad eines Magisters der Philosophie. Nachdem er einige Zeit im Leipziger Schulwesen aktiv gewesen war, erhielt er 1607 die Stelle eines Konrektors des Gymnasiums in Thorn. 1610 zog er wieder nach Lützen, wo er die Familie seiner ersten Frau unterstützte. Im Sommersemester 1615 wurde er Adjunkt der philosophischen Fakultät der Universität Jena und erhielt dort 1617 die Professur der Physik. Als Hochschullehrer der Salana beteiligte er sich auch an den organisatorischen Aufgaben der Hochschule. So war er einige Male Dekan der philosophischen Fakultät und im Sommersemester 1621 Rektor der Alma Mater.

Aenetius war zwei Mal verheiratet. 

Seine erste Ehe schloss er am 7. September 1602 in Lützen mit Justina Günther (* 1585 in Lützen; † 18. Oktober 1605), die Tochter des Stadtrichters in Lützen Michael Günther (* 4. Oktober 1546 in Lützen; † 15. März 1610 ebd.) und dessen 1572 geheirateten Frau Prisca Steinbach, die Tochter des Bürgermeisters in Lützen Thomas Steinbach. 

Seine zweite Ehe ging er am 25. Juni 1623 in Jena mit Maria Rinck (* um 1600 in Eisleben begr. 4. Januar 1648 in Jena) die Tochter des Ratsherrn in Eisleben Philipp Rinck, ein. Aus der Ehe stammen zwei Söhne und drei Töchter. Von den Kindern kennt man den Sohn Johannes Baptist Aenetius (1624–1625), sowie die Töchter Margarethe Aenetius († 1629), Maria Dorothea Aenetius und Justina Aenetius, welche in erster Ehe mit Valerian Theodor Clement verheiratet war und ihre zweite Ehe 1638 mit dem Jenaer Professor Gottfried Möbius einging. Seine Witwe verheiratete sich am 3. November 1634 in Jena mit Georg Pascasius (* 6. Oktober 1613 in Sangerhausen; † 18. Dezember 1667 in Jena).




</doc>
<doc id="9035361" url="https://de.wikipedia.org/wiki?curid=9035361" title="Clo (Einheit)">
Clo (Einheit)

Das Clo ist die veraltete Maßeinheit für den Kehrwert des Wärmeübergangskoeffizienten und wurde vor allem in der Textilindustrie verwendet.

1 Clo = 0,155 K·Meter²/Watt


</doc>
<doc id="9039874" url="https://de.wikipedia.org/wiki?curid=9039874" title="Wenedikt Petrowitsch Dschelepow">
Wenedikt Petrowitsch Dschelepow

Wenedikt Petrowitsch Dschelepow, "", englische Transkription "Venedikt Dzhelepov", (* 12. April 1913 in Moskau; † 12. März 1999 in Dubna (Moskau)) war ein russischer Physiker, bekannt zu Forschungen zur Myonen-Kernfusion, zur experimentellen Kern- und Elementarteilchenphysik und Beschleunigerphysik.

Dschelepow studierte von 1932 bis 1937 nach einer Elektrikerlehre in Leningrad am dortigen Polytechnikum. Er arbeitete am Radium-Institut der Akademie der Wissenschaften unter Igor Wassiljewitsch Kurtschatow. Von 1937 bis 1941 war er Soldat unter anderem im Krieg gegen Finnland. Von 1941 bis 1943 war er am Joffe-Institut in Leningrad und mit diesem nach Kasan evakuiert. Von 1943 bis 1948 war er unter Kurtschatow stellvertretender Leiter des Labors Nr. 2, an dem die geheime sowjetische Forschung zur Kerntechnik stattfand (das spätere Kurtschatow-Institut). Von 1948 bis 1956 war er stellvertretender Leiter des Hydrauliklabors der Akademie der Wissenschaften, das ab 1954 Institut für Kernprobleme in Dubna war. Von 1956 bis 1989 war er dessen Direktor am nunmehrigen Vereinigten Institut für Kernforschung, in dessen Rat er von 1957 bis 1975 war. 1947 wurde er promoviert und 1954 habilitierte er (russischer Doktortitel). 1961 wurde er Professor.

Er befasste sich mit experimenteller Kern- und Elementarteilchenphysik und Beschleunigerphysik und war am Bau des ersten sowjetischen Synchrotrons in Dubna beteiligt. Dschelepow befasste sich auch mit medizinischen Anwendungen der Kernphysik in der Krebstherapie. Mitte der 1960er Jahre fand er das überraschende und damals nicht erklärbare Phänomen eines Anstiegs der Fusionsraten bei Myon-katalysierter Fusion in Deuteriummolekülen mit der Temperatur. 1967 fand E. A. Vesman eine Erklärung in Resonanzeffekten mit komplizierteren Molekülen. 1979 bestätigte seine Gruppe einen von Ponomarjow vorhergesagten Resonanzeffekt bei Tritium-Deuterium-Gemischen, der die Fusionsrate erheblich steigerte. Das trug damals zur Wiederbelebung des Interesses an Myon-katalysierter Fusion auch im Westen bei.

Von 1977 bis 1982 war er in der internationalen Kommission für zukünftige Beschleuniger der IUPAP.

1986 erhielt er die Kurtschatow-Goldmedaille mit Leonid Iwanowitsch Ponomarjow (der führend in der theoretischen Forschung zur Myon-katalysierten Fusion in der Sowjetunion war). 1951 und 1953 erhielt er den Stalinpreis, 1953 den Leninorden, 1983 den Orden der Oktoberrevolution und zweimal den Orden des Roten Banners der Arbeit (1962, 1974). Er war Ehrenbürger von Dubna, wo 2013 ihm und Bruno Pontecorvo ein Denkmal errichtet wurde. Außerdem ist dort nach ihm eine Straße benannt. 1966 wurde er korrespondierendes Mitglied der Russischen Akademie der Wissenschaften. Seit 1946 war er Mitglied der KPdSU.

1961 bis 1988 war er im Herausgebergremium der Zeitschrift "Journal of Experimental and Theoretical Physics" (JETP). Der Kernphysiker Boris Sergejewitsch Dschelepow war sein älterer Bruder.



</doc>
<doc id="9277408" url="https://de.wikipedia.org/wiki?curid=9277408" title="Leiterschaukel">
Leiterschaukel

Die Leiterschaukel ist ein physikalisches Experiment aus dem Bereich der elektromagnetischen Induktion.

Eine Leiterschaukel besteht aus einem Hufeisenmagneten, zwischen dessen Polen ein nicht magnetisierbarer, aber leitfähiger Stab befestigt wird, der an einen Stromkreis angeschlossen wird.

Sobald der Stromkreis geschlossen wird, was oft durch das Leuchten einer Lampe oder durch ein Strommessgerät angezeigt wird, beginnt der Stab sich in eine Richtung zu bewegen. Diese Richtung wird durch die Drei-Finger-Regel bestimmt, Ursache der Bewegung ist die Lorentzkraft. 

Wenn der Stab im Magnetfeld manuell bewegt wird, werden sich durch die Lorentzkraft Elektronen im Stab bewegen. Bei geöffnetem Stromkreis werden sie sich – anschaulich gesprochen – an einem Ende sammeln. Die Ladungstrennung wird enden, wenn die Lorentzkraft mit der elektrischen Kraft im Gleichgewicht steht, also wenn formula_1 gilt, anders gesagt formula_2. Es ergibt sich das Induktionsgesetz für den bewegten Leiter.



</doc>
<doc id="9452325" url="https://de.wikipedia.org/wiki?curid=9452325" title="Daniel Alpert">
Daniel Alpert

Daniel Alpert (* 10. April 1917 in Hartford in Connecticut; † 4. November 2015 in Eugene, Oregon) war ein US-amerikanischer Physiker, der heute vor allem durch das von ihm beschriebene Ionisationsvakuummeter bekannt ist, das nach dem Erfinder Robert T. Bayard und nach ihm Bayard-Alpert-Vakuummeter genannt wird.

Daniel Alpert erhielt 1937 am Trinity College in seiner Geburtsstadt Hartford den Grad des Bachelor of Science. Von 1937 bis 1941 arbeitete er an der Stanford University bei William Webster Hansen im Bereich der Mikrowellen; dort lernte er auch von Sigurd Fergus Varian, der damals das erste Klystron baute. Im Jahr 1942 erhielt er an der Stanford University den Doktorgrad. Ab Oktober 1941 war er in den "Westinghouse Research Laboratories" in Pittsburgh tätig, wo er Bauteile von Radaranlagen entwickelte. 1945 arbeitete er zwischenzeitlich auch am Manhattan Project, wobei er mit Ernest Lawrence an der Trennung der Uranisotope arbeitete. Ab 1957 arbeitete er als Professor der Physik an der University of Illinois. Dort leistete er Pionierarbeit im Bereich des Lernens mit Hilfe des Computers, er war ein Mitbegründer eines entsprechenden Forschungslabors und trug zur Entwicklung des Computersystems PLATO bei. 1987 wurde Alpert emeritiert, er arbeitete aber weiterhin, z. B. als Berater.

Besonders bekannt ist Alpert für seinen Beitrag zur Entwicklung und Beschreibung des Ionisationsvakuummeters.

1957 erhielt Alpert eine Ehrendoktorwürde des Trinity College, 1965 ein Guggenheim-Stipendium für Physik. 1980 erhielt er den Gaede-Langmuir-Preis der "American Vacuum Society" AVS für seine Beiträge zur Erzeugung, Messung und Anwendung von Ultrahochvakuum.


</doc>
<doc id="9656700" url="https://de.wikipedia.org/wiki?curid=9656700" title="Institute of Acoustics">
Institute of Acoustics

Das Institute of Acoustics (IOA) ist die britische Gesellschaft für Ingenieure und Wissenschaftler im Bereich der Akustik. 

Sie ging 1974 aus dem Zusammenschluss der 1963 gegründeten Society of Acoustic Technology (ab 1965 British Acoustic Society) und der Akustik-Gruppe des Institute of Physics hervor. Sie haben rund 3000 Mitglieder (2016).

Es gibt Untergruppen für Raumakustik, Elektroakustik, Umweltlärm, Messinstrumente, Musikalische Akustik, Physikalische Akustik, Sprache und Hören, Unterwasser-Akustik und Schwingungen und Lärm im Ingenieurwesen (Vibration and Noise Engineering).

Sie vergeben verschiedene Preise, darunter die Rayleigh-Medaille, die Tyndall-Medaille und die A. B. Wood Medal.

Ihr Sitz ist in St Albans.


</doc>
<doc id="10007254" url="https://de.wikipedia.org/wiki?curid=10007254" title="Crater erasure">
Crater erasure

Crater erasure ist in der Planetologie der Vorgang des Einebnens von Einschlagskratern auf atmosphärenlosen planetaren Körpern unter der Abwesenheit von vulkanischen geologischen Prozessen.

Die Oberfläche atmosphärenloser planetarer Körper wird in Abwesenheit von geologischer Erosion durch Einschlagskrater geprägt, die durch den Aufprall von Kometen, Asteroiden und Mikrometeoroiden verursacht werden. Trotz der Abwesenheit von Erosion werden die Krater eingeebnet durch die folgenden Prozesse:

Das Auslöschen von Einschlagskratern begrenzt die Genauigkeit der aus der Kraterstatistik (genauer aus der Durchmesser-Häufigkeitsverteilung) abgeleiteten Alter der planetaren Oberfläche. Die aus der Statistik berechnete Alter ist nur eine Untergrenze seit der Erreichung eines Gleichgewichts zwischen dem Entstehen neuer und dem Einebnen alter Krater.




</doc>
<doc id="10351826" url="https://de.wikipedia.org/wiki?curid=10351826" title="Anthony Raymond Bell">
Anthony Raymond Bell

Anthony Raymond „Tony“ Bell ist ein britischer Astro- und Plasmaphysiker.

Bell wurde 1977 an der Universität Cambridge in Radioastronomie promoviert (Thema: ). Danach arbeitete er als Physiker bei GEC-Marconi unter anderem an Signalverarbeitung für Radar und Laser-Plasma-Wechselwirkung. 1985 wurde er Lecturer am Imperial College London und 2007 wurde er Professor an der Universität Oxford (Clarendon Laboratory) und ist außerdem an der des Rutherford Appleton Laboratory.

Er hatte eine führende Rolle in der Entwicklung des Standardmodells über den Ursprung kosmischer Strahlung und Beschleunigungsmechanismen kosmischer Strahlung durch Stoßwellen () über magnetische Spiegel-Effekte. Außerdem zeigte er 2004 wie dabei Magnetfelder verstärkt werden können, was für die Erklärung ultrahochenergetischer kosmischer Strahlung herangezogen wurde.

Von ihm stammen bedeutende Beiträge zur Theorie des Elektronentransports in laserinduzierten Plasmen wie bei der Trägheitsfusion. Er initiierte die nicht-lokale Theorie des Elektronentransports bei Laser-Plasma-Wechselwirkung und zeigte wie selbstinduzierte Magnetfelder Elektronenstrahlen hoher Energie in den Fusionsbrennstoff fokussieren können. Mit John G. Kirk zeigte er die Möglichkeit der Elektron-Positron-Paarerzeugung in intensiven Laser-Plasma-Wechselwirkungen, also quantenelektrodynamische Effekte.

2014 erhielt er die Fred Hoyle Medal and Prize, 2016 die Eddington Medal der Royal Astronomical Society und 2018 den Hannes-Alfvén-Preis. 2017 wurde er Fellow der Royal Society.



</doc>
<doc id="10572147" url="https://de.wikipedia.org/wiki?curid=10572147" title="Paul R. Woodward">
Paul R. Woodward

Paul R. Woodward ist ein US-amerikanischer Astrophysiker und angewandter Mathematiker.

Woodward studierte Mathematik und Physik an der Cornell University mit dem Bachelor-Abschluss 1967 und wurde 1973 an der University of California, Berkeley in Physik promoviert. Danach war er am Lawrence Livermore National Laboratory tätig (unterbrochen von drei Jahren am Observatorium der Universität Leiden ab 1975). Ab 1985 war er Professor für Astronomie an der University of Minnesota. 1995 gründete er dort das "Laboratory for Computational Science and Engineering" und ist dessen Direktor. Er ist Fellow des "Minnesota Supercomputing Institute".

Woodward befasst sich mit numerischer Hydrodynamik und war einer der Entwickler der "Piecewise-Parabolic Method" (PPM). Ab Mitte der 1980er Jahre befasste er sich auch mit Visualisierung im wissenschaftlichen Rechnen und nutzte dazu den Cray-2-Supercomputer des "Minnesota Supercomputing Institute", um Flüssigkeitsströmungen (berechnet mit PPM) zu visualisieren. Um den Schritt von zwei auf drei Dimensionen zu vollziehen, nutzte er ab den 1990er Jahren massiv parallele Rechner und passte den PPM- Code dafür an. Dabei befasste er sich auch mit der dazu nötigen Compiler-Technik.

Neben Anwendungen in Astrophysik befasste er sich auch mit solchen in Meteorologie, Gasströmungen hoher Geschwindigkeit, Plasmaphysik, Magnetohydrodynamik, Laserfusion und Scherströmungen. 

1995 erhielt er den Sidney Fernbach Award.




</doc>
<doc id="10732057" url="https://de.wikipedia.org/wiki?curid=10732057" title="Pjotr Petrowitsch Lasarew">
Pjotr Petrowitsch Lasarew

Pjotr Petrowitsch Lasarew (; * in Moskau; † 23. April 1942 in Alma-Ata) war ein russischer Physiker, Geophysiker, Biophysiker und Hochschullehrer.

Lasarew, Sohn eines Vermessungsingenieurs, besuchte 1888–1896 das Gymnasium und studierte dann an der medizinischen Fakultät der Universität Moskau (MGU) mit Abschluss 1901. Sein Interesse an der Physiologie führte ihn zur Physik, verkörpert durch Hermann von Helmholtz und Jacques Loeb. 1902 wurde er zum Doktor der Medizin promoviert. Darauf wurde er Assistent der von Julija Iwanowna Basanowa gestifteten Ohrenklinik der medizinischen Fakultät der MGU. 1903 legte er als Externer die Prüfung für den gesamten Kurs der physikalisch-mathematischen Fakultät der MGU ab. Seine ersten beiden wissenschaftlichen Untersuchungen befassten sich mit der Unabhängigkeit des Höreindrucks von den Phasendifferenzen der Obertöne und der gegenseitigen Beeinflussung der Hör- und Sehwahrnehmung.

1903 begann Lasarew, das Kolloquium Pjotr Nikolajewitsch Lebedews zu besuchen. Im gleichen Jahr ging er nach Straßburg und fertigte an der Universität Straßburg in Ferdinand Brauns Laboratorium eine Doktorarbeit an. 1907 wurde Lasarew Privatdozent an der MGU. Er arbeitete in Lebedews Laboratorium und hielt Vorlesungen zur Einführung in die Physik, zur Photochemie und zur Biophysik. 1911 verteidigte er seine Magisterdissertation über den Temperatursprung bei der Wärmeleitung an der Grenzfläche zwischen Festkörper und Gas im Bereich einer freien Weglänge eines Moleküls. Im gleichen Jahr verließ er die MGU zusammen mit vielen anderen aus Protest gegen die Eingriffe des neuen Bildungsministers Léon Casso in die Autonomierechte der Universität (Affäre Casso). Lasarew arbeitete nun an der "Städtischen Moskauer Schanjawski-Volksuniversität" und mietete für Lebedews Laboratorium einen Souterrain-Raum. 1912 verteidigte er an der Universität Warschau seine Doktor-Dissertation zur Photochemie mit einer quantitativen Untersuchung des Ausbleichens von Farbpigmenten im sichtbaren Teil des Spektrums.

Nach dem Tode Lebedews 1912 arbeitete Lasarew im physikalischen Laboratorium der Moskauer Technischen Hochschule. Dort lehrte er bereits seit 1908 und wurde 1912 zum Professor gewählt. 1916 lehnte er einen Ruf an die Universität Petrograd als Nachfolger Orest Danilowitsch Chwolsons ab. Als er im März 1917 als Nachfolger des verstorbenen Boris Borissowitsch Golizyn zum Mitglied der Russischen Akademie der Wissenschaften für den Physik-Lehrstuhl gewählt wurde, stellte er die Bedingung, in Moskau bleiben zu können. 1917–1920 war er Professor des Lehrstuhls für Physik der MGU. 1917 leitete er das erste russische Forschungsinstitut für Physik, für das Ende 1916 mit privaten Mitteln von dem Architekten Alexander Nikolajewitsch Sokolow ein Gebäude gebaut wurde. Hier waren Sergei Iwanowitsch Wawilow, Grigori Alexandrowitsch Gamburzew, Wassili Wladimirowitsch Schuleikin, Michail Alexandrowitsch Leontowitsch, Boris Wladimirowitsch Derjagin, Pjotr Alexandrowitsch Rehbinder, Alexander Sawwitsch Predwoditelew, Michail Pawlowitsch Wolarowitsch und Eduard Wladimirowitsch Schpolski seine Schüler.

Bald nach der Oktoberrevolution wurde Lasarew Vorsitzender der Kommission für die Verbesserung der Lebensbedingungen der Mediziner und Mitglied der Zentralkommission für die Verbesserung der Lebensbedingungen der Wissenschaftler. Er organisierte die Einrichtung von Röntgenkabinetten für Kranke und Verwundetet der Roten Armee. Das Laboratorium des bisherigen Forschungsinstituts für Physik war nun eine Einrichtung der Roten Armee und wurde weiter von Lasarew als Direktor geleitet. Untersucht wurden die Sinneswahrnehmungen unter militärischen Bedingungen. Die Untersuchung des spektralen Reflexionsvermögens diverser farbiger Objekte wurde die wissenschaftliche Grundlage für die Auswahl von Tarnfarben. Auf Vorschlag des Volkskommissars für Gesundheit Nikolai Alexandrowitsch Semaschko übernahm Lasarew die allgemeine Leitung der Sektionen für Röntgen, Elektromedizin und Photobiologie des Volkskommissariats für Gesundheit. Die Röntgenuntersuchung des erkrankten Lenins wurde in Lasarews Laboratorium durchgeführt.

Als die sowjetische Regierung aufgefordert wurde, die von Ernst Gustav Leyst angefertigten Karten der Kursker Magnetanomalie, die sich nun in Deutschland befanden, für einen beträchtlichen Betrag zu kaufen, bot sich Lasarew an, diese Karten für einen geringeren Betrag neu zu erstellen, wofür er 1918 ein großes geophysikalisches Projekt organisierte. Im gleichen Jahr organisierte er die Herausgabe der neuen Zeitschrift für Physik "Uspechi Fisitscheskich Nauk (UFN)", deren englische Übersetzung "Physics-Uspekhi (Advances in Physical Sciences)" eine international anerkannte Fachzeitschrift geworden ist. Auch organisierte Lasarew Untersuchungen zur Ermittlung der Ursachen der Meeresströmungen in Abhängigkeit von den Passatwinden.

Auf Lasarews Initiative gründete das Volkskommissariat für Gesundheit 1919 das Forschungsinstitut für Physik und Biophysik, das dann Lasarew leitete. Zu den wissenschaftlichen Mitarbeitern gehörten Grigori Samuilowitsch Landsberg, Alexander Lwowitsch Minz, Sergei Nikolajewitsch Rschewkin, Sergei Wassiljewitsch Krawkow und Trofim Kononowitsch Molody.

Im Januar 1929 kritisierte Lasarew, dass bei der Wahl zu Mitgliedern der Akademie der Wissenschaften der UdSSR (AN-SSSR, ab 1991 Russische Akademie der Wissenschaften (RAN)) durch eine Nachwahl nur Kommunisten Vollmitglieder der AN-SSSR wurden. In einer Vorlesung über die Dialektik der Natur wies er auf einen Fehler von Friedrich Engels beim Problem √−1 hin, worauf er mehrfach denunziert wurde. Besonders aufmerksam wurde die GPU durch Lasarews umfangreichen Briefwechsel mit ausländischen Wissenschaftlern. In der Nacht des 5. März 1931 wurde Lasarew in seiner Wohnung im Institut für Physik und Biophysik verhaftet. Auf Beschluss des Volkskommissariats für Gesundheit verlor er das Direktorenamt und den Lehrstuhl am Moskauer Elektromechanik-Institut (MEMI). Das Institut für Physik und Biophysik wurde dem Obersten Rat für Volkswirtschaft übergeben und in ein chemisches Institut für Spezialaufgaben umgewandelt. Alle wissenschaftlichen Mitarbeiter Lasarews wurden entlassen. Die reichhaltige wissenschaftliche Ausstattung des Instituts war verschwunden. Nach Wladimir Iwanowitsch Wernadskis Tagebuch musste Lasarew seine Auslandsbeziehungen detailliert und vollständig darlegen. Lasarews Frau Olga Alexandrowna setzte sich vergeblich für ihren Mann ein bei der GPU und dem Volkskommissar für Gesundheit Nikolai Alexandrowitsch Semaschko, der Lasarew unterstützte. Sie erhängte sich am 13. Juni 1931, was ihr Mann erst spät erfuhr. Eine Gruppe von Wissenschaftlern unter der Führung von Alexei Nikolajewitsch Bach setzte sich für Lasarew ein und richtete Gesuche nicht an Walerian Wladimirowitsch Kuibyschew, der kein Freund der AN-SSSR war, sondern an dessen Rivalen Wjatscheslaw Michailowitsch Molotow. Im September 1931 wurde Lasarew aus dem Gefängnis entlassen und nach Swerdlowsk in die Verbannung geschickt. Lasarew litt unter Epilepsie und dachte an Suizid. In Swerdlowsk hielt er Vorlesungen am Institut für Geologische Prospektion und am Institut für Berufskrankheiten und arbeitete an Beiträgen der Biophysik zur Medizin. Zu seinem letzten nichtöffentlichen Vortrag vor seiner Freilassung in der GPU über Anfang und Ende des Universums kamen auch die örtlichen Diamatschiki und Geologen. Ende Februar 1932 konnte Lasarew nach Moskau zurückkehren aufgrund der Bemühungen Michail Alexandrowitsch Menzbier und seiner Kollegen und Studenten, wie Wernadski in seinem Tagebuch schrieb. Lasarew blieb in der Kritik, und 1938 wurden ihm pseudowissenschaftliche Theorien vorgeworfen.

Nach seiner Rückkehr 1932 leitete Lasarew den Lehrstuhl für Physik der geophysikalischen Abteilung des Moskauer Instituts für Geologische Prospektion (MGRI), das ihn kurz vor seiner Verhaftung auf den Lehrstuhl für Physik der Erde berufen hatte. 1934 wurde er Leiter der Biophysik-Abteilung des Allunionsinstituts für Experimentelle Medizin in Moskau. 1938 wurde diese Abteilung das Spezialbiophysiklaboratorium der AN-SSSR. Lasarew untersuchte die Prozesse der physiologischen Anpassung der Sinneswahrnehmungen an die einwirkenden Reize. Er wendete die Gesetze der Thermodynamik auf die biologischen Prozesse an. Er untersuchte die Wirkung des elektrischen Stroms auf das Nervengewebe und entwickelte eine theoretische Begründung für die Nernst-Gleichung und das Pflüger-Zuckungsgesetz.

Lasarew verfasste eine Reihe von populärwissenschaftlichen Werken zur Geschichte der russischen und ausländischen Wissenschaft. Er schrieb ein Erinnerungsbuch über P. N. Lebedew, Biografien von Nikolai Alexejewitsch Umow, Alexander Grigorjewitsch Stoletow und anderen. In sein Werk zur Geschichte der exakten Wissenschaft in Russland im Verlaufe von 200 Jahren fügte er selbst gezeichnete Porträts russischer Wissenschaftler von Michail Wassiljewitsch Lomonossow bis Ilja Iljitsch Metschnikow ein. Ein besonderes Buch widmete er dem Leben und Werk des von ihm verehrten Hermann von Helmholtz.

1940 wurde Lasarew zum Vizepräsidenten der Moskauer Gesellschaft der Naturforscher gewählt. Nach Beginn des Deutsch-Sowjetischen Krieges wurde Lasarew mit dem Spezialbiophysiklaboratorium nach Alma-Ata evakuiert. Dort starb Lasarew an einem Magenkarzinom mit Metastasen im Gehirn. Er wurde in Moskau auf dem Nowodewitschi-Friedhof begraben.



</doc>
<doc id="16633" url="https://de.wikipedia.org/wiki?curid=16633" title="Elektrischer Widerstand">
Elektrischer Widerstand

Der elektrische Widerstand ist in der Elektrotechnik ein Maß dafür, welche elektrische Spannung erforderlich ist, um eine bestimmte elektrische Stromstärke durch einen elektrischen Leiter (Bauelement, Stromkreis) fließen zu lassen. Dabei sind Gleichgrößen zu verwenden oder Augenblickswerte bei mit der Zeit veränderlichen Größen.

Wenn die Spannung von einem Anschlusspunkt A zu einem Anschlusspunkt B gezählt wird, wird die Stromstärke in dem Leiter positiv gezählt, wenn er von A nach B fließt; der Widerstand kann nicht negativ sein.

Als Formelzeichen für den elektrischen Widerstand wird in der Regel formula_1 – abgeleitet vom Lateinischen "resistere" für „widerstehen“ – verwendet. Der Widerstand hat die SI-Einheit Ohm, ihr Einheitenzeichen ist das Ω (großes Omega).
Auf historische Zusammenhänge wird im Artikel „ohmsches Gesetz“ eingegangen.

Ein elektrischer Widerstand ist dann ein ohmscher Widerstand, wenn sein Wert unabhängig von der Spannung, der Stärke des Stromes und irgendwelchen Parametern ist. An einem solchen Widerstand gilt das ohmsche Gesetz. Wird in einem Liniendiagramm die Spannung formula_2 über der Stromstärke formula_3 aufgetragen, entsteht bei einem ohmschen Widerstand eine Ursprungsgerade; die an einem Bauteil mit ohmschem Widerstand abfallende Spannung ist proportional zur Stromstärke im Widerstand mit dem Proportionalitätsfaktor formula_1; dieser ist zugleich der Anstieg der Geraden:

Näherungsweise und mit Einschränkungen kann ein ohmscher Widerstand durch ein Bauelement, im einfachsten Fall einen Metalldraht, realisiert werden. Dieses wird üblicherweise ebenfalls als Widerstand – siehe Widerstand (Bauelement) – bezeichnet.

Wenn durch den Strom im Widerstand ein Spannungsabfall entsteht, wird elektrische Energie
in thermische Energie umgesetzt.

Der Kehrwert des ohmschen Widerstands, also der Proportionalitätsfaktor zwischen Stromstärke und Spannung, heißt elektrischer Leitwert formula_6 eines Leiters. Es gilt also:

Der ohmsche Widerstand eines Körpers lässt sich aus seinen geometrischen Abmessungen und einer Material-Konstante, dem spezifischen Widerstand formula_8, berechnen.
Für einen in Längsrichtung durchflossenen geraden Leiter mit konstanter Querschnittsfläche formula_9 und der Länge formula_10 gilt:

Der spezifische Widerstand selbst ist im Allgemeinen von der Temperatur und eventuell noch weiteren Größen abhängig.

Die oben aufgestellte Gleichung für den Gleichstromwiderstand eines geraden Leiters wird dann beispielsweise ersetzt durch

wobei der Index die Celsius-Temperatur kennzeichnet, für die die Größen gelten. In Tabellenbüchern ist die übliche Bezugstemperatur 20 °C. Die Werte sind abhängig von Reinheitsgrad sowie thermischer und mechanischer Behandlung; deshalb sind die Tabellenwerte nur als Richtwerte zu verstehen.

Der Einfluss der Temperatur formula_14 auf den Widerstand formula_15 lässt sich in einfachen Fällen mit dem "Linear-Temperaturkoeffizienten" formula_16 und dem Temperaturunterschied formula_17 darstellen. Dann wird der Zusammenhang durch eine lineare Gleichung beschrieben

Für die meisten Anwendungen mit metallischen Materialien bei nicht zu großen Temperaturbereichen reicht diese lineare Näherung aus; sonst sind Glieder höherer Ordnung in die Gleichung einzubeziehen. (Ein Beispiel mit Summanden bis zur vierten Potenz siehe Platin im Artikel Widerstandsthermometer.)

Je nachdem, ob der Widerstandswert mit steigender Temperatur größer oder kleiner wird, wird unterschieden zwischen

In der Mess- und Regelungstechnik wird die Temperaturabhängigkeit des elektrischen Widerstandes als Messeffekt ausgenutzt, zum Beispiel bei Widerstandsthermometern, weiteren Temperatursensoren, thermischen Anemometern oder Einschaltstrombegrenzern.

Es gibt auch verschiedene spezielle Legierungen, die sich durch einen über weite Temperaturbereiche annähernd konstanten spezifischen elektrischen Widerstand auszeichnen, wie das für einen Messwiderstand erforderlich ist.

An einem rein ohmschen linearen Widerstand formula_1, der von Wechselstrom durchflossen wird, haben Spannung und Stromstärke denselben Phasenwinkel. Wenn sich allerdings frequenzabhängig der Widerstand ändert und die Phasenlage verschiebt, ist zum ohmschen Anteil am Widerstand eine Komponente formula_21 hinzugekommen, die auf Spannungs- oder Stromänderungen verzögernd reagiert. Bei sinusförmigem Verlauf von Spannung und Stromstärke wird der Quotient aus den Amplituden oder Effektivwerten als Scheinwiderstand formula_22 bezeichnet. In der komplexen Wechselstromrechnung wird der Scheinwiderstand mit dem Phasenverschiebungswinkel formula_23 als Impedanz oder komplexer Widerstand formula_24 zusammengefasst:
In einer anderen Darstellung werden die zwei Komponenten in der komplexen Ebene zueinander rechtwinklig zu formula_24 zusammengefasst:
Darin werden formula_1 als Wirkwiderstand und formula_21 als Blindwiderstand bezeichnet. Der Wirkwiderstand, welcher nicht phasenverschiebend arbeitet, wird auch als "ohmscher Anteil" der Impedanz bezeichnet.

Werden die Spannung formula_30 und die Stromstärke formula_31 als sinusförmige Größen mit der Frequenz formula_32 oder der Kreisfrequenz formula_33 in der komplexen Ebene durch Zeiger formula_34 und formula_35 dargestellt, so entsteht unter Einbeziehung der Eulerschen Formel

mit

Bei einer Spule mit der Induktivität formula_38 gilt
Aufgrund einer Spannung wächst die Stromstärke mit der Zeit an. Bei Wechselstrom folgt dieser verzögert. Mit dem Ansatz in komplexer Schreibweise formula_34 und formula_41 wie oben ergibt sich nach der Differenziation

Das formula_21 wird hier als induktiver Blindwiderstand bezeichnet
Zusammen mit dem Faktor formula_46 bedeutet das Ergebnis, dass eine Induktivität für sinusförmige Wechselgrößen wie ein phasendrehender Blindwiderstand wirkt.
Mit formula_47 ergibt sich formula_48

Der Scheinwiderstand einer Induktivität ist ein zur Frequenz proportionaler, aber im Übrigen linearer Widerstand.

Entsprechend gilt bei einem Kondensator mit der Kapazität formula_49
Aufgrund eines Stromes wächst die Spannung mit der Zeit an. Bei Wechselspannung folgt diese verzögert.
In komplexer Schreibweise und nach der Integration ergibt sich

Das formula_21 wird hier als kapazitiver Blindwiderstand bezeichnet
Zusammen mit dem Faktor formula_46 bedeutet das Ergebnis, dass eine Kapazität für sinusförmige Wechselgrößen wie ein phasendrehender Blindwiderstand wirkt. Hier ist formula_56

Der Scheinwiderstand einer Kapazität ist ein zur Frequenz umgekehrt proportionaler, aber im Übrigen linearer Widerstand.

Durch Vergleich der Darstellungen
ergeben sich

und für den "Scheinwiderstand":
und für den Phasenverschiebungswinkel zwischen formula_34 und formula_41:



Als Ersatzwiderstand wird der komplexe elektrische Widerstand bezeichnet, der denselben Widerstand besitzt wie eine elektrische Schaltung oder der Teil einer elektrischen Schaltung, den er ersetzt. Ein Ersatzwiderstand kann das Verhalten komplexer elektrischer Anordnungen veranschaulichen und eine Berechnung ermöglichen; siehe auch Ersatzschaltbild.

Tatsächlich auftretende Wechselstromwiderstände lassen sich häufig durch Reihenschaltung oder Parallelschaltung aus einem ohmschen Widerstand mit einer Induktivität oder mit einer Kapazität beschreiben. Welches der Bilder verwendet wird, ist eine Frage der besseren Annäherung an die Wirklichkeit mit möglichst frequenzunabhängigen Größen und der Zweckmäßigkeit für die mathematische Behandlung.

Bei genauer Betrachtung hat aber auch jeder Kondensator einen kleinen induktiven Anteil, so wie eine Spule auch einen kapazitiven Anteil hat. Selbst ein Stück Draht muss exakt mit formula_1, formula_49 und formula_38 beschrieben werden; siehe auch Leitungsbelag. Dies zeigt sich im Besonderen dann, wenn die Bauelemente mit ihren geometrischen Abmessungen in den Bereich der Wellenlänge der angelegten Wechselspannung kommen; dann besitzen sie eine nicht zu vernachlässigende Induktivität und Kapazität. Sie werden gegebenenfalls zum Schwingkreis, als Beispiel sei hier die Antenne genannt. Deren Enden dürfen als Kondensatorplatten gesehen werden, der Draht dazwischen als Spule.

Werden ein ohmscher Widerstand und ein Blindwiderstand zusammengeschaltet, so können in komplexer Schreibweise die weiter unten folgenden Regeln für Reihen- und Parallelschaltung angewendet werden.

Werden eine kapazitive und eine induktive Impedanz zusammengeschaltet, so entsteht bei genügend kleiner ohmscher Belastung ein Schwingkreis; die Reihen- und Parallelschaltung und die weiteren Konsequenzen werden unter diesem Stichwort behandelt.

Ein anschauliches Hilfsmittel zur Analyse und Beschreibung von Schaltungen mit Wechselstromwiderständen ist die Ortskurve.

Komplexe Größen lassen sich durch Zeiger in der komplexen Ebene darstellen. Wenn die komplexe Größe eine Funktion eines (reellen) Parameters ist und wenn dieser Parameter variiert wird, verschiebt sich die Spitze des Zeigers. Eine Linie durch alle denkbaren Zeigerspitzen wird als Ortskurve bezeichnet.

Die Bilder zeigen Ortskurven der Impedanz als Funktion der Frequenz für die angegebenen Schaltungen. Bei einer RL- oder RC-Reihenschaltung mit einem von der Frequenz unabhängigen ohmschen Widerstand ist auch der Wirkanteil der Impedanz von der Frequenz unabhängig. Bei der entsprechenden Parallelschaltung sind der Wirk- und der Blindanteil der Impedanz ersichtlich beide von der Frequenz abhängig.

Werden formula_79 ohmsche Widerstände hintereinander geschaltet, so addieren sich die Widerstände:
Dieses lässt sich an der Reihenschaltung zweier Widerstände veranschaulichen, die sich nur in der Länge formula_10 unterscheiden.

Die Reihenschaltung ergibt einen Widerstandskörper der Länge formula_82. Dann gilt:

Bei formula_79 gleichen Widerständen (formula_85) ist der Gesamtwiderstand so groß wie der mit der Anzahl der Widerstände multiplizierte Einzelwiderstand:
Der Widerstand einer Reihenschaltung ist stets größer als der größte Einzelwiderstand. Eine Ausnahme gibt es bei Wechselstromwiderständen im Reihenschwingkreis.

Werden formula_79 ohmsche Widerstände nebeneinander geschaltet, so addieren sich die Leitwerte beziehungsweise die reziproken Widerstände:
Dieses lässt sich an der Parallelschaltung zweier Widerstände veranschaulichen, die sich nur in ihrer Querschnittsfläche formula_9 unterscheiden.

Die Parallelschaltung ergibt einen Widerstandskörper der Querschnittsfläche formula_91. Dann gilt:

und umgestellt
Für die Parallelschaltung gibt es eine alternative Schreibweise mit dem "Parallel"-Zeichen formula_94:
Speziell für zwei parallele Widerstände gilt:
Bei formula_79 gleichen Widerständen ist der Gesamtwiderstand so groß wie der durch die Anzahl der Widerstände dividierte Einzelwiderstand:
Der Widerstand einer Parallelschaltung ist stets kleiner als der kleinste Einzelwiderstand. Eine Ausnahme gibt es bei Wechselstromwiderständen im Parallelschwingkreis.

Bei nichtlinearen Strom-Spannungs-Kennlinien – wie zum Beispiel von Dioden – ist der Quotient für jedes Strom-Spannungs-Paar unterschiedlich. In diesem Fall gilt das ohmsche Gesetz nicht, und man kann nicht von einem linearen Widerstand formula_1 sprechen. Kleine Spannungsänderungen sind jedoch näherungsweise proportional zu damit verbundenen kleinen Stromstärkeänderungen. Der Quotient aus kleiner Spannungsänderung und zugehöriger Stromstärkeänderung bei einer bestimmten Spannung wird als differentieller Widerstand formula_100 bezeichnet. In einem Diagramm, in dem formula_2 über formula_3 aufgetragen wird, entspricht er der Steigung der Tangente am betrachteten Punkt der Kennlinie.

Der differentielle Widerstand kann in einem Teil der Kennlinie negativ sein, so dass die Stromstärke bei steigender Spannung sinkt beziehungsweise die Stromstärke bei sinkender Spannung steigt. Im Bild ist das im Bereich "U" < "U" < "U" der Fall. Ein negativer differentieller Widerstand kann zum Anregen (Entdämpfen) von Schwingkreisen oder zur Erzeugung von Kippschwingungen verwendet werden (Oszillator). Der negative differentielle Widerstand tritt zum Beispiel bei Gasentladungen oder bei Bauteilen wie Avalanche- und Tunneldioden auf, in einfachen elektronischen Schaltungen wie der Lambda-Diode, aber auch bei komplexeren Modulen wie z. B. Schaltnetzteilen auf der Eingangsseite.

Bei positiven differentiellen Widerständen nimmt die Stromstärke mit zunehmender Spannung zu. Alle real existierenden Schaltungselemente besitzen in einem Teil ihrer Kennlinie, jedoch stets für sehr große Werte, einen positiven differentiellen Widerstand. Die meisten Elemente in der Schaltungstechnik besitzen einen ausschließlich positiven differentiellen Widerstand.

Beispiele: realer Widerstand, Diode, Zener-Diode, alle halbleitenden Keramiken.

Die physikalische Beschreibung benutzt die Vorstellung, dass sich die Valenzelektronen im Metall wie ein Gas (Elektronengas) verhalten. Im einfachsten Modell bildet das Metall ein positiv homogen geladenes Volumen, in denen sich die Elektronen frei bewegen können. In dieses Volumen sind die Atomrümpfe eingebettet, die aus dem Atomkern und den stärker gebundenen Elektronen auf den tieferen, vollbesetzten Schalen bestehen.

Ohne äußere elektrische Spannung bewegen sich die Elektronen ungeordnet im Metall (siehe brownsche Bewegung). Legt man nun eine Spannung an, so werden die freien Elektronen durch das elektrische Feld in Richtung der Feldlinien beschleunigt. Es fließt ein elektrischer Strom.

Auf ihrem Weg durch das Metall kommt es zu elastischen Stößen der Elektronen mit anderen Elektronen, den Atomrümpfen und Phononen. Dabei geben die Elektronen Energie an ihre Stoßpartner ab, werden gestreut und wieder durch das elektrische Feld beschleunigt. Die Elektronen werden durch diese Wechselwirkung dauernd abgebremst und es stellt sich eine mittlere Strömungsgeschwindigkeit ein.

Die bei diesen Stößen an die Atomrümpfe beziehungsweise Phononen übertragene Energie führt zu einer größeren Eigenschwingung um ihre Gleichgewichtslage, ihre Temperatur erhöht sich. Durch die stärkeren Schwingungen erhöht sich die Querschnittsfläche für mögliche Stöße, deren Anzahl mit steigender Temperatur zunimmt und den Widerstand steigen lässt (Kaltleiter). Der Leitungsvorgang in Heißleitern kann mit diesem Modell nicht vollständig erklärt werden, da es hier mit steigender Temperatur zu einer deutlichen Ladungsträgergeneration kommt, die den eben beschriebenen Vorgang überlagern.

Bei sehr hohen Temperaturen, bei denen die Atome des Materials ionisiert werden (Plasma), ist jeder Stoff elektrisch leitend, da die vorher gebundenen Elektronen nun für den Ladungstransport zur Verfügung stehen. Umgekehrt sind Metalle und Oxide bekannt, für die der elektrische Widerstand bei sehr niedrigen Temperaturen unterhalb einer spezifischen Sprungtemperatur verschwindet: Supraleiter besitzen bei Gleichstrom keinen ohmschen Widerstand, Strom fließt bei dieser tiefen Temperatur ohne Verluste.

Durch die thermische Bewegung der Elektronen entsteht ein temperaturabhängiger Rauschstrom, der als Widerstandsrauschen bezeichnet wird.

Der Hall-Widerstand gibt das Verhältnis Spannung zu Stromstärke eines Hallelementes bei einer bestimmten magnetischen Flussdichte an, wobei diese Spannung "quer" zur Stromdichte auftritt. Er charakterisiert das Hall-Element bzw. die magnetische Flussdichte, hat jedoch mit dem elektrischen Widerstand dieses Hall-Elementes nichts zu tun.

Der Quanten-Hall-Effekt äußert sich dadurch, dass bei tiefen Temperaturen und starken Magnetfeldern die senkrecht zur Stromdichte auftretende Spannung nicht wie beim klassischen Hall-Effekt linear mit der Flussdichte anwächst, sondern in Stufen. Dieses Phänomen führt auf eine universelle Naturkonstante, die „Von-Klitzing-Konstante“ von der Dimension Widerstand. Da die Von-Klitzing-Konstante relativ einfach gemessen werden kann, wurde vorgeschlagen, sie als Normal für Messungen des elektrischen Widerstands zu verwenden.



</doc>
<doc id="30924" url="https://de.wikipedia.org/wiki?curid=30924" title="Quant">
Quant

In der Physik wird unter Quant (von ‚wie groß‘, ‚wie viel‘) ein Objekt verstanden, das durch einen Zustandswechsel in einem System mit diskreten Werten einer physikalischen Größe erzeugt wird. Quantisierte Größen werden im Rahmen der Quantenmechanik und davon inspirierten Teilgebieten der theoretischen Physik wie der Quantenelektrodynamik beschrieben. Quanten können immer nur in bestimmten Portionen dieser physikalischen Größe auftreten, sie sind mithin die Quantelung dieser Größen.

Oft wird mit dem physikalischen Begriff Quant ein Teilchencharakter der betrachteten Größe assoziiert. Dies ist jedoch nur ein Teil der eigentlichen Bedeutung des Begriffs. Ein Beispiel für ein Quant, dem man keinen Teilchencharakter zuschreiben kann, ist das Drehimpulsquant.

Als physikalischer Terminus wird Quant nicht zur Bezeichnung der atomaren Struktur der Materie verwendet, obwohl auch hier eine kleinste Mengeneinheit (Quantelung) auftritt.


Jürgen Audretsch: "Verschränkte Welt Faszination der Quanten", Wiley-VCh, Weinheim 2002, ISBN 3-527-40318-3



</doc>
<doc id="55064" url="https://de.wikipedia.org/wiki?curid=55064" title="Kilometer pro Stunde">
Kilometer pro Stunde

</math>

Kilometer pro Stunde, umgangssprachlich Stundenkilometer, seltener Kilometer durch Stunde, Kilometer je Stunde oder Kilometer in der Stunde (Einheitenzeichen: km/h), ist eine Maßeinheit der Geschwindigkeit. Sie wird vor allem im Verkehrswesen, beispielsweise für die Geschwindigkeit im Straßenverkehr oder für die Reisegeschwindigkeit verwendet, da dort Entfernungen meist in Kilometern und Zeiten in Stunden angegeben werden – statt in Metern und Sekunden, was der SI-Einheit der Geschwindigkeit m/s zugrunde liegt. Die beiden zugehörigen Maßzahlen haben dieselbe Größenordnung.

Ein Objekt, das sich eine Stunde lang mit 1 km/h bewegt, legt eine Strecke von einem Kilometer zurück. Die Strecke und die Geschwindigkeit bleiben proportional.

Umgangssprachlich wird häufig auch die gesprochene Form „ka-em-ha“ verwendet, was auch zu der Verschriftlichung „kmh“ statt „km/h“ führt. Ebenso wie "Stundenkilometer" ist das (physikalisch) fachsprachlich falsch, da diese Aneinanderreihung dort für das Produkt "Kilometer mal Stunde" stünde. Die Gesellschaft für deutsche Sprache verweist darauf, dass in der Wortbildungslehre für Komposita die Beziehung der Wortbestandteile keinen Regeln unterliege und der Begriff Stundenkilometer "durchaus richtig und angemessen" ist. Dabei warnt sie vor sprachlicher Pedanterie, da diese zu nichts führe. Bei Geschwindigkeiten von Kraftfahrzeugen wird häufig ganz auf eine Bezeichnung verzichtet („Er ist in der Baustelle mit 120 geblitzt worden.“), ebenso wie bei der Darstellung von Geschwindigkeiten auf den entsprechenden Verkehrszeichen.

Lange waren in Deutschland und Österreich Verkehrsschilder gebräuchlich, auf denen nur die Einheit „km“ (ohne das „/h“) enthalten war. In Deutschland geht dies auf die "Verordnung über Warnungstafeln für den Kraftfahrzeugverkehr vom 8. Juli 1927" zurück, einem Vorläufer der heutigen Straßenverkehrs-Ordnung. Wichtiger als diese nationalen Regelungen waren jedoch die weltweiten Übereinkommen über Straßenverkehrszeichen. Sowohl das 1971 in der Bundesrepublik Deutschland eingeführte Zeichen 274 als auch das damals in Österreich verwendete Vorschriftszeichen deckten sich in dieser Weise auch mit dem Wiener Übereinkommen über Straßenverkehrszeichen vom 8. November 1968, das unter anderem auch Österreich und die Bundesrepublik Deutschland (1977) unterzeichnet und ratifiziert hatten. Dies ergab im Januar 1985 eine Anfrage im Deutschen Bundestag, die sich mit der Problematik der falschen Einheitsangabe beschäftigte. Der ratifizierte Text lautete: „Nach oder unter der Geschwindigkeitszahl kann ,km‘ (Kilometer) oder ,m‘ (Meilen) hinzugefügt werden.“ Unabhängig davon hatte das Bundesverkehrsministerium zu diesem Zeitpunkt bereits geplant, auf jegliche Einheitsangabe zu verzichten. Der Grund lag jedoch nicht bei der Nutzung der Begrifflichkeit „km“, sondern bei den immer stärker genutzten elektronischen Wechselverkehrszeichen. Hier hätte eine zusätzliche Einheitsangabe unnötige technische Probleme bereitet. Daher wurde in der Neunten Verordnung zur Änderung der Straßenverkehrs-Ordnung vom 22. März 1988 festgelegt, dass das bisherige Zeichen mit Ablauf des 31. Dezember 1998 seine Bedeutung als amtliches Verkehrszeichen verliert.

Bis in die 1960er, vereinzelt bis in die 1980er, wurden auch die Abkürzungen km/St oder km/st verwendet.

Die Einheit km/h wird oft der SI-Einheit m/s (Meter pro Sekunde) vorgezogen. So kann man die beiden zugehörigen Maßzahlen ineinander umrechnen:
Also:

 In einigen Ländern, wie zum Beispiel in den USA oder in Großbritannien, in denen das metrische System nicht obligatorisch ist, werden Geschwindigkeiten oft in "Meilen pro Stunde" (mph) angegeben (1 mph = 1,609 km/h). In der Nautik und Luftfahrt sind Geschwindigkeitsangaben in Knoten (kn), das heißt Seemeilen pro Stunde, üblich (1 kn = 1,852 km/h).


</doc>
<doc id="56568" url="https://de.wikipedia.org/wiki?curid=56568" title="Navier-Stokes-Gleichungen">
Navier-Stokes-Gleichungen

Die Navier-Stokes-Gleichungen [] (nach Claude Louis Marie Henri Navier und George Gabriel Stokes) sind ein mathematisches Modell der Strömung von linear-viskosen newtonschen Flüssigkeiten und Gasen (Fluiden). Die Gleichungen sind eine Erweiterung der Euler-Gleichungen der Strömungsmechanik um Viskosität beschreibende Terme.

Im engeren Sinne, insbesondere in der Physik, ist mit Navier-Stokes-Gleichungen die Impulsgleichung für Strömungen gemeint. Im weiteren Sinne, insbesondere in der Numerischen Strömungsmechanik, wird diese Impulsgleichung um die Kontinuitätsgleichung und die Energiegleichung erweitert und bildet dann ein System von nichtlinearen partiellen Differentialgleichungen zweiter Ordnung. Dieses ist das grundlegende mathematische Modell der Strömungsmechanik. Insbesondere bilden die Gleichungen Turbulenz und Grenzschichten ab. Eine Entdimensionalisierung der Navier-Stokes-Gleichungen liefert diverse dimensionslose Kennzahlen wie die Reynolds-Zahl oder die Prandtl-Zahl.

Die Navier-Stokes-Gleichungen bilden das Verhalten von Wasser, Luft und Ölen ab und werden daher in diskretisierter Form bei der Entwicklung von Fahrzeugen wie Autos und Flugzeugen angewendet. Dies geschieht in Näherungsform, da keine exakten analytischen Lösungen für diese komplizierten Anwendungsfälle bekannt sind. Die Existenz und Eindeutigkeit einer Lösung der Gleichungen ist außerdem im allgemeinen Fall noch nicht erwiesen, was zu den wichtigsten ungelösten mathematischen Problemen, den Millennium-Problemen, gehört.

Isaac Newton veröffentlichte 1686 seine dreibändige Principia mit den Bewegungsgesetzen und definierte zudem im zweiten Buch die Viskosität einer linear viskosen (heute: "newtonschen") Flüssigkeit. 1755 leitete Leonhard Euler aus den Bewegungsgesetzen die Euler-Gleichungen her, mit denen sich das Verhalten viskositätsfreier Fluide (Flüssigkeiten und Gase) berechnen lässt. Voraussetzung dafür war seine bis heute gültige Definition des Drucks in einem Fluid. Jean-Baptiste le Rond d’Alembert (1717–1783) führte die eulersche Betrachtungsweise ein, leitete die lokale Massenbilanz her und formulierte das d’Alembert’sche Paradoxon, demgemäß von der Strömung viskositätsfreier Flüssigkeiten auf einen Körper keine Kraft in Richtung der Strömung ausgeübt wird (was Euler schon vorher bewies). Wegen dieser und anderer Paradoxien viskositätsfreier Strömungen war klar, dass die Euler’schen Bewegungsgleichungen zu ergänzen sind.

Claude Louis Marie Henri Navier, Siméon Denis Poisson, Barré de Saint-Venant und George Gabriel Stokes formulierten unabhängig voneinander in der ersten Hälfte des 19. Jahrhunderts den Impulssatz für newtonsche Fluide in differentieller Form. Navier (1827) und Poisson (1831) stellten die Impulsgleichungen nach Betrachtungen über die Wirkung von intermolekularen Kräften auf. 1843 veröffentlichte Barré de Saint-Venant eine Herleitung der Impulsgleichungen aus Newtons linearem Viskositätsansatz, zwei Jahre bevor Stokes dies (1845) tat. Es setzte sich allerdings der Name Navier-Stokes-Gleichungen für die Impulsgleichungen durch.

Einen wesentlichen Fortschritt im theoretischen und praktischen Verständnis viskoser Fluide lieferte Ludwig Prandtl 1904 mit seiner Grenzschichttheorie. Ab Mitte des 20. Jahrhunderts entwickelte sich die numerische Strömungsmechanik so weit, dass mit ihrer Hilfe für praktische Probleme Lösungen der Navier-Stokes-Gleichungen gefunden werden können, die – wie sich zeigt – gut mit den realen Strömungsvorgängen übereinstimmen.

Die Navier-Stokes-Gleichung im engeren Sinne ist der Impulssatz als Anwendung der newtonschen Axiome auf ein Kontinuum. Eine verwendete Form für kompressible Fluide ist:

Hier ist formula_2 die Dichte, formula_3 der (statische) Druck, formula_4 die Geschwindigkeit eines Teilchens in der Strömung, der Überpunkt genauso wie formula_5 unten die substantielle Zeitableitung, formula_6 die partielle Ableitung nach der Zeit bei festgehaltenem Ort des Fluidelements, „formula_7“ das (formale) Skalarprodukt mit dem Nabla-Operator formula_8 und formula_9 der Laplace-Operator. Links der Gleichheitszeichen steht die substantielle Beschleunigung der Fluidelemente und der mit dem Nabla-Operator gebildete Term stellt ihren konvektiven Anteil dar. Der Vektor formula_10 steht für eine Volumenkraftdichte wie beispielsweise die Gravitation oder die Corioliskraft jeweils bezogen auf das Einheitsvolumen und besitzt die Einheit Newton/Kubikmeter. Bei den Parametern formula_11 und formula_12 handelt es sich um die dynamische Viskosität und die erste Lamé-Konstante. In der Literatur werden sie auch als Lamé-Viskositäts-Konstanten bezeichnet.

Eine andere Schreibweise für die in der Literatur verwendete Form ist:

Darin ist "ζ" die Volumenviskosität. Mit der Kontinuitätsgleichung und Anwendung der Stokes’schen Hypothese "ζ" = 0 wird hieraus die Gleichung für die "Impulsdichte" formula_14:

Das Rechenzeichen formula_16 bildet das dyadische Produkt. Zur Vervollständigung der Gleichungen müssen noch die Massenbilanz oder Kontinuitätsgleichung (Massenerhaltungssatz) und bei Gasen die Energiebilanz (Energieerhaltungssatz) hinzugefügt werden. Je nach weiteren Annahmen, die an das Fluid gestellt werden, ergibt sich das vollständige System in unterschiedlicher Form. Die am häufigsten verwendete Form sind die Navier-Stokes-Gleichungen für inkompressible Fluide, denn sie sind für Unterschallströmungen gut geeignet und ihre Berechnung ist einfacher als die kompressibler Fluide.

Die Vektorform der Gleichungen gelten in jedem Koordinatensystem. Hier sollen die Komponentengleichungen der Impulsgleichung speziell für kartesische Koordinaten angegeben werden.

Darin sind formula_18 und formula_19 die Vektorkomponenten in den räumlichen formula_20-, formula_21- bzw. formula_22-Richtungen. In dieser Form kann eine mögliche Ortsabhängigkeit der Scherviskosität infolge ihrer Temperaturabhängigkeit und Temperaturschwankungen im Fluid berücksichtigt werden.

Die Navier-Stokes-Gleichungen können mit charakteristischen Maßen des gesamten Strömungsgebiets für die Länge formula_23, die Geschwindigkeit formula_24 und die Dichte formula_25 entdimensionalisiert werden. Damit entstehen die dimensionslosen Größen

die zu der dimensionslosen Impuls-Gleichung führen:

Darin charakterisiert die dimensionslose Reynolds-Zahl

die Strömung hinsichtlich des Verhältnisses von Trägheits- zu Scherkräften.

Bei Strömungen mit freier Oberfläche enthält die dimensionslose Kraftdichte formula_29 die Froude-Zahl, die das Verhältnis von Trägheits- zu Schwerekräften charakterisiert.

Die "Chapman-Enskog-Entwicklung" der Boltzmann-Gleichungen der kinetischen Gastheorie führt auf die Navier-Stokes-Gleichungen mit verschwindender Volumenviskosität, also formula_30. Diese Entwicklung basiert auf einer Verteilungsfunktion, die nur von der Geschwindigkeit der Teilchen abhängt, also deren Rotationsdrehimpuls vernachlässigt. Dies ist in einatomigen Gasen bei niedrigem bis mittlerem Druck eine probate Annahme, gilt jedoch nicht bei mehratomigen Gasen. Die Chapman-Enskog-Entwicklung ist mathematisch so anspruchsvoll, dass sie hier nicht vorgestellt werden kann.

Im phänomenologischen kontinuumsmechanischen Ansatz ergeben sich die Navier-Stokes-Gleichungen mit Volumenviskosität wie folgt aus der Newton’schen Annahme der linearen Viskosität. Die Viskosität begründet sich aus dem Experiment, nach dem zur Aufrechterhaltung einer Scherströmung eine Kraft erforderlich ist, die, bezogen auf ihre Wirkfläche, einer Schubspannung entspricht. Im Fluid wirkt daneben auch noch der Druck, der eine gleichförmige Normalspannung in allen Raumrichtungen darstellt. Der Cauchy’sche Spannungstensor formula_31 fasst den Spannungszustand in einem Fluidelement zu einem mathematischen Objekt zusammen und seine Divergenz verkörpert gemäß

den Kraftfluss im Fluid. Die Kraft formula_33, die mit flächenverteilten Kräften formula_34 auf der Oberfläche formula_35 des Volumens formula_36 wirkt, ist das Volumenintegral über die Divergenz des Spannungstensors. Diese trägt demnach zur substantiellen Beschleunigung

der Fluidelemente bei. Neben der Divergenz des Spannungstensors kann noch eine volumenverteilte Kraft formula_10 wie die Schwerkraft auf ein Fluidelement wirken und so ergibt sich mit der Dichte formula_2 das erste Cauchy-Euler’sche Bewegungsgesetz:

Ein newtonsches Fluid vermag Kräfte über den Druck im Fluid und über Spannungen zu übertragen, die von der räumlichen Änderung der Strömungsgeschwindigkeit abhängen und die sich makroskopisch als Viskosität bemerkbar machen. Die räumliche Änderung der Strömungsgeschwindigkeit ist im Geschwindigkeitsgradient formula_41 zusammengefasst. Allerdings treten keine Spannungen bei einer starren Rotation auf, die vom schiefsymmetrischen Anteil des Geschwindigkeitsgradienten bemessen wird, siehe Kinematik in der Strömungsmechanik. Demnach trägt nur der symmetrische Anteil formula_42 des Geschwindigkeitsgradienten, der "Verzerrungsgeschwindigkeitstensor"

zur Viskosität bei. In einem bezugssysteminvarianten Materialmodell der linearen Viskosität kann der Spannungstensor nur von formula_42 und seiner linearen Hauptinvariante formula_45 abhängen. Das Materialmodell der klassischen Materialtheorie für das linear viskose, isotrope Fluid lautet demgemäß

Darin bezeichnet formula_3 den (statischen) Druck, formula_48 den Einheitstensor, formula_49 die Spur, das hochgestellte formula_50 den Deviator, formula_11 die Scherviskosität, formula_12 die erste Lamé-Konstante und formula_53 die Volumenviskosität.

Einsetzen der Divergenz des Spannungstensors in das erste Cauchy-Euler’sche Bewegungsgesetz liefert die Navier-Stokes-Gleichungen.

Der Druck, die Dichte und der Verzerrungsgeschwindigkeitstensor formula_42 sind objektiv, siehe Euklidische Transformation, werden also von verschiedenen Beobachtern in gleicher Weise wahrgenommen. Deshalb sind die Navier-Stokes-Gleichungen invariant gegenüber einer Galilei-Transformation.

Falls sich die Dichte entlang von Teilchenbahnen nicht ändert, heißt die Strömung inkompressibel. Dies ist beispielsweise eine sinnvolle Annahme für Wasser oder Gase weit unterhalb der Schallgeschwindigkeit (Mach-Zahl < 0,3). Die Kontinuitätsgleichung vereinfacht sich zur Divergenzfreiheit des Geschwindigkeitsfeldes

Die Impulsgleichung vereinfacht sich zu:

Hierbei steht formula_3 für den physikalischen Druck, formula_10 ist eine Volumenkraft bezogen auf das Einheitsvolumen und formula_11 ist die dynamische Viskosität. Damit wird eine inkompressible Strömung vollständig durch ein partielles Differentialgleichungssystem mit zwei Gleichungen für die zwei Größen Geschwindigkeit formula_4 und Druck formula_3 in Abhängigkeit von Ort und Zeit beschrieben. Die Energieerhaltung wird nicht zum Schließen des Systems benötigt. Dieser Satz von Gleichungen wird auch als "inkompressible Navier-Stokes-Gleichungen mit variabler Dichte" bezeichnet. Anwendungsbeispiele für diese Gleichung sind Probleme der Ozeanographie, wenn Wasser unterschiedlichen Salzgehalts zwar inkompressibel ist, aber keine konstante Dichte hat.

In vielen praktischen Problemen ist die Strömung nicht nur inkompressibel, sondern hat sogar konstante Dichte. Hier kann man durch die Dichte dividieren und sie in die Differentialoperatoren einbeziehen:

In dieser Gleichung steht formula_63 für den Quotienten aus physikalischem Druck und Dichte und formula_64 ist eine Schwerebeschleunigung. Diese Größen stellen somit den Druck bzw. die Volumenkraft bezogen auf die Einheitsmasse dar. Die Größe formula_65 ist die kinematische Viskosität und bemisst den diffusiven Impulstransport.

Die zuletzt genannten Gleichungen werden in der Literatur auch als "inkompressible Navier-Stokes-Gleichungen" oder einfach nur als "die" Navier-Stokes-Gleichungen bezeichnet, weil sie die am besten untersuchten und in der Praxis am häufigsten benutzten sind. Zudem sind sie einfacher zu lösen als die Gleichungen für kompressible Fluide. Anwendbar sind die Gleichungen bei vielen wichtigen Strömungsproblemen, beispielsweise bei Luftströmungen weit unterhalb der Schallgeschwindigkeit (Mach-Zahl < 0,3), für Wasserströmungen sowie für flüssige Metalle. Sobald sich die Dichten der betrachteten Fluide jedoch stark ändern, wie zum Beispiel bei Überschallströmungen oder in der Meteorologie, stellen die Navier-Stokes-Gleichungen für inkompressible Fluide kein geeignetes Modell der Wirklichkeit mehr dar und müssen durch die vollständigen Navier-Stokes-Gleichungen für kompressible Fluide ersetzt werden.

Die Vektorform der Gleichungen gelten in jedem Koordinatensystem. Hier sollen die Komponentengleichungen der Impulsgleichung bei Inkompressibilität in kartesischen, zylindrischen und sphärischen Koordinaten angegeben werden.

In einem kartesischen formula_66-System schreibt sich die Impulsbilanz:

Der Operator formula_68 steht für die Substantielle Ableitung.

In Zylinderkoordinaten (formula_69) lauten die Gleichungen

In Kugelkoordinaten (formula_71) lauten die Gleichungen

Für kompressible Gase werden die obigen Impulsgleichungen um die Energiebilanz und die Zustandsgleichung eines idealen Gases erweitert. Der komplette Satz an Gleichungen besteht also aus der Kontinuitätsgleichung (Massenerhaltung), Impulsbilanz (Impulserhaltung), Energiebilanz (Energieerhaltung) und einer Zustandsgleichung. Die in Klammern angegebenen Gesetze gelten in abgeschlossenen Systemen, aber an einem Fluidteilchen sind die ein und aus gehenden Flüsse zu bilanzieren, was auf Bilanzgleichungen führt, die unter Strömungsmechanik nachzuschlagen sind. Unter der Annahme, dass die Dichte entlang der Teilchenbahnen konstant ist, entstehen wieder die Gleichungen für inkompressible Fluide.

Im Folgenden bedeutet formula_73 die Ableitung einer Größe nach der Zeit und formula_8 ist der Nabla-Operator, der die Ableitung nach dem Ort bildet, also je nach Verknüpfung die Divergenz oder den Gradient, und formula_75 sind die drei Ortskoordinaten in einem kartesischen Koordinatensystem. Die angegebenen Bilanzgleichungen führen in abgeschlossenen Systemen zu Erhaltungsgleichungen.

Die Kontinuitätsgleichung entspricht der Massenerhaltung und wird hier mit der Impulsdichte formula_14 formuliert:

Die Impulsbilanz entspricht der Impulserhaltung und lautet in Indexschreibweise

wobei formula_79 das Kronecker-Delta und

der Reibtensor oder viskose Spannungstensor sind. Der Materialparameter formula_11 ist die dynamische Viskosität, formula_12 die erste Lamé-Konstante und formula_83 ist die formula_84-te Komponente des Volumenkraftvektors. In der alternativen koordinatenfreien Schreibweise lautet die Impulsbilanz

wobei

der viskose Spannungstensor, d der Verzerrungsgeschwindigkeitstensor, der der symmetrische Anteil des Geschwindigkeitsgradienten formula_87 ist und die Spur formula_88 besitzt, formula_89 der Spannungstensor, 1 der Einheitstensor und formula_16 das dyadische Produkt ist, siehe #Herleitung der Impulsgleichung oben.

Die Energiebilanz am Fluidteilchen im Schwerefeld der Erde lautet

wobei formula_92 die Schwerebeschleunigung und

die Enthalpie pro Einheitsmasse ist. Das negative Vorzeichen vor der Schwerebeschleunigung resultiert aus dem "abwärts" gerichteten Vektor formula_92, so dass in einer "aufwärts" führenden Strömung potentielle Energie "hinzu gewonnen" wird. Der Wärmefluss formula_95 kann mittels des Wärmeleitkoeffizienten formula_96 als

geschrieben werden. Mit dem Quellterm formula_98 kann beispielsweise die Absorption und Emission von Wärme aus Treibhausgasen infolge von Einstrahlung beschrieben werden. Die totale Energie pro Einheitsmasse formula_99 ist die Summe von innerer (formula_100), kinetischer und potentieller Energie, sie lässt sich (mit der Höhe formula_101) also schreiben als

Nun liegen also vier Gleichungen für fünf Variablen vor und das System wird durch die folgende Zustandsgleichung abgeschlossen:

Die thermodynamischen Größen Dichte, Druck und Temperatur sind durch das "ideale Gasgesetz" verbunden:

Oft geht man zusätzlich von einem perfekten Gas mit konstanter spezifischer Wärmekapazität formula_105 aus. Dann vereinfacht sich das Integral und es gilt:

In beiden Fällen hängen der adiabatische Exponent formula_107 und die Gaskonstante formula_108 durch den spezifischen Wärmekoeffizienten für konstanten Druck formula_109 respektive konstantes Volumen formula_105 durch formula_111 und formula_112 zusammen.

Ein wesentlicher Punkt bei den Navier-Stokes-Gleichungen ist die experimentell sehr gut nachgewiesene Haftbedingung ("No-Slip-Bedingung"), bei der an einer Wand sowohl in Normalenrichtung, als auch insbesondere in tangentialer Richtung als Relativgeschwindigkeit Null vorgeschrieben werden. Die Fluidteilchen kleben also an der Wand. Dies führt zur Bildung einer Grenzschicht, die für wesentliche, nur durch die Navier-Stokes-Gleichungen modellierte Phänomene verantwortlich ist. Nur wenn die freie Weglänge bewegter Moleküle groß ist zur charakteristischen Länge der Geometrie (z. B. für Gase mit extrem niedrigen Dichten oder Strömungen in extrem engen Spalten) ist diese Bedingung nicht mehr sinnvoll.

Durch dynamische (also Kraft-) Randbedingungen auf einer Fläche wird die Fläche im Allgemeinen deformiert und die Strömung folgt ihr. Zum Problem gehört dann die Bestimmung der Fläche dazu. Sie ergibt sich aus der Vorgabe des Flächenkraft- oder Spannungsvektors formula_113 für alle Punkte auf der Fläche und der Tatsache, dass die Fläche eine materielle Fläche ist, denn Flächenkräfte können nur auf Fluidteilchen aufgebracht werden. Auf der Fläche gilt also formula_114, wobei formula_115 der Normaleneinheitsvektor der Fläche ist und sich der Spannungstensor aus der Materialgleichung formula_116 berechnet. Zumeist, vor allem im technischen Bereich wie z. B. am Auslass eines durchströmten Rohres, ist die Fläche bekannt, was die Aufgabenstellung erheblich vereinfacht.

Bei entsprechend kleinskaligen Strömungen ist die Oberflächenspannung zu berücksichtigen, die nach der Young-Laplace-Gleichung von der Krümmung der Oberfläche abhängt. Bei schwacher Krümmung entsteht für den Druck an der Oberfläche die Gleichung

Hier ist formula_118 der vorgegebene Druck auf der Fläche formula_101, die hier die Flächenparameter formula_20 und formula_21 besitzt, und formula_107 ist ein Parameter, der die Stärke der Oberflächenspannung skaliert.

Zusätzlich muss gegebenenfalls am Rand noch entweder eine Temperatur oder ein Wärmefluss vorgeschrieben werden.

Es ist bis heute nicht gelungen, die Existenz globaler Lösungen nachzuweisen. Mathematiker wie P.-L. Lions (siehe Literaturliste) betrachten im Wesentlichen den wichtigen Spezialfall der "inkompressiblen" Navier-Stokes-Gleichungen. Während hier für den zweidimensionalen Fall unter anderem von Olga Alexandrowna Ladyschenskaja, Roger Temam und Ciprian Foias bereits weitreichende Existenz-, Eindeutigkeits- und Regularitätsaussagen bewiesen werden konnten, gibt es bislang keine Resultate für den allgemeinen dreidimensionalen Fall, da hier einige fundamentale Einbettungssätze für sogenannte Sobolevräume nicht mehr eingesetzt werden können. Allerdings gibt es für endliche Zeiten oder spezielle, insbesondere kleine, Anfangsdaten auch im dreidimensionalen Fall – vor allem für schwache Lösungen – Existenz- und Eindeutigkeitsaussagen. Den Fall schwacher Lösungen der Navier-Stokes-Gleichungen auch in drei Dimensionen behandelte Jean Leray 1934. Er zeigte, dass die von ihm eingeführten schwachen Lösungen kein pathologisches Verhalten in zwei Dimensionen zeigen (keine Divergenz (blow up) in endlicher Zeit) und somit global in der Zeit existieren. Allerdings zeigten Untersuchungen von Tristan Buckmaster und Vlad Vicol, dass bei einer anderen Art schwacher Lösungen (schwächer als die Definition von Leray) die Navier-Stokes-Gleichungen in drei Dimensionen pathologisches Verhalten (Mehrdeutigkeit) zeigen.

Das Problem des allgemeinen, inkompressiblen Existenzbeweises in drei Dimensionen gehört laut dem Clay Mathematics Institute zu den wichtigsten ungelösten mathematischen Problemen zur Zeit der Jahrtausendwende.

In der Praxis gewinnt man analytische Lösungen, indem man die physikalischen Modelle/Randbedingungen vereinfacht (Spezialfälle). Besondere Schwierigkeit bereitet hier die Nichtlinearität der konvektiven Beschleunigung formula_123. Nützlich ist hierbei die Darstellung mit Hilfe der Vortizität formula_124:

Geschlossene analytische Lösungen existieren fast nur für Fälle, in denen der zweite Term verschwindet. Dies ist bei der Annahme, dass bei 3-dimensionalen Strömungen die Wirbel sich immer entlang der Stromlinie ausbilden (also dem Helmholtz-Wirbelsatz), also formula_126 der Fall. Diese Annahme trifft aber nicht bei allen realen Strömungen zu. Eine analytische Lösung mit formula_127 liegt im Hamel-Oseenschen-Wirbel vor.

Die Navier-Stokes-Gleichungen sind ein wichtiges Anwendungsfeld der numerischen Mathematik (die Theorie beschäftigt sich mit Existenz und Eindeutigkeit von Lösungen; in aller Regel gibt es jedoch keine geschlossenen Lösungsformeln). Der Teilbereich, der sich mit der Konstruktion numerischer Näherungsverfahren für die Navier-Stokes-Gleichungen beschäftigt, ist die numerische Strömungsmechanik oder Computational Fluid Dynamics (CFD).

Bei der numerischen Lösung der Navier-Stokes-Gleichungen kommen Verfahren der numerischen Strömungsmechanik zum Einsatz. Als Diskretisierungen werden sowohl Finite-Differenzen-, Finite-Elemente- und Finite-Volumen-Verfahren sowie für spezielle Aufgabenstellungen auch Spektralmethoden und weitere Techniken verwendet. Die Gitter müssen, um die Grenzschicht korrekt auflösen zu können, in Normalenrichtung nahe der Wand extrem fein aufgelöst sein. In Tangentialrichtung wird darauf verzichtet, so dass die Zellen an der Wand extrem große Seitenverhältnisse haben.

Die feine Auflösung erzwingt wegen der Einhaltung der CFL-Bedingung bei expliziter Zeitintegration extrem kleine Zeitschritte. Deswegen werden in der Regel implizite Verfahren eingesetzt. Wegen der Nicht-linearität des Gleichungssystems muss das System iterativ (z. B. mit Mehrgitter- oder Newton-Verfahren) gelöst werden. Die Kombination aus Impuls- und Kontinuitätsgleichung bei den inkompressiblen Gleichungen weist eine Sattelpunktstruktur auf, die hierbei ausgenutzt werden kann.

Ein einfaches Modell zur Simulation von Flüssigkeiten, das im hydrodynamischen Limit die Navier-Stokes-Gleichung erfüllt, ist das FHP-Modell. Dessen Weiterentwicklung führt auf die Lattice-Boltzmann-Methoden, welche besonders im Kontext der Parallelisierung zur Ausführung auf Supercomputern attraktiv sind.

Im Bereich der Computergrafik wurden mehrere numerische Lösungsverfahren verwendet, bei denen durch bestimmte Annahmen eine Echtzeit-Darstellung erreicht werden kann, wobei jedoch teilweise die physikalische Korrektheit nicht immer gewährt ist. Ein Beispiel hierfür ist das von Jos Stam entwickelte "Stable Fluids"-Verfahren. Hierbei wurde die Chorin’sche Projektionsmethode für den Bereich der Computergrafik verwendet.

Um turbulente Strömungen zu berechnen, können die Navier-Stokes-Gleichungen direkt numerisch berechnet werden. Jedoch erzwingt die Auflösung der einzelnen Turbulenzen ein sehr feines Gitter, so dass dies nur in der Forschung unter Zuhilfenahme von Supercomputern und bei kleinen Reynolds-Zahlen wirtschaftlich ist.

In der Praxis hat sich die Lösung der Reynolds-Gleichungen durchgesetzt. Hier ist jedoch ein Turbulenzmodell nötig, um das Gleichungssystem zu schließen.

Als Mittelweg gilt die Large Eddy Simulation, die zumindest die großen Wirbel direkt numerisch berechnet und erst die kleinen Skalen über ein Turbulenzmodell simuliert.

Eine viel untersuchte Konvektion, welche sich mit der Navier-Stokes-Gleichung beschreiben lässt, ist die Rayleigh-Bénard-Konvektion. Sie ist ein wichtiges Beispiel für selbstorganisierende Strukturen und die Chaostheorie.

Auf Grund der schwierigen Lösbarkeitseigenschaften der Navier-Stokes-Gleichungen wird man in den Anwendungen (soweit dies physikalisch sinnvoll ist) versuchen, vereinfachte Versionen der Navier-Stokes-Gleichungen zu betrachten.

Wird die Viskosität vernachlässigt (formula_128), so erhält man die Euler-Gleichungen (hier für den kompressiblen Fall)

Die Euler-Gleichungen für kompressible Fluide spielen insbesondere in der Aerodynamik eine Rolle als Approximation der vollen Navier-Stokes-Gleichungen.

Eine andere Art von Vereinfachungen ist zum Beispiel in der Geodynamik üblich, wo der Mantel der Erde (oder anderer terrestrischer Planeten) als eine extrem zähe Flüssigkeit behandelt wird (Schleichende Strömung). In dieser Näherung ist die Diffusivität des Impulses, d. h. die kinematische Viskosität, viele Größenordnungen höher als die thermische Diffusivität, und der Trägheitsterm kann vernachlässigt werden. Führt man diese Vereinfachung in die stationäre Navier-Stokes-Impulsgleichung ein, erhält man die "Stokes-Gleichung":
Wendet man die Helmholtz-Projektion formula_131 auf die Gleichung an, verschwindet der Druck in der Gleichung:
mit formula_133.
Dies hat den Vorteil, dass die Gleichung nur noch von formula_4 abhängt. Die ursprüngliche Gleichung erhält man mit
formula_136 wird auch "Stokes-Operator" genannt.

Andererseits haben Geomaterialien eine komplizierte Rheologie, die dazu führt, dass die Viskosität nicht als konstant angesehen wird. Für den inkompressiblen Fall ergibt dies:

Für gravitationsabhängige Strömungen mit kleinen Dichtevariationen und nicht zu großen Temperaturschwankungen wird häufig die Boussinesq-Approximation verwendet.

Da es bis heute keinen Existenzbeweis für Lösungen der allgemeinen Navier-Stokes-Gleichungen gibt, ist auch nicht gesichert, dass sie Turbulenz von Fluiden wiedergeben und wenn ja, wie realistisch. Des Weiteren können zufällige äußere Störungen die Strömung beeinflussen (Schmetterlingseffekt) und es ist bekannt, dass Fluidelemente eine zufällige brownsche Bewegung ausführen. Solche zufälligen Fluktuationen können mit einem stochastischen Ansatz erfasst werden. Es wird eine stochastische Differentialgleichung in differentieller Schreibweise

betrachtet. Der Term in der eckigen Klammer repräsentiert die Navier-Stokes-Gleichungen bei Inkompressibilität und der folgende Term einen stochastischen Einfluss wie die brownsche Bewegung. Dieser Ansatz ist zur Jahrtausendwende Gegenstand reger Forschungsaktivität.




</doc>
<doc id="80660" url="https://de.wikipedia.org/wiki?curid=80660" title="Kreisel">
Kreisel

Ein Kreisel ist ein (starrer) Körper, der um eine Achse rotiert. Er kann sich ansonsten frei bewegen (freier Kreisel), kann aber auch mit einer Achse in eine bestimmte Richtung gezwungen sein (gefesselter Kreisel). Kreisel müssen in der Physik nicht notwendigerweise rotationssymmetrisch sein.

Als "Kreisel" oder Gyro (griechisch.) werden auch Messinstrumente bezeichnet, die ähnliche Aufgaben wie ein Kreiselinstrument wahrnehmen, auch wenn sie keinen rotierenden Kreisel enthalten. Beispiele sind Laserkreisel, Faserkreisel oder Vibrationskreisel.


Ein um seine Figurenachse rotierender Kreisel behält bei kardanischer Aufhängung seine Orientierung im Raum bei, auch wenn das Tragegestell verdreht wird. Das schwache Drehmoment, das durch die Lagerreibung der Aufhängung wirkt, erzeugt eine vernachlässigbar kleine Änderung des Drehimpulses, die nicht zu einer beobachtbaren Veränderung der Rotationsachse führt. Verglichen mit dem ruhenden Kreisel sind große äußere Momente erforderlich, um die Ausrichtung zu ändern.

Weiterhin lässt sich beobachten: Wenn beim rotierenden Kreisel versucht wird, seine Rotationsachse zu kippen, dann lässt sich eine Kraftwirkung senkrecht zur Kipprichtung der Rotationsachse registrieren. Je schneller der Kreisel rotiert, desto größer sind die auftretenden Kräfte (die auch Kreiselkräfte genannt werden). Erklären lässt sich das mit dem hohen Drehimpuls des Kreisels, der in seiner Richtung geändert werden muss. Dessen Änderung erfolgt in der Richtung, in der die Rotationsachse gekippt wird, und erfordert ein Drehmoment, das in der Kippebene liegt. Das aufzubringende Drehmoment bedingt die Kraftwirkung senkrecht zur Kipprichtung.

Umgekehrt bewirkt ein Drehmoment senkrecht auf einen rotierenden Kreisel nicht, dass er seine Ausrichtung um die Achse des Drehmoments ändert, sondern in die Richtung der Drehmomentachse kippt.

Die Erklärung des Kreiselverhaltens mag zwar rechnerisch logisch sein, aber schon der Drehimpuls selbst ist eine wenig anschauliche Größe. Daher sei nun zur Plausibilisierung der Abläufe ein Körper angenommen, der im Kreisel eingeschlossen ist. Solange der Kreisel stabil um seine Figurenachse rotiert, muss der Kreisel auf den eingeschlossenen Körper nur eine Zentripetalkraft ausüben. Spannend wird es, wenn nun die Rotationsachse des Kreisels gekippt wird und die Bewegung des Körpers dabei analysiert wird. Dann bewegt sich der eingeschlossene Körper auch in Kipprichtung, wechselt aber ständig die Seite und damit seine Bewegungsrichtung, also seine Geschwindigkeit. In Richtung der Senkrechten zur Kippebene führt der eingeschlossene Körper eine sinusförmige Schwingung aus. Das bedeutet, im Scheitel gibt es einen Ruhepunkt und im „Nulldurchgang“, beim Wechsel der Kippseite, findet die größte Änderung der „Kippgeschwindigkeit“ und damit die größte Kraftwirkung statt. Der Kreisel will also beim Kippen zur Seite ausbrechen.

Wenn die Winkelgeschwindigkeit des Kreisel "ω" deutlich größer als die Kippwinkelgeschwindigkeit "Ω" ist, dann gilt die im Weiteren folgende Näherungsrechnung. Die Drehimpulsänderung formula_1 ergibt sich aus der Winkeländerung d"φ" und der Ausrichtung der Kippachse formula_2 nach der folgenden Formel. Das Kreuzprodukt bedeutet, hier interessiert nur die Komponente des Drehimpulses, die senkrecht zur Kippachse formula_2 steht. Der Anteil parallel zur Kippachse sei vernachlässigt.

Die Winkeländerung d"φ" über die Zeit d"t" stellt des Weiteren die Kippwinkelgeschwindigkeit "Ω" dar. Im nächsten Schritt sei die Drehimpulsänderung eingesetzt in den Eulerschen Drehimpulssatz. Damit folgt das resultierende Drehmoment M aus den Kreiselparametern Rotationsrate "ω" und Trägheitsmoment der Figurenachse "I", verbunden mit der Kippwinkelgeschwindigkeit "Ω".

Das Deviationsmoment ist ein Maß für das Bestreben eines Kreisels, seine Rotationsachse zu verändern, wenn er nicht um eine seiner Hauptträgheitsachsen rotiert.

Eine Verallgemeinerung der Kreiselbewegung ergibt sich aus dem Drehimpulssatz. Dabei folgt der Drehimpuls formula_9 aus dem Produkt von Trägheitstensor formula_10 und Drehgeschwindigkeit formula_11 des Kreisels. Wie die Masse für die translatorische Bewegung angibt, wie „schwer“ ein Körper zu beschleunigen ist, so beschreibt der Trägheitstensor für die Drehbewegung, wie „schwer“ es ist, die Drehung eines Kreisels zu ändern. Im Trägheitstensor sind die Trägheitsmomente für Drehungen um die verschiedenen Drehachsen des Kreisels zusammengefasst.

Berechnet man die Änderung des Drehimpulses durch dessen Ableitung nach der Zeit, ergibt sich:formula_12Das heißt, die Änderung des Drehimpulses formula_13 setzt ein Drehmoment formula_14 voraus. Sie ist abhängig von der Änderung der räumlichen Richtung des Drehimpulses (Term formula_15) sowie von der Änderung der momentanen Winkelgeschwindigkeit formula_16 (Term formula_17).

Durch die Rotation in ein Bezugssystem, in dem der Trägheitstensor eine Diagonalmatrix bildet, können die einzelnen Komponenten der Vektoren in ein besonders einfaches System von Differentialgleichungen umformuliert werden. Sie sind nach ihrem Entdecker Leonhard Euler als die Eulerschen Kreiselgleichungen bekannt.

Nach Eigenschaften:

Nach Aufhängung:


Das Spielzeug gibt es in zahlreichen Bauformen und Varianten:


Anfang des 20. Jahrhunderts wurden Kreisel – z. B. der Musilsche Farbkreisel – zur Untersuchung der menschlichen Farbwahrnehmung eingesetzt.

In dem Film Inception aus dem Jahr 2010 benutzt der Protagonist Cobb einen Kreisel als „Totem“.




</doc>
<doc id="92472" url="https://de.wikipedia.org/wiki?curid=92472" title="Heliosphäre">
Heliosphäre

Die Heliosphäre (aus dem Griechischen: "Sonnenkugel") ist die Astrosphäre der Sonne. Sie bezeichnet im Weltraum einen weiträumigen Bereich um die Sonne, in dem der Sonnenwind mit seinen mitgeführten Magnetfeldern wirksam ist. In diesem Bereich verdrängt der Teilchenstrom der Sonne das interstellare Medium. Die Umlaufbahnen der Planeten liegen weit innerhalb der Heliosphäre.
Das Sonnensystem ist in das interstellare Medium eingebettet, das vorwiegend aus extrem verdünntem Gas sowie Staub und Magnetfeldern besteht.

Die Sonne wiederum emittiert einen konstanten Strom von Partikeln, den Sonnenwind. Dieser besteht hauptsächlich aus ionisiertem Wasserstoff und Helium (Protonen, Heliumkerne und Elektronen). In einem Abstand von 1 AE von der Sonne (Umlaufbahn der Erde) beträgt die Teilchendichte des Sonnenwindes ein bis zehn Millionen Teilchen pro Kubikmeter. Bei koronalen Massenauswürfen kann die Teilchendichte in diesem Abstand auf mehr als das Hundertfache ansteigen. Der Sonnenwind mit seinen elektrisch geladenen Teilchen und dem mitgeführten interplanetaren Magnetfeld verdrängt das interstellare Medium und bildet eine „Blase“ um die Sonne. Diese Blase ist die Heliosphäre.

Das Sonnensystem bewegt sich mit einer Geschwindigkeit von etwa 23 km/s durch das interstellare Medium, aus der Richtung des Sternbilds Stier kommend in die Richtung des Sternbilds Skorpion. Dadurch entsteht ein „Fahrtwind“ (interstellarer Wind). Ob und in welchem Maße die Heliosphäre dadurch verformt wird – vorne eingedrückt ist und hinten einen „Helioschweif“ (engl. "heliotail") ausbildet – ist noch weitgehend unklar.

Die Heliosphäre wird durch zwei Grenzen strukturiert:

Für den Bereich innerhalb der Randstoßwelle gibt es keine besondere Bezeichnung. Der Bereich zwischen Randstoßwelle und Heliopause wird Heliohülle (engl. "heliosheath") genannt. Jenseits der Heliopause endet definitionsgemäß die Heliosphäre, und der interstellare Raum beginnt.

Lange wurde vermutet, dass sich – wie bei anderen Sternen beobachtet – jenseits der Heliopause eine weitere Stoßfront bildet, die Bugstoßwelle "(bow shock)," wo der interstellare Wind von Über- auf Unter­schall­geschwindigkeit abgebremst wird. Nach neueren Erkenntnissen gibt es diese nicht (sondern nur eine Bugwelle), weil sich das Sonnensystem relativ zum interstellaren Medium mit weniger als Schall­geschwindigkeit bewegt.

Im inneren Bereich der Heliosphäre bewegt sich der Sonnenwind unbeeinflusst durch den Raum, da er mit Überschall­geschwindigkeit strömt, d. h. seine Strömungs­geschwindig­keit ist größer als die Geschwindigkeit, mit der sich Störungen der Dichte bzw. des Druckes im Plasma fortbewegen (Schallgeschwindigkeit). Nur elektrisch neutrale Atome aus dem interstellaren Medium und ein geringer Teil der galaktischen kosmischen Strahlung können so weit in die Heliosphäre eindringen. Abgesehen von den wenigen Partikeln, die das schaffen, stammt die gesamte Teilchenmenge dort von der Sonne.

An der Randstoßwelle (Terminationsschock, "termination shock") sinkt die Strömungs­geschwindig­keit unter die Schall­geschwindig­keit, sodass zum ersten Mal eine Beeinflussung durch das interstellare Medium auftritt. Die Partikel des Sonnenwindes werden abrupt abgebremst – in niedrigen Breiten (d. h. nahe der Ekliptik) von ca. 350 km/s auf ca. 130 km/s. Durch dieses Abbremsen und das weitere Nachströmen von Materie verdichtet und erhitzt sich das Medium des Sonnenwindes. Als Folge kommt es weiterhin zu einem deutlichen Anstieg des Magnetfeldes. 

Die Raumsonde Voyager 2 maß beim Durchqueren der Randstoßwelle einen sprunghaften Anstieg der Temperatur von ca. 11 000 K auf 180 000 K, was allerdings deutlich unter den Vorhersagen einiger Modelle lag, die Temperaturen von einigen Millionen Kelvin vorhergesagt hatten. Zusammen mit den Ergebnissen der STEREO-Sonden ergab sich, dass 70 % der Bewegungsenergie des Sonnenwindes nicht in Wärme übergehen, sondern in die Ionisation von dort angetroffener Materie. Dies könnten elektrisch neutrale Wasserstoffatome sein, die mit einer Geschwindigkeit von etwa 25 km/s in die Heliohülle eingedrungen und bis zur Randstoßwelle vorgestoßen sind. 

Voyager 1 wurde beim Vorbeiflug am Saturnmond Titan 34° nördlich aus der Ekliptikebene abgelenkt und erreichte die Randstoßwelle bei 94 AE Entfernung von der Sonne; Voyager 2 hingegen, die am Neptun 26° südlich abgelenkt wurde, erreichte sie schon bei 84 AE Entfernung. Eine mögliche Erklärung für diesen Unterschied ist, dass das interstellare magnetische Feld die südliche Hälfte der Heliosphäre nach innen drückt und die nördliche Hälfte nach außen wölbt. Eine andere mögliche Ursache ist die variable Sonnenaktivität, da die Messungen der beiden Voyager­sonden im Abstand von drei Jahren vorgenommen wurden.

Ebenso zeigte sich am Beispiel von Voyager 2, dass die Randstoßwelle keine konsistente feste Grenze, sondern ein dynamisches Ereignis ist, das sich ähnlich der Brandung an einem Strand verhält. So gibt es Dichte­schwankungen im Sonnenwind, hervorgerufen durch koronale Massenausbrüche oder Überlagerung der schnellen und langsamen Sonnenwinde, die mit den Wellen im Meer vergleichbar sind und somit weiter in die Heliohülle hinausreichen. Durch die differentielle Rotation der Sonne und die große Entfernung von der Sonne können so in relativ kurzen Abständen große Sprünge in der absoluten Entfernung von der Sonne möglich sein. Voyager 2 passierte die Randstoßwelle innerhalb einiger Tage fünf Mal, bevor sie am 30. August 2007 endgültig durchschritten war.

Jenseits der Randstoßwelle befindet sich die Heliohülle "(heliosheath)," in deren Bereich weiterhin Sonnenwind­teilchen vorkommen, nun jedoch mit einer reduzierten Strömungs­geschwindigkeit bei höherer Dichte und Temperatur. Dieser Bereich wird noch vom Sonnenwind dominiert, aber es mischen sich Partikel des lokalen interstellaren Mediums hinein. Messungen der Voyager-Raumsonden ergaben, dass sich in der Heliohülle eine Art „Schaum“ aus magnetischen Blasen mit einem Durchmesser von typischerweise 1 AE befindet, in denen die geladenen Partikel des Sonnenwinds gefangen sind.

Aufgrund von Modellrechnungen und Beobachtungen an anderen Sternen hat man angenommen, dass die Heliohülle in Richtung der Eigenbewegung der Sonne nur 10 AE dick sei, während sie in entgegen­gesetzter Richtung vom interstellaren Wind zu einem langen „Helioschweif“ von bis zu 100 AE verformt werde. Messungen der Sonde IBEX im Jahr 2013 gaben Hinweise auf einen Helioschweif mit einer kleeblattförmigen Struktur, als deren Ursache die ungleichmäßige Aktivität der Sonne während ihres 11-jährlichen Zyklus angenommen wurde. Kombinierte Langzeitmessungen mit den Raumsonden Voyager 1 und 2, Cassini und IBEX legen andererseits nahe, dass die Heliohülle eher kugelförmig ist. Grund ist das mit etwa 0,5 Nanotesla unerwartet starke interstellare Magnetfeld, das die Heliosphäre in Form hält.

Die Heliopause ist die äußerste Grenze der Heliosphäre. Dahinter beginnt definitionsgemäß der interstellare Raum. Der Sonnenwind übt nun keine materiellen Einflüsse auf das interstellare Gas mehr aus. Die Partikel des Sonnenwindes vermischen sich mit dem interstellaren Gas und haben keine erkennbare herausstechende Strömungsrichtung im Vergleich mit dem die Heliosphäre umgebenden Gas.

Voyager 1 passierte die Heliopause um den 25. August 2012 in einem Abstand von 121,7 AE von der Sonne. Dabei verzeichneten die Messgeräte einen dramatischen Abfall der Zählrate solarer Teilchen um mehr als einen Faktor 100 sowie einen signifikanten Anstieg energiereicher kosmischer Strahlung. Voyager 2 erreichte die Heliopause am 5. November 2018 im Abstand von 119,0 AE. Das Plasmaspektrometer verzeichnete dabei einen scharfen Abfall der Geschwindigkeit solarer Teilchen. In radialer Richtung (aus dem Sonnensystem heraus) wurde der Sonnenwind komplett gestoppt. Mit Voyager 1 konnte diese Messung nicht durchgeführt werden, da das Plasmaspektrometer der Sonde bereits in den 1980er Jahren komplett ausfiel.

Voyager 2 durchquerte die Heliopause innerhalb von nur einem halben Tag, verzeichnete also eine dünne, stabile Grenzschicht. Voyager 1 hingegen hatte schon fast zwei Jahre vor der Passage der Heliopause ein Abflauen der Plasmaströme registriert und danach Turbulenzen im umgebenden interstellaren Plasma – ein Indiz für eine instabile, aber dicke Grenzregion. Grund für den Unterschied könnte die zeitlich schwankende Sonnenaktivität sein. Nach Messungen von Voyager 1 waren die magnetischen Feldlinien der Heliosphäre mit denen des interstellaren Raumes verbunden. Entlang einer so entstandenen „magnetischen Autobahn“ "(magnetic highway)" können geladene Teilchen aus dem Sonnensystem in den interstellaren Raum gelangen und umgekehrt. Voyager 2 fand keine solche Verbindung vor aber eine magnetische Barriere diesseits der Heliopause, die sich vermutlich mit dem Zyklus der Sonnenaktivität jeweils neu aufbaut und als zusätzliche Abschirmung gegen galaktische kosmische Strahlung wirkt.

Die Heliopause wird manchmal als „Grenze des Sonnensystems“ bezeichnet. In der Tat liegen die Umlaufbahnen der bekannten Planeten weit innerhalb der Heliosphäre (Neptun als äußerster Planet mit 30 AE), und Gleiches gilt für Pluto und den Kuipergürtel allgemein (30–50 AE). Mittlerweile sind aber transneptunische Objekte gefunden worden, deren Orbits über die Heliopause hinaus reichen, und noch viel weiter entfernt befindet sich die von Astrophysikern vermutete Oortsche Wolke. Eine Schwerkrafteinwirkung der Sonne ist auch bei diesen Objekten noch gegeben, daher ist es nicht allgemein akzeptiert, die Heliopause als Grenze zu definieren.

Die Erforschung mit Raumsonden vor Ort ist schwierig, weil immense Distanzen überwunden werden müssen, obendrein gegen die bremsende Gravitationskraft der Sonne. Die beiden Sonden des Voyager-Programms sind die einzigen von Menschen gebauten Objekte, die nachweislich jemals in die Heliohülle eingedrungen sind. Obwohl sie durch mehrere Swing-by-Manöver beschleunigt wurden, brauchten sie hierfür mehr als ein Viertel­jahrhundert; die Heliopause erreichten sie nach 35 bzw. 41 Jahren. Pioneer 10 und 11 lieferten Daten bis zu einer Entfernung von 63 AE bzw. 35,6 AE, bevor der Kontakt zu ihnen abbrach. Ob New Horizons noch ausreichend Energie hat, wenn die Sonde ca. 2035 die Randstoßwelle erreicht, ist unsicher.

Die beiden Voyager-Sonden wurden 1977 gestartet. Sie waren nur für die Erforschung der äußeren Planeten und eine Lebensdauer von wenigen Jahren konzipiert, blieben aber weit länger funktionstüchtig. Mit ihren Detektoren für Magnetfelder, kosmische Strahlung, Plasmateilchen (nur Voyager 2) sowie Plasmawellen übermittelten die Sonden Messdaten aus der Heliohülle und dem interstellaren Raum. 

Voyager 1 erreichte die Randstoßwelle am 16. Dezember 2004, Voyager 2 am 30. August 2007. Ungefähr am 25. August 2012 passierte Voyager 1 die Heliopause und trat damit in den interstellaren Raum ein; Voyager 2 folgte am 5. November 2018. 
Eigentlich dafür ausgelegt, die Magnetosphäre der Erde in Verbindung mit Ausbrüchen der Sonne zu untersuchen, konnte das Solar Terrestrial Relations Observatory (STEREO) indirekt neutrale Atome aus dem interstellaren Gas nachweisen. Die Sonden detektierten hochenergetische elektrisch neutrale Atome, die vor allem aus der Richtung kamen, in die sich die Sonne bewegt. Unbeeinflusst vom Magnetfeld der Heliosphäre konnten diese Atome bis zu den STEREO-Sonden vordringen. Offenbar handelt es sich um ursprünglich geladene Teilchen (Ionen) aus dem Sonnenwind, die in der Region der Randstoßwelle auf hohe Energien aufgeheizt wurden, in der Heliohülle ihre Ladung an nieder­energetische neutrale Atome aus dem interstellaren Gas verloren und zurückgestreut wurden. Dies stimmt mit den Messergebnissen von Voyager 2 überein, die jenseits der Randstoßwelle eine niedrigere Temperatur als erwartet lieferten.

Der NASA-Forschungssatellit Interstellar Boundary Explorer (IBEX, zu deutsch etwa "Erforscher der interstellaren Grenze") kartiert das interstellare Medium um die Sonne mittels der Messung neutraler Atome aus einer Erdumlaufbahn heraus. Es gab 2013 erste Hinweise auf einen Helioschweif. 2016 wurde eine Bandstruktur entdeckt, die von der Umströmung der Heliosphäre im interstellaren Magnetfeld herrühren soll. Außerdem ergab sich 2012 durch IBEX das überraschende Ergebnis, dass sich das Sonnensystem so langsam durch das interstellare Medium bewegt, dass es keine Stoßfront (Bugstoßwelle) gibt.

2016 beobachtete IBEX die Auswirkungen einer erhöhten Sonnenaktivität: In der zweiten Jahreshälfte 2014 hatten sich Dichte und Geschwindigkeit des Sonnenwindes erhöht, wodurch sein Druck um 50 % zunahm. Zwei Jahre später detektierte IBEX Sonnenwind­teilchen, die den Rand der Heliosphäre erreicht hatten und von dort als neutrale Atome zurückgestreut worden waren. Modellrechnungen ergaben, dass der verstärkte Sonnenwind die Randstoßwelle um 7 AE und die Heliopause um bis zu 4 AE nach außen verschoben hatte.

Die Heliosphäre – vor allem die Heliohülle – schirmt die Erde vor ca.  der galaktischen kosmischen Strahlung ab. Derzeit bewegt sich das Sonnensystem im interstellaren Raum durch die Lokale Blase, die eine relativ geringe Dichte hat. Würde die Sonne einen Bereich mit einer weitaus höheren Dichte durchqueren, könnte die Heliosphäre an der Front weiter zurückgedrückt werden. Für das Durchqueren einer Molekülwolke mit 30-mal höherer Dichte ergeben Modellrechnungen beispielsweise, dass die Heliopause in Bewegungsrichtung um einen Faktor 4–5 näher wäre. Die galaktische kosmische Strahlung würde auf der Erde um einen Faktor 1,5–3 ansteigen, die anomale kosmische Strahlung um einen Faktor 10. Diesen Umstand, wäre er jemals in den 4,5 Milliarden Jahren seit Bestehen des Sonnensystems vorgekommen, könnte man durch Untersuchung von Sedimenten nachweisen. Allerdings gibt es keine Anzeichen dafür, dass die Sonne in ihrer bisherigen Lebensspanne eine Molekülwolke durchquert hat. Ebenso ist es nicht zu erwarten, dass die Sonne in den nächsten Jahrmillionen in eine Region mit größerer Dichte eintauchen wird.




</doc>
<doc id="150712" url="https://de.wikipedia.org/wiki?curid=150712" title="Größenordnung (Länge)">
Größenordnung (Länge)

Dies ist eine Zusammenstellung von Längen verschiedener Größenordnungen zu Vergleichszwecken. Die Angaben sind oft als „typische Werte“ zu verstehen; die umgerechneten Werte sind gerundet.

Grundeinheit der Länge im internationalen Einheitensystem ist 1 Meter (Einheitenzeichen "m").

1 Zeptometer ist ein trilliardstel Meter (1 zm = 10 m).
1 Attometer ist ein trillionstel Meter (1 am = 10 m).
1 Femtometer ist ein billiardstel Meter (1 fm = 10 m).
1 Pikometer ist ein billionstel Meter (1 pm = 10 m).
1 Nanometer ist ein milliardstel Meter (1 nm = 10 m) und Namensgeber der Nanotechnologie.


subatomarer bis atomarer Bereich:























</doc>
<doc id="172877" url="https://de.wikipedia.org/wiki?curid=172877" title="Gravity Probe">
Gravity Probe

Gravity Probe hießen zwei Raumfahrtmissionen der NASA zum Test der allgemeinen Relativitätstheorie von Albert Einstein. Eine vorgeschlagene dritte Mission wurde bisher nicht realisiert. 

Gravity Probe A (GP-A) flog am 16. Juni 1976 mit einer extrem genauen Atomuhr in einer steilen, ballistischen Bahn mit 10.000 km Gipfelhöhe.
Während des knapp zweistündigen Fluges wurde der Gang der Uhr mittels Mikrowellenverbindung mit zwei gleichen Uhren am Boden verglichen.
Dazu wurde das Uhrensignal der Sonde durch einen Transponder einem vom Boden empfangenen Signal aufgeprägt und wieder zurückgesendet. Dieses Verfahren vermied die störende Wirkung des Doppler-Effekts und erlaubte die Messung der auf dem Äquivalenzprinzip beruhenden Gravitationsrotverschiebung mit einer Genauigkeit von 0,02 %. 1965 lag die Genauigkeit noch bei 1 %, gemessen mittels Mößbauer-Effekt über eine Fallhöhe von lediglich 15 Metern. Wenig später erlaubte das GPS-Satellitensystem weitaus genauere Messungen.

Im Satelliten Gravity Probe B (GP-B) befanden sich frei schwebend vier schnell rotierende Quarzkugeln.
Die extrem genaue Beobachtung ihrer Drehachsen gab Aufschluss auf Drehmomente, die durch zwei relativistische Effekte verursacht wurden.
Die Idee dazu stammt von dem Theoretiker Leonard Schiff, der Anfang der 1960er Jahre mit William Fairbank Sr. darüber diskutierte. Die wissenschaftliche Leitung der NASA-Mission lag bei einem früheren Mitarbeiter von Fairbank an der Stanford University, Francis Everitt. Die Auswertung der 2004/2005 aufgenommenen, schwer gestörten Daten zog sich bis 2011 hin. Das Ergebnis war wie erwartet in Übereinstimmung mit der Theorie, aber nicht genauer als frühere Messungen mittels der geodätischen Satelliten LAGEOS und GRACE.

Die Satelliten-Mission Gravity Probe B sollte – zur Zeit ihrer Planung durch Fairbank "erstmals" – eine experimentelle Überprüfung zweier Aussagen der Allgemeinen Relativitätstheorie ermöglichen:


Nach den Vorhersagen der Physiker sollten sich die Rotationsachsen der vier Gyroskope wegen der Raumzeit-Krümmung pro Jahr um 6606,1 Millibogensekunden neigen und zusätzlich, durch den Lense-Thirring-Effekt, um 39,2 Millibogensekunden in eine andere Richtung.
Die Messung derartig kleiner Änderungen der Rotationsachse ist eine extreme Herausforderung an die Experimentiertechnik.
Die eigens für diese Mission entwickelten Gyroskope bestanden aus Quarzkugeln von der Größe eines Tischtennisballs (3,8 cm), die im Vakuum mit 10.000 Umdrehungen pro Minute rotierten.
Sie wurden auf 1,8 K abgekühlt, um ihre mit Niob beschichtete Oberfläche supraleitend zu machen und durch das London-Moment ein Magnetfeld in Richtung der Drehachse zu erzeugen.
Veränderungen der Rotationsachse wurden mittels hochempfindlicher supraleitender Quanteninterferenz-Detektoren, sogenannter SQUIDs, erfasst. Äußere Magnetfelder wurden durch eine zweifache Abschirmung aus supraleitendem Material um 240 dB reduziert. Auf diese Weise waren Veränderungen von einer Millibogensekunde messbar (bei 10 Stunden Integrationszeit). Unter diesem Winkel erscheint ein Stecknadelkopf im Abstand von 1.000 km.

Die vier Kugeln befanden sich in der Rotationsachse des Satelliten und rotierten paarweise gleich- bzw. gegensinnig. Die Drehachsen wurden gegenüber dem Satelliten gemessen und mit einem kleinen Teleskop in dessen Rotationsachse auf den spektroskopischen Doppelstern IM Pegasi
bezogen.
Zur Lagekontrolle des Satelliten um seine Achse und während der Bedeckungen von IM Pegasi durch die Erde dienten mehrere Startracker und Gyroskope. IM Pegasi wurde gewählt, weil er dicht an der Ebene des Erdäquators liegt, hell genug für die genaue Peilung ist und seine starke Radioemissionen per VLBI detektiert werden können, sodass seine Bewegung auf das Referenzsystem ferner Quasare bezogen werden konnte.

Der Satellit wurde am 20. April 2004 vom US-Luftwaffenstützpunkt Vandenberg an Bord einer Delta II 7920-Rakete erfolgreich gestartet.
Die Bahn des Satelliten führte in einer Höhe von ca. 740 km über die Pole.

Am 28. August 2004 waren die Vorbereitungen für die eigentlichen Messungen abgeschlossen.
Die zuvor durchgeführten Kalibriermessungen hatten jedoch schon gezeigt, dass unvorhergesehene Effekte (später beschrieben als "misalignment torque" und "roll-polhode resonance torque") die Rotationsrichtung der Gyroskope beeinflussten. Nachdem die Ursache verstanden war, konnten die Effekte modelliert und zunächst grob herausgerechnet werden. Es handelte sich um eine unerwünschte Kopplung zwischen Kugeloberfläche und Wandung durch inhomogene elektrische Felder aufgrund von lokalen Variationen der Austrittsarbeit.
Zudem führte eine Wechselwirkung dieses Feldes mit der Aktivität der elektrostatischen Zentrierung der Kugel zu einer Dämpfung ihrer Präzessionsbewegung, was die Analyse der Messwerte erschwerte und verzögerte.

Die aus den eigentlichen Messungen nicht präzise genug ermittelbaren Kopplungskonstanten mit der Wandung, also mit dem rotierenden Satelliten, wurden in einer gegen Ende der Mission vorgesehenen Kalibrierphase gesondert vermessen, indem die Rotationsachse des Satelliten grob falsch eingestellt wurde, was die Störeffekte vervielfachte. Es schloss sich eine weitere Messphase an, bis das Helium für die Kühlung Ende 2005 verbraucht war.

Das öffentliche Eingeständnis der Probleme übernahm Everitt selbst.

Kurz bevor die NASA-Mission mit dem Jahr 2007 endete, beantragte er eine Verlängerung um ein Jahr, da eine weitere Auswertung genauere Ergebnisse versprach. Dies wurde von anderen Forschern, die um die Finanzierung ihrer Missionen bangten, mit dem Argument kritisiert, dass LAGEOS bereits ähnlich genaue Tests der Relativitätstheorie ermöglicht hatte. Ein 15-köpfiges Gremium der NASA bewilligte nur noch spärliche Mittel, die durch eine private Spende von Fairbanks Sohn sowie Mittel der Universität ergänzt wurden. Der Schlussbericht der Mission erschien im Dezember 2008,
die Auswertung wurde aber weitergeführt, u. a. mit Mitteln aus Saudi-Arabien.
Der Schlussbericht der Universität
erschien unter erneuter öffentlicher Aufmerksamkeit
im Mai 2011.

Der Effekt der Raumzeit-Krümmung (Theorie: 6606,1, alles in Millibogensekunden/Jahr) hatte mit einer Präzision von 0,01 % bestimmt werden sollen, der schwächere Lense-Thirring-Effekt (Theorie: 39,2) auf 1 %. Herausgekommen sind 6601,8 ± 18,3 (0,28 %), bzw. 37,2 ± 7,2 (19 %), Fehlerangaben jeweils 1 σ.
Bereits 2004 hatte eine Auswertung von über elf Jahre gesammelten LAGEOS-Bahndaten die Vorhersage des Lense-Thirring-Effekts auf 1 % getroffen mit einer Unsicherheit von 5 %.

Gravity Probe C(lock) ist ein Vorschlag aus dem Jahr 1997 für eine dritte Mission. Das Experiment würde aus zwei Satelliten bestehen, die auf äquatorialen Umlaufbahnen in entgegengesetzter Richtung die Erde umkreisen. Laut Allgemeiner Relativitätstheorie sollten sich die Umlaufzeiten der Satelliten durch gravitomagnetische Effekte (verursacht durch die Erdrotation) um ca. 100 Nanosekunden unterscheiden. Um andere Effekte herausrechnen zu können, müsste vor der Mission das Gravitationsfeld der Erde noch genauer untersucht werden.




</doc>
<doc id="210490" url="https://de.wikipedia.org/wiki?curid=210490" title="Zerfallsreihe">
Zerfallsreihe

Eine Zerfallsreihe im allgemeinen Sinn ist die Abfolge der nacheinander entstehenden Produkte eines radioaktiven Zerfalls. Sie bildet sich, indem ein Radionuklid sich in ein anderes, dieses in ein drittes umwandelt usw. („zerfällt“). Das zuerst entstehende Nuklid wird Tochternuklid genannt, das dem Tochternuklid folgende Enkelnuklid, das dem Enkelnuklid folgende Urenkelnuklid usw.

Aus einer vorhandenen Menge eines instabilen Nuklids bildet sich durch Zerfall ein Gemisch der Nuklide, die ihm in der Zerfallsreihe folgen, bevor irgendwann alle Atomkerne die Reihe bis zum Endnuklid durchlaufen haben. In dem Gemisch sind Nuklide mit kurzer Halbwertszeit nur in geringer Menge vorhanden, während solche mit längerer Halbwertszeit sich entsprechend stärker ansammeln.

Praktisch und historisch wichtig sind die Zerfallsreihen der drei primordialen Radionuklide Uran-238, Uran-235 und Thorium-232, auch "Natürlich radioaktive Familien" genannt. Sie entstehen durch Alpha- und Beta-Zerfälle, die mehr oder weniger regelmäßig abwechselnd aufeinander folgen. Manche der beteiligten Nuklide haben auch die alternativ mögliche, aber seltene Zerfallsart Spontanspaltung; sie führt aus der jeweiligen Zerfallsreihe hinaus und wird hier nicht beachtet.

Ein Alphazerfall verringert die Massenzahl des Atomkerns um 4 Einheiten, ein Betazerfall lässt sie unverändert. Schreibt man die Massenzahl "A" als "A" = 4"n"+"m" (dabei ist "n" irgendeine natürliche Zahl und "m" eine der Zahlen 0, 1, 2 oder 3), bleibt deshalb "m" innerhalb einer solchen Zerfallsreihe stets konstant. Die drei genannten Anfangsnuklide haben verschiedene Werte von "m". Daher erzeugt

Thorium-232 ist zwar primordial, aber nach heutiger Kenntnis sind auch seine Vorgängernuklide bis zum Plutonium-244 auf der Erde vorhanden.

In der obigen (4"n"+"m")-Systematik „fehlt“ eine Reihe mit "m" = 1. Da es im Massenzahlbereich von Uran und Thorium kein primordiales Nuklid mit "A" = 4"n"+1 gibt, kommt eine solche Zerfallsreihe in der Natur nicht (mehr) vor. Der Systematik zuliebe wird aber die Zerfallsreihe der künstlich erzeugbaren Nuklide Plutonium-241 oder Neptunium-237, die Neptunium-Reihe, als diese fehlende vierte Reihe betrachtet. Nur das letzte Radionuklid dieser Reihe, Bismut-209, ist wegen seiner extrem langen Halbwertszeit noch vorhanden. Es wurde lange für das Endnuklid gehalten, bis 2003 entdeckt wurde, dass es ein Alphastrahler mit 19 Trillionen Jahren Halbwertszeit ist. Das Endnuklid ist daher Thallium-205.

In der klassischen Zeit der Erforschung der radioaktiven Zerfallsreihen – also im frühen 20. Jahrhundert – wurden die verschiedenen Nuklide mit anderen Namen bezeichnet, an denen sich die Zugehörigkeit zu einer natürlichen Zerfallsreihe und die Ähnlichkeit in den Eigenschaften erkennen ließ (z. B. sind Radon, Thoron und Actinon allesamt Edelgase):

Die drei natürlichen Zerfallsreihen sähen in dieser alten Bezeichnungsweise folgendermaßen aus:

Nuklide zerfallen nach einer Kinetik erster Ordnung (vgl. Zerfallsgesetz), so dass die zeitabhängige Konzentration eines einzelnen Nuklids recht einfach zu berechnen ist. Die Fragestellung wird deutlich komplizierter, wenn das Nuklid als Glied einer Zerfallsreihe aus Vorläufernukliden laufend nachgebildet wird. Ein kurzer und übersichtlicher Weg zur Berechnung seiner Konzentration unter dieser Voraussetzung findet sich bei Jens Christoffers (1986); der Autor gibt auch einen Algorithmus zur Programmierung der Berechnung an. 



</doc>
<doc id="275832" url="https://de.wikipedia.org/wiki?curid=275832" title="Rayl">
Rayl

Das Rayl (benannt nach dem englischen Physiker John William Strutt, 3. Baron Rayleigh) ist eine veraltete und ungesetzliche Einheit der spezifischen Schallkennimpedanz. Dabei gibt es zwei unterschiedliche Definitionen, die sich um eine Größenordnung unterscheiden:


</doc>
<doc id="292715" url="https://de.wikipedia.org/wiki?curid=292715" title="Elsa Neumann">
Elsa Neumann

Elsa Neumann (* 23. August 1872 in Berlin; † 23. Juli 1902 ebenda) war eine deutsche Physikerin. Im Jahre 1899 wurde sie als erste Frau im Fach Physik an der Berliner Universität promoviert.

Als Frau blieb Elsa Neumann in ihrem Heimatland der Zugang zu höherer Bildung generell verwehrt. Im Jahre 1890 absolvierte sie daher zunächst die Lehrerinnenprüfung, eine Ausbildung, die zur damaligen Zeit keine höhere Bildung verlangte und vom Stellenwert her unterhalb des Realgymnasiums angesiedelt war. In der Folge nahm sie bei verschiedenen Professoren Privatunterricht, um sich die für ein Studium notwendigen Kenntnisse anzueignen. Ab 1894 studierte sie neun Semester Physik, Mathematik, Chemie und Philosophie an den Universitäten in Berlin und Göttingen. Da Frauen in Preußen zu diesem Zeitpunkt ein reguläres Universitätsstudium noch untersagt war, musste sie von jedem Professor besondere Erlaubnis einholen, bei ihm Vorlesungen besuchen zu dürfen. Die Physik-Professoren Emil Warburg und Max Planck gehörten zu ihren einflussreichsten Förderern. 1898 erhielt sie mit spezieller Erlaubnis des Kultusministeriums die Genehmigung zur Promotion, die sie noch im selben Jahr cum laude abschloss; die feierliche Promotion fand am 18. Februar 1899 statt. Ihre Arbeit "„Über die Polarisationskapazität umkehrbarer Elektroden“" wurde 1899 in der angesehenen Fachzeitschrift Annalen der Physik veröffentlicht.

Auf Grund der schlechten Berufsaussichten für promovierte Frauen an akademischen Einrichtungen arbeitete Elsa Neumann ab 1899 als Privatgelehrte und führte ihre Forschungen in dem chemischen Privat-Laboratorium von Arthur Rosenheim und Richard Joseph Meyer durch, das sich seit 1891 in der Chausseestraße 2e in Berlin befand.
Hier starb Elsa Neumann am Mittwoch, dem 23. Juli 1902, an den Folgen eines Unfalls beim Experimentieren mit Blausäure.

Ihre Mutter stiftete nach dem Tod ihrer Tochter den "Elsa-Neumann-Preis", der jeweils am 18. Februar eines Jahres für die beste mathematisch-physikalische Arbeit der Universität Berlin verliehen werden sollte, ausdrücklich unabhängig von Geschlecht oder Religion des Verfassers. Die insgesamt zwölf Träger des von 1906 bis 1918 verliehenen Preises waren allerdings allesamt männlich. Ein bekannter Preisträger ist der Kernphysiker Walther Bothe.

Elsa Neumann war sich ihrer Ausnahmeposition bewusst und engagierte sich für die Durchsetzung des Frauenstudiums in Preußen. Obwohl oder weil sie aus vermögendem Elternhaus kam, war ihr bewusst, dass das Frauenstudium wirtschaftlich unterstützt werden musste. Sie war am 26. April 1900 Gründerin, erste Vorsitzende und später Ehrenmitglied des „Vereins zur Gewährung zinsfreier Darlehen an studierende Frauen“. Der Verein hatte sich am 30. April 1900 beim Amtsgericht I in Charlottenburg eintragen lassen. Im Statut definierte er im Paragraph 3: „Der Zweck des Vereins ist, studierenden Frauen durch Gewährung von zinsfreien Darlehen das Studium zu erleichtern und die Ablegung eines Abschlußexamens zu ermöglichen.“ Elsa Neumann war von April 1900 bis März 1902 die 1. Vorsitzende und wurde im März 1902 „als Ehrenmitglied mit dem Recht an den Vorstandsverhandlungen teilzunehmen“ aufgenommen. Ab 1902 war die Mikrobiologin Lydia Rabinowitsch-Kempner (1871–1935) die 1. Vorsitzende. Sie übte dieses Amt bis 1930 aus, d. h. zumindest solange der Verein über nennenswerte Mittel verfügte. Er wurde am 26. März 1930 faktisch neu gegründet; das Vermögen betrug nur noch 3000 Mark. Lydia Rabinowitsch-Kempner war laut Protokoll 1934 noch „Ehrenmitglied“. Ihr Tod am 3. August 1935 verhinderte den Ausschluss auf Grund der rassistischen NS-Gesetze.

Während der Nazi-Herrschaft beging ihre Schwester Alice aufgrund der einsetzenden Verfolgung Selbstmord und ihre Brüder wurden in den Konzentrationslagern ermordet.

Das Land Berlin vergibt seit Juli 2010 an besonders qualifizierte Nachwuchskräfte Promotionsstipendien und Sonderzuwendungen als „Elsa-Neumann-Stipendium des Landes Berlin“.




</doc>
<doc id="296206" url="https://de.wikipedia.org/wiki?curid=296206" title="Datenerfassung">
Datenerfassung

Datenerfassung bezeichnet alle Methoden der zeitgleichen oder zeitfolgerichtigen Messungen und Zählungen, gegebenenfalls einschließlich Zeitstempel für messbare oder zählbare Daten und Gruppen von zusammenhängenden Daten.

Charakteristisch ist die unmittelbare und direkte Zugänglichkeit der Messgröße oder einer damit physikalisch eindeutig verbundenen Ersatzgröße. Das ist Gegenstand der Messtechnik in allen bekannten Ausprägungen.

Zur Erfassung gehört auch das Lesen (englisch: "reading") von Identmerkmalen mittels geeigneter Kennzeichen und Lesegeräte.

In abweichender bzw. allgemeinerer Bedeutung ist "Datenerfassung" ein „Arbeitsvorgang, mit dem anfallende Daten in eine maschinenlesbare Form gebracht und auf Datenträgern gespeichert werden“. Häufig versteht man darunter die Erfassungstätigkeit eines Benutzers am Computer. Siehe auch Eingabe (Computer).

Im Gegensatz zur Erfassung ist die Datenerhebung (englisch: "inquiry") eine mathematisch qualifizierte Ersatzmethode, welche lediglich eine nicht zugängliche Messgröße in einer für den intendierten Zweck hinreichenden Eindeutigkeit mit einer erfassten Hilfsgröße verknüpft und ohne direkte Zeitbindung berechnet. Dies ist Gegenstand der repräsentativen Stichprobe oder der retrospektiven Datenanalyse und den Methoden der deskriptiven Statistik in allen bekannten Ausprägungen.

Eine Datenerhebung erfolgt somit nie zeitgleich (online). Im allgemeinen (nicht technischen) Sprachgebrauch wird die Datenerhebung häufig mit "Datenerfassung" gleichgesetzt.

In der Messtechnik versteht man unter "Datenerfassung" (engl. "data acquisition", kurz DAQ) die Aufnahme analoger Signale mittels geeigneter Hardware (z. B. einer Datenerfassungskarte). Hierbei werden mit Hilfe eines Analog-digital-Umsetzers digitale Messdaten erzeugt, die dann per Software weiterverarbeitet werden können. Es wird zwischen Off-Line- und On-Line-Datenerfassung unterschieden.

Für die Leittechnik wurde der Begriff der Datenerfassung in DIN 19222 definiert:
Anmerkung: DIN 19222 ist ersetzt worden durch DIN EN 60050-351.

In Logistik und Handel wird unter "Datenerfassung" das Auslesen von Barcodes bzw. Transpondern mittels automatischer Systeme verstanden. Dabei sind zwei Fälle zu unterscheiden. Häufig wird nur eine Zeichenfolge gelesen, die als Identifikator für ein Objekt, etwa einen Artikel an einer Scannerkasse dient. Mit dem Aufkommen von EAN128, zweidimensionaler Barcodes und mit Daten frei beschreibbarer Transponder werden zunehmend Informationen transportiert. Dies können z. B. Mindesthaltbarkeitsdaten, Sender- und Empfängeradressen aber auch Produktions- und Sicherheitsinformationen sein. Vergleichbar der Schrift zur Informationsspeicherung und -übertragung zwischen Menschen ist entscheidend, dass die Informationen auf dem Datenträger (Barcode/Transponder) für ein späteres maschinelles Auslesen gespeichert werden.

Im Zuge der Industrie 4.0 nimmt die Datenerfassung sprunghaft zu, da immer mehr Materialflussobjekte (z. B. Einzelteile, Produkte, Pakete, Behälter, Transportmittel) durch eine maschinenlesbare Kennzeichnung identifiziert werden können (z. B. Barcode, QR-Code). Immer mehr Materialflussobjekte werden inzwischen durch elektronische Identifizierungsmedien (z. B. RFID) gekennzeichnet und können so elektronisch erkannt und erfasst werden. Mit Hilfe von elektronischen Ortungssystemen (z. B. GPS) und dem Internet können bei cyber-physikalische Objekte oftmals auch die örtliche Position bestimmt werden, wodurch die Verfolgung eines Objektes (Tracing)in der gesamten Lieferkette möglich wird.



</doc>
<doc id="366328" url="https://de.wikipedia.org/wiki?curid=366328" title="Thomsonsche Schwingungsgleichung">
Thomsonsche Schwingungsgleichung

Mit der Thomsonschen Schwingungsgleichung lässt sich die Resonanzfrequenz formula_1 eines Schwingkreises (Reihenschwingkreis und idealer Parallelschwingkreis) mit der Kapazität "C" und der Induktivität "L" berechnen. Sie wurde 1853 von dem britischen Physiker William Thomson erstmals formuliert und lautet:

Oder umgeformt für die Periodendauer (Schwingungszeit):

Im Resonanzfall ist der Resonanzwiderstand so groß wie der Serienwiderstand. Der kapazitive Widerstand des Kondensators und induktiver Widerstand der Spule innerhalb des Schwingkreises kompensieren sich auf null:

Betrachten wir den elektrischen Schwingkreis als ein geschlossenes System, so ist die Summe aller Energieformen in diesem System zu jeder Zeit "t" konstant.

Setzt man die entsprechenden Formeln ein, so kommt man auf folgende Differentialgleichung:

Aus

folgt:

Nun leitet man diese Gleichung nach der Zeit ab und erhält:

Um diese Gleichung zu lösen, müssen wir einen Zusammenhang zwischen formula_22 und formula_23 herstellen. Dazu verwenden wir eine Sinusfunktion als Lösungsansatz, da sie sich auf Grund ihrer Periodizität gut zur Beschreibung einer Schwingung eignet.

Durch Einsetzen ergibt sich:

Daraus folgt mit formula_34:

Die thomsonsche Schwingungsgleichung gilt nur für Serienschwingkreise und ideale Parallelschwingkreise. Bei komplexeren Topologien muss, ausgehend von formula_38, die Frequenz abgeleitet werden.

Des Weiteren muss bei der Anwendung der thomsonschen Schwingungsgleichung darauf geachtet werden, dass sich das jeweilige System im Schwingfall befindet – die Dämpfung durch den ohmschen Widerstand also nicht zu groß ist. Bei nicht zu großer Dämpfung kann die beim Parallelschwingkreis veränderte Resonanzfrequenz mit dem Verlustwiderstand "R" von "L" berechnet werden:



</doc>
<doc id="692723" url="https://de.wikipedia.org/wiki?curid=692723" title="Landauer-Prinzip">
Landauer-Prinzip

Das Landauer-Prinzip ist eine 1961 von Rolf Landauer formulierte Hypothese, die die Informationstheorie mit der Thermodynamik und der statistischen Physik verknüpft. Sie besagt, dass das Löschen eines Bits an Information zwangsläufig die Abgabe einer Energie von
in Form von Wärme an die Umgebung bedeutet.

Dabei ist

Landauers Thesen sind von erheblicher theoretischer Tragweite und bilden einen Schlüsselbaustein für eine Reihe weitergehender Theorien, z. B. bei Grundlagenarbeiten zu Quantencomputern.

Durch das Landauer-Prinzip besteht für irreversibel arbeitende Computer, wie es heute fast alle sind, eine theoretische Untergrenze der Verlustleistung; in der Praxis liegt diese bis auf Weiteres um Größenordnungen höher.

Unterschreiten lässt sich diese Grenze nur mit grundlegenden technischen Neuerungen wie Quantencomputern oder reversibel arbeitenden Computern nach Charles H. Bennett. Letztere sind unmittelbar vom Landauer-Prinzip abgeleitet. Um ein Löschen von Information zu vermeiden, laufen sie nach dem Ende einer Berechnung rückwärts wieder in den Anfangszustand zurück. Dazu müssen vom logischen Gatter bis zur Programmiersprache alle Elemente reversibel neu entwickelt werden.

Ebenfalls von Charles Bennett vorgeschlagen wurde die Interpretation des Maxwellschen Dämons mit dem Landauer-Prinzip. Aus der oben angegebenen Formel für den Energieverlust folgt unmittelbar für die beim Löschen eines Bits abgegebene Entropie:
Diese Entropie wird durch das Überschreiben, also das implizite Löschen des Gedächtnisses des Dämons für die Geschwindigkeit der anfliegenden Teilchen, freigesetzt. Die dadurch verursachte Entropieerhöhung hebt die durch seine Sortiertätigkeit verursachte Verringerung genau auf.

Positive Unterstützung finden Landauers Thesen in den theoretischen Arbeiten des amerikanischen Physikers Edwin Thompson Jaynes.

Kritik wurde von der Wissenschaftsphilosophin Orly Shenker geäußert, der zufolge Landauer unzulässig den thermodynamischen und den informationstheoretischen Entropiebegriff vermengte.

Auf theoretischer Ebene konnte gezeigt werden, dass durch Verschränkung und Quanteninformation eine Verletzung des Landauer-Prinzips möglich ist in Abhängigkeit von dem Wissen, das ein Beobachter schon über das System hat.

Eine erste experimentelle Bestätigung von Landauers Thesen wurde im März 2012 durch Physiker aus Augsburg, Lyon und Kaiserslautern präsentiert. In ihrem Experiment wurde eine Mikro-Glaskugel in einem durch fokussiertes Laserlicht erzeugten Doppelmuldenpotential betrachtet, wobei 1 Bit Information der Position in der einen Mulde, 0 Bit der Position der Kugel in der energetisch tieferen Mulde entsprach.

Auf Quantenebene konnte der Effekt von einer chinesischen Arbeitsgruppe an einem auf wenige Mikrokelvin abgekühlten Kalziumatom in einer Magnetfalle nachgewiesen werden.



</doc>
<doc id="716124" url="https://de.wikipedia.org/wiki?curid=716124" title="Kristian Birkeland">
Kristian Birkeland

Kristian Olaf Bernhard Birkeland (* 13. Dezember 1867 in Kristiania; † 15. Juni 1917 in Tokio) war ein norwegischer Physiker, der ab 1898 einen Lehrstuhl an der damaligen Königlichen Friedrichs-Universität Kristiania (seit 1939 Universität Oslo) innehatte. Er wurde sieben Mal für den Physik-Nobelpreis vorgeschlagen, ohne die begehrte Auszeichnung zu erhalten.

Kristian Birkeland war das zweite Kind von Reinert Birkeland (1838–1899) und dessen Ehefrau Ingeborg, geborene Ege (1842–1913). Nach der Schulausbildung in seiner Heimatstadt Kristiania (heute Oslo) begann er an der dortigen Universität ein Studium der Chemie, Mathematik und Physik. Zwischen 1893 und 1895 setzte er seine akademische Ausbildung in Deutschland, Frankreich (wo er bei Henri Poincaré studierte) und der Schweiz fort. Nach seiner Rückkehr nach Norwegen wurde er 1896 das bis dahin jüngste Mitglied der Norwegischen Akademie der Wissenschaften.
Birkeland beschäftigte sich ab der Mitte der 1890er Jahre intensiv mit dem Polarlicht. Er ging davon aus, dass Elektronen der Sonne das Gasgemisch der oberen Atmosphäre zum Leuchten anregen. Da die Existenz des Sonnenwindes zu dieser Zeit noch nicht bekannt war, wurde seine Theorie jedoch bezweifelt. Im Februar 1897 fuhr Birkeland erstmals in den hohen Norden Norwegens und kam beinahe in einem Schneesturm ums Leben. Bis 1899 richtete er ein Nordlicht-Observatorium auf dem Berg Haldde bei Alta ein. Im Winter 1902/1903 betrieb er schließlich ein Netz aus vier Stationen. Neben Haldde gab es Observatorien auf Island, Nowaja Semlja und der zu Spitzbergen gehörenden Insel Akseløya. Daneben führte er Laborexperimente an einem kugelförmigen Magneten als Modell der Erde, genannt Terrella, durch. Es gelang ihm, dem Polarlicht ähnliche Lichterscheinungen künstlich zu erzeugen.

Zusammen mit Sam Eyde entwickelte er 1903 das Birkeland-Eyde-Verfahren zur Herstellung von künstlichem Salpeter und Düngesalz. Dabei wird der Stickstoff aus der Luft mithilfe eines Lichtbogens oxidiert. Gemeinsam gründeten sie 1905 das Unternehmen Norsk Hydro, das die für das Verfahren erforderlichen hohen Elektrizitätsmengen aus Wasserkraft erzeugte. Die Beteiligung an der Firma machte Birkeland wohlhabend.

Ab 1910 ließen Birkelands wissenschaftliche Aktivitäten deutlich nach, möglicherweise infolge einer Quecksilbervergiftung, die er sich in Zusammenhang mit den Terrella-Experimenten zugezogen hatte. Forschungsreisen brachten ihn dennoch nach Jordanien, Japan, Indien und vor allem nach Ägypten, wo er zwischen 1914 und 1917 lebte und ein eigenes Observatorium betrieb.

Birkeland meldete 59 Patente an, darunter auch eine elektromagnetische Kanone, die bei einer öffentlichen Vorführung für potenzielle Kunden allerdings aufgrund eines Kurzschlusses spektakulär versagte (wenn auch das Projektil korrekt auftraf). Er wollte mit den Mitteln aus dem Verkauf seine Polarlicht-Expeditionen und Experimente finanzieren. 

Seine wissenschaftlichen Abhandlungen schrieb er überwiegend auf Französisch. Die Technische Hochschule Dresden ernannte ihn bereits 1908 zum Ehrendoktor.

Birkeland starb 1917, vermutlich durch eine Überdosis Veronal in Kombination mit Alkohol.

Er war seit 1905 mit Ida Charlotte Hammer verheiratet, die Ehe wurde aber 1911 geschieden und blieb kinderlos. Der Mathematiker Richard Birkeland war sein Cousin.

Birkeland ist auf den 200-Kronen-Noten der Norwegischen Bank abgebildet, die von 1994 bis 2017 im Umlauf waren. Der Asteroid (16674) Birkeland und ein Krater auf der Mondrückseite wurden nach ihm benannt.




</doc>
<doc id="861181" url="https://de.wikipedia.org/wiki?curid=861181" title="Freies Teilchen">
Freies Teilchen

Als freie Teilchen werden in der Physik Objekte (z. B. Ladungsträger) bezeichnet, die sich nicht in einem Potential befinden. Somit wirkt keine Kraft auf das Teilchen und man muss nur gegen seine Trägheit Arbeit leisten, um seine Geschwindigkeit zu ändern; die Geschwindigkeit bleibt konstant, wenn keine Arbeit aufgewendet wird. Da die Potentiale in der Regel unendlich weit reichen, handelt es sich in den meisten Fällen um Näherungen. In Fällen, wo diese Näherung nur notdürftig ausreicht, spricht man auch von "quasifreien" Teilchen.

Das Gegenteil eines freien Teilchens ist der gebundene Zustand.



</doc>
<doc id="940897" url="https://de.wikipedia.org/wiki?curid=940897" title="Young-Tableau">
Young-Tableau

Ein Young-Tableau oder Young-Diagramm (nach Alfred Young) ist ein grafisches Werkzeug der Darstellungstheorie der symmetrischen Gruppe formula_1. Jedes Young-Tableau wird dabei durch eine bestimmte Zahl von Zellen (meist symbolisiert durch Quadrate) bestimmt, die von oben nach unten und linksbündig so angeordnet sind, dass deren Anzahl in jeder neuen Zeile nicht zunimmt.

Beispiele für gültige Young-Tableaux:
Beispiele für "nicht" gültige Young-Tableaux:

Die "Partition" eines Young-Tableau ist die Aufzählung der Zahl der Zellen jeder Zeile und dient der kompakten Beschreibung seiner Struktur. In den gezeigten Beispielen ergeben sich folgenden Partitionen: a) formula_2 b) formula_3 c) formula_4 und d) formula_5. Die Ordnung formula_6 des Tableaux bezeichnet die Zahl aller Zellen. Die Anzahl gültiger Tableaux mit der Ordnung formula_6 kann durch die Partitionsfunktion formula_8 angegeben werden.

Die wichtigsten Zusammenhänge zwischen den irreduziblen Darstellungen der formula_1 und den Young-Tableaux der Ordnung formula_6 seien hier skizziert.

Ein "Young-Schema" ist ein Young-Tableau, dessen formula_6 Zellen mit den Zahlen von formula_12 bis formula_6 zunächst willkürlich besetzt sind. Beispiele für Young-Schemata:

Nun werden Operatoren aus diesen Schemata gebildet. Dabei bilden die "Zeilen" im Schema die Grundlage zur Bildung eines Operators formula_14. Pro Zeile werden aus allen Kombinationen der Zellenindizes Permutationen gebildet und summiert. Die so entstehenden Summen von Permutationen werden multipliziert. Ganz analog bilden die "Spalten" im Schema die Grundlage zur Bildung eines Operators formula_15. Pro Spalte werden aus allen Kombinationen der Spaltenindizes Permutationen gebildet und summiert. Bei der Summation wird aber ein negatives Vorzeichen verwendet, wenn die Permutation ungerade ist. Die so entstehenden Summen von Permutationen werden multipliziert.

Beispiel:

Hier gilt (in der Zyklennotation)
und

Ein Standardschema ist ein Young-Schema, bei dem die Nummerierung der Zellen derart durchgeführt wird, dass in jeder Spalte von oben nach unten und in jeder Zeile von links nach rechts die Zahlen größer werden.

Beispiele für Standardschemata:

Für die Schemata lässt sich Folgendes zeigen


Damit sind die formula_24 die Projektoren der irreduziblen Darstellungen der formula_1.

Zwei Darstellungen von zwei (im Allgemeinen verschiedenen) symmetrischen Gruppen formula_1 und formula_35 kann man zu einer Darstellung der symmetrischen Gruppe formula_36 "verknüpfen", dem sogenannten "äußeren Tensorprodukt" dieser beiden Darstellungen. Die genaue Definition dieser Darstellung verläuft folgendermaßen:

Für je zwei Permutationen formula_37 und formula_38 definieren wir das "äußere Produkt" formula_39 als die Permutation der Menge formula_40, welche jedes formula_41 auf formula_42 abbildet und jedes formula_43 auf formula_44 abbildet. Anschaulich gesprochen ist also formula_45 die Permutation, die auf den ersten formula_6 Zahlen wie formula_47 wirkt und auf den letzten formula_48 Zahlen wie (eine um formula_6 verschobene Permutation) formula_50 wirkt.

Wir können die Gruppe formula_51 als Untergruppe von formula_36 ansehen (vermöge der Einbettung formula_53).

Für jede Darstellung formula_54 von formula_1 und jede Darstellung formula_56 von formula_35 definieren wir nun das "äußere Tensorprodukt" von formula_54 und formula_56 als die Darstellung formula_60 (hierbei ist formula_61 auf kanonische Weise eine Darstellung der Gruppe formula_51: die Gruppe formula_1 wirkt auf dem ersten Tensoranden, während die Gruppe formula_35 auf dem zweiten Tensoranden wirkt).

Das "äußere Produkt" der formula_1 verknüpft Permutationen der formula_66, die auf die Indizes formula_12 bis formula_25 wirken, mit Permutationen der formula_69, die auf Indizes formula_70 bis formula_71 wirken und zusammen Permutationen der formula_72 beschreiben. Dabei stellt sich die Frage, in welche irreduziblen Darstellungen der formula_72 das äußere Produkt einer irreduziblen Darstellung von formula_66 und formula_69 zerfällt. Im Folgenden wird das äußere Produkt mit dem Symbol formula_76 dargestellt.

Als Beispiel wählen wir formula_77. Sei formula_54 die triviale Darstellung von formula_79 (also der eindimensionale Vektorraum, auf dem jedes Element von formula_79 als Identität wirkt) und sei formula_56 die alternierende Darstellung (auch Signum-Darstellung oder Signatur-Darstellung genannt) von formula_79 (also der eindimensionale Vektorraum, auf dem jede gerade Permutation als Identität und jede ungerade Permutation als Punktspiegelung am Ursprung wirkt). Dann ist formula_61 eine eindimensionale Darstellung der Gruppe formula_84, und das äußere Produkt von formula_54 und formula_56 ist eine sechsdimensionale Darstellung formula_87 von formula_88.

Nun stellt sich die Frage, wie das äußere Tensorprodukt zweier irreduzibler Darstellungen in irreduzible Darstellungen zerlegt werden kann (dieses Tensorprodukt ist selber nur selten irreduzibel, aber nach dem Satz von Maschke zerfällt es in eine direkte Summe irreduzibler Darstellungen). Da die irreduziblen Darstellungen von formula_89 (bis auf Isomorphie) eindeutig den Young-Tableaux der Ordnung formula_18 entsprechen, können wir also folgende Frage stellen:

Seien formula_91 und formula_92 zwei Young-Tableaux der Ordnungen formula_6 bzw. formula_94. Seien formula_54 und formula_56 die irreduziblen Darstellungen von formula_1 bzw. formula_35, die zu diesen Young-Tableaux gehören. Das äußere Produkt formula_60 von formula_54 und formula_56 ist dann eine Darstellung von formula_36, und somit eine direkte Summe irreduzibler Darstellungen von formula_36. Diese irreduziblen Darstellungen entsprechen wiederum Young-Tableaux der Ordnung formula_104. Welche Young-Tableaux sind diese? Wir schreiben kurz

formula_105

um zu sagen, dass formula_106 die Young-Tableaux zu den irreduziblen Darstellungen von formula_36 sind, in welche das äußere Produkt von formula_54 und formula_56 zerfällt. Dabei kann unter den Young-Tableaux formula_106 auch ein und das gleiche Tableau mehrfach vorkommen - nämlich dann, wenn in der Zerlegung des äußeren Produktes von formula_54 und formula_56 eine irreduzible Darstellung mehrfach vorkommt. Manchmal fasst man in diesem Fall diese gleichen Tableaux zusammen (statt formula_113 schreibt man also formula_114, falls formula_115 ist). Dadurch wird aus der Summe formula_116 eine Summe paarweise verschiedener Young-Tableaux mit Koeffizienten - diese Koeffizienten nennt man "Littlewood-Richardson-Koeffizienten".

Die Frage ist nun, wie man anhand von formula_91 und formula_92 die Young-Tableaux formula_106 bestimmt. Es gibt unterschiedliche Antworten auf diese Frage; sie werden allgemein als "Littlewood-Richardson-Regeln" (nach Dudley Littlewood und A. R. Richardson) bezeichnet. Wir geben im Folgenden eine solche Regel, die rekursiv ist (es gibt auch explizite Regeln, die allerdings eine langwierige kombinatorische Formulierung haben).

Zuerst ein Beispiel: Seien formula_91 und formula_92 die Young-Tableaux

Die zu formula_91 bzw. formula_92 gehörenden irreduziblen Darstellungen formula_54 und formula_56 sind dann die triviale Darstellung von formula_79 (als formula_54) und die alternierende Darstellung von formula_79 (als formula_56). Wir sind also in dem Beispiel weiter oben, wo wir festgestellt haben, dass das äußere Produkt von formula_54 und formula_56 eine formula_132-dimensionale Darstellung von formula_88 ist. Man kann feststellen (z. B. mit Charaktertheorie), dass diese Darstellung sich als direkte Summe formula_134 schreiben lässt, wobei formula_135 die irreduzible Darstellung von formula_88 zum Young-Tableau

ist, und formula_137 die irreduzible Darstellung von formula_88 zum Young-Tableau

ist. Wir können also schreiben:

wobei wir P (X) Q für formula_139 schreiben.

Seien nun die Young-Tableaux formula_91 und formula_92 gegeben. Wir wollen die Summanden formula_106 in der Zerlegung formula_105 bestimmen (im obigen Beispiel konnte man dies noch recht leicht per Hand erledigen, vor allem mit Charaktertheorie, aber für größere Tableaux wird dies schnell sehr mühsam).

Die sogenannte "Pieri-Regel" erledigt dies im Sonderfall, wenn das Tableau formula_92 nur aus einer Zeile besteht: In diesem Fall ist formula_145 die Summe aller Youngtableaus, die aus dem Youngtableau formula_91 durch Anfügen von insgesamt formula_94 neuer Zellen entstehen (wobei formula_94 die Ordnung von formula_92 ist), und zwar höchstens einer neuen Zelle pro Spalte.

Beispiel (der Stern dient nur als Orientierung bei der Zuordnung der Zellen):

Eine Kombination wie

kommt in der Entwicklung "nicht" vor, weil in ihr die erste Spalte "zwei" hinzugefügte Zellen [*] enthält.

Zur Bildung des äußeren Produkts formula_150 zwischen beliebigen Tableaux zerlegt man zunächst eines der beiden Tableaux in eine alternierende Summe von äußeren Produkten von einzeiligen Tableaux nach folgender Vorschrift: Haben wir ein Tableau der Form formula_151 vor uns, dann berechnen wir das äußere Produkt formula_152. Wir bekommen eine Summe von Tableaux, darunter unser Ausgangstableau formula_151, aber auch einige weitere Tableaux. Diese weiteren Tableaux werden nun abgezogen:

formula_154.

Auf die so entstandene Summe wird die Prozedur rekursiv angewandt. Diese Rekursion kommt immer zu einem Ende, weil mit jedem Schritt Tableaux entstehen, die in der letzten Zeile mindestens eine Zelle weniger haben.

Beispiel (der Stern dient nur als Orientierung bei der Zuordnung der Zellen):

Nach dieser Zerlegung kann man unter Ausnutzung der Assoziativität des äußeren Produktes und mithilfe der Pieri-Regel die eigentliche Multiplikation durchführen. Eine Anwendung des äußeren Produkts findet man bei der Zerlegung der Tensordarstellung eines Vielteilchensystems.

Das äußere Tensorprodukt zweier Darstellungen formula_54 und formula_56 zweier symmetrischer Gruppen formula_1 und formula_35 ist nicht zu verwechseln mit dem "inneren Tensorprodukt" zweier Darstellungen formula_54 und formula_56 einer und der gleichen symmetrischen Gruppe formula_89. Letzteres ist (wie gesagt) nur für zwei Darstellungen "der gleichen" symmetrischen Gruppe definiert, und auch dann unterscheidet es sich vom äußeren Tensorprodukt (es ist eine Darstellung von formula_89, während das äußere Tensorprodukt eine Darstellung von formula_163 ist). Die Zerlegung dieses inneren Tensorproduktes in irreduzible Darstellungen ist noch um einiges schwieriger als die des äußeren Tensorproduktes. Statt der Littlewood-Richardson-Koeffizienten kommen hier sogenannte "Kronecker-Koeffizienten" ins Spiel.

Der Einsatz von Young-Tableaux ist vielfältig. Sie dienen unter anderem


Darüber hinaus wird zum Beispiel in der Elementarteilchenphysik mit der Technik der Young-Tableaux eine Dekomposition der Tensordarstellung von Mehrteilchensystemen ermöglicht. Unter anderem wurden sie benutzt, um die Quark-Struktur von Hadronen aufzuklären. Quarks wurden anfangs nicht durch Hochenergiestreuexperimente direkt beobachtet, sondern mussten zunächst aus der Systematik der als Darstellungen der zugrundeliegenden Gruppe realisierten zusammengesetzten Teilchen erschlossen werden.




</doc>
<doc id="1384510" url="https://de.wikipedia.org/wiki?curid=1384510" title="D-Meson">
D-Meson

Als D-Mesonen bezeichnet man die Mesonen, welche genau ein Charm-Quark als schwerstes Quark enthalten. Ihre Masse liegt im Bereich von etwa 1,9 GeV/c² und sie leben im Mittel einige 10 Sekunden lang.

D-Mesonen sind Testteilchen für die Erforschung der Quark-Umwandlung über die schwache Wechselwirkung.

Ähnlich wie Kaonen und B-Mesonen mischen auch neutrale D-Mesonen, sie können also in ihre eigenen Antiteilchen übergehen. Dies wird unter anderem untersucht, um Hinweise auf eine CP-Verletzung sowie auf Physik außerhalb des Standardmodells zu finden.




</doc>
<doc id="1477267" url="https://de.wikipedia.org/wiki?curid=1477267" title="Marangoni-Zahl">
Marangoni-Zahl

Die Marangoni-Zahl formula_1 oder formula_2 (benannt zu Ehren des italienischen Physikers Carlo Marangoni) ist eine dimensionslose Kennzahl aus dem Bereich der Strömungsmechanik. Sie ist ein Maß für die Stärke der kapillaren Konvektion an Grenzflächen (Marangoni-Konvektion).

Die Marangoni-Konvektion ist eine Strömung an Grenzflächen, die durch lokale Unterschiede der Grenzflächenspannung formula_3 verursacht wird. Da die Grenzflächenspannung der meisten Stoffe bei zunehmender Temperatur formula_4 abnimmt, entsteht eine Strömung von warmen zu kalten Bereichen der Grenzfläche. In diesem Fall der thermokapillaren Konvektion, die durch Temperaturdifferenzen formula_5 bedingt sind, lässt sich die Maragoni-Zahl definieren als:

Dabei bezeichnet

Analog können die lokale Unterschiede in der Grenzflächenspannung auch durch Konzentrationsunterschiede gelöster Stoffe (z. B. Detergentien) oder der Ladungsdichte entstehen und durch eine entsprechende Definition der Marangoni-Zahl ausgedrückt werden.


</doc>
<doc id="1513834" url="https://de.wikipedia.org/wiki?curid=1513834" title="Frequenzauflösung">
Frequenzauflösung

Die Frequenzauflösung bezeichnet den geringsten Frequenzabstand zweier Töne (oder sinusförmiger Vorgänge), die noch unterschieden werden können. Sie bezeichnet die Anwendung des physikalischen Begriffs Auflösung auf die Dimension der Frequenz. Der Begriff wird vor allem in der Akustik (insbesondere Psychoakustik) und der Signalverarbeitung verwendet.

Die Auflösung wird zum einen durch die Quantisierung der Frequenz vorgegeben. So kann das Frequenzspektrum mit Hilfe einer Filterbank bestimmt worden sein (z. B. Terzspektrum). Dann ist die resultierende Frequenzauflösung im ganzen Spektralbereich jeweils eine Terz.

Zum anderen bestimmt die Analysezeit die Auflösung. Die Frequenzauflösung kann nicht wesentlich besser sein als der Kehrwert der Analysezeit. Dies ist eine Eigenschaft der diskreten Fourier-Transformation, mit der meist Frequenzspektren bestimmt werden. Die Auflösung kann demgegenüber verbessert werden, wenn zusätzliche Kenntnisse vorliegen oder Annahmen getroffen werden können. Dann können hochauflösende Methoden (z. B. Maximum-Likelihood-Methode oder Maximum-Entropie-Methode) die Auflösung erhöhen. Fensterfunktionen zur Verringerung der Nebenzipfel im Spektrum verringern die Auflösung. 

Die Fähigkeit des Menschen oder von Tieren zur Unterscheidung zweier Töne mit geringfügig unterschiedlichen Frequenzen ist sowohl individuell als auch über den Hörbereich unterschiedlich ausgeprägt: der Zusammenhang zwischen der physikalischen Frequenz eines Tones und seiner physiologisch empfundenen Tonheit ist nicht linear. Diese Gesetzmäßigkeit ist in der Bark-Skala dargestellt.

Die Ausprägung dieser Fähigkeit zur Frequenzunterscheidung ist entscheidend für den Grad der akustischen Orientierungsfähigkeit, beispielsweise hinsichtlich des Richtungshörens oder beim Cocktail-Party-Effekt, der die Spracherkennung bei lauten Hintergrund- und Störgeräuschen beschreibt.



</doc>
<doc id="1567148" url="https://de.wikipedia.org/wiki?curid=1567148" title="Stille elektrische Entladung">
Stille elektrische Entladung

Die stille elektrische Entladung (auch dielektrische Barriereentladung, DBE, , "DBD") oder Plasmaentladung ist eine Wechselspannungs-Gasentladung, bei der mindestens eine der Elektroden vom Gasraum durch galvanische Trennung mittels eines Dielektrikums elektrisch isoliert ist.

Ein gas- oder luftgefüllter Raum zwischen isolierend umhüllten Elektroden kann ionisiert werden beziehungsweise gelangt in einen Plasmazustand (Niedertemperaturplasma ähnlich einer Glimmentladung), wenn eine Wechselspannung an den Elektroden im Gasraum ausreichende Feldstärken erzeugt. Durch Verschiebungsströme wird die Entladung auch durch die Isolation hindurch aufrechterhalten und es kann kontinuierlich elektrische Leistung in das Plasma übertragen werden. Man kann sich entsprechende Anordnungen als Kondensator mit inhomogenem Dielektrikum vorstellen, weshalb man auch von kapazitiver Anregung oder (etwas unzutreffend) elektrodenloser Anregung spricht. Die Feldstärke ist umgekehrt proportional zu den Dielektrizitätskonstanten und daher im Gas stets höher als im Dielektrikum. Dennoch wird die Oberfläche des Dielektrikums durch Ionenbombardement und Ultraviolettstrahlung belastet, was je nach Anwendung entweder ausgenutzt oder vermieden werden muss.

DBE haben folgende Eigenschaften:

Zur Erzeugung einer homogenen Entladung ist eine gepulste Anregung vorteilhaft. Die DBE wird dabei mit uni- oder bipolaren Pulsen mit Pulsdauern von wenigen Mikrosekunden bis hinunter zu einigen zehn Nanosekunden und Amplituden im einstelligen Kilovoltbereich beaufschlagt. Das Puls-Pausenverhältnis ist in der Regel sehr klein und liegt unterhalb von zehn Prozent.

Die hohe elektrische Wechselspannung (einige Kilovolt) hoher Frequenz (etwa 10 bis 1000 kHz) oder die Hochfrequenzpulse können mit hoher Effizienz mit Resonanzwandlern erzeugt werden.

Vorteile je nach Anwendung:

Ein Vorteil ist auch das Arbeiten an normaler Luftatmosphäre.

Der Frequenzbereich ist zwar nach oben nicht begrenzt, effektive elektrische Anregungsschaltungen arbeiten mit Halbleitern bis zu wenigen 100 kHz und bei Generatoren mit Elektronenröhren bei 10…100 MHz. Ähnlich arbeiten jedoch auch Verfahren mit Magnetrons im ISM-Band 2,4…2,5 GHz. Es ist jedoch zu beachten, dass das bei DBE mögliche Nichtgleichgewichtsplasma vornehmlich nur durch Pulsanregung erreicht wird. Im Gegensatz zu kontinuierlicher Anregung, beispielsweise mit einem Sinus- oder Rechtecksignal, weist der Pulsbetrieb ein kleines Puls-zu-Pause Verhältnis (duty cycle) auf. Nach erfolgter Anregung und Herbeiführung des Plasmazustandes können die im Gas gebildeten Ladungsträger während der Pause wieder abgebaut werden und ein wirkungsgrad-schädigendes Thermalisieren des Plasmas wird verhindert.

Die Elektrodenkonfigurationen einer stillen Entladung können je nach Anwendung stark variieren:



Einsatz in der Gaschromatographie als "Barrier Ionisation Discharge Detektor" (BID) mit kalter Plasmaentladung.
Dieser Detektor nutzt die energiereichen Photonen des Heliumplasmas zur Ionisierung der Probemoleküle. Da praktisch alle Substanzen (außer Neon und Helium selbst) ein geringeres Ionisierungspotential haben, ist dieser Detektor als universell zu bezeichnen.
Die japanische Firma Shimadzu hat das Prinzip der Barrier Ionisations Plasmaentladung weiterentwickelt und sich diese Technik seit 2013 durch zahlreiche Patente exklusiv gesichert.



Erzeugung von Licht und ultravioletter optischer Strahlung:
Anregung von Gaslasern.

Betriebsgeräte für dielektrisch behinderte Gasentladungslampen sind in der Regel Leistungs-Hochfrequenzgeneratoren, die einen Transformator zur Ausgangsspannungserhöhung enthalten. Die einfachsten Betriebsgeräte generieren eine nieder- oder hochfrequente kontinuierliche Sinusspannung. Pro Sinus-Halbwelle kommt es in der Regel zu mehreren Lampenzündungen. Eine Alternative dazu stellen Generatoren mit Rechteck-Ausgangsspannungssignal dar. Die Lampenzündungen erfolgen hier zu Zeiten hoher Spannungsanstiegsgeschwindigkeiten und somit außerhalb der Plateauspannungen. Die zur Anwendung kommenden leistungselektronischen Topologien basieren – wie auch bei Betriebsgeräten für andere Gasentladungslampen – auf halb- oder vollbrückengetriebenen Resonanzkreisen. Die Lampenkapazität wird sinnvollerweise als kapazitiver Teil des Resonanzkreises genutzt. Bei kontinuierlich arbeitenden Betriebsgeräten muss somit die in der Kapazität der Lampe gespeicherte Energie nicht zurückgewonnen werden. Sie verbleibt im Resonanzkreis und lediglich die durch die Lampenzündungen konsumierte Wirkleistung muss in den Resonanzkreis eingekoppelt werden. Der für die Lampeneffizienz vorteilhafte gepulste Betrieb von DBE-Lampen ist nachteilig für die Effizienz des Pulsbetriebsgerätes, da die Lampe in der Regel einen sehr geringen Leistungsfaktor (typisch 10 %) aufweist und somit 90 % der zur Erreichung der Zündspannung notwendigen Energie wieder aus dem Resonanzkreis entnommen werden muss. Typische Topologien sind der Sperrwandler (Flyback converter) und resonante Halbrückenschaltungen (half bridge). Eine flexible Topologie, die diese beiden Schaltungsansätze vereint, wird in und vorgestellt und kann für DBE-Lampen mit variabler Kapazität genutzt werden. Eine Übersicht über mögliche Topologien und Ansteuerkonzepte für den gepulsten DBE-Betrieb liefert.

Eine recht neue Methode zur Erzeugung von Barriereentladungen ist die direkte Nutzung eines piezoelektrischen Transformators (PT) als aktive dielektrische Komponente.




</doc>
<doc id="1579814" url="https://de.wikipedia.org/wiki?curid=1579814" title="COMSOL Multiphysics">
COMSOL Multiphysics

COMSOL Multiphysics, vormals FEMLAB, ist eine Software zur Simulation physikalischer Vorgänge, die mittels Differentialgleichungen beschrieben werden können.

Vertrieben wird das Programm heute durch direkte Niederlassungen (zum Beispiel COMSOL Multiphysics GmbH) und durch ein weltweites Netz von Distributoren (z. B. Humusoft (Prag), Addlink (Madrid), Pitotech (Taipeh)…).

Im November 2005 fand die erste FEMLAB-Konferenz (heute: COMSOL-Konferenz) statt, die sich zum Ziel gesetzt hat, Interessenten aus Forschung, Lehre und Entwicklung eine gemeinsame Plattform zu bieten.

Das Programm basiert auf der sogenannten Finite-Elemente-Methode (FEM) und wird in Forschung, Lehre und Entwicklung eingesetzt. Neben einfachen FEM-Berechnungen ist eine Kopplung von verschiedenen physikalischen Problemen (Multiphysik) auf einfache Art und Weise möglich. Dabei ist ein iteratives Hin- und Herschalten zwischen verschiedenen Anwendungen nicht nötig, gekoppelte Gleichungssysteme können simultan gelöst werden.

Die Simulationssoftware ist modular aufgebaut. Neben dem Basispaket „COMSOL Multiphysics“ werden folgende optionale Pakete angeboten:

Bereich Elektromagnetik
Bereich Mechanik und Akustik

Strömungsmechanik und Wärmetransport

Bereich Chemie und Verfahrenstechnik
Multifunktional
Programmschnittstellen

Die vorherigen Versionen von COMSOL Multiphysics wurden FEMLAB (FEM-laboratory) genannt. FEMLAB entstand aus einer Toolbox für Matlab, der Partial Differential Equation (PDE) Toolbox. Die Eingabe eigener partieller Differentialgleichungen ist Bestandteil des Basispakets.

COMSOL Multiphysics bietet eine bidirektionale Schnittstelle zu MATLAB und Simulink, welche eine Skriptsteuerung von COMSOL Multiphysics über die MATLAB-Skriptsprache ermöglicht (Erweiterung um ca. 650 neue Funktionen).

Weitere Schnittstellen:




</doc>
<doc id="1606929" url="https://de.wikipedia.org/wiki?curid=1606929" title="Fluoreszenzlebensdauer">
Fluoreszenzlebensdauer

Die Fluoreszenzlebensdauer, früher auch Fluoreszenzabklingzeit, gibt die mittlere Zeit an, die ein Molekül in einem angeregten Zustand bleibt, bevor es ein Photon emittiert und damit in den Grundzustand zurückkehrt. Sie liegt in der Größenordnung von 10 - 10 s. Der Zerfall der Fluoreszenz folgt dabei einem exponentiellen Gesetz:

Hierbei ist formula_2 die Fluoreszenzintensität unmittelbar nach einem Anregungsblitz (z. B. ein Laserimpuls), formula_3 die Zeit und formula_4 die Fluoreszenzlebensdauer. Für diese gilt

Das bedeutet, es existieren strahlende Prozesse, die mit der Rate formula_6 zerfallen und nicht-strahlende Prozesse, die mit der Rate formula_7 zerfallen. Bei stark fluoreszierenden Stoffen, wie Fluoreszenzfarbstoffen, ist formula_7 verschwindend gering. Bei nicht-fluoreszierenden Stoffen (also den meisten Dingen unserer Umgebung) ist hingegen formula_6 viel kleiner als formula_7.

Typische Fluoreszenzlebensdauern liegen im Bereich von wenigen Nanosekunden. Dabei ist zu beachten, dass es sich hier um einen spinerlaubten Vorgang (Fluoreszenz) handelt. Beim spinverbotenen Vorgang (Phosphoreszenz) ergeben sich um Größenordnungen längere Lebenszeiten im Bereich von Millisekunden bis Stunden.

Die Fluoreszenzlebensdauer ist in der Spektroskopie und Mikroskopie (Fluoreszenzlebensdauer-Mikroskopie) ein wichtiger Messparameter, der zur Unterscheidung verschiedener (auch gleichfarbiger) Fluorophore dient. Darüber liefert die Fluoreszenzlebensdauer wichtige Informationen über die chemische Umgebung eines Fluorophors und kann Energietransfermechanismen, wie den Förster-Resonanzenergietransfer, aufdecken.

Zum Beispiel wird in einer Zelle die Fluoreszenzlebensdauer durch die nähere Umgebung des Fluorophors beeinflusst, d. h. die Fluorophore können als Messsonden der Umgebung dienen.

Die Ermittlung von Fluoreszenzlebensdauern erfordert die zeitaufgelöste Aufzeichnung der Intensität emittierter Strahlung. Ein gängiges Verfahren dafür ist die zeitkorrelierte Einzelphotonenzählung (TCSPC). Dabei erfolgt die Anregung der Probe periodisch mit monochromatischen Lichtblitzen geringer Intensität (Laser, Nanosekunden-Blitzlampe). Die Detektion der Fluoreszenz erfolgt bei einer größeren als der zur Anregung verwendeten Wellenlänge (Monochromator auf Emissionsseite der Versuchsanordnung) mit einem Sekundärelektronenvervielfacher (Photomultiplier, PMT), der in der Lage ist, einzelne Photonen zu registrieren. 

Wird das Licht der Anregungslichtquelle so stark abgeschwächt, dass nur noch nach ein bis fünf Prozent der Lichtblitze ein Signal registriert wird, so kann davon ausgegangen werden, dass es sich um die Registrierung einzelner Photonen handelt. Mit einer elektronischen Schaltung werden Zeitmessungen durchgeführt, die von einem zusätzlichen Detektor (Photodiode) direkt an der Lichtquelle gestartet und vom Signal des Fluoreszenzdetektors gestoppt werden. Durch die Diskretisierung des Zeitsignals erhält man nach Durchlaufen vieler Anregungs-/Messzyklen ein (von der Auflösung der eingesetzten Analog-Digital-Wandler abhängiges) Histogramm, dessen Einhüllende dem Signal einer analogen Aufzeichnung des zeitaufgelösten Intensitätsverlaufs der Fluoreszenz nach einem einzelnen Anregungsimpuls hoher Leistung entspricht.

Aus dem dabei erhaltenen Histogramm (siehe Abbildung) kann auf graphischem Wege oder durch Regressionsanalyse die Fluoreszenzlebensdauer formula_4 ermittelt werden.

Eine andere Methode ist die Messung im Frequenzbereich (Phasenfluorometrie). Hierbei wird die Probe mit einem intensitätsmodulierten Licht formula_12 der Frequenz formula_13 bestrahlt. Detektiert wird das emittierte Fluoreszenzlicht formula_14, das mit der gleichen Frequenz moduliert ist. Allerdings wird die Modulationsamplitude verringert und es tritt eine Phasenverschiebung auf. Das System wird wie folgt beschrieben (lineare Antwort):

formula_16 ist die Suszeptibilität, die sich aus einem Dispersionsterm und einem Absorptionsterm zusammensetzt. Wird nun als Zeitantwort auf eine Delta-Störung eine Debye-Relaxation angenommen:

dann folgt für den Frequenzbereich (Fouriertransformation):

Für Phase und Demodulation formula_19 ergibt sich dann:

Dabei sind formula_22 und formula_23 die Abklingzeiten für die Phase bzw. die Demodulation. Im Falle nur eines Fluorophors sind diese gleich und frequenzunabhängig.



</doc>
<doc id="2706308" url="https://de.wikipedia.org/wiki?curid=2706308" title="Molekulare Strömung">
Molekulare Strömung

Molekulare Strömung herrscht im Gegensatz zur viskosen Strömung, wenn die mittlere freie Weglänge der Gasteilchen deutlich größer ist als der Durchmesser der Strömung, d. h. wenn die Knudsen-Zahl deutlich größer als 1 ist:

Molekulare Strömung ist im Hoch- und Ultrahochvakuumbereich vorherrschend.

Anschaulich bedeutet dies, dass aufgrund des Mangels an Teilchen in einer Vakuumkammer annähernd keine Wechselwirkungen mehr "untereinander" stattfinden, sondern nur noch Stöße mit den begrenzenden Wänden. Bei einfachen Sorptionspumpen wird diese Eigenschaft genutzt, um mittels hochporöser Oberflächen Teilchen zu binden. Mit diesem passiven System kann bereits ein schwaches Hochvakuum erzeugt werden. Ionengetterpumpen basieren auf dem gleichen Prinzip, forcieren die Physisorption von freien Teilchen aber durch Ionisation und elektrische Felder. Hiermit ist auch Ultrahochvakuum erreichbar.

Für Abschätzungen in der Raumfahrt wird gemäß ECSS-E-10-04A mit einer freien Molekularströmung gerechnet, wenn die Knudsen-Zahl größer ist als 3. Für kleine Satelliten trifft dies ab einer Orbithöhe von 150 km zu, für große Objekte (ISS) ab 250 km.


</doc>
<doc id="3167745" url="https://de.wikipedia.org/wiki?curid=3167745" title="Innere Umwandlung">
Innere Umwandlung

Innere Umwandlung (engl. "internal conversion", „IC“) ist ein Begriff aus der Photochemie und der Spektroskopie. Er bezeichnet den isoenergetischen und daher strahlungslosen Übergang vom Schwingungsgrundzustand eines elektronisch angeregten Zustands (z. B. S) in einen hoch angeregten Schwingungszustand des nächst tieferliegenen elektronischen Zustands (z. B. S = elektronischer Grundzustand), ohne Änderung der Multiplizität (keine Spinumkehr). Wenn sich die Multiplizität ändert, handelt es sich dagegen um ein "Intersystem Crossing".

Innere Umwandlungen treten mit der größten Wahrscheinlichkeit in der Nähe von Schnittpunkten zweier Potentialkurven auf. Gehört die zweite Potentialkurve zu einem ungebundenen Kontinuumszustand, so tritt Dissoziation ein. Andernfalls erfolgt der Übergang in einen vibronisch angeregten Zustand, der anschließend, unter Energieabgabe durch Stöße mit umgebenden Teilchen, strahlungslos innerhalb von 10 Sekunden in den Schwingungsgrundzustand des jeweiligen elektronischen Zustands übergeht (Schwingungsrelaxation). Ist das Ausgangsniveau ein höher angeregter elektronischer Zustand (z. B. S), finden so lange hintereinander Innere Umwandlungen mit anschließender Schwingungsrelaxation statt, bis der tiefste elektronisch angeregte Zustand (S) erreicht ist. Da große Moleküle mehr Vibrationsmoden als kleine haben, ist bei ihnen die Wahrscheinlichkeit für einen solchen Übergang i. A. höher. Eine Übersicht über die möglichen Übergänge gibt das nebenstehende Jabłoński-Schema.

Das Melanin der Haut und die DNA nutzen die innere Umwandlung, um die für den Organismus schädliche UV-Strahlung in unschädliche Wärme umzuwandeln. Diese ultraschnelle innere Umwandlung ist verantwortlich für den exzellenten UV-Schutz dieser großen natürlichen Moleküle.



</doc>
<doc id="3411990" url="https://de.wikipedia.org/wiki?curid=3411990" title="Zacharias Nordmark">
Zacharias Nordmark

Zacharias Nordmark (auch "Sakarias"; * 29. Oktober 1751 in Luleå; † 26. Juni 1828 in Uppsala) war ein Physiker und Astronom. 

Er studierte an der Universität Uppsala und beendete dort auch 1776 seine Doktorarbeit in Astronomie, bevor er ebenfalls in Uppsala als Adjunkt für Astronomie arbeitete. 

1783 wurde er Professor für Experimentalphysik an der Universität Greifswald und 1787 in Uppsala. Im Jahr 1790 sowie von 1799 bis 1807 war er Rektor der Universität Uppsala. Er traf Bernouilli und Lagrange und untersuchte theoretisch das Gleichgewicht schwimmender Körper. In den Wasserkünsten der Bergwerke von Falun führte er hydraulische Experimente aus.



</doc>
<doc id="3903497" url="https://de.wikipedia.org/wiki?curid=3903497" title="NA11">
NA11

NA11 war ein Fixed-Target-Experiment am H6-Strahl des Super Proton Synchrotron am CERN. Das Ziel war die Messung verschiedener Wirkungsquerschnitte in hadronischen Wechselwirkungen zwischen Teilchen mit von Null verschiedener Charmquantenzahl. Das Experiment wurde am 3. Februar 1978 vorgeschlagen und im Dezember 1982 abgeschlossen. Es war das erste Experiment, bei dem im Planarverfahren gefertigte Halbleiterdetektoren verwendet wurden.



</doc>
<doc id="4202657" url="https://de.wikipedia.org/wiki?curid=4202657" title="Siemens Argonaut Reaktor">
Siemens Argonaut Reaktor

Der Siemens Argonaut Reaktor (SAR) war ein Forschungsreaktor, der von 1959 bis 1968 in Garching bei München durch die Technische Universität München betrieben wurde.

Im Gegensatz zum benachbarten Forschungsreaktor München (FRM), der als Komplettanlage von einer amerikanischen Firma geliefert worden war, wurde der SAR als erster Kernreaktor in Deutschland allein von einheimischen Wissenschaftlern und Technikern gebaut. Als Grundlage dienten Konstruktionspläne aus dem Argonne National Laboratory, wo der erste Reaktor des anspielungsvoll Argonaut genannten Typs
im Februar 1957 in Betrieb gegangen war. Am 23. Juni 1959 erreichte der Reaktor seine erste Kritikalität.

Der Reaktor wurde nach neun Jahren und sieben Monaten Betrieb am 31. Oktober 1968 abgeschaltet. Er wurde daraufhin stillgelegt und schließlich am 8. Januar 1992 aus dem Geltungsbereich des deutschen Atomgesetzes entlassen. Bis zum 20. März 1998 wurde die Reaktoranlage zur grünen Wiese vollständig zurückgebaut.

Ab 1962 wurde in Graz ein Forschungsreaktor mit gleichem Namen und ähnlichem Aufbau errichtet, der von 1965 bis 2005 in Betrieb war.

Der Siemens Argonaut Reaktor wurde mit leichtem Wasser moderiert und hatte eine thermische Leistung von einem Kilowatt. Der maximale thermische Neutronenfluss betrug 2,4 × 10 n/cm s. Die Neutronen wurden dabei durch Graphit-Reflektoren gebündelt.



</doc>
<doc id="4440437" url="https://de.wikipedia.org/wiki?curid=4440437" title="Pierre Bergé (Physiker)">
Pierre Bergé (Physiker)

Pierre Bergé (* 16. September 1934 in Pau; † 4. September 1997 in Paris) war ein französischer Experimentalphysiker, der die Chaos-Physik erforschte.

Bergé studierte Physik an der École Centrale in Nantes. Danach war er während seiner gesamten Karriere (ab 1957) am Kernforschungszentrum in Saclay des Commissariat à l'énergie atomique. Von 1979 bis 1990 war er dort Leiter der Abteilung "Kondensierte Materie" und von 1991 bis 1994 des umbenannten "Départments für kondensierte Materie, Atome und Moleküle". Er war auch in nationalen Programmen zur Verbesserung der Physikdidaktik engagiert.

Bergé beschäftigte sich mit chaotischen dynamischen Systemen und Turbulenz, unter anderem im Rayleigh-Benard-Experiment, wo er mit anderen Forschern verschiedene Intermittenz-Übergange zum Chaos entdeckte.

1974 erhielt er den "Prix Louis Ancel" und 1990 den Gentner-Kastler-Preis, beide von der der Société française de physique vergeben. 1995 erhielt er den von der CEA gestifteten "Prix science et innovation"




</doc>
<doc id="5449535" url="https://de.wikipedia.org/wiki?curid=5449535" title="Reinhard Stock">
Reinhard Stock

Reinhard Stock (* 1938) ist ein deutscher Experimentalphysiker, der sich mit Schwerionenphysik beschäftigt.

Stock studierte an der Ruprecht-Karls-Universität Heidelberg, wo er bei Rudolf Bock über Schwerionenreaktionen promovierte. Danach war er als Post-Doc an der University of Pennsylvania, wo er sich mit Biophysik beschäftigte. Er war 1985 bis zu seiner Emeritierung 2004 Professor am Institut für Kernphysik der Johann Wolfgang Goethe-Universität Frankfurt am Main, wo er zeitweise Leiter des Instituts war. Er ist seit 2007 Senior Fellow am Frankfurt Institute for Advanced Studies (FIAS).

Stock leistete mit Hans Gutbrod und Rudolf Bock Pionierarbeit beim Aufbau relativistischer Schwerionenexperimente in Berkeley (Bevalac, mit Arthur Poskanzer), fortgesetzt ab Mitte der 1980er Jahre am CERN mit höheren Energien. Er war dort langjähriger Sprecher des NA49 Experiments. In den 1990er Jahren gelang ihnen damit bei Kollisionen von Bleikernen der Nachweis eines Quark-Gluon-Plasmas, wie das CERN 1999 bekanntgab.

1999 bis 2004 war er Vorsitzender des Wissenschaftlichen Rats des GSI Helmholtzzentrum für Schwerionenforschung.

1988 erhielt er den Robert-Wichard-Pohl-Preis mit Gutbrod. 1989 erhielt er den Leibniz-Preis. 2008 erhielt er mit Walter Greiner den Lise-Meitner-Preis für "Pionierarbeiten zur Etablierung der relativistischen Schwerionenphysik", insbesondere für die Experimentelle Bestätigung der von Greiner und Mitarbeitern vorhergesagten und untersuchten Stoßwellen und des Flusses hochkomprimierter Kern- und Quarkmaterie.




</doc>
<doc id="5491114" url="https://de.wikipedia.org/wiki?curid=5491114" title="Pythagoreischer Becher">
Pythagoreischer Becher

Der Pythagoreische Becher (auch als Becher der Gerechtigkeit bekannt) ist ein Trinkgefäß, welches seinen Benutzer dazu zwingt, nur die übliche Menge einzuschenken. Dank seiner Pythagoras von Samos zugeschriebenen Konstruktion erlaubt der Becher seinem Benutzer, ihn bis zu einer bestimmten Höhe zu füllen. Wenn der Benutzer den Becher nur bis zu dieser Höhe befüllt, kann er sein Getränk in Ruhe genießen. Schüttet er noch mehr ein, dann läuft der gesamte Inhalt des Bechers unten aus. Mit diesem Becher, heißt es, wollte Pythagoras gierige Menschen Bescheidenheit lehren.

Ein pythagoreischer Becher sieht aus wie ein normales Trinkgefäß, außer dass das Gefäß eine Mittelsäule besitzt. Diese sitzt direkt über dem Schaft und über dem Loch am Fuße des Schaftes. Ein kleines offenes Röhrchen läuft von diesem Loch fast bis in die Spitze der Mittelsäule, wo sich eine offene Kammer befindet. Diese Kammer ist verbunden mit einem zweiten Röhrchen, das zum Boden der Mittelsäule führt, wo eine Öffnung in der Säule den Inhalt des Bechers einlässt.

Wenn der Becher gefüllt wird, steigt die Flüssigkeit durch das zweite Röhrchen hoch bis in die Kammer in der Spitze der Mittelsäule, gemäß Pascals Prinzip der kommunizierenden Röhren. So lange die Flüssigkeit nicht höher als das Niveau dieser Kammer steigt, funktioniert der Becher normal. Sobald man noch mehr eingießt, ergießt sich der gesamte Inhalt des Bechers durch das erste Röhrchen und läuft unten nach dem Saugheberprinzip aus.

Der Legende nach wurde Pythagoras' Erfindung bei den Bauarbeiten an der Wasserversorgung der Insel Samos eingesetzt, die er geplant hatte und deren Ausführung er betreute. Er sorgte sich um den großen Weinkonsum der Arbeiter. Auf der Insel Samos wird der Becher als "Díkea Koúpa" (Δίκαια Κούπα του Πυθαγόρα) bezeichnet und auch als Andenken verkauft. 

Heron von Alexandria (60 v. Chr.) beschreibt in seiner Abhandlung Pneumatika den Becher als Tantalos-Becher, in Anlehnung an die Figur der griechischen Mythologie. Tantalos wurde von den Göttern bestraft und musste ewigen Durst inmitten von greifbar nahem Wasser (und ewigen Hunger) erleiden.

Heutzutage werden Versionen des Bechers in Spielzeuggeschäften als Scherzartikel verkauft.



</doc>
<doc id="5533215" url="https://de.wikipedia.org/wiki?curid=5533215" title="Eric Becklin">
Eric Becklin

Eric E. Becklin (* 6. April 1940) ist ein US-amerikanischer Astrophysiker.

Becklin wurde am California Institute of Technology zum Ph.D. in Physik promoviert. Er war seit 1989 Professor für Physik und Astronomie an der University of California, Los Angeles (UCLA). Becklin ist Chefwissenschaftler und "Director Designate" des Stratospheric Observatory for Infrared Astronomy (SOFIA). Nach Gerry Neugebauer und ihm ist das Becklin-Neugebauer-Objekt (Becklin und Neugebauer 1968) benannt.

1975 wurde Becklin mit dem Newton-Lacy-Pierce-Preis für Astronomie ausgezeichnet. 2009 wurde er in die American Academy of Arts and Sciences gewählt. Für 2017 wurde ihm die Henry Norris Russell Lectureship zugesprochen.




</doc>
<doc id="5663664" url="https://de.wikipedia.org/wiki?curid=5663664" title="Blasiussche Formeln">
Blasiussche Formeln

Die 1. und 2. Blasiussche Formel geben den dynamischen Auftrieb und das Drehmoment an, die ein langgestreckter Körper (z. B. Tragflügel) in einem strömenden Medium erfährt, wenn bestimmte Voraussetzungen an den Strömungstyp erfüllt sind. Die Formeln sind benannt nach dem Ingenieur und Hochschullehrer Heinrich Blasius, der von 1912 bis zu seinem Tod im Jahre 1970 an der Hochschule für Angewandte Wissenschaften Hamburg (damals Ingenieurschule Hamburg) lehrte.

Die Kräfte und Drehmomente an einem Körper in einem strömenden Medium sind im Allgemeinen kompliziert zusammengesetzt und lassen sich nur mit Hilfe von Computern berechnen. 
Unter Bedingungen, die bei Flugzeugen näherungsweise realisiert sein können, gibt es jedoch analytische Formeln, die noch aus der Pionierzeit (um 1900) stammen. Für den dynamischen Auftrieb sind das die Kutta-Joukowski-Formel und die 1. Blasiussche Formel, für das Nickmoment ist es die 2. Blasiussche Formel. Die 1. Blasiussche Formel gestattet eine exakte Herleitung der in Physik-Lehrbüchern häufiger benutzten Kutta-Joukowski-Formel.

Physikalische Grundlage der Formeln ist die Druckverteilung in einem strömenden Medium nach der Bernoulli-Gleichung. Die Druckkräfte wirken auf die Oberflächenelemente des Körpers und werden aufsummiert, wodurch sich Integrale über die Profilkurve des Körpers ergeben. Nur in Spezialfällen, etwa beim Joukowski-Profil, sind diese Integrale leicht auszuwerten. 

Die Voraussetzungen für die Anwendbarkeit der Blasiusschen Formeln sind folgende: Der umströmte Körper muss (wie ein Tragflügel) langgestreckt sein und ein einheitliches Profil haben; genaugenommen muss es sich um einen verallgemeinerten Zylinder mit dem Profil als Grundfläche und unendlicher Ausdehnung in der dritten Dimension handeln. Die Formeln geben deswegen Kräfte und Drehmomente "pro Längeneinheit in der dritten Dimension" an. Der Körper muss genau in Querrichtung angeströmt werden, und die Strömung muss stationär, inkompressibel, reibungslos, wirbelfrei und im Wesentlichen zweidimensional (2D) sein. 
Die 2D-Voraussetzung bedeutet, dass in Richtung der dritten Dimension die Komponente der Geschwindigkeit und alle Variationen von Größen klein und vernachlässigbar sind.

Die Massendichte des strömenden Mediums sei formula_1 (konstant weil inkompressibel), 
die Profilkurve ("contour") des Körpers sei formula_2. Dann gilt für die reellen Komponenten formula_3 und formula_4 der Kraft pro Längeneinheit auf den Körper, ausgedrückt durch die reellen Komponenten formula_5 und formula_6 der lokalen Strömungsgeschwindigkeit:

1. Blasiussche Formel 

Für das Drehmoment um die Längsrichtung, wiederum pro Längeneinheit, gilt die 

2. Blasiussche Formel
Der Index 0 steht für den Koordinatenursprung als Bezugspunkt des Drehmoments. 

Die komplexe Schreibweise ist dem Problem besonders angemessen. 

Ein Punkt in der Ebene des Profils wird als komplexe Zahl
dargestellt; analog eine Geschwindigkeit und eine Kraft (jeweils am Punkt formula_10) 
durch 
In den Blasiusschen Formeln erscheint das komplex Konjugierte formula_12 der Geschwindigkeit und analog der Kraft. Unter den genannten Voraussetzungen ist formula_13 eine holomorphe Funktion (formula_14 selbst ist es nicht). 

Für die Kraft pro Längeneinheit gilt die 

1. Blasiussche Formel 
Dabei ist formula_16 die Massendichte des strömenden Mediums und formula_17 die Profilkurve des Körpers. 

Für das Drehmoment um die Längsrichtung, wiederum pro Längeneinheit, gilt die 

2. Blasiussche Formel
Der Index 0 steht für den Koordinatenursprung als Bezugspunkt des Drehmoments. 

Die Integrationskurve lässt sich wegen der Holomorphie von formula_13 im Bereich des Strömungsfeldes nach Belieben deformieren (Cauchyscher Integralsatz). 

Die Argumentation ist hier durchgehend komplex formuliert. Da die Reibung vernachlässigt wird, gibt es keine Scherkräfte. 
An einem Element formula_20 der Profilkurve erzeugt der Druck formula_21 eine Kraft senkrecht zu formula_20 und betragsmäßig gleich formula_23. Umfährt man das Profil entgegen dem Uhrzeigersinn, so erhält man die Richtung der Kraft in der komplexen Ebene durch Multiplikation mit der imaginären Einheit (entsprechend einer 90°-Drehung). Daher gilt an einem Linienelement:
Nach der Bernoulli-Gleichung ist der lokale Druck durch das Betragsquadrat der Geschwindigkeit gegeben. Entlang einer Stromlinie, die vom Unendlichen vor dem Körper bis ins Unendliche hinter dem Körper verläuft, gilt
Für die Physik des Fliegens ist der Fall interessant, dass das Medium in großem Abstand vom Körper mit konstanter Geschwindigkeit formula_26 strömt (aus dem Flugzeug betrachtet) und der (Luft-)Druck im Unendlichen ebenfalls konstant ist. Dann sind formula_26 und formula_28 für alle Stromlinien gleich, und es gilt für den lokalen Druck:
Aufsummieren aller an dem Profil angreifenden Druckkräfte ergibt somit zunächst
wobei berücksichtigt wurde, dass die Konstante keinen Beitrag leistet, weil die Summe aller formula_20 entlang einer geschlossenen Linie null ergibt. 

Nun muss die Geschwindigkeit auf der Profilkurve überall parallel zu formula_2 verlaufen, d. h. es haben formula_33 und formula_20 dasselbe 
Argument und es ist das folgende Produkt reell: 
Substituiert man im Integral dementsprechend und bildet man auf beiden Seiten das komplex Konjugierte, so erhält man die 1. Blasiussche Formel. 

Für das Drehmoment mit Bezugspunkt 0 gilt an einem Linienelement des Profils:
Für den Druck lässt sich wieder der Ausdruck aus der Bernoulli-Gleichung einsetzen und im Integral über die Profilkurve die Substitution wie oben ausführen. 
Auch hier verschwindet der Beitrag der Konstanten, weil er auf ein Integral über ein vollständiges Differential führt:
Indem man unter dem Realteil alle Faktoren komplex konjugiert, erhält man die 2. Blasiussche Formel.



</doc>
<doc id="6167484" url="https://de.wikipedia.org/wiki?curid=6167484" title="Gerhard Becherer">
Gerhard Becherer

Gerhard Becherer (* 15. Juni 1915 in Ahlsdorf; † 14. Juli 2003 in Rostock) war ein deutscher Experimentalphysiker. Er unterrichtete an der Universität Rostock.

1935 bestand Becherer das Abitur und begann an der Universität Halle Physik zu studieren. Das Studium wurde unterbrochen, damit er als Soldat am Zweiten Weltkrieg teilnehmen konnte. Dabei wurde er 1942 schwer verwundet.

Nach dem Krieg ging Becherer als Assistent an das Institut für Physik an der Universität Halle. Die Universität promovierte ihn später und habilitierte ihn, woraufhin er dort dozierte.

1958 wurde Becherer Professor mit Lehrstuhl für Experimentalphysik am Institut für Physik an der Universität Rostock. Sein Vorgänger war Paul Kunze. Ferner wurde er als Direktor des Instituts eingesetzt. An der Universität konzentrierte er sich bei der Experimentalphysik besonders auf Festkörperphysik, während sich sein Vorgänger auf Kernphysik spezialisierte. In den 1970er Jahren fand er eine Röntgenstrukturanalyse.

Becherer wurde 1960 als Dekan der Fakultät für Naturwissenschaft/Mathematik eingesetzt, an der er wirkte. Von 1962 bis 1964 fungierte er als Prodekan, 1968 gab er die Direktion ab. 1975 gründete er den "Arbeitskreis Nichtkristalline und Partiellkristalline Strukturen" und wurde als deren Leiter eingesetzt, ferner leitete er seither die "Vereinigung für Kristallographie". Beides übte er bis 1980 aus.

Die Rostocker Professur hielt Becherer bis 1980 inne, dann wurde er emeritiert. Der Wissenschaft kehrte er aber nicht den Rücken, sondern hielt noch Kontakte zu seinem Arbeitskreis. 2003 starb er in Rostock nach einer längeren Krankheit im Alter von 88 Jahren. Während seiner Zeit als Professor hatte er etwa 400 Studenten, wobei bei ihm 70 promovierten und 26 habilitierten.




</doc>
<doc id="6694933" url="https://de.wikipedia.org/wiki?curid=6694933" title="Twyman-Green-Interferometer">
Twyman-Green-Interferometer

Das Twyman-Green-Interferometer (benannt nach Frank Twyman und Alfred Green) ist ein Interferometer, das heißt ein optisches Messgerät, das zur Charakterisierung und Prüfung von optischen Komponenten, wie Linsen oder Prismen, eingesetzt wird.

Der Aufbau eines Twyman-Green-Interferometers entspricht im Wesentlichen dem des Michelson-Interferometers, bestehend aus einer Lichtquelle, einem Strahlteiler, zweier Spiegel und einem Schirm/Detektor. Es gibt jedoch zwei Unterschiede, diese liegen zum einen in der Nutzung einer monochromatischen Punktlichtquelle, zum anderen an Linsen am Interferometereingang und -ausgang.

Die Funktionsweise kann wie folgt zusammengefasst werden. Das Licht trifft von der Lichtquelle auf die Eingangslinse, die aus dem Licht der Punktquelle einen kollimierten Strahl (Planwelle) endlicher Ausdehnungen erzeugt. Anschließend trifft das kollimierte Licht auf einen Strahlteiler auf, wo es in zwei gleichartige Teilstrahlen aufgespaltet wird, die jeweils einen der beiden Spiegel treffen. Nach der Reflexion an den beiden Spiegeln trifft das Licht abermals auf den Strahlteiler und wird wieder vereinigt auf den Interferometerausgang gelenkt. Die Linse am Interferometerausgang sorgt abschließend für eine Fokussierung des Lichts auf einen Schirm oder Detektor. Wie bei allen Interferometern erfolgt bei der Vereinigung der beiden Teilstrahlen eine Überlagerung der Teilwellen und somit Interferenz in Abhängigkeit vom optischen Wegunterschied beider Teilstrahlen. Diese Unterschiede sind anschließend als charakteristisches Streifen- oder Ringmuster beobachtbar und erlauben Rückschlüsse auf die Oberflächenformen der Spiegel und der Probe.

Die Prüfung von optischen Bauteilen erfolgt durch die Platzierung des Bauteils in einem der Strahlwege. Dabei muss sichergestellt werden, dass beispielsweise bei der Prüfung eines Prismas das Licht senkrecht auf den zweiten Referenzspiegel trifft. Andernfalls gelangt nur ein Teil des Lichts oder gar kein Licht erneut zum Strahlteiler.
Bei der Prüfung von Linsen muss statt eines planen Referenzspiegels ein zur Linse passender, gewölbter Referenzspiegel eingesetzt werden.

Ein ähnlicher Aufbau wird auch beim Fizeau-Interferometer genutzt.

Ein leicht abgewandelter Aufbau nutzt statt eines planen Strahlteilers einen polarisierenden kubusförmigen Strahlteiler und 45°-linearpolarisiertes Licht. Durch diese Änderung im Aufbau sind die beiden Teilstrahlen nach der Strahlteilung unterschiedlich, das heißt horizontal und vertikal, linear polarisiert und werden am Ausgang des kubusförmigen Strahlteilers durch eine λ/4-Verzögerungsplatte in unterschiedliche drehendes, zirkular polarisiertes Licht umgewandelt. Das vom Referenzspiegel und der Probe zurückfallende Licht durchläuft dann nochmals die λ/4-Verzögerungsplatte und den Strahlteiler und gelangt als unterschiedlich linear polarisiertes Licht zum Detektor. Es ist anzumerken, dass diese beiden Teilstrahlen aufgrund ihrer unterschiedlichen Polarisation nicht miteinander interferieren und kein Licht wieder zur Lichtquelle gelangt. Ein interferierendes Signal kann durch eine nochmalige Linearpolarisation erreicht werden.

Twyman-Green-Interferometer sind im Bereich der optischen Prüfverfahren von großer Bedeutung. Es wird daher unter anderem in der Fertigung von optischen Bauteilen, wie Linsen, Prismen oder spiegelnder Oberflächen, zur Prüfung der optischen Eigenschaften, wie Homogenität ebener Oberflächen, eingesetzt.


</doc>
<doc id="6832488" url="https://de.wikipedia.org/wiki?curid=6832488" title="Helmholtz-Lagrangesche Invariante">
Helmholtz-Lagrangesche Invariante

Die Helmholtz-Lagrangesche Invariante (nach Helmholtz und Lagrange) stellt in der paraxialen Optik den Zusammenhang zwischen Abbildungsmaßstab und Winkelverhältnis dar:

mit

Bei einer optischen Abbildung ist demnach das Produkt aus Objekt/Bild-Größe, Brechungsindex und Strahlwinkel auf der Objekt- und der Bildseite gleich.

Eine ähnliche Grundaussage der paraxialen Optik ist die Abbesche Invariante.


</doc>
<doc id="7218778" url="https://de.wikipedia.org/wiki?curid=7218778" title="Journal of Statistical Mechanics: Theory and Experiment">
Journal of Statistical Mechanics: Theory and Experiment

Journal of Statistical Mechanics: Theory and Experiment (J. Stat. Mech. Theor. Exp.) ist eine begutachtete wissenschaftliche Fachzeitschrift mit inhaltlichem Schwerpunkt auf statistischer Physik. Die Zeitschrift wird vom Institute of Physics (IOP) zusammen mit der International School for Advanced Studies (SISSA) herausgegeben. Die Veröffentlichung erfolgt dabei ausschließlich online.

Wie schon dem Titel der Zeitschrift zu entnehmen ist, werden sowohl experimentelle wie auch theoretische Arbeiten im Bereich der statistischen Physik veröffentlicht.

Der Impact Factor beträgt 2,371 (Stand: 2018). Im Science Citation Index belegte das Journal mit diesem Impact Factor Rang 19 von 139 Zeitschriften in der Kategorie "Mechanik" und Platz 9 von 55 Journals in der Kategorie "mathematische Physik".



</doc>
<doc id="8252553" url="https://de.wikipedia.org/wiki?curid=8252553" title="Formelsammlung Tensoralgebra">
Formelsammlung Tensoralgebra

Diese Formelsammlung fasst Formeln und Definitionen der Tensoralgebra für Tensoren zweiter Stufe in der Kontinuumsmechanik zusammen.


Für Summen gilt dann z. B.

Dies gilt für die anderen Indexgruppen entsprechend.

Kreuzprodukt:

Die hier verwendeten Vektoren sind Spaltenvektoren

Drei Vektoren formula_36 können spaltenweise in einer 3×3-Matrix formula_37 arrangiert werden:

Die Determinante der Matrix

ist


Also gewährleistet formula_40, dass die Vektoren formula_36 eine rechtshändige Basis bilden.

Die Spaltenvektoren bilden eine Orthonormalbasis, wenn

worin formula_43 die transponierte Matrix ist. Bei der hier vorausgesetzten Rechtshändigkeit gilt dann zusätzlich formula_44

Basisvektoren formula_15

Duale Basisvektoren formula_13

Beziehungen zwischen den Basisvektoren

mit dem Spatprodukt

Trägt man die Basisvektoren spaltenweise in eine Matrix ein, dann finden sich die dualen Basisvektoren in den Zeilen der Inversen oder den Spalten der transponiert Inversen:

mit der transponiert inversen formula_52

In der Standardbasis formula_53 sind die Basisvektoren zu sich selbst dual:

Beziehung zwischen den Skalarprodukten der Basisvektoren:

Wechsel von

Basis formula_13 mit dualer Basis formula_15

nach

Basis formula_62 mit dualer Basis formula_12:

Matrizengleichung:

Abbildung formula_66

Dyade: formula_67

Abbildung formula_74

Abbildung formula_76 oder formula_77

Abbildung formula_79 oder formula_80

Abbildung formula_83

Abbildung formula_85

Abbildung formula_87

Abbildung formula_83

Abbildung formula_83

Durch die Eigenschaften des dyadischen Produktes wird formula_94 zu einem Vektorraum und entsprechend kann jeder Tensor komponentenweise bezüglich einer Basis dargestellt werden.

mit Komponenten formula_96

Meistens ist aber:
siehe auch #Axialer Tensor oder Kreuzproduktmatrix

Siehe auch #Vektorinvariante
Das Skalarkreuzprodukt mit dem #Einheitstensor vertauscht das dyadische Produkt durch das Kreuzprodukt:

Mit der Formel für das Produkt zweier #Permutationssymbole:

Eigenschaften:
Aber meistens:

Siehe auch #Hauptinvarianten, #Kofaktor eines Tensors.

Determinantenproduktsatz:

Multiplikation mit Skalaren formula_135:

Charakteristische Gleichung:

mit der Hauptinvariante formula_139. Spezialfall:

Zusammenhang mit dem Spatprodukt:

Zusammenhang mit dem äußeren Tensorprodukt:

Zusammenhang mit dem Kofaktor formula_144:

Spatprodukt und #Determinante eines Tensors:
Kreuzprodukt und #Kofaktor eines Tensors:

Die Komponenten formula_162 ergeben sich durch Vor- und Nachmultiplikation mit der Transformationsmatrix
die ein #Einheitstensor ist:

Definition für einen Tensor formula_17:

Zwei Tensoren formula_17 und formula_168 sind identisch wenn gilt:

Definition

Wenn λ die Eigenwerte des Tensors A sind, dann hat cof(A) die Eigenwerte λλ, λλ, λλ.


Betrag: formula_172

Eigenschaften:
Kreuzprodukt und Kofaktor:

Definition:


Betrag: formula_181

Eigenschaften:

Definition

Die Inverse ist nur definiert, wenn formula_188

Zusammenhang mit dem adjungierten Tensor formula_189:

Werden die Spalten von formula_17 mit Vektoren bezeichnet

dann gilt:

Satz von Cayley-Hamilton:

worin formula_195 die drei Hauptinvarianten sind.

Inverse des transponierten Tensors:

Inverse eines Tensorprodukts:

Spezialfälle:

mit Eigenwert formula_203 und Eigenvektor formula_204. Die Eigenvektoren werden auf die Länge eins normiert.

Jeder Tensor hat drei Eigenwerte und drei dazugehörige Eigenvektoren. Mindestens ein Eigenwert und Eigenvektor sind reell. Die beiden anderen Eigenwerte und -vektoren können reell oder komplex sein.

"Charakteristische Gleichung"

Die Koeffizienten sind die "#Hauptinvarianten":

Satz von Cayley-Hamilton:

Sei formula_210 "symmetrisch".

Symmetrische Tensoren haben reelle Eigenwerte und paarweise zueinander senkrechte oder orthogonalisierbare Eigenvektoren, die also eine Orthonormalbasis aufbauen. Die Eigenvektoren werden so nummeriert, dass die ein Rechtssystem bilden.

Hauptachsentransformation mit Eigenwerten formula_211 und Eigenvektoren formula_212 des symmetrischen Tensors formula_17:

bzw.

Sei formula_216 "schiefsymmetrisch".

Schiefsymmetrische Tensoren haben einen reellen und zwei konjugiert komplexe, rein imaginäre Eigenwerte. Der reelle Eigenwert von formula_17 ist null zu dem ein Eigenvektor gehört, der proportional zur reellen "#Vektorinvariante" formula_218 ist. Siehe auch #Axialer Tensor oder Kreuzproduktmatrix.

Sei formula_219 und formula_220 eine Basis und formula_221 die dazu duale Basis.

Der Tensor

hat die Eigenwerte

und Eigenvektoren

Der Tensor

hat die Eigenwerte

und Eigenvektoren

Definition
Invarianten:

Eigensystem:

mit formula_236

Allgemein:

Es gilt:

Vektortransformation

Tensorprodukt

Skalarprodukt

Invarianten:

Eigenwerte:
Jeder Vektor ist Eigenvektor.

Definition

Determinantenproduktsatz:

Definition
Invarianten (formula_247 ist der Drehwinkel):

"Eigentlich orthogonaler" Tensor formula_249, entspricht einer Drehung.

"Uneigentlich orthogonaler" Tensor formula_250, entspricht einer Drehspiegelung.

Spatprodukt:

Kreuzprodukt und #Kofaktor eines Tensors:

Gegeben ein Einheitsvektor formula_253 und Drehwinkel formula_247. Dann sind die folgenden Tensoren formula_255 orthogonal und drehen um die Achse formula_253 mit Winkel formula_247:

Drehung von Vektorraumbasis formula_259 mit Drehachse formula_253:

Gegeben Orthonormalbasis formula_262, Drehwinkel formula_247 und formula_264 sei die Drehachse:
formula_266: Drehung, formula_267: Drehspiegelung um formula_264

Wenn formula_262 ein Rechtssystem (Mathematik) bilden, dann dreht Q gegen den Uhrzeigersinn, sonst im Uhrzeigersinn um die Drehachse.

Eigensystem:

Drehwinkel:

Drehachse formula_253:

Definition

Notwendige Bedingungen für positive Definitheit:

Notwendige und hinreichende Bedingung für positive Definitheit: Alle Eigenwerte von formula_17 sind größer als null.

Immer positiv definit falls formula_188:

Definition
In kartesischen Koordinaten:
Invarianten:

Bilinearform:

Ein Eigenwert ist null, zwei imaginär konjugiert komplex, siehe #Axialer Tensor oder Kreuzproduktmatrix.

Dualer axialer Vektor:

mit #Vektorinvariante formula_218. Der zum Eigenwert null gehörende Eigenvektor ist proportional zum dualen axialen Vektor formula_293 denn

Kreuzproduktmatrix formula_297 eines Vektors formula_298:
Invarianten:
Eigensystem:

Eigenschaften:

Potenzen von formula_306

Definition

Alle Eigenwerte sind reell.

Alle Eigenvektoren sind reell und paarweise orthogonal zueinander oder orthogonalisierbar.

Symmetrische Tensoren haben keine #Vektorinvariante:

Definition

Mit den Eigenwerten formula_316, den Eigenvektoren formula_317 und einer reellwertigen Funktion formula_318 eines reellen Argumentes formula_135 definiert man über die Hauptachsentransformation

den Funktionswert des Tensors:

Insbesondere mit dem Deformationsgradienten formula_322:

Rechter Strecktensor

Linker Strecktensor

Henky-Dehnung

Die Tensoren

bilden eine Basis im Vektorraum formula_327 der symmetrischen Tensoren zweiter Stufe. Bezüglich dieser Basis können alle symmetrischen Tensoren zweiter Stufe in Voigt'scher Notation dargestellt werden:

Diese Vektoren dürfen addiert, subtrahiert und mit einem Skalar multipliziert werden. Beim Skalarprodukt muss
berücksichtigt werden.

Definition

Definition

Gegeben ein beliebiger Tensor formula_341

Symmetrische und schiefsymmetrische Tensoren sind orthogonal zueinander:

Deviatoren und Kugeltensoren sind orthogonal zueinander:

Gegeben sei die Gerade durch den Punkt formula_369 mit Richtungsvektor formula_370 und ein beliebiger anderer Punkt formula_371.

Dann ist
Der Punkt formula_373 ist die senkrechte Projektion von formula_371 auf die Gerade. Der Tensor G extrahiert den Anteil eines Vektors in Richtung von formula_370 und I-G den Anteil senkrecht dazu.

Gegeben sei die Ebene durch den Punkt formula_369 und zwei die Ebene aufspannende Vektoren formula_298 und formula_378 sowie ein beliebiger anderer Punkt formula_371. Dann verschwindet die Normale
nicht. Dann ist
Der Punkt formula_373 ist die senkrechte Projektion von formula_371 auf die Ebene. Der Tensor P extrahiert den Anteil eines Vektors in der Ebene und I-P den Anteil senkrecht dazu.

Die Projektion der Geraden, die durch die Punkte formula_369 und formula_371 verläuft, liegt in der Ebene in Richtung des Vektors formula_11.

Falls formula_387 und formula_388 folgt:

Eigenwerte

Falls formula_341:

Falls formula_398:

Falls formula_403:

Falls formula_405:

Falls formula_407:

Falls formula_409:

Für #Orthogonale Tensoren Q gilt:

Definition:

Kreuzprodukt von Vektoren:




Tensoren zweiter Stufe sind ebenfalls Elemente eines Vektorraums formula_94 wie im Abschnitt Tensoren als Elemente eines Vektorraumes dargestellt. Daher kann man Tensoren vierter Stufe definieren, indem man in dem Kapitel formal die Tensoren zweiter Stufe durch Tensoren vierter Stufe und die Vektoren durch Tensoren zweiter Stufe ersetzt, z. B.:

mit Komponenten formula_424 und die Tensoren formula_425 sowie formula_426 bilden eine Basis von formula_94.

Standardbasis in formula_94:

Tensortransformation:

Tensorprodukt:

Übliche Schreibweisen für Tensoren vierter Stufe:

Transposition:

Spezielle Transposition formula_435 vertauscht formula_436-tes mit formula_437-tem Basissystem.

Beispielsweise:

Definition: formula_441

Dann gilt: formula_442

Für beliebige Tensoren zweiter Stufe formula_17 gilt:

Diese fünf Tensoren sind sämtlich symmetrisch.

Mit beliebigen Tensoren zweiter Stufe formula_446 und formula_447 gilt:

In dem in diesen Formeln im Tensor vierter Stufe formula_168 durch formula_450 und die Transpositionen formula_451 durch formula_452 ersetzt werden, entstehen die Ergebnisse mit transponiertem formula_447:

Mit den Spannungen formula_456 und den Dehnungen formula_457 im Hooke'schen Gesetz gilt:

mit den Lamé-Konstanten formula_203 und formula_460. Dieser Elastizitätstensor ist symmetrisch.

Invertierungsformel mit formula_461, formula_462 und formula_463:

mit der Querdehnzahl formula_465 und dem Elastizitätsmodul formula_466.

Aus der Basis formula_467 des Vektorraums formula_468 der symmetrischen Tensoren zweiter Stufe kann eine Basis des Vektorraums formula_469 der linearen Abbildungen von symmetrischen Tensoren auf symmetrische Tensoren konstruiert werden. Die 36 Komponenten der Tensoren vierter Stufe aus formula_470 können als Voigt'scher Notation in eine 6×6-Matrix einsortiert werden:

Die Vektoren und Matrizen in Voigt'scher Notation können addiert, subtrahiert und mit einem Skalar multipliziert werden. Das Matrizenprodukt von Matrix und Vektor ist ebenfalls möglich. Beim Skalarprodukt muss
berücksichtigt werden.



</doc>
<doc id="8379009" url="https://de.wikipedia.org/wiki?curid=8379009" title="Poincaré-Kugel">
Poincaré-Kugel

Die Poincaré-Kugel ist ein Werkzeug zur Darstellung der Polarisationszustände elektromagnetischer Wellen wie zum Beispiel Licht. Jeder Polarisationszustand entspricht einem Punkt auf der Kugel, wobei sich vollständig polarisierte Zustände auf der Oberfläche, teilpolarisierte Zustände innerhalb der Kugel und der unpolarisierte Zustand im Zentrum befinden. Lineare Polarisationen befinden sich am Äquator der Kugel, zirkulare Polarisationen an den Polen und elliptische Polarisationen dazwischen. Orthogonale Polarisationen befinden sich auf der Kugeloberfläche einander gegenüber.
Die Poincaré-Kugel ist benannt nach dem Physiker Henri Poincaré.

Polarisationszustände elektromagnetischer Wellen lassen sich unter anderem mit Hilfe normierter Stokes-Parameter beschreiben, wo zur vollständigen Festlegung die drei Parameter "S", "S" und "S" benötigt werden. Diese Parameter spannen einen dreidimensionalen Vektorraum auf, in dem sich die Polarisationszustände als Punkte befinden. Die Stokes-Vektoren aller physikalisch möglichen Polarisationen befinden sich dann innerhalb einer Einheitskugel, der Poincaré-Kugel.

Auf den Schnittpunkten der Koordinatenachsen mit der Kugeloberflächen befinden sich folgende Polarisationen:

Den Punkten der Kugeloberfläche entsprechen die reinen Polarisationszustände, dem Innern die gemischten Polarisationen. Nach innen nimmt die Reinheit kontinuierlich ab bis zum unpolarisierten Zustand im Mittelpunkt.

Die Poincaré-Kugel eignet sich zur intuitiven Veranschaulichung von Polarisationszuständen und insbesondere von Transformationen zwischen verschiedenen Polarisationen. Ähnliche Polarisationszustände sind stets benachbart und kontinuierliche Transformationen wie zum Beispiel beim Durchgang einer Welle durch ein doppelbrechendes Medium entsprechen kontinuierlichen Verschiebungen auf der Kugel. Lineare Transformationen der Polarisation entsprechen einer Drehung auf der Poincaré-Kugel um eine feste Drehachse. Beispiele:

Die Poincaré-Kugel ist mathematisch äquivalent zur Bloch-Kugel, die den Hilbert-Raum der möglichen Zustände eines quantenmechanischen Zweizustandssystems (Qubit) veranschaulicht. Die Orientierung mit der die beiden Kugeln aufeinander abgebildet werden können, hängt hierbei davon ab, welche beiden orthogonalen (gegenüberliegenden) Polarisationen als Basiszustände ausgewählt werden. Diese Abbildung entspricht der Verwendung der Polarisation eines Photons als Qubit.

Wird die Polarisation anstelle von Stokes-Parametern mathematisch äquivalent mit Jones-Vektoren beschrieben, so entspricht der Jones-Vektor gerade dem Bloch-Vektor sofern als Basiszustände auf der Poincaré-Kugel die Polarisationen H und V ausgewählt werden.

Überträgt man Punkte der Poincaré-Kugel auf die Riemannschen Zahlenkugel, dann erhält man eine komplexe Zahl, die die Amplituden- und Phasenbeziehung von horizontaler und vertikaler Schwingung beschreibt.


</doc>
<doc id="8539536" url="https://de.wikipedia.org/wiki?curid=8539536" title="Marion Asche">
Marion Asche

Marion Asche (* 7. Januar 1935 in Berlin; † 11. Dezember 2013 ebenda) war eine deutsche Physikerin und Professorin für Festkörperphysik. Sie widmete sich insbesondere der Forschung auf dem Gebiet der Halbleiterphysik und erbrachte hier Pionierleistungen.

Marion Asche wurde als Kind der Lisa Asche und ihres Ehemannes, des Gewerbeoberlehrers Werner Asche in Berlin geboren. Der Vater hatte eine Ausbildung als Grafiker und die Mutter als "Modegestalterin" an der Kunstgewerbeschule in Magdeburg erhalten. Bereits der Vater von Lisa Asche arbeitete zuvor an der Kunstgewerbeschule in Karlsruhe und später dann in Magdeburg als Lithograf. Dieser künstlerische familiäre Hintergrund war für Marion Asche und ihren Bruder "Peter R. Asche", der in Magdeburg Regelungstechnik bei Heinrich Wilhelmi studiert hat, von großem Einfluss auf ihre anhaltend enge Verbindung zur Kunst.

Im Jahre 1941 wurde sie in Berlin-Prenzlauer Berg eingeschult, aber bereits ein Jahr später siedelt die Mutter wegen der Bombenangriffe auf Berlin zusammen mit der Tochter und dem 1940 geborenen Sohn nach Lauterbach bei Putbus auf die Insel Rügen um, sodass sie den Schulbesuch in Putbus fortsetzte. 1945 erfolgte die Rückkehr in ihre Wohnung nach Ost-Berlin, jedoch war der Vater noch bis 1950 in sowjetischer Kriegsgefangenschaft. Er erhielt danach eine Anstellung als Dozent an der Fachschule für Werbung und Gestaltung in Ost-Berlin, die er bis zu seinem Eintritt ins Rentenalter innehatte.

Wegen ihrer guten Leistungen kam Marion Asche 1949 auf die Käthe-Kollwitz-Oberschule und legte dort 1953 das Abitur mit dem Prädikat „sehr gut“ ab.

Danach begann sie ein Studium der Physik an der Humboldt-Universität zu Berlin (HUB). Von 1957 bis 1959 führte Marion Asche im Institut für Festkörperforschung der Deutschen Akademie der Wissenschaften (DAW) Untersuchungen an Cadmiumsulfid (CdS)-Kristallen durch und erhielt mit einer Arbeit über dieses Thema von der HUB das Diplom als Physikerin mit dem Gesamtprädikat „ausgezeichnet“.

Sie begann im Herbst 1959 ihre Berufstätigkeit an der DAW als wissenschaftliche Assistentin im Physikalisch-Technischen Institut (Leiter: Robert Rompe), dem späteren Zentralinstitut für Elektronenphysik der Akademie der Wissenschaften der DDR. Hier traf sie auf einen Kreis profilierter Wissenschaftler wie Klaus Thiessen u. a., und sie übernahm zunächst die Aufgabe, den "Piezowiderstand" von verschiedenen Halbleitermaterialien wie p-Germanium u. a. zu erforschen. Dazu hat sie eine spezielle Anlage für die Durchführung ihrer Experimente entworfen und nach deren Fertigstellung die langandauernden Messungen durchgeführt sowie die ersten wissenschaftlichen Resultate bereits ab dem Jahre 1963 publiziert.

Gegen Ende 1963 kam als Gastwissenschaftler der promovierte "Oleg Sarbey" aus Kiew in das Physikalisch-Technische-Institut. Seit dieser Zeit hat Marion Asche bis zum Ende ihrer wissenschaftlichen Tätigkeit sehr eng mit den Mitarbeitern der Ukrainischen Akademie der Wissenschaften zusammengearbeitet. Im Jahre 1964 verbrachte Marion Asche ein halbes Jahr in Kiew, wo sie die angefangenen Experimente und theoretischen Überlegungen mit den dortigen Kollegen fortsetzen konnte. Die Zusammenarbeit erwies sich als sehr erfolgreich und Ende 1965 promovierte sie mit einer Dissertation über „Heiße Elektronen in Silizium“ an der HUB mit „Auszeichnung“.

In der Folgezeit wurden die experimentellen und theoretischen Untersuchungen von Halbleitern in starken elektrischen Feldern erweitert. Die publizierten Ergebnisse fanden internationale Anerkennung und bildeten die Grundlage für ihre Habilitationsschrift. 1970 erhielt Marion Asche den akademischen Grad "Doktor sc. nat." an der HUB.

Im Laufe von zehn Jahren untersuchte Marion Asche in Kooperation mit den ukrainischen Kollegen die Transporterscheinungen in starken elektrischen und magnetischen Feldern. Diese Untersuchungen führten zu einer Reihe bisher nicht beobachteter Erscheinungen, und einige Resultate wurden in Lehrbücher aufgenommen, z. B. vom Lehrstuhl Seeger an der Universität Wien.

Die Einbeziehung von tiefen Temperaturen in die Messungen führte Marion Asche im Jahre 1984 zu der Beobachtung einer spontanen Symmetriebrechung in der Verteilung von Elektronen in "Vieltal-Halbleitern", die früher von den Kiewer Theoretikern vorausgesagt war und die jetzt als Entdeckung Nummer 294 vom Staatskomitee für Erfindungs- und Entdeckungswesen der Sowjetunion im Jahre 1986 anerkannt und registriert wurde (mit der Priorität von 1984). Es war hier der erste Fall einer physikalischen Entdeckung mit Beteiligung von deutschen Wissenschaftlern zu diesem Zeitpunkt.

Marion Asche wurde daher im Jahre 1987 für „hervorragende wissenschaftliche Leistungen auf dem Gebiet der Halbleiterphysik“ durch den Präsidenten der Akademie der Wissenschaften der DDR (AdW) Werner Scheler zur Professorin ernannt.

Sie erweiterte den Kreis ihrer Interessen in der Festkörperphysik, und in Kooperation mit den Kiewer Kollegen nahm sie teil an Untersuchungen von ballistischen Phononen, nichtlinearen optischen Prozessen, Abkühlung von Elektron-Hole-Plasma in Halbleitern und anderen zeitgemäßen Themen der Festkörperphysik in den 1980er und 1990er Jahren. In der letzten Zeit ihrer wissenschaftlichen Tätigkeit war ihr Interesse den Untersuchungen zweidimensionaler Systeme gewidmet, wo sie auch bedeutende Resultate erzielte.

Marion Asche hat etwa 85 Veröffentlichungen in internationalen Fachzeitschriften publiziert, davon zwei Review-Artikel, die von vielen Spezialisten zitiert wurden. Sie hat an drei Monographien mitgearbeitet, die in Verlag „Naukowa Dumka“ in Kiew und beim Springer-Verlag in Heidelberg erschienen sind. Auch für das ABC der Physik im Brockhaus Verlag und für das ukrainische enzyklopädische Wörterbuch schrieb sie Fachartikel. Bemerkenswert sind auch ihre Beiträge zur Geschichte der Physikalischen Gesellschaft zu Berlin (PGzB) anlässlich der Feiern zum 150-jährigen Bestehen der Berliner und damit der Deutschen Physikalischen Gesellschaft im Jahre 1995.

Sie war auch Mitautor von drei Patenten. Zahlreiche Diplom- und Promotionsarbeiten wurden von ihr betreut.

Sie bekam viele Einladungen für Vorträge auf internationalen Konferenzen und in renommierten Institutionen in der Bundesrepublik Deutschland und im Ausland. Auf Grund ihrer herausragenden Leistungen fand sie internationale Anerkennung und wurde bereits vor der Deutschen Wiedervereinigung zu Vorträgen nach Frankreich, Italien, Österreich, Dänemark, in die Sowjetunion und in die Bundesrepublik Deutschland eingeladen.

Marion Asche arbeitete nicht nur mit ukrainischen Physikern eng zusammen. Obwohl sie bis 1989 nur sporadisch Diskussionen mit Physikern aus der westlichen Welt führen konnte, hat sie nach der Grenzöffnung die Zusammenarbeit mit englischen und westdeutschen Wissenschaftlern intensiv betrieben, und dies brachte zugleich eine Reihe von gemeinsamen wissenschaftlichen Veröffentlichungen hervor. Diese entstanden insbesondere aus der langjährigen Kooperation mit den Lehrstühlen von Frederick Koch an der TU München und von "Eckehard Schöll" an der TU Berlin.

Charakteristisch für Marion Asche als Wissenschaftlerin war die allseitige Kompetenz in den von ihr betriebenen Untersuchungen – vom Schleifen der Proben, was sie häufig selbst gemacht hat, bis zur theoretischen Bearbeitung der Resultate sowie dem Niederschreiben der Forschungsergebnisse und Fachartikel.

Von 1971 bis 1979 leitete sie eine Arbeitsgruppe für die Untersuchung der heißen Elektronen bei der AdW und ab Dezember 1990 bis 1992 war sie Leiterin der Abteilung für Halbleitertransport im Zentralinstitut für Elektronenphysik der AdW. Sie hat sehr aktiv auch bei der Gründung des Paul-Drude-Instituts für Festkörperelektronik (PDI) (Leiter: Klaus Ploog) der Leibniz-Gemeinschaft im Zuge der Wiedervereinigung mitgewirkt, hier war sie bis zu ihrem Eintritt in den Ruhestand im Jahre 2000 tätig.


Als am Ende der 1980er Jahre in der Sowjetunion (SU) die „Perestrojka“ aufkam und sie darüber durch ihre Kenntnisse der Umstände in der SU besser informiert war, hat sie es für wichtig betrachtet, die Mitarbeiter des Instituts darüber möglichst ausführlich zu informieren.

Das Preisgeld, das sie im Jahre 1986 für die zuvor bereits erwähnte Auszeichnung für ihre Entdeckung in der SU erhielt, spendete sie für die Opfer der Tschernobyl-Atomkatastrophe in der Ukraine.

Marion Asche heiratete im Jahre 2005 den Kiewer Halbleiter-Physiker und Professor "Oleg Sarbey", mit dem sie bereits seit 1965 verbunden war.




</doc>
<doc id="8782701" url="https://de.wikipedia.org/wiki?curid=8782701" title="Größenordnung (Äquivalentdosis)">
Größenordnung (Äquivalentdosis)

Dies ist eine Zusammenstellung von Äquivalentdosen ionisierender Strahlung zu Vergleichszwecken. Die Angaben sind oft als „typische Werte“ zu verstehen; die umgerechneten Werte sind gerundet.

Wenn kein Zeitraum angegeben ist, handelt es sich um "kurzzeitige" Bestrahlungen, also Zufuhr der genannten Dosis innerhalb höchstens einiger Stunden. Bei Verteilung der gleichen Dosis über längere Zeit setzen biologische Heilungsvorgänge schon während der Bestrahlung ein, so dass die Auswirkungen geringer sind.

Grundeinheit der Äquivalentdosis "H" im internationalen Einheitensystem ist das Sievert (Einheitenzeichen Sv). Die Äquivalentdosis ist nicht mit der Energiedosis "D" (Einheit: Gray, Gy), der Organdosis, der Äquivalentdosisleistung (Äquivalentdosis pro Zeit) oder der Aktivität "A" (Einheit: Becquerel, Bq) zu verwechseln.











</doc>
<doc id="8805988" url="https://de.wikipedia.org/wiki?curid=8805988" title="Zelladhäsionstest">
Zelladhäsionstest

Ein Zelladhäsionstest (synonym "Zellbindungstest") ist eine biochemische und biophysikalische Methode zur Bestimmung der Zelladhäsion.

Gewebszellen bilden untereinander Zellkontakte aus, die unter anderem den Zusammenhalt des jeweiligen Gewebes und eine Zell-Zell-Kommunikation bewirken. Dagegen liegen Suspensionszellen (z. B. Blutzellen wie PBMC) meistens einzeln vor. Durch geeignete Messverfahren können Zellkontakte bestimmt werden. Mit einem Rasterkraftmikroskop können einzelne Zellpaare auseinandergezogen werden und die dafür notwendigen Kräfte bestimmt werden. Durch Ansaugen von Flüssigkeit mit einer Mikropipette kann ein konstanter Sog zur Trennung von Zellverbänden verwendet werden. Durch eine Zentrifugation können Zellen bei einer definierten Kraft getrennt werden. Durch eine Fluoreszenzmarkierung von Zellen oder über eine Isotopenmarkierung (meistens Cr) können Zellkontakte nachgewiesen werden.

Die Hämagglutination (z. B. beim Influenzavirus) und die Rosettierung (z. B. bei Pockenviren oder Plasmodien) sind charakteristische Eigenschaften von manchen Infektionserregern und werden zur Bestimmung anhand ihrer Zelladhäsion verwendet.


</doc>
<doc id="9356836" url="https://de.wikipedia.org/wiki?curid=9356836" title="Leo (Einheit)">
Leo (Einheit)

</math>

Das Leo (Einheitenzeichen: leo), benannt nach dem Vornamen des italienischen Physiker Galileo Galilei, war eine vom englischen Metrologen Francis John Welsh Whipple vorgeschlagene Einheit für die Beschleunigung.



</doc>
<doc id="10256526" url="https://de.wikipedia.org/wiki?curid=10256526" title="Itzhak Bars">
Itzhak Bars

Itzhak Bars (* 31. August 1943 in Izmir) ist ein US-amerikanischer theoretischer Physiker.

Itzhak Bars studierte am Robert College in Istanbul mit dem Bachelor-Abschluss 1967 sowie an der Yale University mit dem Master-Abschluss 1969 und der Promotion bei Feza Gürsey 1971. Als Postdoktorand war er an der University of California, Berkeley. 1973 wurde er Assistant Professor an der Stanford University, 1975 Assistant Professor und später Associate Professor in Yale sowie ab 1983 Professor an der University of Southern California (USC). Von 1999 bis 2003 war er Direktor des "Center for Theoretical Physics" von Caltech und USC.

Er befasst sich mit Symmetrien in der Elementarteilchenphysik, mit Modellen zusammengesetzter Quarks und Leptonen, GUTs, Supersymmetrie (auch in der Kernphysik), Supergruppen, Nichtkommutativer Geometrie, Twistoren, Superstrings und Brane-Theorien, Stringfeldtheorie, Kosmologie und Physik mit zwei Zeitdimensionen und vier und mehr Raumdimensionen. Bars bezeichnet speziell die Erweiterung auf 4 Raum- und 2 Zeitdimensionen, also sowohl einer zusätzlichen Zeit- als auch einer zusätzlichen Raumdimension, als "2T-Physik", mit möglichen weiteren kompaktifizierten Dimensionen. Die 4+2 Dimensionen sind nicht "klein" wie in Kaluza-Klein-Theorien. Die Theorien haben zusätzliche Eichsymmetrien (Sp(2,formula_1), die symplektische Gruppe in 2 Dimensionen, sie vertauscht Energie-Impuls und Zeit-Ort) und können durch Eichfixierung wieder auf Theorien in 3+1 Dimensionen reduziert werden, was allerdings nicht eindeutig ist. Bars konnte zeigen, dass das Standardmodell in 4+2 Dimensionen auf diese Weise einem "emergenten" Standardmodell 3+1 Dimensionen entspricht, das dasselbe Teilchenspektrum wie das übliche Standardmodell hat. Durch die zusätzlichen Symmetrien in den beiden Zusatzdimensionen folgen aber auch über das Standardmodell hinausgehende Eigenschaften, zum Beispiel wird das starke CP-Problem der Quantenchromodynamik gelöst ohne dass es erforderlich wäre die Peccei-Quinn-Symmetrie und Axionen einzuführen. Bars behandelte auch supersymmetrische Feldtheorien und M-Theorie, die im Rahmen der 2T-Physik durch 11 Raum- und 2 Zeitdimensionen (statt wie üblich durch 10 Raum- und eine Zeitdimension) beschrieben würde.

Bars trug auch dazu bei, die Beiträge der schwachen Wechselwirkung zum anomalen magnetischen Moment des Myons (dessen Messung ist eines der Präzisionsexperimente zur Quantenelektrodynamik) zu berechnen.

1979 und 1990 war er Gast am Institute for Advanced Study und 1978 in Harvard und im selben Jahr Junior Faculty Fellow in Yale. Von 1976 bis 1980 war er Sloan Research Fellow. 1986 war er am "Center for Theoretical Physics" der University of California, Santa Barbara.

1988 erhielt er den ersten Preis der Gravity Research Foundation. Er ist Fellow der American Physical Society.

Er ist US-amerikanischer Staatsbürger.




</doc>
<doc id="10737880" url="https://de.wikipedia.org/wiki?curid=10737880" title="Chris Ferrie">
Chris Ferrie

Chris Ferrie (* 1983) ist ein kanadischer Physiker, Mathematiker und Sachbuchautor.

Ferrie studierte an der University of Waterloo in Waterloo, Ontario, Kanada, an der er seinen BSC in Mathematical Physics und seinen Master Applied Mathematics ablegte. Danach promovierte er 2012 in mathematischer Physik über "Some Theory and Applications of Probability in Quantum Mechanics" an der University of Waterloo.

Von 2013 bis 2014 arbeitete er als Postdoctoral Fellow am Center for Quantum Information and Control der University of New Mexico. Von 2015 bis 2017 war er Postdoctoral Research Associate und seit 2017 ist er als Dozent am Centre for Engineer Quantum Systems (Zentrum für Quanteninformatik) der University of Technology in Sydney tätig. In Deutschland wurde er als Verfasser der Buchreihe Baby-Universität für Kleinkinder bekannt, die in deutscher Übersetzung im Loewe-Verlag erschienen.

Ferrie ist verheiratet und Vater von vier Kindern.




</doc>
<doc id="10853050" url="https://de.wikipedia.org/wiki?curid=10853050" title="William Matthaeus">
William Matthaeus

William Henry Matthaeus (* um 1950) ist ein US-amerikanischer Astro- und Plasmaphysiker.

Matthaeus besuchte die Schule in Philadelphia mit dem Abschluss 1968 und studierte mit einem Stipendium des Bürgermeisters von Philadelphia Physik und Philosophie an der University of Pennsylvania mit dem Bachelor-Abschluss 1973. Er erhielt 1975 einen M.A. in Physik an der Old Dominion University in Norfolk in Virginia und 1977 einen Master of Science in Physik am College of William and Mary, an dem er 1979 bei David Campbell Montgomery in Plasmaphysik promovierte ("Nonlinear Evolution of the Magnetohydrodynamic Sheet Pinch"). Er ist "Unidel Professor" für Physik und Astronomie an der University of Delaware, an der er seit 1983 lehrt und am Bartol Research Institute ist.

Er befasst sich mit Plasmaphysik (Turbulenz in der Magnetohydrodynamik (MHD) mit numerischer Simulation, kinetischer Theorie) mit astrophysikalischen Anwendungen (wie dem Sonnenwind und dessen Fluktuationen). Er ist am Swarthmore Spheromak Experiment und seit 2004 wesentlich an der 2018 gestarteten Parker Solar Probe beteiligt zur Untersuchung der Korona der Sonne. Seit 2016 ist er Direktor des Delaware Space Grant der NASA.

In den 1980er Jahren wandte er die Lattice-Boltzmann-Methode in der Magnetohydrodynamik an und 1992 veröffentlichte er eine vielzitierte Arbeit über die Ableitung der Navier-Stokes-Gleichung mit der Lattice-Boltzmann-Methode.

Für 2019 erhielt er den James-Clerk-Maxwell-Preis für Plasmaphysik für "Pionierforschung zur Natur der Turbulenz in Weltraumplasmen und astrophysikalischen Plasmen, die zu wichtigen Fortschritten im Verständnis von Teilchentransport, Dissipation der Turbulenzenergie und magnetischer Rekonnexion" (Laudatio) führte. 1985 erhielt er den James B. MacElwane Award der American Geophysical Union und wurde deren Fellow.

Außer die in den Fußnoten zitierten Arbeiten.




</doc>
<doc id="10918516" url="https://de.wikipedia.org/wiki?curid=10918516" title="Gert Viertel">
Gert Viertel

Gert Henning Michael Viertel, auch "Gert M. Viertel", (* 12. Juli 1943 in Berlin; † 31. Juli 2019 in Sigmaringen) war ein deutscher Astroteilchenphysiker.

Gert M. Viertel studierte Naturwissenschaften an der Johannes Gutenberg-Universität Mainz und der Universität Bern. Bei Beat Hahn am Institut für Hochenergiephysik der Universität Bern wurde er promoviert. Seit 1978 war er an der ETH Zürich tätig.

Er war Sektionschef am Labor für Hochenergiephysik und Leiter der Abteilung Technik und Entwicklung. 1996 wurde er an der ETH Zürich zum Professor ernannt.

Viertel beschäftigte sich als Teilchenphysiker mit der Entwicklung und dem Bau von Detektoren für Experimente an den Grundlagenforschungszentren Deutsches Elektronen-Synchrotron DESY in Hamburg und CERN bei Genf sowie für die International Space Station Alpha (ISSA).



</doc>
<doc id="4189" url="https://de.wikipedia.org/wiki?curid=4189" title="Quantenfeldtheorie">
Quantenfeldtheorie

Die Quantenfeldtheorie (QFT) ist ein Gebiet der theoretischen Physik, in dem Prinzipien klassischer Feldtheorien (zum Beispiel der klassischen Elektrodynamik) und der Quantenmechanik zur Bildung einer erweiterten Theorie kombiniert werden. Sie geht über die Quantenmechanik hinaus, indem sie Teilchen und Felder einheitlich beschreibt. Dabei werden nicht nur sog. Observablen (also beobachtbare Größen wie Energie oder Impuls) quantisiert, sondern auch die wechselwirkenden (Teilchen-)Felder selbst; Felder und Observable werden also analog behandelt. Die Quantisierung der Felder bezeichnet man auch als Zweite Quantisierung. Diese berücksichtigt explizit die Entstehung und Vernichtung von Elementarteilchen (Paarerzeugung, Annihilation).

Die Methoden der Quantenfeldtheorie kommen vor allem in der Elementarteilchenphysik und in der statistischen Mechanik zur Anwendung. Man unterscheidet dabei zwischen "relativistischen Quantenfeldtheorien", die die spezielle Relativitätstheorie berücksichtigen und häufig in der Elementarteilchenphysik Anwendung finden, und "nicht-relativistischen Quantenfeldtheorien", die beispielsweise in der Festkörperphysik relevant sind.

Die Objekte und Methoden der QFT sind physikalisch motiviert, auch wenn viele Teilbereiche der Mathematik zum Einsatz kommen. Die Axiomatische Quantenfeldtheorie versucht dabei, Grundlagen und Konzepte in einen mathematisch rigorosen Rahmen zu fassen.

Die Quantenfeldtheorie ist eine Weiterentwicklung der Quantenphysik über die Quantenmechanik hinaus. Die vorher existierenden Quantentheorien waren ihrem Aufbau nach Theorien für Systeme mit wenigen Teilchen. Um Systeme mit vielen Teilchen zu beschreiben, ist zwar prinzipiell keine neue Theorie nötig, doch die Beschreibung von bspw. 10 Teilchen in einem Festkörper ist mit den Methoden der Quantenmechanik ohne Näherungen aufgrund des hohen Rechenaufwands rein technisch unmöglich.

Ein fundamentales Problem der Quantenmechanik ist ihre Unfähigkeit, Systeme mit variierender Teilchenzahl zu beschreiben. Die ersten Versuche einer Quantisierung des elektromagnetischen Feldes zielten darauf ab, die Emission von Photonen durch ein Atom zu beschreiben. Außerdem gibt es nach der relativistischen Klein-Gordon-Gleichung und der Dirac-Gleichung die oben erwähnten Antiteilchen-Lösungen. Bei ausreichender Energie ist es dann möglich, Teilchen-Antiteilchen-Paare zu erzeugen, was ein System mit konstanter Teilchenzahl unmöglich macht.

Zur Lösung dieser Probleme behandelt man das Objekt, das in der Quantenmechanik als Wellenfunktion eines Teilchens interpretiert wurde, als "Quantenfeld". Das heißt, dass man es ähnlich behandelt wie eine Observable der Quantenmechanik. Dies löst nicht nur die zuvor genannten Probleme, sondern beseitigt auch Inkonsistenzen der klassischen Elektrodynamik, wie zum Beispiel die Strahlungsrückwirkung. Außerdem erhält man Begründungen für das Pauli-Prinzip und das allgemeinere Spin-Statistik-Theorem.

Die Quantenfeldtheorien sind ursprünglich als relativistische Streutheorien entwickelt worden. In gebundenen Systemen sind die Teilchenenergien im Allgemeinen deutlich kleiner als die Massenenergien "mc". Daher ist es in solchen Fällen meist ausreichend genau, in der nichtrelativistischen Quantenmechanik mit der Störungstheorie zu arbeiten. Bei Kollisionen zwischen kleinen Teilchen können jedoch sehr viel höhere Energien auftreten, so dass relativistische Effekte berücksichtigt werden müssen.

Im folgenden Abschnitt wird erklärt, welche Schritte zur Entwicklung einer relativistischen Streutheorie nötig sind. Zunächst wird dazu die Lagrangedichte aufgestellt, dann werden die Felder quantisiert. Zuletzt wird mit den quantisierten Feldern eine Streutheorie beschrieben und ein dabei auftretendes Problem durch die Renormierung gelöst.

Der erste Schritt zu einer Quantenfeldtheorie besteht darin, Lagrangedichten für die Quantenfelder zu finden. Diese Lagrangedichten müssen als Euler-Lagrange-Gleichung die im Allgemeinen bekannte Differentialgleichung für das Feld liefern. Das sind für ein Skalarfeld die Klein-Gordon-Gleichung, für ein Spinorfeld die Dirac-Gleichung und für das Photon die Maxwellgleichungen.

Im Folgenden wird immer die 4er-(Raumzeit)-Vektoren-Schreibweise verwendet. Dabei werden die üblichen Kurzschreibweisen benutzt, nämlich die Kurzschreibweise formula_1 für Differentiale und die Einsteinsche Summenkonvention, die besagt, dass über einen oben und einen unten stehenden Index (von 0 bis 3) summiert wird. Im verwendeten Einheitensystem gilt: formula_2.
Dabei bezeichnet formula_3 die Dirac-Matrizen. formula_4 ist der sogenannte adjungierte Spinor. formula_5 sind die Komponenten des Feldstärketensors. Dabei wurden hier die Maxwellgleichungen in kovarianter Formulierung ohne die Quellenterme (Ladungs- und Stromdichte) benutzt.

Die oben aufgeführten Lagrangedichten beschreiben freie Felder, die nicht wechselwirken. Sie ergeben die Bewegungsgleichungen für freie Felder. Für Wechselwirkungen der Felder untereinander müssen den Lagrangedichten zusätzliche Terme hinzugefügt werden. Dabei ist auf folgende Punkte zu achten:

Erlaubte Terme sind zum Beispiel formula_6 wobei "m" und "n" natürliche Zahlen sind (einschließlich Null) und "k" die Kopplungskonstante ist. Wechselwirkungen mit dem Photon werden meist durch die kovariante Ableitung (formula_7) in der Lagrangedichte für das freie Feld realisiert. Dabei ist die elektrische Ladung "e" des Elektrons hier zugleich die Kopplungskonstante des elektromagnetischen Feldes.

Bisher wurde noch keine Aussage über die Eigenschaften der Felder gemacht. Bei starken Feldern mit einer großen Zahl von Bosonen-Anregungen können diese halbklassisch behandelt werden, im Allgemeinen muss man aber zunächst einen Mechanismus entwickeln, um die Auswirkungen der Quantennatur der Felder zu beschreiben. Die Entwicklung eines solchen Mechanismus bezeichnet man als "Feldquantisierung" und sie ist der erste Schritt, um das Verhalten der Felder berechenbar zu machen. Es gibt dabei zwei verschiedene Formalismen, die unterschiedliches Vorgehen beinhalten.


Im Folgenden werden die Grundlagen der Feldquantisierung für freie Felder in beiden Formalismen erklärt.

Für die Feldquantisierung im kanonischen Formalismus benutzt man den Hamilton-Formalismus der klassischen Mechanik. Man ordnet dabei jedem Feld (formula_8 bzw. formula_9) ein "kanonisch konjugiertes Feld" formula_10 analog dem kanonischen Impuls zu. Das Feld und sein kanonisch konjugiertes Feld sind dann im Sinne der Quantenmechanik konjugierte Operatoren, sogenannte Feldoperatoren, und erfüllen eine Unschärferelation, wie Ort und Impuls in der Quantenmechanik. Die Unschärferelation kann entweder durch eine Kommutatorrelation (für Bosonen nach dem Spin-Statistik-Theorem) oder eine Antikommutatorrelation (für Fermionen) analog zum Kommutator von Ort und Impuls realisiert werden. Den Hamilton-Operator, der die Energie des Systems charakterisiert, erhält man, indem man die Hamilton-Funktion bildet und darin die Felder durch die Feldoperatoren ersetzt. Er ist in der Regel positiv definit oder darf zumindest keine unbeschränkt negativen Eigenwerte haben, da ein solches System unter beliebig großer Energieabgabe an die Umgebung in immer tiefere Energieeigenzustände fallen würde.

Für skalare Felder erhält man formula_11 als kanonisch konjugiertes Feld zu formula_12 und formula_13 als kanonisch konjugiertes Feld zu formula_14. Die geforderte Kommutatorrelation lautet
Es ist in Quantenfeldtheorien üblich, im Impulsraum zu rechnen. Dazu betrachtet man die Fourier-Darstellung des Feldoperators, die für das Skalarfeld lautet
Dabei sind formula_17 der Impuls und formula_18 die Stufenfunktion, die bei negativem Argument 0 und sonst 1 ist.
Da formula_19 und formula_20 Operatoren sind, trifft dies auch auf formula_21, formula_22, formula_23 und formula_24 zu. Ihre Kommutatoren folgen aus dem Kommutator der Feldoperatoren. Der Operator formula_22 kann als Operator interpretiert werden, der ein Teilchen mit Impuls formula_17 erzeugt, während formula_24 ein Antiteilchen mit Impuls formula_17 erzeugt. Entsprechend können formula_21 und formula_23 als Operatoren interpretiert werden, die ein Teilchen oder Antiteilchen mit Impuls formula_17 vernichten.
Die Verwendung der Kommutatorrelationen führt wie gewünscht zu einem positiv definiten Hamilton-Operator. Es können beliebig viele Skalarfelder im selben Zustand sein (Bose-Einstein-Statistik).

Wenn man für ein Spinorfeld analog vorgeht, erhält man formula_32 als kanonisch konjugiertes Feld zu formula_33 und formula_34 als kanonisch konjugiertes Feld zu formula_35. Damit ergeben sich die geforderten (Anti-)Kommutatorrelationen zu
Dabei sind formula_37 und formula_38 Spinorindizes. Man betrachtet dann wieder analog die Fourier-Darstellung des Feldoperators und berechnet den Hamilton-Operator. Einen positiven Hamilton-Operator erhält man beim Spinorfeld jedoch nur, wenn man Antikommutatoren benutzt. Diese werden mit geschweiften Klammern geschrieben, was in den obigen Formeln bereits vorweggenommen wurde. Aufgrund dieser Antikommutatoren ergibt die zweimalige Anwendung desselben Erzeugungsoperators auf einen Zustand den Nullzustand. Das bedeutet, dass nie zwei Spin-1/2-Teilchen im selben Zustand sein können (Pauli-Prinzip). Spinorfelder gehorchen daher der Fermi-Dirac-Statistik.

Für Eichfelder lauten die geforderten Kommutatorrelationen
wobei formula_40 die Komponenten der Minkowski-Metrik bezeichnet. Allerdings erhält man aus der Lagrangedichte formula_41, was die geforderte Kommutatorrelation nicht erfüllen kann. Die Quantisierung von Eichfeldern ist daher nur bei Festlegung einer Eichbedingung möglich. Die Festlegung einer geeigneten Eichbedingung, die den Zugang über Kommutatorrelationen von Feldern ermöglicht und gleichzeitig die Lorentzinvarianz der Lagrangedichte erhält, ist kompliziert.

Man verwendet meist eine Abwandlung der Lorenz-Eichung, um sinnvoll ein kanonisch konjugiertes Feld definieren zu können. Der Formalismus wird nach seinen Entwicklern Suraj N. Gupta und Konrad Bleuler als Gupta-Bleuler-Formalismus bezeichnet.

Eine Alternative stellt eine physikalische Eichung wie z. B. die temporale plus eine weitere Eichbedingung dar. Hier werden zwei der vier Polarisationen des Eichfeldes als physikalische Freiheitsgrade direkt durch die Wahl der Eichung formula_42 sowie durch die anschließende Implementierung des Gaußschen Gesetzes formula_43 als Bedingung an die physikalischen Zustände eliminiert. Der wesentliche Vorteil ist die Reduzierung des Hilbertraumes auf ausschließlich physikalische, transversale Freiheitsgrade. Dem steht als Nachteil der Verlust einer manifest kovarianten Formulierung gegenüber.

Im Pfadintegralformalismus werden die Felder nicht als Operatoren, sondern als einfache Funktionen behandelt. Das Pfadintegral stellt im Wesentlichen eine Übergangsamplitude von einem Vakuumzustand zum Zeitpunkt formula_44 zu einem Vakuumzustand zum Zeitpunkt formula_45 dar, wobei über alle dazwischen möglichen Feldkonfigurationen ("Pfade") integriert wird, mit einem Phasenfaktor, der durch die Wirkung festgelegt wird. Es hat für das Skalarfeld die Form

Um allerdings überhaupt Wechselwirkungen bei einem Übergang vom Vakuum zum Vakuum zu erhalten, müssen Felder erzeugt und vernichtet werden können. Dies wird im Pfadintegralformalismus nicht mithilfe von Erzeugungs- und Vernichtungsoperatoren, sondern durch Quellenfelder erzielt. Es wird also zur Lagrangedichte ein Quellenterm der Form formula_47 hinzugefügt. Das Quellenfeld "J(x)" soll nur in einem endlichen Intervall auf der Zeitachse von Null verschieden sein. Das bedeutet, dass die wechselwirkenden Felder genau innerhalb dieses Zeitintervalls existieren. Das volle Pfadintegral für ein freies Skalarfeld hat damit die Form

Das lässt sich wegen der Integration über formula_12 mit einem Analogon des gaußschen Fehlerintegrals in eine Form bringen, die in bestimmter Weise nur noch vom Quellenfeld "J(x)" abhängt, und zwar:

Dabei ist formula_51 gegeben durch formula_52 also gewissermaßen als das Inverse des Klein-Gordon-Operators (formula_53 ist der D’Alembert-Operator). Dieses Objekt wird als zeitgeordnete Greensche Funktion oder Feynman-Propagator bezeichnet. Man bezeichnet das Pfadintegral daher auch als "Erzeugendenfunktional des Propagators", da die Ableitungen nach formula_54 und formula_55 effektiv einer Multiplikation mit dem Propagator entsprechen.

Das Verhalten des freien Feldes in Anwesenheit von Quellen wird nur durch den Propagator und das Quellenfeld bestimmt. Dieses Ergebnis entspricht der Erwartung, denn das Verhalten eines Feldes, das nicht wechselwirkt, ist offenbar nur durch seine Eigenschaften bei Erzeugung und Vernichtung und seine freie Bewegung bestimmt. Erstere stecken im Quellenfeld und das Bewegungsverhalten wird durch den Klein-Gordon-Operator bestimmt, dessen Informationsgehalt hier durch sein Inverses gegeben ist.

Bei der Quantisierung des Spinorfeldes im Pfadintegral-Formalismus tritt das Problem auf, dass die Felder einerseits wie normale zahlenwertige Funktionen behandelt werden, auf der anderen Seite jedoch antikommutieren. Normale Zahlen kommutieren jedoch. Diese Schwierigkeit lässt sich lösen, indem man die Fermionfelder als Elemente einer Graßmann-Algebra, sogenannte Graßmann-Zahlen, auffasst. Rechnerisch bedeutet das nur, dass man sie wie antikommutierende Zahlen behandelt. Durch die Graßmann-Algebra ist diese Vorgehensweise theoretisch abgesichert. Das Pfadintegral mit Quellenfeldern formula_56 und formula_57 hat dann die Form

Daraus lässt sich, wie beim skalaren Feld, eine Form ableiten, die in bestimmter Weise nur noch von formula_59 und formula_60 abhängt. Dabei lässt sich erneut ein Analogon des gaußschen Integrals anwenden, das allerdings nicht dem gewohnten Formalismus entspricht, sondern in gewisser Weise dazu „invers“ ist. Zunächst ist es jedenfalls nötig, einen Integralbegriff für Graßmann-Zahlen zu entwickeln. Dann lässt sich das Pfadintegral in die folgende Form bringen:

Dabei ist formula_62 das Inverse des Dirac-Operators, das auch als Dirac-Propagator bezeichnet wird. Analog zum skalaren Feld ergibt sich auch hier eine Form, die erwartungsgemäß nur von den Quellenfeldern und der Dynamik der Felder bestimmt ist.

Das Pfadintegral für ein Eichfeld ist von der Form

Der Operator formula_64 hat jedoch kein Inverses. Das erkennt man daran, dass er bei Anwendung auf Vektoren des Typs formula_65 Null ergibt. Mindestens einer seiner Eigenwerte ist also Null, was analog einer Matrix dafür sorgt, dass der Operator nicht invertierbar ist.

Daher lässt sich hier nicht dieselbe Vorgehensweise anwenden, wie beim skalaren Feld und beim Spinorfeld. Man muss der Lagrangedichte einen zusätzlichen Term hinzufügen, so dass man einen Operator erhält, zu dem es ein Inverses gibt. Dies ist äquivalent dazu, eine Eichung festzulegen. Daher bezeichnet man den neuen Term als "eichfixierenden Term". Er ist allgemein von der Form formula_66. Die dazu korrespondierende Eichbedingung lautet formula_67.

Das führt jedoch dazu, dass die Lagrangedichte von der Wahl des Eichterms "f" abhängt. Dieses Problem lässt sich durch das Einführen von sogenannten Faddejew-Popow-Geistern beheben. Diese Geister sind antikommutierende skalare Felder und widersprechen damit dem Spin-Statistik-Theorem. Sie können daher nicht als freie Felder auftreten, sondern nur als sogenannte virtuelle Teilchen. Durch die Wahl der sogenannten Axial-Eichung lässt sich das Auftreten dieser Felder vermeiden, was ihre Interpretation als mathematische Artefakte naheliegend erscheinen lässt. Ihr Auftreten in anderen Eichungen ist jedoch aus tieferliegenden theoretischen Gründen (Unitarität der S-Matrix) zwingend notwendig für die Konsistenz der Theorie.

Die vollständige Lagrangedichte mit eichfixierendem Term und Geistfeldern ist von der Eichbedingung abhängig. Für die Lorenz-Eichung lautet sie bei nichtabelschen Eichtheorien
Dabei ist formula_57 das Geistfeld und formula_70 das Anti-Geistfeld.

Für abelsche Eichtheorien wie den Elektromagnetismus nimmt der letzte Term unabhängig von der Eichung die Form formula_71 an. Daher kann dieser Teil des Pfadintegrals einfach integriert werden und trägt nicht zur Dynamik bei.

Das Pfadintegral liefert auch einen Zusammenhang mit den Verteilungsfunktionen der statistischen Mechanik. Dazu wird die "imaginäre" Zeitkoordinate im Minkowskiraum analytisch in den euklidischen Raum fortgesetzt und statt komplexer Phasenfaktoren im Wegintegral erhält man reelle ähnlich den Boltzmann-Faktoren der statistischen Mechanik. In dieser Form ist diese Formulierung auch Ausgangspunkt von numerischen Simulationen der Feldkonfigurationen (meist zufällig im Monte-Carlo-Verfahren mit einer Wichtung über diese "Boltzmannfaktoren" ausgewählt) in Gitter-Rechnungen. Sie liefern die bisher genauesten Methoden z. B. für die Berechnung von Hadronmassen in der Quantenchromodynamik.

Wie oben schon ausgeführt, ist das Ziel der vorangegangenen Verfahren die Beschreibung einer relativistischen Streutheorie. Obwohl die Methoden der Quantenfeldtheorien heute auch in anderen Zusammenhängen genutzt werden, ist die Streutheorie noch heute eines ihrer Hauptanwendungsgebiete. Daher werden die Grundlagen derselben an dieser Stelle erläutert.

Das zentrale Objekt der Streutheorie ist die sogenannte "S-Matrix" oder "Streumatrix", deren Elemente die Übergangswahrscheinlichkeit von einem Anfangszustand formula_72 in einen Ausgangszustand formula_73 beschreiben. Die Elemente der S-Matrix bezeichnet man als Streuamplituden. Auf der Ebene der Felder ist die S-Matrix also bestimmt durch die Gleichung

Die S-Matrix lässt sich im Wesentlichen als Summe von Vakuumerwartungswerten von zeitgeordneten Feldoperatorprodukten (auch n-Punkt-Funktionen, Korrelatoren oder Greensche Funktionen genannt) schreiben. Ein Beweis dieser sogenannten LSZ-Zerlegung ist einer der ersten großen Erfolge der axiomatischen Quantenfeldtheorie. Im Beispiel einer Quantenfeldtheorie, in der es nur ein Skalarfeld gibt, hat die Zerlegung die Form
Dabei ist "K" der Klein-Gordon-Operator und T der Zeitordnungsoperator, der die Felder aufsteigend nach dem Wert der Zeit formula_76 ordnet. Falls noch andere Felder als das Skalarfeld vorkommen, müssen jeweils die entsprechenden Hamilton-Operatoren verwendet werden. Für ein Spinorfeld muss z. B. der Dirac-Operator statt des Klein-Gordon-Operators verwendet werden.

Zur Berechnung der S-Matrix genügt es also, die zeitgeordneten n-Punkt-Funktionen formula_77 berechnen zu können.

Als nützliches Werkzeug zur Vereinfachung der Berechnungen der n-Punkt-Funktionen haben sich die Feynman-Diagramme erwiesen. Diese Kurzschreibweise wurde 1950 von Richard Feynman entwickelt und nutzt aus, dass sich die Terme, die bei der Berechnung der n-Punkt-Funktionen auftreten, in eine kleine Anzahl elementarer Bausteine zerlegen lassen. Diesen Term-Bausteinen werden dann Bildelemente zugeordnet. Diese Regeln, nach denen diese Zuordnung geschieht, bezeichnet man als Feynman-Regeln. Die Feynman-Diagramme ermöglichen es damit, komplizierte Terme in Form kleiner Bilder darzustellen.

Dabei gibt es zu jedem Term in der Lagrangedichte ein entsprechendes Bildelement. Der Massenterm wird dabei zusammen mit dem Ableitungsterm als ein Term behandelt, der das freie Feld beschreibt. Diesen Termen werden für verschiedene Felder meist verschiedene Linien zugeordnet. Den Wechselwirkungstermen entsprechen dagegen Knotenpunkte, sogenannte "Vertices", an denen für jedes Feld, das im Wechselwirkungsterm steht, eine entsprechende Linie endet. Linien, die nur an einem Ende mit dem Diagramm verbunden sind, werden als reale Teilchen interpretiert, während Linien, die zwei Vertices verbinden als virtuelle Teilchen interpretiert werden. Es lässt sich auch eine Zeitrichtung im Diagramm festlegen, so dass es als eine Art Veranschaulichung des Streuprozesses interpretiert werden kann. Dabei muss man jedoch zur vollständigen Berechnung einer bestimmten Streuamplitude alle Diagramme mit den entsprechenden Anfangs- und Endteilchen berücksichtigen. Wenn die Lagrangedichte der Quantenfeldtheorie Wechselwirkungsterme enthält, sind dies im Allgemeinen unendlich viele Diagramme.

Wenn die Kopplungskonstante kleiner ist als eins, werden die Terme mit höheren Potenzen der Kopplungskonstante immer kleiner. Da nach den Feynmanregeln jeder Vertex für die Multiplikation mit der entsprechenden Kopplungskonstante steht, werden die Beiträge von Diagrammen mit vielen Vertices sehr klein. Die einfachsten Diagramme liefern also den größten Beitrag zur Streuamplitude, während die Diagramme mit zunehmender Kompliziertheit gleichzeitig immer kleinere Beiträge liefern. Auf diese Weise lassen sich die Prinzipien der Störungstheorie unter Erzielung guter Ergebnisse für die Streuamplituden anwenden, indem nur die Diagramme niedriger Ordnung in der Kopplungskonstanten berechnet werden.

Die Feynman-Diagramme mit geschlossenen inneren Linien, die sogenannten Schleifendiagramme (z. B. Wechselwirkung eines Elektrons mit „virtuellen“ Photonen aus dem Vakuum, Wechselwirkung eines Photons mit virtuell erzeugten Teilchen-Antiteilchen Paaren aus dem Vakuum), sind meist divergent, da über alle Energien/Impulse (Frequenz/Wellenzahl) integriert wird. Das hat zur Folge, dass sich kompliziertere Feynman-Diagramme zunächst nicht berechnen lassen. Dieses Problem lässt sich jedoch häufig durch ein sogenanntes Renormierungsverfahren beheben, nach einer falschen Rückübersetzung aus dem Englischen auch manchmal als „Renormalisierung“ bezeichnet.

Es gibt grundsätzlich zwei verschiedene Sichtweisen auf diese Prozedur. Die erste traditionelle Sichtweise ordnet die Beiträge der divergierenden Schleifendiagramme so an, dass sie wenigen Parametern in der Lagrangefunktion wie Massen und Kopplungskonstanten entsprechen. Dann führt man Gegenterme (counter terms) in der Lagrangefunktion ein, die als unendliche „nackte“ Werte dieser Parameter diese Divergenzen aufheben. Das ist in der Quantenelektrodynamik möglich, ebenso in der Quantenchromodynamik und anderen solchen Eichtheorien, bei anderen Theorien wie der Gravitation dagegen nicht. Dort wären unendlich viele Gegenterme nötig, die Theorie ist „nicht renormierbar“.

Eine zweite neuere Sichtweise aus dem Umfeld der Renormierungsgruppe beschreibt die Physik je nach Energiebereich durch verschiedene „effektive“ Feldtheorien. Beispielsweise ist die Kopplungskonstante in der Quantenchromodynamik energieabhängig, für kleine Energien geht sie gegen Unendlich (confinement), für hohe Energien gegen Null (Asymptotische Freiheit). Während in der QED die „nackten“ Ladungen durch die Vakuumpolarisation (Paarerzeugung und -vernichtung) wirksam abgeschirmt werden, liegt der Fall bei Yang-Mills-Theorien wie der QCD wegen der Selbstwechselwirkung der geladenen Eichbosonen komplizierter.

Man vermutet, dass sich alle Kopplungskonstanten physikalischer Theorien bei genügend hohen Energien annähern, und dort wird die Physik dann durch eine große vereinheitlichte Theorie der Grundkräfte beschrieben. Das Verhalten von Kopplungskonstanten und die Möglichkeit von Phasenübergängen mit der Energie wird durch die Theorie der Renormierungsgruppe beschrieben. Aus solchen theoretischen Extrapolationen hat es in den 1990er Jahren erste Hinweise auf die Existenz supersymmetrischer Theorien gegeben, für die sich die Kopplungskonstanten am besten in einem Punkt treffen.

Die technische Vorgehensweise ist jedoch unabhängig von der Sichtweise. Es wird zunächst eine Regularisierung vorgenommen, indem ein zusätzlicher Parameter in die Rechnung eingeführt wird. Dieser Parameter muss zuletzt wieder gegen null oder unendlich laufen (je nach Wahl) um die ursprünglichen Terme wieder zu erhalten. Solange der Regularisierungsparameter jedoch als endlich angenommen wird, bleiben die Terme endlich. Man formt dann die Terme so um, dass die Unendlichkeiten nur noch in Termen auftreten, die reine Funktionen des Regularisierungsparameters sind. Diese Terme werden dann weggelassen. Danach setzt man den Regulierungsparameter null bzw. unendlich, wobei das Ergebnis nun endlich bleibt.

Diese Vorgehensweise wirkt auf den ersten Blick willkürlich, doch das „Weglassen“ muss nach bestimmten Regeln erfolgen. Dadurch wird sichergestellt, dass die renormierten Kopplungskonstanten bei niedrigen Energien den gemessenen Konstanten entsprechen.

Ein spezielles Gebiet der relativistischen Quantenmechanik betrifft Lösungen der relativistischen Klein-Gordon-Gleichung und der Dirac-Gleichung mit negativer Energie. Dies würde es Teilchen erlauben, zu unendlicher negativer Energie abzusteigen, was in der Realität nicht beobachtet wird. In der Quantenmechanik löst man dieses Problem, indem man die entsprechenden Lösungen willkürlich als Entitäten mit positiver Energie interpretiert, die sich rückwärts in der Zeit bewegen; man überträgt also in der Wellenfunktion das negative Vorzeichen von der Energie "E" auf die Zeit "t", was wegen der Beziehung formula_78 naheliegend ist ( "h" ist die Plancksche Konstante und formula_79 das der Energiedifferenz formula_80 zugeordnete Frequenzintervall).

Paul Dirac interpretierte diese rückwärts bewegten Lösungen als Antiteilchen.

Durch Kombination des elektroschwachen Modells mit der Quantenchromodynamik entsteht eine vereinte Quantenfeldtheorie, das so genannte Standardmodell der Elementarteilchenphysik. Es enthält alle bekannten Teilchen und kann die meisten bekannten Vorgänge erklären.

Gleichzeitig ist aber bekannt, dass das Standardmodell nicht die endgültige Theorie sein kann. Zum einen ist die Gravitation nicht enthalten, zum anderen gibt es eine Reihe von Beobachtungen (Neutrinooszillationen, Dunkle Materie), nach denen eine Erweiterung des Standardmodells notwendig scheint. Außerdem enthält das Standardmodell viele willkürliche Parameter und erklärt z. B. das sehr unterschiedliche Massenspektrum der Elementarteilchenfamilien nicht.

Die im Folgenden erläuterten Quantenfeldtheorien sind alle im Standardmodell enthalten.

Die Lagrangedichte der formula_81-Theorie lautet
Diese Quantenfeldtheorie besitzt große theoretische Bedeutung, da sie die einfachste denkbare Quantenfeldtheorie mit einer Wechselwirkung ist und hier im Gegensatz zu realistischeren Modellen einige exakte mathematische Aussagen über ihre Eigenschaften gemacht werden können. Sie beschreibt ein selbstwechselwirkendes reelles oder komplexes Skalarfeld.

In der statistischen Physik spielt sie eine Rolle als einfachstes Kontinuumsmodell für die (sehr allgemeine) Landau-Theorie der Phasenübergänge zweiter Ordnung und der kritischen Phänomene. Von der statistischen Interpretation aus bekommt man zugleich einen neuen und konstruktiven Zugang zum Renormierungsproblem, indem gezeigt wird, dass die Renormierung der Massen, Ladungen und Vertex-Funktionen durch Eliminierung kurzwelliger Wellenphänomene aus der sog. Zustandssumme formula_83 (englisch: „Partition Function“) erreicht werden kann. Auch das Higgsfeld des Standardmodells hat eine formula_81-Selbstwechselwirkung, die allerdings noch um Wechselwirkungen mit den anderen Feldern des Standardmodells ergänzt wird. In diesen Fällen ist die Kopplungskonstante "m" negativ, was einer imaginären Masse entspräche. Diese Felder werden daher als tachyonische Felder bezeichnet. Diese Bezeichnung bezieht sich jedoch auf das "Higgsfeld" und nicht auf das "Higgs-Teilchen", das sogenannte Higgs-Boson, welches kein Tachyon, sondern ein gewöhnliches Teilchen mit reeller Masse ist. Das Higgsteilchen wird auch nicht durch das Higgsfeld beschrieben, sondern nur durch einen bestimmten Anteil dieses Feldes.

Die Lagrangedichte der Quantenelektrodynamik (QED) lautet
Die QED ist die erste physikalisch erfolgreiche Quantenfeldtheorie. Sie beschreibt die Wechselwirkung eines Spinorfeldes mit Ladung "-e", das das Elektron beschreibt, mit einem Eichfeld, das das Photon beschreibt. Man erhält ihre Bewegungsgleichungen aus der Elektrodynamik durch Quantisierung der maxwellschen Gleichungen. Die Quantenelektrodynamik erklärt mit hoher Genauigkeit die elektromagnetische Wechselwirkung zwischen geladenen Teilchen (zum Beispiel Elektronen, Myonen, Quarks) mittels Austausch von virtuellen Photonen sowie die Eigenschaften von elektromagnetischer Strahlung.

Dadurch lassen sich etwa die chemischen Elemente, ihre Eigenschaften und Bindungen und das Periodensystem der Elemente verstehen. Auch die Festkörperphysik mit der wirtschaftlich bedeutsamen Halbleiterphysik leiten sich letztendlich von der QED ab. Konkrete Rechnungen werden allerdings in der Regel im vereinfachten, aber ausreichenden Formalismus der Quantenmechanik durchgeführt.

Die schwache Wechselwirkung, deren bekanntester Effekt der Betazerfall ist, nimmt eine physikalisch geschlossene Formulierung nach Vereinheitlichung mit der QED im elektroschwachen Standardmodell an.
Die Wechselwirkung wird hier durch Photonen, W- und Z-Bosonen vermittelt.

Ein anderes Beispiel einer QFT ist die Quantenchromodynamik (QCD), welche die Starke Wechselwirkung beschreibt.
In ihr wird ein Teil der im Atomkern auftretenden Wechselwirkungen zwischen Protonen und Neutronen auf die subnukleare Wechselwirkung zwischen Quarks und Gluonen reduziert.

Interessant ist in der QCD, dass die Gluonen, welche die Wechselwirkung vermitteln, selbst miteinander wechselwirken. (Das wäre am Beispiel der QED etwa so, als ob sich zwei durchdringende Lichtstrahlen direkt beeinflussen würden.) Eine Konsequenz dieser gluonischen Selbstwechselwirkung ist, dass die elementaren Quarks nicht einzeln beobachtet werden können, sondern immer in Form von Quark-Antiquark-Zuständen oder Zuständen dreier Quarks (oder Antiquarks) auftreten (Confinement). Auf der anderen Seite folgt daraus, dass die Kopplungskonstante bei hohen Energien nicht zunimmt, sondern abnimmt. Dieses Verhalten wird als "asymptotische Freiheit" bezeichnet.

Wie oben schon angesprochen, eignet sich die formula_81-Theorie zur Beschreibung von Systemen mit spontaner Symmetriebrechung oder kritischen Punkten. Der Massenterm wird dazu als Teil des Potentials verstanden. Für eine reelle Masse hat dieses Potential dann nur ein Minimum, während bei imaginärer Masse das Potential eine w-förmige Parabel vierten Grades beschreibt. Wenn das Feld mehr als eine reelle Komponente hat, erhält man noch mehr Minima. Bei einem komplexen Feld (mit zwei reellen Komponenten) erhält man zum Beispiel die Rotationsfigur der w-förmigen Parabel mit einem Minimakreis. Diese Form wird auch als "Mexican Hat Potential" bezeichnet, da das Potential an die Form eines Sombrero erinnert.

Jedes Minimum entspricht nun einem Zustand niedrigster Energie, die vom Feld alle mit gleicher Wahrscheinlichkeit angenommen werden. In jedem dieser Zustände hat das Feld jedoch ein geringeres Maß an Symmetrie, da die Symmetrie der Minima untereinander durch Auswahl eines Minimums verloren geht. Diese Eigenschaft der klassischen Feldtheorie überträgt sich auf die Quantenfeldtheorie, so dass sich die Möglichkeit ergibt, Quantensysteme mit gebrochener Symmetrie zu beschreiben. Beispiele für solche Systeme sind das Ising-Modell aus der Thermodynamik, das die spontane Magnetisierung eines Ferromagneten erklärt, und der Higgs-Mechanismus, der die Massen der Eichbosonen in der schwachen Wechselwirkung erklärt. Durch die erhaltenen Massenterme der Eichbosonen wird nämlich die Eichsymmetrie reduziert.

Die Axiomatische Quantenfeldtheorie versucht, ausgehend von einem Satz möglichst weniger, als mathematisch oder physikalisch unumgänglich angesehener Axiome, eine konsistente Beschreibung der Quantenfeldtheorie zu erzielen.

Die axiomatische Quantenfeldtheorie wurde u. a. aus den Wightman-Axiomen, entstanden im Jahr 1956, begründet. Ein weiterer Zugang ist die von Haag und Araki 1962 formulierte algebraische Quantenfeldtheorie, die durch die Haag-Kastler-Axiome charakterisiert wird. Die Osterwalder-Schrader-Axiome stellen einen dritten axiomatischen Zugang zur Quantenfeldtheorie dar.

Etliche konkrete Ergebnisse konnten mit dieser Herangehensweise erzielt werden, zum Beispiel die Herleitung des Spin-Statistik-Theorems und des CPT-Theorems alleine aus den Axiomen, d. h. unabhängig von einer speziellen Quantenfeldtheorie. Ein früher Erfolg war die 1955 von Lehmann, Symanzik und Zimmermann entwickelte LSZ-Reduktionsformel für die S-Matrix. Außerdem existiert ein von Bogoliubov, Medvedev und Polianov begründeter funktionalanalytischer Zugang zur S-Matrix-Theorie (auch BMP-Theorie genannt).

Weitere Anwendungen im Bereich der klassischen Statistik und der Quantenstatistik sind schon sehr weit fortgeschritten. Sie reichen von der allgemeinen Ableitung der Existenz thermodynamischer Größen, Satz von Gibbs, Zustandsgrößen wie Druck, innerer Energie und Entropie bis zum Beweis der Existenz von Phasenübergängen und der exakten Behandlung wichtiger Vielteilchensysteme:

Versuche, diese Quantenfeldtheorien mit der allgemeinen Relativitätstheorie (Gravitation) zur Quantengravitation zu vereinen, sind bisher ohne Erfolg geblieben. Nach Ansicht vieler Forscher erfordert die Quantisierung der Gravitation neue, über die Quantenfeldtheorie hinausgehende Konzepte, da hier der Raum-Zeit Hintergrund selbst dynamisch wird. Beispiele aus der aktuellen Forschung sind die Stringtheorie, die M-Theorie und die Loop-Quantengravitation. Weiter liefern die Supersymmetrie, die Twistor-Theorie, die Finite Quantenfeldtheorie und die Topologische Quantenfeldtheorie wichtige konzeptionelle Ideen, die zurzeit in der Fachwelt diskutiert werden.

Auch in der Festkörpertheorie finden sich Anwendungen der (nicht-relativistischen) Quantenfeldtheorie, und zwar hauptsächlich in der Vielteilchentheorie.

Allgemeine Einführungen in das Thema (jeweils in alphabetischer Reihenfolge der (Erst-)Autoren)

Deutsch:

Englisch:

Speziellere und verwandte Themen und



</doc>
<doc id="13234" url="https://de.wikipedia.org/wiki?curid=13234" title="Fresnel-Linse">
Fresnel-Linse

Eine Fresnel-Linse [] oder genauer eine Fresnelsche Stufenlinse ist eine volumen- und massereduzierte Bauform einer optischen Linse.
Sie wurde um 1822 vom französischen Physiker Augustin Jean Fresnel ursprünglich für Leuchttürme entwickelt. Das Funktionsprinzip erdachte Georges-Louis Leclerc de Buffon im Jahre 1748.

Da Licht nur beim Passieren der Linsen-Oberflächen gebrochen wird, ist der Brechungswinkel nicht von der Dicke, sondern nur von dem Winkel zwischen den beiden Oberflächen abhängig. Das Volumen bei der Fresnel-Linse ist durch eine Aufteilung in ringförmige Bereiche verringert, deren maximale Glasdicke ungefähr gleich ist. Durch die erforderliche Krümmung der Oberflächen erhält die Linse eine Reihe ringförmiger Stufen. Die Linse behält ihre Brennweite bei, aber die Abbildungsqualität wird durch die Stufenstruktur verschlechtert.

Heute unterscheidet man im Wesentlichen zwei Bauformen von Fresnel-Linsen: "Scheinwerferlinsen" und "Gürtellinsen".

Im 19. Jahrhundert wurden Fresnel-Linsen vor allem in Leuchttürmen eingesetzt. Sie bestanden aus handpolierten Prismen aus Glas, die in einen Rahmen aus Metall, meist Messing, zusammengefügt wurden. Durch ihre im Vergleich zu früheren Leuchtfeueroptiken weitaus höhere Lichtstärke revolutionierten sie die Leuchtturmtechnik. Die Herstellung der frühen Fresnel-Linsen war aufwändig und teuer. Die wichtigsten Produktionszentren lagen in Großbritannien und Frankreich, der wichtigste Hersteller in Deutschland war die Firma Weule.

Mit dem Begriff "Ordnung" wird bei Leuchtturmlinsen deren geometrische Größe – mit der die optische Größe Brennweite einhergeht – gekennzeichnet. Große Linsen (große Brennweite) ergeben bei gleicher Lichtquelle ein helleres, weiter reichendes Leuchtturmlicht als kleine Linsen. In den USA sind sechs Ordnungen standardisiert:

Fresnel-Linsen werden in einigen Fahrzeugen (insb. LKWs) eingesetzt, um den toten Winkel zu reduzieren. Aufgrund der 400 Verkehrstoten jährlich in Deutschland bei Abbiegeunfällen im toten Winkel fordert der Deutsche Verkehrssicherheitsrat, dass der Gesetzgeber die Verwendung von Fresnel-Linsen zumindest vorübergehend vorschreibt. Im Internet werden selbstklebende Folien mit einer Fresnel-Linse unter der Bezeichnung „Lescars Fresnellinse“ angeboten. Diese können zum Beispiel auf die Heckscheibe eines Pkws geklebt werden. Dadurch vergrößert sich der Blickwinkel und es können bislang tote Winkel eingesehen werden. Der Hersteller empfiehlt auch das Aufkleben auf Rückspiegel, um den Blickwinkel zu vergrößern und dadurch tote Winkel einsehen zu können. Rückspiegel benötigen eigentlich keine Fresnel-Linse, da die spiegelnde Fläche vom Hersteller ohne zusätzliches Material beliebig geformt werden kann. Durch Aufkleben einer Fresnel-Linse kann der Blickwinkel vergrößert werden und so tote Winkel eingesehen werden. Dabei nimmt man aber eine Verkleinerung der Bildobjekte und eine Verschlechterung der Bildqualität in Kauf. Bei der Anwendung ist also Vorsicht geboten, da der Autofahrer durch die Verkleinerung der Objekte eine verschlechterte Wahrnehmung und eine falsche Entfernungsschätzung haben kann. Bei LKWs gibt es inzwischen zahlreiche bessere Systeme, um beim Abbiegen Radfahrer im toten Winkel zu erkennen, z. B. TURN DETECT.

Weiters finden die Fresnel-Linsen bei optischen Sondersignalanlagen aller Farben Verwendung. Bei Xenonblitz-, aber auch LED-Kennleuchten wird damit ein möglichst homogenes Lichtbild auf der Kennleuchtenoberfläche und damit eine große und vor allem helle Lichtfläche erzeugt. Die Linse kann dabei beispielsweise in der Haube befestigt sein.

Fresnel-Linsen werden heute dort eingesetzt, wo das Gewicht der Linsen ausschlaggebend und die Abbildungsqualität zweitrangig ist.

Gürtellinsen kommen bei Schiffslaternen und Leuchtfeuern vor. Bei Leuchtfeuern umspannen sie oftmals nicht einen vollen Kreis, sondern lassen den Bereich über dem Festland frei und bestehen wegen ihrer Größe vielfach nicht aus einem Stück.

Scheinwerferlinsen kommen bei Leuchtfeuern ebenfalls häufig vor. Bei Richtfeuern wird nur eine Linse verwendet. Bei Drehfeuern sind mehrere Scheinwerferlinsen gleichmäßig kreisförmig angeordnet, von denen jede momentan bevorzugt in einen Sektor strahlt.

Scheinwerferlinsen werden auch in Frontscheinwerfern von Automobilen verwendet.

Hinter einem Rückprojektionsbildschirm befindet sich eine Fresnel-Scheinwerferlinse. Außerdem wird diese Linse in speziellen Scheinwerfern für Veranstaltungs- und Theatertechnik verwendet.

Hochwertige Scheinwerfer für Film- und TV-Beleuchtung sind zum Großteil als Stufenlinsenscheinwerfer ausgeführt. Diese Konstruktion ermöglicht, neben einer gleichmäßig flächigen Ausleuchtung mit weicher Kante, eine stufenlose Regulierung des Lichtaustrittswinkels. Die Fresnel-Linse dieser Scheinwerfer wird mattiert („gefrostet“) um eine Projektion der Stufenringe zu vermeiden.

Billige Scheinwerferlinsen sind aus Kunststoff gepresst und finden in Tageslichtprojektoren, bei Passiv-Infrarotsensoren, bei einfachen Handlupen, bei Automobil-Rücklichtern und als Weitwinkellinsen(-folien) in Automobil-Heckscheiben und Ladenkassen (Kontrolle der Einkaufswagen) Verwendung.

Weitere Anwendungen findet das Prinzip bei Einstellscheiben im Sucher von Spiegelreflexkameras mit mehr oder weniger großem Fresnel-Stufenlinsenbereich oder bei Streufolien mit kleinen Fresnellinsen-Quadraten zum Aufkleben auf Fensterscheiben sowie bei Tageslichtprojektoren.
Sie sind auch Bestandteil von hochwertigen Teleobjektiven mit langer Brennweite, um deren Gewicht zu reduzieren.

Neue Anwendungen ergeben sich in der Solartechnik, da mit Linsen höhere Wirkungsgrade bei Solarstrommodulen erreicht werden können.
Bei Sonnenwärmekraftwerken kommen "Fresnel-Spiegel-Kollektoren" zur Anwendung.



</doc>
<doc id="29823" url="https://de.wikipedia.org/wiki?curid=29823" title="Gewichtskraft">
Gewichtskraft

Die Gewichtskraft, auch Gewicht, ist die durch die Wirkung eines Schwerefeldes verursachte Kraft auf einen Körper. Im rotierenden Bezugssystem eines Himmelskörpers (wie dem der Erde) setzt sich dieses Schwerefeld aus einem Gravitationsanteil und einem kleinen Zentrifugalanteil zusammen. Die Gewichtskraft ist lotrecht nach unten gerichtet, was im Schwerefeld der Erde beinahe, aber nicht genau, der Richtung zum Erdmittelpunkt entspricht.

Als Formelzeichen wird meist formula_1 oder formula_2 verwendet. Die SI-Einheit für die Gewichtskraft ist das Newton (N).

Die Gewichtskraft formula_1 kann als Produkt der Masse formula_4 mit der Schwerebeschleunigung formula_5 berechnet werden:

Abgesehen von geringen Unregelmäßigkeiten ist die Gewichtskraft eines Körpers stets zum Mittelpunkt des Himmelskörpers hin gerichtet, auf dem er sich befindet, da das Schwerefeld in guter Näherung ein Radialfeld ist. In den meisten Anwendungen erreicht man aber auch eine ausreichende Genauigkeit, wenn man das Schwerefeld als homogenes Feld ansieht, nämlich dann, wenn alle Abmessungen viel kleiner als der Radius des Himmelskörpers sind. In diesem Fall hat die Gewichtskraft an jedem Ort gleiche Richtung und gleiche Stärke.

Die Bahnkurve eines bewegten starren Körpers verläuft genau so, als griffe die gesamte Gewichtskraft im Gravizentrum (Schwerpunkt) des Körpers an. Das gilt auch für die Bewegung des Schwerpunkts eines Systems mehrerer Körper. In einem homogenen Gravitationsfeld stimmt das Gravizentrum mit dem Massenmittelpunkt überein. Ist die Gewichtskraft die einzige wirkende Kraft, so befindet sich der Körper bzw. das Mehrkörpersystem im Zustand des freien Falls. Da die Trägheit eines Körpers von der Masse in derselben Weise abhängt wie die Gewichtskraft, sind die Beschleunigungen aller frei fallenden Körper gleich. Die Fallbeschleunigung hängt also nicht von der Masse oder anderen Eigenschaften des Körpers ab, sondern höchstens von seinem Ort.

Auf der Erdoberfläche kann man für die Schwerebeschleunigung den Näherungswert formula_7 verwenden.

Möchte man die Gewichtskraft auf der Erde jedoch genauer bestimmen, so ist die Ortsabhängigkeit der Schwerebeschleunigung (formula_8 am Äquator bzw. formula_9 an den Polen) durch Schwereformeln zu berücksichtigen, beispielsweise durch die Formel von Somigliana. Für diese Ortsabhängigkeit gibt es verschiedene Ursachen:
Auf der Erdoberfläche hängen die ersten beiden Effekte von der geographischen Breite ab: der erste, weil die Breite den Abstand des Standorts von der Erdachse bestimmt; der zweite, weil die Breite den Abstand vom Erdmittelpunkt und die genaue Verteilung der Erdmasse in Bezug zum Standort bestimmt. Dazu kommt eine Abhängigkeit von der Höhe des Standorts über der Erdoberfläche.

In der Alltagssprache wird vom "Gewicht" eines Körpers gesprochen, ohne zu unterscheiden, ob damit seine Masse oder seine Gewichtskraft gemeint ist. Dennoch handelt es sich um sehr unterschiedliche physikalische Begriffe:
Die Masse ist daher eine dem Körper innewohnende Eigenschaft, während die Gewichtskraft Resultat eines äußeren Einflusses auf den Körper ist.

Demzufolge ist die Masse eines Körpers, unabhängig von dem Ort, an dem er sich befindet (Erde, Mond, Schwerelosigkeit,  …), stets gleich, während die auf ihn wirkende Gewichtskraft von der Schwerebeschleunigung abhängt. (Auf dem Mond beträgt die Gewichtskraft nur ungefähr ein Sechstel derjenigen auf der Erde. Ein Körper mit einer Masse von 6 kg ist daher auf dem Mond nur so schwer wie ein Körper mit einer Masse von 1 kg auf der Erde)

Bis 1960 war es üblich, die Kraft in der Einheit Kilopond (kp) anzugeben. Diese war so definiert, dass die Gewichtskraft auf der Erde, gemessen in Kilopond, dieselbe Maßzahl hatte wie die Masse in Kilogramm (formula_10). Danach wurde das Kilopond im SI-Einheitensystem durch die Einheit Newton (1 kp = 9,80665 N ≈ 1 daN) ersetzt. Seither haben die Masse und die Gewichtskraft Maßzahlen, die sich näherungsweise um den o. g. Faktor formula_11 unterscheiden.

Messgeräte zur "direkten" Feststellung einer Gewichtskraft sind Kraftmesser, beispielsweise Federwaagen. Allerdings verfälscht der statische Auftrieb das Ergebnis, was sich insbesondere bei Körpern geringer Dichte bemerkbar macht.

"Indirekt" kann man die Gewichtskraft auch durch Wägung und anschließende Umrechnung des Wägewerts bestimmen. Bei einer genaueren Betrachtung der Funktionsweise einer Waage stellt man fest, dass die eigentliche direkt erfasste Messgröße ohnehin die um den Auftrieb verfälschte Gewichtskraft ist, auch wenn als Wägewert eine Masse angezeigt wird. So vergleicht z. B. eine einfache Balkenwaage die "Kräfte," die die beiden Massen auf ihre jeweilige Waagschale ausüben.

Gewichtskraft wird in vielen in die Mechanik einführenden Büchern erklärt. Beispielhaft seien hier genannt:


</doc>
<doc id="42680" url="https://de.wikipedia.org/wiki?curid=42680" title="Brian May">
Brian May

Brian Harold May, CBE (* 19. Juli 1947 in Hampton, Middlesex, heute im London Borough of Richmond upon Thames) ist ein britischer Gitarrist, Komponist, Sänger, Astrophysiker, Sachbuchautor und Tierschützer. Er ist bekannt als Leadgitarrist der Rockband Queen.

May war zusammen mit Leadsänger Freddie Mercury und Schlagzeuger Roger Taylor Gründungsmitglied von Queen, nachdem er zuvor mit Taylor in der Band Smile gespielt hatte. Nach der Komplettierung der Besetzung durch Bassist John Deacon etablierten sich Queen als eine der führenden britischen Rockbands. Von Mitte der 1970er Jahre bis zu Beginn der 1990er Jahre war Queen fast ständig in den britischen Musikcharts vertreten. Mit Queen erlangte May weltweite Bekanntheit als virtuoser Musiker. Zu seinen typischen Stilmitteln gehören die Rückkopplung und der Echo-Effekt seiner selbstgebauten Gitarre Red Special. Die Verwendung von Münzen anstelle eines Plektrums prägte Queens typischen Gitarren-Klang ebenfalls. Zu Mays Kompositionen für die Band gehören unter anderem "We Will Rock You", "Tie Your Mother Down", "I Want It All", "Fat Bottomed Girls", "Flash", "Hammer to Fall", "Save Me", "Who Wants to Live Forever" und "The Show Must Go On".

Nach Mercurys Tod im Jahr 1991 verfolgte May eine Solo-Karriere, bis er Queen zusammen mit Taylor und Gastsängern wieder aufleben ließ. Nach dreieinhalb Jahrzehnten kehrte May an das Londoner Imperial College zurück, um seine Doktorarbeit zu beenden, die sich mit Radialgeschwindigkeiten im interplanetaren Staub befasst und 2007 angenommen wurde. Von 2008 bis 2013 war er Ehrenkanzler der Liverpool John Moores University. May ist ein Aktivist der Tierrechtsbewegung und seit 2012 Vizepräsident der Royal Society for the Prevention of Cruelty to Animals.

Vater Harold May war Elektroingenieur und diente während des Zweiten Weltkriegs in der Royal Air Force. Danach war er als technischer Zeichner für das britische Luftfahrtministerium tätig und an der Entwicklung der Concorde beteiligt. Mutter Ruth May stammte aus Schottland. Die Familie lebte in Feltham, einem westlichen Vorort von London in der Grafschaft Middlesex. Brian May war ein Einzelkind, bastelte gerne und zeigte früh musikalisches Talent, was seine Eltern mit Klavierunterricht förderten. Als er sechs Jahre alt war, brachte ihm sein Vater die Grundlagen für das Spielen der Banjolele bei, einer Kombination aus Banjo und Ukulele, die in den 1930er Jahren durch George Formby jr. in Großbritannien populär geworden war.

Zu seinem siebten Geburtstag erhielt May das erste eigene Instrument geschenkt, eine spanische Akustikgitarre. Für diese bastelte er einen Single-Coil-Tonabnehmer, um einen elektrischen Klang zu imitieren, wobei er das Radio der Familie als Verstärker benutzte. Er spielte Lieder auf Schallplatten nach und verbesserte so seine eigenen Fähigkeiten als Gitarrenspieler. May begann sich auch für die Sterne und den Nachthimmel zu interessieren und bastelte ein kleines Teleskop.

Nach dem Besuch der Grundschule an der Hanworth Road in Feltham kam May als Elfjähriger an die Grammar School in Hampton, unterstützt durch ein Stipendium, das er für seine hervorragenden schulischen Leistungen erhalten hatte. Bald merkte er, dass seine Akustikgitarre seinen Ansprüchen nicht mehr genügte. Da E-Gitarren teuer waren, kam er auf die Idee, selbst eine zu bauen. Zusammen mit seinem Vater begann er im August 1963 mit dem Bau der später berühmt gewordenen Red Special. Sie entstand aus Gegenständen, die im Haus der Mays zu finden waren, Teilen eines alten Motorrads, einem Eichenbrett und einer über hundertjährigen Mahagoni-Kaminverkleidung. Das Tremolo fertigten sie aus einem Brotmesser. Nur die Burns-Tonabnehmer musste er kaufen. Nach 18 Monaten war die Gitarre vollendet.

Im Herbst 1964 gründete May zusammen mit Tim Staffell und anderen Mitschülern eine Band namens 1984, benannt nach dem gleichnamigen Roman von George Orwell. Ihren ersten Auftritt hatten sie am 28. Oktober in der St Mary’s Hall in Twickenham. In den folgenden drei Jahren hatten sie zahlreiche Auftritte in Pubs, Clubs und Schulen, dabei spielten sie eine Mischung aus Musikstilen von Sam & Dave und Otis Redding. May schloss die Grammar School mit dem General Certificate of Education ab, in den Fächern Physik, Mathematik und angewandte Physik mit dem Advanced Level. Er wollte Astronom werden und begann im Herbst 1965 am Londoner Imperial College mit dem Studium der Physik und der Infrarotastronomie. Am 13. Mai 1967 spielten 1984 im Imperial College als Vorband von Jimi Hendrix, am 23. Dezember desselben Jahres traten sie zusammen mit Tyrannosaurus Rex, The Herd, Jimi Hendrix, Traffic und Pink Floyd bei einem Weihnachtsfestival im "Olympia Theatre" auf.

Anfang 1968 löste sich die Band auf. Ausschlaggebend waren musikalische Differenzen und die Tatsache, dass May mehr Zeit für sein Studium benötigte. Unter anderem leitete er den Bau einer kleinen Schutzhütte auf der Testa Grigia bei Zermatt in der Schweiz, um dort Geräte zur Beobachtung des Zodiakallichts zu lagern. Er verbrachte dort einige Zeit, wegen des andauernd schlechten Wetters wurden die Hütte und die Geräte aber bald nach Teneriffa gebracht und am Teide aufgestellt. May erhielt von Professor Bernard Lovell das Angebot, ihn bei seinen Forschungen im Jodrell-Bank-Radioobservatorium zu unterstützen. Nach langem Zögern lehnte er ab, da er in London bleiben wollte, um sich musikalisch weiterzuentwickeln.

Im Herbst 1968 gründeten May und Staffell die Band Smile. Zu ihnen gesellte sich der Zahnmedizinstudent Roger Taylor als Schlagzeuger. May schloss sein Studium ab und erhielt am 24. Oktober 1968 im Rahmen einer Feier in der Royal Albert Hall von der Königinmutter den akademischen Grad eines Bachelor of Science (BSc) überreicht. Zwei Tage später hatten Smile im Imperial College ihr erstes Konzert, als Vorgruppe von Pink Floyd. Über Staffell lernten May und Taylor den Kunststudenten Farrokh „Freddie“ Bulsara (Freddie Mercury) kennen, der die Band als Fan unterstützte und ihr Erscheinungsbild mitgestaltete. Ein Plattenvertrag für ein Album kam nicht zustande. Staffell trennte sich im Frühjahr 1970 von der Band, wodurch Smile aufhörte zu existieren.

Im April 1970 gründeten May, Taylor und Mercury die Band Queen. Nachdem sie mehrmals den Bassisten wechseln mussten, lernten sie den angehenden Elektrotechniker John Deacon kennen, womit die Besetzung im Februar 1971 komplett war. Über einen Bekannten gelang es May im September 1971, Studiozeit für Demo-Aufnahmen zu organisieren: Mehrere Monate lang konnten sie in den neu eröffneten De Lane Lea Studios unter professionellen Bedingungen arbeiten, was ihnen den Weg zum ersten Plattenvertrag ebnete. Im Juli 1973 wurde das Debütalbum "Queen" veröffentlicht.
May arbeitete in seiner freien Zeit weiterhin an seiner Dissertation, außerdem gab er private Nachhilfestunden am Imperial College. Im September 1973 nahm er für einige Monate eine Teilzeitstelle als Englischlehrer an einer Gesamtschule im Südosten Londons an. Im Mai 1974 mussten Queen einen Teil ihrer US-Tournee absagen, weil May wegen des Gebrauchs nicht-steriler Impfnadeln an Hepatitis erkrankt war und mehrere Wochen im Krankenhaus verbrachte. Auch die nachfolgende US-Tournee wurde abgesagt, weil er im Juli 1974 wegen eines Zwölffingerdarmgeschwürs operiert werden musste. Dessen ungeachtet, schafften Queen den internationalen Durchbruch und wurden zu einer der weltweit bekanntesten Rockbands. May entschloss sich, die Arbeit an seiner Dissertation endgültig abzubrechen. Dies führte vorübergehend zu einem Zwist mit seinem Vater, der ihm vorwarf, seine Ausbildung fortzuwerfen; nach zwei Jahren versöhnten sie sich jedoch wieder.

Im April 1983, als die Bandmitglieder mit verschiedenen Solo-Projekten beschäftigt waren, traf sich May in einem Studio in Los Angeles mit Eddie Van Halen und weiteren Musikern. Die aus der Jamsession resultierenden Songs, die zunächst nur für den privaten Gebrauch aufgenommen worden waren, veröffentlichte May sechs Monate später auf dem Mini-Album "Star Fleet Project". Im selben Jahr produzierte er zusammen mit Reinhold Mack das Album "Lettin Loose" der schottischen Band Heavy Pettin. 1987 produzierte er das Album "Cancel" der japanischen Sängerin Minako Honda und schrieb für sie zwei Songs. Im selben Jahr produzierte May für Anita Dobson (seine spätere zweite Ehefrau) das Album "Talking of Love", wofür er auch die meisten Lieder schrieb. Ebenfalls 1987 produzierte er das selbstbetitelte Album der Comedy-Metal-Band Bad News.; die Single "Anyone Can Fall in Love" erreichte Platz 4 in den britischen Single-Charts.

May sagte in Interviews, dass er in den späten 1980er und frühen 1990er Jahren unter schweren Depressionen litt. Als Gründe nannte er seine kriselnde Ehe, sein vermeintliches Scheitern als Ehemann und Vater sowie den Tod seines Vaters. Nach dem Tod von Freddie Mercury am 24. November 1991 habe er einen Suizid in Betracht gezogen. Er wies sich selbst in eine Klinik in Arizona ein und meinte dazu: „Ich betrachtete mich als vollkommen krank. Ich war verletzt und zerbrochen. Ich verfiel in eine ernsthafte Depression. Ich war von Gefühlen des Verlusts überwältigt.“ Um seine Trauer zu verarbeiten, bürdete er sich ein großes Arbeitspensum auf, indem er sein Soloalbum "Back to the Light" vollendete und damit weltweit auf Promotion-Tour ging. Dies sei die einzige für ihn denkbare Selbsttherapie gewesen. Dazu gehörte auch das Mitorganisieren des Freddie Mercury Tribute Concert zu Ehren des verstorbenen Queen-Sängers, das am 20. April 1992 im Wembley-Stadion stattfand.

Ende 1992 bildete May die "Brian May Band", die ihn auf Solotourneen unterstützte. Ihr gehörten im Laufe der Jahre mehrere renommierte Musiker an, darunter Cozy Powell, Mike Moran, Neil Murray, Spike Edney, Jamie Moses, Miriam Stockley und Chris Thompson. Während eines Teils der US-Tournee 1993 war die "Brian May Band" die Vorgruppe von Guns n’ Roses. Ende Dezember 1993 kehrte May ins Studio zurück, um mit den verbliebenen Queen-Mitgliedern Roger Taylor und John Deacon am Album "Made in Heaven" zu arbeiten. Das Album erschien im November 1995; neben Mercurys letzten gesungenen Stücken enthält es auch von den drei übrigen Bandmitgliedern neu eingespielte Songs.

1995 begann May an einem neuen Solo-Album zu arbeiten; es sollte "Heroes" heißen und ausschließlich Coverversionen enthalten. Daneben war er mit verschiedenen Film- und Fernseh-Soundtracks sowie anderen Gemeinschaftsproduktionen beschäftigt. Nach einiger Zeit ließ er die Coverversionen fallen und konzentrierte sich auf neues Songmaterial. Ergebnis war das im Juni 1998 veröffentlichte Album "Another World". Die Vorbereitungen auf die dazugehörende kurze Promotionstournee der "Brian May Band" erlitten einen schweren Rückschlag, als Cozy Powell am 5. April 1998 bei einem Autounfall nahe Bristol ums Leben kam. Kurzfristig sprang Steve Ferrone als Ersatz ein. Für die eigentliche Tournee verpflichtete May Eric Singer. Nach deren Ende im November 1998 löste sich die "Brian May Band" auf.

Seit der Veröffentlichung seines letzten Albums im Jahr 1998 trat May entweder als Solokünstler, als Teil eines Ensembles oder gelegentlich zusammen mit Roger Taylor als Queen auf. Ab 2000 arbeitete er zusammen mit Taylor und Ben Elton an der Umsetzung des Musicals "We Will Rock You", das am 12. Mai 2002 im Dominion Theatre in London seine Premiere hatte. In den folgenden zwölf Jahren traten May und Taylor in dem Musical gelegentlich als Gastmusiker auf. Einen besonders denkwürdigen Auftritt hatte May am 3. Juni 2002 bei der "Party at the Palace", die zu Ehren des goldenen Thronjubiläums von Königin Elisabeth II. stattfand, als er auf dem Dach des Buckingham Palace stehend die Nationalhymne "God Save the Queen" als Gitarrensolo spielte. Im Juni 2005 wurde er für seine „Verdienste um die britische Musikindustrie“ zum Commander des Order of the British Empire ernannt.

May arbeitete intensiv mit der Bühnenschauspielerin und Sängerin Kerry Ellis zusammen, die er für die Londoner Originalbesetzung von "We Will Rock You" gecastet hatte. 2008 produzierte er ihre EP "Wicked in Rock", zwei Jahre später war er für die Produktion und das Arrangement ihres Debütalbums "Anthems" verantwortlich. Seither tritt er oft als Gastmusiker bei ihren Konzerten auf. Seine erfolgreichste Zusammenarbeit mit einem anderen Künstler (gemessen an der Chartsposition) war 2012 mit dem Rapper Dappy: Die Single "Rockstar", zu der er ein Gitarrensolo beisteuerte, erreichte Platz 2 in den britischen Single-Charts. Am 12. August 2012 trat May bei der Schlussfeier der Olympischen Sommerspiele 2012 auf: Er spielte zunächst "Brighton Rock", danach "We Will Rock You" zusammen mit Taylor und Jessie J.

Ende 2004 gaben May und Taylor bekannt, dass sie sich wiedervereinigen und auf Tournee gehen würden. Sie verpflichteten Paul Rodgers, den früheren Leadsänger von Free und Bad Company, der aber nicht als offizielles Queen-Mitglied in Erscheinung trat. Queen-Bassist John Deacon verzichtete, da er sich komplett aus dem Musikgeschäft zurückgezogen hatte, ließ aber verlauten, dass er sein Einverständnis gegeben habe. 2005 und 2006 gingen "Queen + Paul Rodgers" auf eine ausgedehnte Welttournee durch Europa, Nordamerika und Japan.

Am 15. August 2006 bestätigte May über seine Website und seinen Fanclub, dass "Queen + Paul Rodgers" im Oktober „an einem geheimen Ort“ mit der Produktion eines gemeinsamen Albums beginnen würden. Das Album mit dem Titel "The Cosmos Rocks" erschien am 12. September 2008. Die darauf folgende Tournee begann am selben Tag mit einem Gratiskonzert auf dem Freiheitsplatz in der ukrainischen Stadt Charkiw vor rund 350.000 Zuschauern. Sie führte anschließend durch Europa, die USA, Dubai und Südamerika. "Queen + Paul Rodgers" trennten sich offiziell am 12. Mai 2009. Rodgers schloss eine mögliche Zusammenarbeit zu einem späteren Zeitpunkt nicht aus. Fünf Jahre später meinte May jedoch, Rodgers’ Stimme sei eher nicht für Queen-Songs geeignet gewesen und passe eher zu Blues und Soul.

Am 20. Mai 2009 spielten May und Taylor beim Finale von American Idol live "We Are The Champions", zusammen mit dem Sieger Kris Allen und dem Zweitplatzierten Adam Lambert. Zweieinhalb Jahre später, bei den MTV Europe Music Awards 2011 in Belfast, spielten Queen und Lambert drei Songs. Fans und Medien bewerteten den Auftritt überaus positiv, was zu Gerüchten über eine mögliche zukünftige Zusammenarbeit führte. Im Februar 2012 bestätigten May, Taylor und Lambert eine Europa-Tournee. Die Tournee im Sommer 2012 führte durch Osteuropa und endete in London.

Im Sommer 2014 gingen "Queen + Adam Lambert" auf Welttournee und spielten in Nordamerika, Ostasien und Ozeanien. Es folgten eine Europatournee im Winter 2015 sowie eine Südamerika-Tournee im Herbst 2015. Unter anderem spielten Queen erstmals seit 1985 wieder beim Festival Rock in Rio. May, Taylor und Lambert setzten ihre Zusammenarbeit 2016 mit einer Europa-Tournee fort. Arbeiten an einem Studioalbum sind zurzeit nicht geplant.

2015 spielte Brian May als Gastmusiker ein Gitarrensolo bei dem Song "The Devil" auf dem Album "Bad Magic" der Band Motörhead, dem letzten Album der Band vor dem Tod von Lemmy Kilmister.

Brian May ist für den unverwechselbaren Gitarrensound der Gruppe verantwortlich. Dieser beruht zum einen auf der besonderen Bauweise seiner selbstgebauten Gitarre Red Special, zum anderen auf seiner individuellen Spieltechnik, die seiner Gitarre einen „singenden“ Klang verleiht. Bis Ende der 1970er Jahre erzeugte May mit seiner Gitarre alle Klangeffekte, die bei anderen Gruppen Synthesizer, Streichorchester oder Blechblasinstrumente übernehmen.

Burns London bot zum Preis von etwa 650,– € in der Zeit von Oktober 2001 bis Juli 2004 einen Nachbau der "Red Special" aus koreanischer Fertigung an. Dieser unterschied sich neben einigen optischen Details vom Original im Wesentlichen durch das einfacher gestaltete Vibratosystem. Seit 2004 vermarktet Brian May die Gitarre selbst unter dem Markennamen Brian May Guitars. Im Jahre 2006 wurde dieses Modell optisch noch mehr dem Original angepasst sowie qualitativ verbessert. Auch eine (Elektro-)Akustikgitarre, entworfen in Anlehnung an die Red Special, wurde in diesem Jahr vorgestellt. Außer diesen sehr eigenen Instrumenten sind auf der „Hardware“-Seite an Mays Gitarrensound ein von Bassist John Deacon gefertigter Verstärker für Studioaufnahmen („Deacy Amp“) und traditionelle britische Röhrenverstärker des Typs Vox AC30 in Kombination mit einem Effektpedal (Treble Booster) beteiligt.

Bei vielen Stücken nutzte May im Tonstudio die Overdub-Technik. So weist die Partitur von "Somebody to Love" fünf E-Gitarren-Stimmen auf. In den rein instrumentalen Stücken ("Procession", "The Wedding March", "God Save the Queen"), aber auch in Songs wie "Keep Yourself Alive", "Dreamer’s Ball", "Lazing on a Sunday Afternoon" oder "The Millionaire Waltz" kommt dieser „orchestral-gesangliche“ Gitarrenklang besonders deutlich zur Geltung. Eine weitere Steigerung dieser Gitarren-Effekte erzielte er in "Good Company" („Jazzband“); "The Loser in the End" („Rockorgel“), "All Dead, All Dead" und "Lily of the Valley" („Streichorchester“). In manchen Titeln erhält der Gitarrenklang eine Flexibilität ähnlich einem Chor oder einer Singstimme, so dass der Übergang zwischen Gesang und Instrument kaum zu bemerken ist, wie in den Stücken mit den extrem hohen „Screams“ von Roger Taylor ("Seven Seas of Rhye", "The March of the Black Queen", "The Fairy Feller’s Master-Stroke", "Ogre Battle", "Father to Son").

Zahlreiche Publikationen und Musiker bezeichnen May als virtuosen Gitarristen. Die Musikzeitschrift "Rolling Stone" setzte ihn auf Platz 26 der Liste der besten Gitarristen überhaupt. In einer Liste aus dem Jahr 2003 hatte er Rang 39 belegt. Im Gegensatz zu den meisten anderen Gitarristen verwendet May zum Anspielen der Gitarrensaiten kein Plektrum aus Kunststoff, sondern Münzen. Dabei bevorzugt er Sixpence-Münzen, insbesondere die letzte Prägeserie aus dem Jahr 1970, da deren Rigidität ihm mehr Kontrolle beim Gitarrenspiel erlaube. Außerdem ermögliche deren geriffelter Rand beim Anschlag eine wesentlich obertonreichere Tonentfaltung. Für seine Solotournee im Jahr 1993 ließ er bei der Royal Mint Nachprägungen mit seinem Konterfei herstellen; diese wurden auch als Fanartikel verkauft.

Zu seinen musikalischen Vorbildern in der Kindheit gehörten Lonnie Donegan, Tommy Steele, The Everly Brothers, Buddy Holly und The Crickets. Später kamen Cliff Richard und The Shadows, die Beatles, Jimi Hendrix, Jeff Beck und Eric Clapton hinzu. Besonders beeindruckt zeigte er sich von The Who und Led Zeppelin. Auch von Bluesmusikern ließ er sich inspirieren, insbesondere von Rory Gallagher.

May kann sich mit seinem Gitarrenspiel vielen Stilen anpassen: Von intimen, einfachen Liedern, wie "Love of My Life", die er klassisch auf der akustischen doppelchörigen Gitarre begleitete, bis hin zu den Hardrock-Titeln wie "Tie Your Mother Down" oder "Tear It Up". Bis zur Einführung des Synthesizers bei Queen war Brian May bei Studioaufnahmen auch auf anderen Instrumenten zu hören: Klavier in zahlreichen Songs, Harfe in "Love of My Life", Spielzeug-Koto in "The Prophet’s Song", Ukulele in "Good Company" und Banjolele in "Bring Back That Leroy Brown".

Seine Stimme ist weniger markant ausgeprägt als die von Freddie Mercury und setzt sich deutlich davon ab, so dass auf den Alben immer klar der Leadsänger herausgehört werden kann. In den für Queen typischen dreistimmigen Gesangsharmonien übernahm May üblicherweise den tieferen Begleitgesang. Auf einigen der von ihm geschriebenen Songs war er der Leadsänger, beispielsweise bei der ersten Strophe von "Who Wants to Live Forever", der letzten Strophe von "Mother Love" sowie der Bridge von "I Want It All" und "Flash". Alleiniger Leadsänger war er in "Some Day One Day", "She Makes Me (Stormtrooper in Stilettos)", "’39", "Good Company", "Long Away", "All Dead, All Dead", "Sleeping on the Sidewalk", "Leaving Home Ain’t Easy" und "Sail Away Sweet Sister". Auf den Queen-Alben stammten die meisten Songs entweder von May oder Mercury. Zu Mays bekanntesten Kompositionen gehören "We Will Rock You", "Tie Your Mother Down", "I Want It All", "Fat Bottomed Girls", "Flash", "Hammer to Fall", "Save Me", "Who Wants to Live Forever" und "The Show Must Go On".

May ist in seinen Bewegungen und Gesten sparsamer als manch anderer Rock-Gitarrist, was nicht zuletzt an seiner filigranen Fingertechnik liegt. Auf der Bühne überließ er Freddie Mercury die Show und sang ausschließlich die Nebenstimmen, auch bei den eigenen Titeln überließ er Mercury die Leadstimme. In der Regel sah man ihn nur mit Gitarre auf der Bühne. Gelegentlich spielte er auch Klavier ("Save Me", "Teo Torriatte") und Keyboard ("Who Wants to Live Forever"). Wie schon sein Vorbild Jimi Hendrix nutzt Brian May auch häufig das Feedback seiner Gitarre als Stilmittel. Darüber hinaus ist der Echo-Effekt (Delay) typisch für ihn. So kann er live mit sich selbst mehrstimmig spielen – insbesondere beim Solo von "Brighton Rock" (enthalten u. a. auf dem Album "Live Killers").

Brian May war neben Freddie Mercury der Hauptsongschreiber innerhalb der Band; mehr als zwei Drittel der Songs von Queen stammen von den beiden. Wenn nicht anders angegeben, wurden die folgenden von May komponierten Lieder von Mercury gesungen:

Neben den von 1989 bis 1991 bereits angeführten Titeln wurden weitere lediglich unter der Autorenangabe „Queen“ veröffentlicht, siehe dazu Queen – Die Autoren der Queen-Songs.

Ende 1968, während seiner Zeit bei Smile, lernte May die angehende Lehrerin Christine „Chrissie“ Mullen kennen. Die beiden heirateten 1974 und hatten zusammen einen Sohn und zwei Töchter; das Paar trennte sich 1988. Die Trennung und die spätere Scheidung erregten die Aufmerksamkeit der britischen Boulevardpresse, nachdem bekannt geworden war, dass er seit 1986 eine Affäre mit der Schauspielerin Anita Dobson hatte. Dobson war in den 1980er Jahren durch die Rolle der "Angie Watts" in der Seifenoper EastEnders national bekannt geworden. May und Dobson heirateten am 18. November 2000.

Gemäß der "Sunday Times Rich List" hatte May im Jahr 2011 ein Vermögen von 85 Millionen Pfund. Er ist Vegetarier und lebt in London sowie in Windlesham in der Grafschaft Surrey. Zu seinen Hobbys gehört das Sammeln von Zinnsoldaten und Star-Wars-Fanartikeln.

May studierte Physik und Infrarotastronomie am Imperial College in London und schloss als Bachelor of Science ab. Ab 1970 absolvierte er ein postgraduales Studium mit dem Ziel, als Ph.D. abzuschließen. Dabei untersuchte er Radialgeschwindigkeiten im interplanetaren Staub. Als 1974 der internationale Erfolg mit Queen einsetzte, brach May die Arbeit an seiner Dissertation ab. Er war aber Mitautor zweier wissenschaftlicher Arbeiten, die auf seinen Beobachtungen im Observatorio del Teide auf Teneriffa basieren. Diese erschienen in den Fachzeitschriften "Nature" und "Monthly Notices of the Royal Astronomical Society".

Im Oktober 2006 schrieb sich May wieder am Imperial College ein, um seine Dissertation zu vollenden. Er reichte sie im August 2007 ein, ein Jahr früher, als er ursprünglich angenommen hatte. Dabei musste er den Forschungsfortschritt im Bereich des interplanetaren Staubs in den vergangenen drei Jahrzehnten miteinbeziehen, darunter die vom Infrared Astronomical Satellite der NASA entdeckten Staubstreifen. Am 23. August 2007 bestand er die Disputation, woraufhin seine revidierte Doktorarbeit mit dem Titel "A Survey of Radial Velocities in the Zodiacal Dust Cloud" („Eine Untersuchung von Radialgeschwindigkeiten im interplanetaren Staub“) im folgenden Monat angenommen wurde – 37 Jahre nachdem er daran zu arbeiten begonnen hatte. Sein Doktorvater ist der Astrophysiker Paul Nandra. Die Einreichung seiner Doktorarbeit war nur möglich gewesen, weil in der Zwischenzeit kaum auf diesem Gebiet geforscht worden war. Seinen Doktortitel erhielt May offiziell im Rahmen einer Zeremonie des Imperial College, die am 14. Mai 2008 in der Royal Albert Hall stattfand.

Das Imperial College ernannte May im Oktober 2007 zum Gastwissenschaftler. Er beschäftigt sich weiterhin auf dem Gebiet der Astrophysik und nimmt regelmäßig an Veranstaltungen des Imperial College teil. Am 17. November 2007 ernannte ihn die John Moores University in Liverpool zum Kanzler. Dieses repräsentative Amt übte er von 2008 bis 2013 aus. Nach dem Vorschlag seines Freundes Patrick Moore benannte die Internationale Astronomische Union am 18. Juni 2008 den Asteroiden (52665) Brianmay nach ihm, dies wohl aufgrund der temporären Bezeichnung 1998 BM. Gelegentlich tritt May in der BBC-Fernsehsendung "The Sky at Night" auf, die sich mit Astronomie beschäftigt. 2014 war er Mitbegründer der weltweiten Aufklärungskampagne Asteroid Day, die vor den möglichen Gefahren von Asteroiden warnt. Während einer Pressekonferenz des Applied Physics Laboratory der Johns Hopkins University im Juli 2015 wurde May als Mitarbeiter jenes Teams vorgestellt, welches das NASA-Projekt "New Horizons" betreut. Anlässlich des Vorbeiflugs der Sonde an (486958) Arrokoth (damals inoffiziell "Ultima Thule" genannt) am 1. Januar 2019 veröffentlichte May ein offizielles Musikvideo mit dem Titel "New Horizons (Ultima Thule Mix)".

Zusammen mit Patrick Moore und Chris Lintott ist May Co-Autor zweier populärwissenschaftlicher Bücher. 2006 erschien "The Complete History of the Universe". Es erklärt die Geschichte des Universums, vom Urknall über die anhaltende Expansion des Universums bis hin zum möglichen Big Freeze in ferner Zukunft. 2012 folgte das Buch "The Cosmic Tourist: The 100 Most Awe-inspiring Destinations in the Universe" derselben Autoren, das die Phänomene des Weltalls anhand von 100 Beispielen erklärt. Für das im Jahr 2015 erschienene Buch "How to Read the Solar System: A Guide to the Stars and Planets" von Chris North und Paul Abel schrieb May das Vorwort.

May bezeichnete sich als ehemaligen Wähler der Conservative Party. Aufgrund deren Haltung zur Fuchsjagd und zur Massenkeulung von Dachsen habe er bei den Unterhauswahlen 2010 nicht mehr für sie gestimmt. In jenem Jahr gründete er die Tierschutzorganisation "Save Me", die sich für den Schutz aller Tiere gegen unnötige, grausame und entwürdigende Behandlung einsetzt. Ein besonderes Augenmerk legt sie darauf, dass der Hunting Act 2004 und andere Tierschutzgesetze weiterhin in Kraft bleiben. Im September 2012 wurde May zum Vizepräsidenten der Royal Society for the Prevention of Cruelty to Animals (RSPCA) ernannt. Er unterstützt auch den International Fund for Animal Welfare, die League Against Cruel Sports und PETA.

Im März 2012 schrieb May das Vorwort für ein Strategiepapier der Denkfabrik "Bow Group", das die Regierung dazu auffordert, auf die Keulung tausender Dachse zu verzichten. Dass diese Methode zur Eindämmung der Übertragung von Rindertuberkulose unwirksam sei, hätten bereits Jahre zuvor von der Labour-Regierung angeordnete Versuche gezeigt. Autor des Strategiepapiers war Graham Godwin-Pearson, ergänzt mit Beiträgen führender Tuberkuloseforscher wie John Krebs. Im April 2013 unterstützte May zusammen mit Hank Marvin eine vom französischen Gitarristen Jean-Pierre Danel gegründete Tierrechtsorganisation; zu diesem Zweck signierten sie Gitarren und Kunstfotos.

Im Mai 2013 schlossen sich May, die RSPCA, der Schauspieler Brian Blessed und der Cartoonist Jonti Picking zum „Team Badger“ zusammen, um gegen die geplante Massenkeulung von Dachsen zu protestieren. May, Blessed und Picking nahmen die Single "Save The Badger Badger Badger" auf – ein Mashup von Pickings Flash-Cartoon "Badger Badger Badger", der 2003 viral geworden war, und des Queen-Songs "Flash" mit Blesseds Gesang. Das dazu entstandene Video war eine Parodie sowohl von Pickings eigenem Cartoon als auch von Szenen des Films "Flash Gordon", dessen Soundtrack von Queen stammt. Die Single erreichte Platz 79 der UK Singles Charts, Platz 39 der UK iTunes-Charts und Platz 1 der iTunes Rock-Charts.

Vor den Unterhauswahlen 2015 erwog May ernsthaft, als unabhängiger Kandidat anzutreten. Es wurde auch berichtet, er habe ein Projekt gestartet, um „gute Sitten und Anstand in den Alltag und ins Parlament“ zurückzubringen. May sagte, er wolle die aktuelle Regierung „loswerden“ und wünsche sich ein Unterhaus, das sich aus Einzelpersonen zusammensetzt, die „gestützt auf ihr Gewissen“ abstimmen. May gehörte zu einer Gruppe von Prominenten, welche die Kandidatur der grünen Kandidatin Caroline Lucas bei diesen Wahlen unterstützten. Ebenso warb er für die Wiederwahl des konservativen Abgeordneten Henry Smith wegen dessen Einsatz für den Tierschutz.

Im Juli 2015 kritisierte May den britischen Premierminister David Cameron, weil er für eine Abstimmung über die Änderung des Fuchsjagdverbotsgesetzes in England und Wales den Fraktionszwang aufgehoben hatte. Während eines Live-Fernsehinterviews bezeichnete er die Countryside Alliance, die sich für die Wiedereinführung der Fuchsjagd einsetzt, als einen „Haufen lügender Bastarde“. Die Regierung verschob die Abstimmung nach einer Intervention der Abgeordneten der Scottish National Party, die angekündigt hatten, das Gesetz unverändert belassen zu wollen. Bei einer Protestkundgebung von Fuchsjagdgegnern vor dem Palace of Westminster sagte May, dies sei ein „sehr, sehr wichtiger Tag für unsere Demokratie“, fügte aber hinzu, dass sie „den Krieg noch nicht gewonnen“ hätten und dass es „keinen Platz für Selbstzufriedenheit“ gäbe.

2019 sprach er sich dafür aus, ein erneutes Live-Aid-Konzert zum Kampf gegen den Klimawandel abzuhalten.

May interessiert sich seit Jahrzehnten für die Stereoskopie des viktorianischen Zeitalters. 2009 gab er zusammen mit der Historikerin Elena Vidal das Buch "A Village Lost and Found" heraus, das sich mit dem englischen Stereoskopie-Pionier T. R. Williams befasst. Für seine Verdienste im Bereich der 3D-Fotografie erhielt er im September 2012 die Saxby-Medaille der Royal Photographic Society. May, der Vorsitzender der "Stereoscopic Society" ist, leistete einen bedeutenden technischen Beitrag zum Begleitbuch der Ausstellung "Stereoscopic Photographs of Pablo Picasso by Robert Mouzillat", die von Februar bis Juni 2014 im Holburne Museum in Bath stattfand.

Nach dem Kauf eines ersten Werks im Jahr 1973 begann May mit dem Aufbau einer Sammlung von "Diableries" – stereoskopische Fotos aus dem Frankreich der 1860er Jahre, die den „Alltag in der Hölle“ darstellen. Im Oktober 2013 brachte er zusammen mit Denis Pellerin und Paula Fleming das Buch "Diableries: Stereoscopic Adventures in Hell" heraus.
May erstellte eine 3-D Aufnahme des Asteroiden (162173) Ryugu aus zwei verschiedenen Aufnahmen vom 25. Juni 2018. Es gibt eine Video-Dokumentation von Brian May über die Entwicklung der Stereoskopie, die bei Sky GB ausgestrahlt wurde.

Studioalben




</doc>
<doc id="70857" url="https://de.wikipedia.org/wiki?curid=70857" title="Avicenna">
Avicenna

Abū Alī al-Husain ibn Abd Allāh ibn Sīnā (; geboren kurz vor 980 in Afschana bei Buchara in Chorasan; gestorben im Juni 1037 in Hamadan), auch Ibn Sina und vermutlich über eine hebräische Zwischenstufe wie "Aven Zina" latinisiert Avicenna, war ein persischer Arzt, Naturwissenschaftler, aristotelisch-neuplatonischer Philosoph, Dichter, sunnitisch-hanafitisch ausgebildeter Jurist bzw. Faqīh, Mathematiker, Astronom, Alchemist und Musiktheoretiker sowie Politiker.

Avicenna zählt zu den berühmtesten Persönlichkeiten seiner Zeit, tauschte sich philosophisch mit dem berühmten Gelehrten al-Biruni aus, galt bis weit ins 16. Jahrhundert als medizinisch-philosophische Autorität und hat insbesondere die Geschichte und Entwicklung der Medizin maßgeblich mitgeprägt. Einige seiner philosophischen Ausarbeitungen wurden von späteren Mystikern des Sufismus rezipiert. Zu seinen bedeutendsten Werken gehören das "Buch der Genesung" ("Kitāb aš-šifā’") und der "Kanon der Medizin" ("Qānūn fī aṭ-ṭibb").

Über Avicennas Leben erfahren wir vor allem etwas aus den Angaben in seiner von seinem Schüler Abu Ubaid Abd al-Wahid al-Dschuzdschani verfassten Biographie, deren erster Teil laut jenem von Avicenna selbst stamme, wobei es unklar ist, wann er seinem ihn 25 Jahre begleitenden Schüler den Bericht seiner Jugendjahre diktiert haben könnte. Avicennas Vater war ein aus der chorasanischen Stadt Balch stammender ismailitischer, die "Episteln" der „Lauteren Brüder“ (eine den Ismailiten nahestehende geheime Gelehrtengesellschaft, die sich auch mit Alchemie beschäftigte) lesender, Steuereintreiber, der sich im Dorf Afschāna bei Buchara im persischen Samanidenreich niederließ, dort eine hohe Verwaltungsposition im Staatsdienst innehatte und Abū Alīs Mutter Setāra heiratete. Abū Alī und danach sein Bruder Alī wurden in Afschāna geboren, anschließend zog die Familie (vermutlich um 986) in die Hauptstadt Buchara.

Da seine Muttersprache Persisch war, lernte Avicenna zuerst Arabisch, die damalige Verkehrssprache. Danach wurden ihm zwei Lehrer zugewiesen, die ihm den Koran und Literatur näher bringen sollten. Bereits im Alter von zehn Jahren beherrschte er den Koran und hatte viele Werke der schönen Literatur studiert und sich dadurch die Bewunderung seiner Umgebung erworben. Von einem gelehrten Gemüsehändler lernte er indisches Rechnen (später erfand er eine verbesserte Methode des Fingerrechnens). In die Rechtswissenschaften wurde Avicenna von dem „der Asket“ genannten hanafitischen Juristen Ismail eingeführt. Danach erhielt er Unterricht von dem Wander-Philosophen Abū ’Abdallāh an-Nātilī, der unter anderem eine Bearbeitung der Arzneimittelsammlung "De Materia medica" von Pedanios Dioskurides publiziert hatte. Bei an-Nātilī befasste sich Avicenna mit den Werken "Eisagoge" (eine Einleitung zu den Schriften des Aristoteles über Logik) von Porphyrios, "Elemente" von Euklid und "Almagest" von dem Astronomen Ptolemaios. Nachdem an-Nātilī nach Gurgandsch (heute Kunja Urgentsch), der nordwestlich von Buchara gelegenen Hauptstadt von Choresm, abgereist war, vertiefte Avicenna die folgenden Jahre autodidaktisch seine Studien in Jurisprudenz (Scharia), Philosophie sowie Logik und setzte sein Studium der Werke von Euklid und des Almagest fort. Er beschäftigte sich auch mit der Heilkunde, wandte sich im Alter von 17 Jahren intensiver der Medizin zu und studierte sowohl ihre Theorie als auch ihre Praxis. Zudem erhielt er Unterricht von al-Qumri (Abū Mansūr al-Hasan ibn Nūh al-Qamarī), dem Leibarzt des Samaniden al-Mansur ibn Nuh (al-Malik al-Muzaffar al-Amir as-Sadid Abu Salih Mansur (I.) b. Nuh, reg. 961–976). Mit etwa 18 Jahren soll er einen Samanidenstatthalter erfolgreich von einer schweren Erkrankung kuriert haben. Er beschrieb die Heilkunst als „nicht schwierig“. Avicenna vertiefte sich auch weiterhin in metaphysische Probleme, besonders in die Werke des Aristoteles, wobei ihm erst Schriften von Abu Nasr al-Farabi ("Über die Intentionen des Buches der Metaphysik", wohl preisgünstig durch Avicenna erworben, und/oder "Das Buch der Buchstaben", eine ausführlichere Schrift) zum Verständnis der Metaphysik verhalfen. Bei Unklarheiten auf dem Gebiet der Logik (etwa bei der Suche nach dem Mittelbegriff beim Syllogismus) soll er in der Moschee gebetet und um Eingebung gebeten haben, bei Müdigkeit oder Schwäche während seines Studium half im angeblich ein (nach den Regeln der hanafitischen Schule erlaubter) Becher Wein.

Da er sich bereits einen Ruf als Gelehrter und Heilkundiger erarbeitet hatte, nahm ihn um 996 der in Buchara regierende samanidische Emir Nuh ibn Mansur (Nūḥ ibn Manṣūr) (976–997), der Vater von Abd al-Malik II. als einen seiner behandelnden Arzt in seine Dienste auf. Dieser übertrug ihm dann auch Verwaltungstätigkeiten. Ihm wurde zudem erlaubt, die königliche Bibliothek mit ihren seltenen und einzigartigen Büchern, von denen er die der wichtigsten (griechischen) Autoritäten bis zu seinem 18. Lebensjahr gelesen hatte, zu nutzen. Emir von Buchara war nun Ibn Nuh (Abu l-Harith Mansur (II.) b. Nuh), der von 997 bis 999 regierte. Im Alter von 21 Jahren verfasste Avicenna sein erstes eigenes Buch, genannt "Die Sammlung" oder "Buch über die Seele in Form eines Kompendiums", welches er auf Anregung bzw. im Auftrag des in seiner Nachbarschaft wohnenden Abu l-Hasan al-’Arudi schrieb und das alle Wissenschaften außer der Mathematik enthalten soll. Ebenfalls auf Wunsch eines Nachbarn, der Avicenna in hanafitischen Recht unterrichtete, Abū Bakr al-Baraqī († 986), sollen das beinahe 20-bändige "Buch des Ertrags und Gewinns" und das "Buch der Rechtschaffenheit und der Sünde" entstanden sein. Von al-Baraqī schrieb Avicenna Gedichte ab.

Avicennas Vater starb 1002. Zu dieser Zeit wurde Avicenna in Buchara auch in Regierungsgeschäfte eingebunden. Wahrscheinlich hatte er Buchara bereits verlassen, als die Stadt 999 an die türkischen Karachaniden (geführt von Abu'l-Hasan Nasr b. Ali Arslan Ilek) fiel und sein erst neu eingesetzter Dienstherr (der Emir Abd al-Malik II.) in Gefangenschaft geraten war.

Es wird vermutet, dass Avicenna sich für einige Zeit dem letzten Samaniden, Ismail Muntasir (Ismāʿīl ibn Nūḥ al-Muntaṣir), der von 1000 bis 1005 regierte, angeschlossen hatte. Er begab sich 1005, nach der Ermordung Muntasirs durch Angehörige eines Araberstammes, somit nach Aussterben der samanidischen Dynastie und nachdem er seine Anstellung verloren hatte, über Nischapur und Merv in Chorasan nach Gurgandsch (auch Gurgentsch) im mit den Samaniden befreundeten Choresm am Aralsee. Dort trug er (wie schon in Gurgandsch) die Tracht eines Rechtsgelehrten, was ihn als Anwärter einer theologisch-juristischen Laufbahn als rechtsgelehrten "faqī" auswies. Über das reiche Oasengebiet südlich des Aralsees herrschte damals (von 997 bis 1009) der Emir ʿAlī ibn Maʾmūn als Schah (siehe auch Choresm-Schahs). In seiner Biographie hebt Avicenna hervor, dass am dortigen Hof der gebildete Wesir Abu l-Husain as-Suhaili ein Liebhaber der Wissenschaften gewesen sei. Eine Audienz beim Herrscher bewirkte keine Anstellung Avicennas am Hof. Avicenna, der das Wohlwollen von as-Suhaili hatte (1013 von ʿAlī ibn Maʾmūns Nachfolger ʿMaʾmūn ibn Maʾmūn abgesetzt), verfasste für diesen in Gurgandsch drei kleinere Abhandlungen zur Logik (in Gedichtform), Diätetik und, den Stillstand der Erde in der Mitte des Kosmos betreffend, Astronomie. Avicenna diente dann ʿAlī ibn Maʾmūn in Kath, bis er etwa 1012 aus Choresm floh, vielleicht um nicht in den Dienst des dort inzwischen wirkenden Sultans Mahmud von Ghazna, den Sohn des Sebüktigin, der Avicenna angeblich mit Hilfe eines Bildnisses hat suchen lassen, treten zu müssen (1017 eroberte Mahmud Choresm).

Bei seiner Flucht durch die Wüste Karakum soll Avicenna laut dem Geschichtenerzähler Nizamī-i Arūzī-i Samarqandī in Begleitung des christlichen Arztes Abū Sahl ʿĪsā ibn Yahyā al-Masihi al-Dschurdschānī gewesen sein. Nach erneuter Wanderung durch verschiedene Städte Chorasans (Nisā, Abiward, Tūs und Samanqān) kam er über Ğāğarm (Dschādscharm, englisch umschrieben Jajarm, in Nord-Chorasan) 1012 oder 1013 nach Gorgan (auch Dschurdschān; arabisch "Ǧurǧān") am Südrand des Kaspischen Meers, wo er viele seiner bedeutendsten Werke verfasste.

Angezogen hatte ihn der Ruhm des dortigen Herrschers Qabus ibn Voschmgir (oder Qābūs ibn Wušmagīr, kurz auch Wuschmagir) (reg. 977/978–981 und 997/998–1012/1013), der als Förderer von Literatur und Wissenschaft galt und bei dem sich auch al-Biruni aufgehalten hatte, der ab etwa 998 in einem Aristoteles ("Über den Himmel" und "Physikvorlesung") behandelnden Briefwechsel mit Avicenna (und dessen brieflich diesem sekundierenden Schüler Maʿṣūmī oder al-Maʿṣūmī, von den Truppen Mahmuds bei der Besetzung Reys 1029 ermordet) stand. Der Fürst aus der Dynastie der Ziyariden war jedoch kurz vor Avicennas Ankunft von Aufständischen im Winter 1012/1013 in einer Festung festgesetzt, wo er zu Tode kam. In Gorgan hielt Avicenna Vorlesungen in Logik und Astronomie, schrieb einen Teil des "Qānūn" und traf nach einem Aufenthalt in Dihistan in Gorgan, nun bis 1029 regiert von Falak al-Maali Manutschehr b. Qabus, auf seinen Freund und Schüler al-Dschuzdschani. In Gorgan wohnte er in einem ihm von einem privaten Gönner gekauften Haus. Diesem widmete er das philosophische "Buch des Ausgangs und der Heimkehr" und das "Buch der gesamten astronomischen Beobachtungen". 1014 (oder 1013) bewarb er sich mit einem in Gorgan ausgestellten Empfehlungsschreiben am Hof von Rey um eine Stellung.

Von 1014 bis 1015 hielt sich Avicenna als Arzt in Rey auf und stand im Dienst des noch minderjährigen Herrschers aus der schiitischen Buyiden-Dynastie, Madsch ad-Daula (997–1029), und dessen regierender verwitweter Mutter. Dort behandelte er den kleinen, an „Melancholie“ leidenden Fürsten. Avicenna gründete als – wie er sich selbst bezeichnete – "mutaṭabbib" (im 11. Jahrhundert soviel wie „praktizierender Arzt“) eine medizinische Praxis und verfasste 30 kurze Werke. Als Rey von ad-Daulas Bruder, Schams ad-Daula (regierte 997–1021), 1015 belagert wurde, sah sich Avicenna gezwungen, Rey zu verlassen und er ging über Qazwin nach Hamadan.

Avicenna wurde 1015 Leibarzt und medizinischer Berater des nun als Emir von Hamadan herrschenden Schams ad-Daula, dessen Kolik er vierzig Tage behandelte, wonach er zum Dank zum "nadīm" (ein mit „Zechgenosse“ übersetzbares Amt) ernannt wurde. Avicenna stieg, nachdem er den Herrscher auf einem unzufriedenstellend verlaufenem Kriegszug begleitet hatte und die Regierung daraufhin umgebildet wurde, schließlich sogar zu dessen Wesir auf. Eine Meuterei von Soldaten führte zu seiner Absetzung und Verhaftung, wobei Schams ad-Daula eine geforderte Hinrichtung Avicennas verweigerte. Doch als der Emir wieder einmal an einer Kolik litt, soll Avicenna zur Behandlung herangezogen und nach erfolgreicher Heilung freigelassen und wieder in sein altes Amt eingesetzt worden sein.

Sein Leben in jener Zeit war anstrengend: Tagsüber war er mit Diensten für den Emir beschäftigt, während er einen großen Teil der Nächte mit Vorlesungen und dem Diktieren von Notizen für seine Bücher verbrachte. Studenten sammelten sich in seinem Haus, um von al-Dschuzdschani aus Avicennas Hauptwerken, dem "Kitāb asch-Schifā" und dem "Qanun", vorgetragene Ausschnitte und die anschließenden Erläuterungen des Meisters zu hören. Daran schloss sich dann ein Symposion an, ein Weingelage bei dem auch Sänger erschienen.

Nach dem Tod Schams ad-Daulas (1021) nach einem wegen Erkrankung des Emirs abgebrochenen Kriegszug wohnte Avicenna im Haus eines Gewürzkrämers, von wo aus er dem Dynastiegründer, Erzfeind Hamadans und seit etwa 1008 regierenden Kakuyiden-Emir ʿAlā ad-Daula Muḥammad von Isfahan seine Dienste in einem Brief anbot, während er eine ihm angebotene erneute Anstellung als Wesir bzw. "nadīm" am Hamadaner Hof, nun unter einem der Söhne Schams’, abgelehnt hatte (Avicennas Nachfolger als Wesir wurde der Philosophie-Feind Tādsch al-Mulk). Nachdem al-Mulk von der geheimen Korrespondenz Avicennas mit dem Emir von Isafahan erfahren hatte, wurde Avicennas Unterschlupf denunziert, dieser geriet in den Verdacht des Hochverrats und wurde vom neuen Herrscher Hamadans in der nahegelegenen Festung "Fardadschān" eingekerkert. In Festungshaft verfasste Avicenna mehrere Schriften, darunter die (von Avicennas Schüler Abu Mansur ibn Zaila kommentierte und erklärte) allegorisch-mystische Erzählung vom "Ḥayy ibn Yaqẓān" (siehe auch "Der Philosoph als Autodidakt") mit Parallelen zu Dantes "Göttlicher Komödie" (Die Erzählung fand, vor allem in der Bearbeitung von Abraham ibn Esra als "Chaj ben Mekitz", „Der Lebendige, der Sohn des Wachenden“, auch Eingang ins Hebräische Schrifttum und liegt seit 1889 als europäischer Druck vor). Als ʿAlā ad-Daula vier Monate später gegen Hamadan marschierte (1023), kam Avicenna frei nachdem ad-Daula die Stadt wieder geräumt hatte, verließ mit dem ihm nun wieder gewogenen Wesir al-Mulk die Festung, setzte in einem Privatquartier seine Arbeit am "Buch der Genesung" fort und verfasste seinen Traktat über Herzmedikamente.

Avicenna, sein Freund und Biograf al-Dschuzdschani, sein Bruder und zwei sie begleitende Sklaven verließen, verkleidet als wandernde Derwische, Hamadan und zogen nach Isfahan. Während der Reise verfasste Avicenna eine Abhandlung "Über Schicksal und Vorherbestimmung". In Isfahan hieß ʿAlā ad-Daula Muhammad 1024 Avicenna am Kakuyidenhof willkommen. Avicenna wurde Leibarzt und auch hier wieder "nadīm" im Dienst des als Freigeist geltenden, sich über Religionsgesetze hinwegsetzenden Kakuyiden, den er auch in wissenschaftlichen und literarischen Fragen beriet. Ihm widmete er eine Zusammenfassung der Philosophie (statt in der arabischen Sprache der Wissenschaft) in der persischen Volkssprache. Diese kurzgefasste Enzyklopädie nannte er "Dāneschnāme-ye ‘Alā’ī" („Das Buch des Wissens für ʿAlā ad-Daula“), kurz auch "Dāniš-nāmeh". Außerdem begleitete er den zu seinem Freund Gewordenen auf strapaziösen Kriegszügen. Zudem wurden seine Kräfte wohl auch durch sein Sexualleben beeinflusst. In Isfahan vollendete er seinen "Kanon" und das "Buch der Genesung". Freunde rieten ihm, sich zu schonen und ein gemäßigtes, weniger unstetes Leben zu führen, aber das entsprach nicht Avicennas Charakter: „Ich habe lieber ein kurzes Leben in Fülle als ein karges langes Leben“ antwortete er. Während der Teilnahme an einem Feldzug gegen Masud I. von Ghazni erkrankte Avicenna 1034 an einer langwierig werdenden und mit schmerzhaften Koliken verbundenen Darmerkrankung. Im Juni 1037 starb, einige Tage nach einem weiteren Kriegszug mit ʿAlā ad-Daula (gegen Hamadan), der unverheiratete und kinderlose Avicenna erschöpft an den Folgen seines Darmleidens im Alter von 57 Jahren, vermutlich an der Ruhr oder an Darmkrebs. Angeblich wurde sein Ende durch die übermäßige Gabe eines Medikaments (ein Mithridatikum mit einer Überdosis Opium) durch einen seiner Schüler beschleunigt.

Avicenna wurde an der Stadtmauer von Hamadan in einem kleinen Grabmal beigesetzt, das 1877 erstmals renoviert wurde. Besonders bemüht um die Restaurierung des Monuments hatte sich der kanadische Mediziner und Medizinhistoriker William Osler. Von 1951 bis 1953 wurde ein neues, mit einem 64 Meter hohen Turm versehenes Mausoleum im Zentrum der Stadt fertiggestellt, wohin Avicennas Gebeine überführt wurden. Usbekische Anthropologen rekonstruierten anhand zweier Fotografien des Schädels von Avicenna seinen Kopf in Form einer Büste.

Schwerpunkte des literarischen Schaffens Avicennas bilden Texte zu Philosophie und Medizin.
Von 456 Titeln sind 258 (Stand: 1999) erhalten. Es wird behauptet, dass Avicenna 21 Haupt- und 24 Nebenwerke in Philosophie, Medizin, Theologie, Geometrie, Astronomie und anderen Gebieten vollendet hat. Andere Autoren schreiben Avicenna 99 Bücher zu: 16 über Medizin, 68 über Theologie und Metaphysik, 11 über Astronomie und 4 über das Drama. Die meisten von ihnen wurden in arabischer Sprache verfasst; aber auch in seiner Muttersprache Persisch schrieb er eine große Auswahl philosophischer Lehren, genannt "Dāneschnāme-ye ‘Alā’ī", und eine kurze, ʿAlā ad-Daula Muḥammad von Isfahan gewidmete, Abhandlung über den Puls.

Die unterschiedlichen Angaben dazu hängen zusammen mit der kurz nach Avicennas Tod einsetzenden Überlieferung von Texten unter seinem Namen, die sein Werk zwar im Kern enthalten, aber von Autoren unterschiedlicher Herkunft stammen. Die ursprüngliche Werkeliste in seiner Biografie enthielt etwa 40 Titel, deren Zahl sich mit der Entwicklung des unter seinem Namen tradierten Textkorpus auf über 200 erhöhte.

Eine die Grammatik des Arabischen behandelnde Schrift mit dem Titel "Die Sprache der Araber" blieb ein Entwurf.

Zwei unterschiedliche Erzählungen sind unter dem Titel "Salaman und Absal" und Avicennas Namen überliefert worden. Die eine davon war angeblich von Hunain ibn Ishāq aus dem Griechischen übersetzt worden und der berühmtgewordene Titel wurde später auch für sein gleichnamiges Epos von Dschāmi benutzt. Avicenna verfasste zudem eine allegorische Erzählung mit dem Titel "Die Vögel". Auch verschiedene Gedichte werden Avicenna zugeschrieben.

Etwa 100 Jahre nach Avicennas Tod fanden seine Schriften über lateinische Übersetzungen Eingang in die abendländische Rezeption. Nachweislich zum medizinischen Unterricht eingesetzt wurde Avicenna in Europa ab dem 14. Jahrhundert, nachdem Papst Clemens V. die Universität von Montpellier angewiesen hatte, unter anderem Schriften von Galen und Avicenna zu verwenden. Die ersten gedruckten Übersetzungen entstanden um die Wende vom 15. zum 16. Jahrhundert.

Der "Kanon der Medizin", arabisch "Qānūn at-Tibb" () ist eines der berühmtesten von Avicennas Werken, woraus auch sein Beiname "al-Qānūni" herrührt. Das von Schipperges als eine „Summa medicinae“ bezeichnete und eine Zusammenfassung und Systematisierung des damaligen medizinischen Wissensstandes darstellende Werk ist in fünf Bücher unterteilt:


Jedes Buch (persisch "kitāb") ist in "Fen" genannte Abschnitte ("funūn") weiter untergliedert und jeder "Fen" besteht aus Unterweisungen ("ta’līm", lateinisch "doctrinae"). Jede dieser Doktrinen gliedert sich in Summen ("ğumal", lateinisch "summae") und diese bestehen aus Kapiteln ("fuṣūl", Einzahl: "fasl")

Im Buch über die allgemeinen Prinzipien der Medizin stellt Avicenna, dessen Morphologie und Physiologie vor allem auf Galen beruhen, fest, dass diese den Säfteverhältnissen der Humoralpathologie und dem Kräftepotential des Organismus unterworfen sind, was als physiologische Grundlage für die Entstehung und Symptome von Krankheiten zu verstehen ist. Sowohl im "Kanon" als auch in anderen seiner medizinischen Werke zeigt Avicenna zudem Ansätze einer Psychosomatik.

Im die Medizin systematisch zusammenfassenden "Kanon der Medizin" wird beispielsweise beschrieben, dass Tuberkulose ansteckend ist und dass Krankheiten von Wasser und Erde übertragen werden können. Er gibt eine wissenschaftliche Diagnose von Ankylostomiasis (Hakenwurmbefall) und beschreibt die Bedingungen des Auftretens von Eingeweidewürmern. Zudem behandelte er die Drakunkulose, eine auch in der Gegend von Buchara vorkommende Parasitose mit Befall durch den Medinawurm. Der "Kanon" behandelt die Wichtigkeit diätetischer Maßnahmen, den Einfluss des Klimas und der Umwelt auf die Gesundheit und den chirurgischen Gebrauch von peroral zugeführten Anästhetika. Avicenna rät Chirurgen, Krebs in seinen frühesten Stadien zu behandeln und sicherzustellen, dass alles kranke Gewebe entfernt worden ist. Erstmals wird von ihm die Harnfistel, wie sie bei Verletzungen der Harnblase durch die Geburt auftreten kann, beschrieben. Des Weiteren wird die Anatomie des Auges richtig beschrieben, und es werden verschiedene Augenkrankheiten (wie Katarakt) beschrieben. Außerdem werden Symptome ansteckender und sexuell übertragbarer Krankheiten genannt sowie auch diejenigen von Diabetes mellitus. Bei einem lebensbedrohlichen Verschluss der Atemwege empfiehlt Avicenna den Luftröhrenschnitt. Das Herz wird als Pumpe aufgefasst, allerdings waren Avicennas Vorstellungen zur Herzanatomie und -physiologie eher auf Aristoteles als auf den diesbezüglich fortschrittlicheren Galen gegründet und gingen noch von antiken Vorstellungen einer „Bewässerung“ des Körpers und noch nicht von einem Blutkreislauf oder (wie bei Galen postuliert) gerichteten Bewegungen des Blutes außerhalb des Herzens aus. Auch die chirurgische Behandlung von Mastdarmfisteln, die Repositione ausgerenkter Gelenke und die geburtshilfliche Kindsentwicklung bei abnormen Geburtslagen werden von Avicenna beschrieben.

Die "Materia Medica" („Medizinisches Material“) des "Qānūn" enthält 760 Medikamente mit Angaben zu deren Anwendung und Wirksamkeit. Avicenna war der erste, der Regeln aufstellte, wie ein neues Medikament zu prüfen sei, bevor es Patienten verabreicht wird.

Avicenna bemerkte die enge Beziehung zwischen Gefühlen und dem körperlichen Zustand, befasste sich im Sinne der griechischen Humoralpathologie mit der positiven physischen und psychischen Wirkung der Musik auf Patienten und stellte auch Beziehungen der menschlichen Temperamente (deren Beschaffenheit auf dem Mengenverhältnis der Körpersäfte sowie dem bei deren Umwandlung beruhen) zu den unterschiedlichen modalen Tonsystemen und tradierten Melodien her, die sich heute noch in den Dastgahha der persischen und Maqamat der arabischen Musik finden. Zu den vielen psychischen Störungen, die er im "Qānūn" beschreibt, gehört auch die Liebeskrankheit. Wie es den volkstümlichen anekdotenhaften "Vier Abhandlungen" des Niẓāmī ʿArūḍī (um 1100/1160) heißt, habe Avicenna die Krankheit eines jungen Verwandten des Herrschers von Gorgan diagnostiziert, der bettlägerig war und dessen Leiden die örtlichen Ärzte verwirrte. Avicenna bemerkte ein Flattern im Puls des Jünglings, als er die Adresse und den Namen seiner Geliebten erwähnte. Der große Arzt hatte ein einfaches Heilmittel: Der Kranke sollte mit seiner Geliebten vereint werden. Allerdings war der mit Avicenna die Hochzeit ausrichtende Herrscher Qabus, der den aus Choresm gekommenen Arzt zu sich gerufen hatte, gar nicht mehr am Leben, als Avicenna in Gorgan eingetroffen war. Den Kern dieser Erzählung bildet eine ältere Wanderanekdote um den Arzt Erasistratos, der den Prinzen Antiochos behandelt.

Avicenna, der davon ausging, dass zur Zeugung eines Kindes ein Orgasmus auch bei der Frau erforderlich ist, äußert sich im "Kanon der Medizin" auch zu dazugehörigen Methoden. Eigene, teils polemische gegen Galen formulierte Überlegungen zur Zeugungslehre und Embryologie finden sich ebenfalls in seinem Werk.

Vor 1180 entstand, verfasst von Guido von Arezzo dem Jüngeren, ein "Liber mitis" genannter Purgiertraktat, der die medizinische Avicenna-Rezeption einleitete. Ebenfalls im 12. Jahrhundert (vor 1187) wurde der "Kanon" von Gerhard von Cremona in Toledo ins Lateinische übersetzt. Das Werk, von dem 1470 im gesamten Abendland 15–30 lateinische Ausgaben existierten, galt bis ins 17. Jahrhundert als wichtiges Lehrbuch der Medizin. 1491 wurde in Neapel eine hebräische Fassung gedruckt, 1593 wurde es als eines der ersten persischen Werke in Rom in arabischer Sprache gedruckt. In der zweiten Hälfte des 16. Jahrhunderts war – vermutlich auch infolge der Konfrontation mit den Türken – zugunsten der Lehren Galens, die („arabische“) Medizin Avicennas in den Lehrplänen rückläufig, kleinere Universitäten wie die in Frankfurt an der Oder, wo Avicenna noch 1588 der Vorzug gegeben wurde, ausgenommen. 1650 wurde der "Kanon" zum letzten Mal an den Universitäten von Löwen und Montpellier benutzt.

An der Universität Wien musste Pius Nikolaus von Garelli (Dr. med. et phil. der Universität Bologna) zur Aufnahme in die medizinische Fakultät noch am 18. Februar 1696 eine feierliche „Repetition“ über einen Abschnitt des Canon des Avicenna mit anschließender Argumentation der Gründe abhalten, die für oder wider die darin enthaltenen Thesen sprechen.

Avicenna beschäftigte sich in seinem Werk "Liber Primus Naturalium" mit der Frage, ob Ereignisse wie Krankheiten oder Missbildungen Zufallsereignisse sind und ob sie natürliche Ursachen haben. Er analysierte dies am Beispiel Polydaktylie. Seine Erkenntnis war: Wenn ein Ereignis selten ist, hat es unabhängig davon eine natürliche Ursache, auch wenn eine solche unnatürlich erscheint. Krankheiten oder Missbildungen werden von Avicenna am Beispiel Polydaktylie unter neuem Vorzeichen gesehen: Sie sind keine übernatürlichen und keine zufälligen Phänomene. Die Erkenntnis, dass solche Phänomene natürlich sind, ist ein fundamentaler Schritt in Richtung einer konsequent naturalistischen Betrachtung medizinischer Phänomene.

Neben dem "Kanon" und dem "Liber Primus Naturalium" gibt es noch 14 weitere medizinische Werke Avicennas, von denen acht in Versen geschrieben sind. Sie enthalten unter anderem die 25 Zeichen der Erkennung von Krankheiten, hygienische Regeln, nachgewiesene Arzneien sowie anatomische Notizen. Unter seinen Prosa-Werken fand die ab etwa 1023 entstandene Abhandlung über Herzmedikamente "De medicinis cordialibus" ("Medikamente für das Herz"), die gemeinsam mit dem durch an der venezianischen Botschaft in Damaskus tätigen Arzt Andrea Alpago von Belluno (1450–1521/1522) neuübersetzten und ergänzten "Kanon" 1521 und in einer Gesamtausgabe durch dessen Neffen Paolo Alpago 1527 publiziert wurde, besondere Beachtung. Auch Arnald von Villanova bearbeitete die "Medikamente für das Herz".

In einem Lehrgedicht gibt er die Empfehlung zur Mäßigung: „Hüte dich davor, immerzu betrunken zu sein. Und wenn es sich so ergibt, dann einmal im Monat“. Das für Studenten der Medizin zum leichteren Erlernen von Theorie und Praxis konzipierte "Lehrgedicht über die Heilkunde" ("Urğūza fi’ṭ-ṭibb") besteht aus 1326 Versen und behandelt die mit dem Messer, mit Medikamenten und diätetischen Maßnahmen durchzuführende ärztliche Praxis. Mit diesen einfachen Versen fasste er knapp die ganze Medizin zusammen. Jeder Vers (رجز, raǧaz) besteht aus einer Doppelzeile. Die Verbreitung von Avicennas Lehrgedicht zeigt sich zum einen in der Kommentierung durch Averroes in Andalusien und zum anderen in der Veröffentlichung als mit "Cantica Avicennae" betitelter Anhang zu lateinischen Ausgaben des "Kanon der Medizin".

Des Weiteren bearbeitete Avicenna den "Grabesbrief" (die sogenannte pseudohippokratische "Elfenbeinkapsel" aus dem angeblichen Grab des Hippokrates) poetisch – ebenfalls als "raǧaz"-Gedicht. Es handelt sich um 25, ähnlich den "hippokratischen Aphorismen" formulierten, den nahen Tod prognostizierenden Symptomkombinationen.

Dem im Jahr 1034 Isfahan plündernden Sultan Masud von Ghazna ("al-Masʿūd") hatte Avicenna in den 1030er Jahren einen noch erhaltenen, aber (1999) noch nicht edierten "Traktat zur sexuellen Potenz" gewidmet.

Avicenna beschäftigte sich auch mit Naturwissenschaften. Den Neigungen oder Anweisungen von ʿAlā ad-Daula Muhammad (seines Gönners in Isfahan ab 1024) folgend, schrieb er über Ergebnisse seiner Beobachtung der Gestirne und fügte diesen antike astronomische Tabellen hinzu. In der Astronomie arbeitete er seinem Schüler al-Dschuzdschani zufolge an Ptolemäus’ Sternenmodell und vermutete, dass die Venus der Erde näher stehe als die Sonne. Die Astrologie seiner Zeit kritisierte er, unter anderem weil ihre Brauchbarkeit nicht empirisch bzw. durch Berechnungen nachweisbar und sie mit der islamischen Theologie unvereinbar sei. Avicenna zitierte einige Passagen aus dem Koran, um dieses Urteil religiös zu untermauern.

Für die Tochter von Qabus (s. o.) soll er eine neue Methode der Längengradbestimmung entwickelt haben. Er entwickelte zudem eine dem späteren Jakobsstab ähnliche Visiervorrichtung.

Er beschrieb die Wasserdampfdestillation zur Erzeugung von Ölen bzw. öligen Auszügen. Andererseits stand er der Alchemie relativ skeptisch gegenüber. So glaubte er nicht an einen Stein der Weisen. Alchemistisch erzeugtes Gold sei, wie er in seinem "Kitab asch-Schifa" schrieb, nur eine Imitation und er bestritt die Gleichheit natürlicher und künstlicher Stoffe. Sie war in den frühen Übersetzungen oft der Meteorologica von Aristoteles beigefügt, da die Herausgeber sie für aristotelisch hielten, und übte einen beträchtlichen Einfluss auf die alchemistische Literatur aus als Gegenpol, gegenüber dem man sich rechtfertigte. Einige der ihm später zugeschriebenen alchemistischen Schriften sind spätere Unterschiebungen (wie "De anima in arte alkemia"), beeinflussten aber z. B. Roger Bacon.

In der Geologie gab er zwei Ursachen für die Entstehung von Bergen an: „Entweder entstehen sie durch das Aufbäumen von Erdschichten, wie es bei schweren Erdbeben geschieht, oder sie sind die Folge von Wasser, das neue Wege suchte und Täler herausgewaschen hat, wo weichere Gesteinsschichten zu finden sind […] Dies muss jedoch eine große Zeit in Anspruch nehmen, in der die Berge selbst geringer werden könnten.“

Auch in der Physik war Avicenna vielfältig tätig; so verwendete er Thermometer, um die Temperatur bei seinen Experimenten zu messen, und stellte eine Theorie über Bewegung auf. Darin befasste er sich mit der Kraft und der Bahnneigung eines Geschosses und zeigte, dass ein Geschoss sich in einem Vakuum ewig fortbewegt. In der Optik argumentierte er, dass die Lichtgeschwindigkeit endlich sei, und gab eine Beschreibung des Regenbogens (im Anschluss an Avicenna entwickelte Dietrich von Freiberg seine Theorie des doppelten Regenbogens).

Eine Schrift mit dem Titel "Richtmaß der Vernunft", das die fünf heronischen Grundformen behandelt, wird Avicenna zugeschrieben.

Avicenna beschäftigte sich ausgiebig mit philosophischen Fragen, sowohl mit Metaphysik als auch mit Logik und Ethik. Bereits in Buchara verfasste er in Folge seiner Beschäftigung mit Aristoteles erste philosophische Schriften. Seine Kommentare zu Werken des Aristoteles enthielten konstruktive Kritik an dessen Auffassungen und schufen Voraussetzungen für eine neue Aristoteles-Diskussion. Avicennas philosophische Lehren werden sowohl von westlichen als auch von muslimischen Forschern als weiterhin aktuell eingeschätzt.

Avicenna schrieb seine frühesten Arbeiten in Buchara unter dem Einfluss von al-Farabi. Das erste, ein "Kompendium über die Seele" (), ist eine kurze Abhandlung, die er den samanidischen Herrschern widmete und in der er sich mit neuplatonischem Gedankengut beschäftigte. Das zweite ist die "Philosophie für den Prosodisten" (), in der er sich mit der Metaphysik des Aristoteles auseinandersetzt.

Nach seinem Aufbruch aus Buchara verfasste Avicenna weitere philosophische Werke, darunter das sein philosophisches Hauptwerk darstellende, während seiner Wanderjahre (beginnend als Wesir unter Schams ad-Daula und das Werk weiterführend in seinem Unterschlupf bei einem Gewürzkramer in Hamadan) entstandene "Buch der Heilung" (), eine achtzehnbändige wissenschaftliche Enzyklopädie. Es handelt sich dabei hauptsächlich um „die Heilung der Seele von Irrtum“ und zeigt auch Wege zur Genesung von den „Krankheiten des Zweifels und der Verzweiflung“ auf (das von Avicennas Schüler al-Dschuzdschani endredigierte und mit einem Vorwort versehene Werk wurde auch als "Buch der Genesung" bezeichnet, enthält aber noch keine medizinischen Artikel). Das "Kitāb aš-šifā" behandelt überdies Arithmetik, Astronomie, Geometrie, Logik, Musik, Naturwissenschaften bzw. Naturkunde (Physik, Botanik und Zoologie, Geologie, Mineralogie, Geographie und Geodäsie), Philosophie und Psychologie. Der dritte Teil des Buchs enthält eine zusammenfassende Darstellung der "Elemente" des Euklid unter Einbezug späterer griechischer Mathematiker, eine verkürzte "Almagest"-Bearbeitung sowie einen Traktat über Arithmetik und Musik. Seine musiktheoretischen Ausführungen behandeln unter anderem die mathematische Erfassung der Intervalle, aber auch die Wirkung der verschiedenen Modi (siehe oben) auf den Menschen. Wie seinem diesbezüglichen Vorgänger al-Farabi diente auch Avicenna hierzu die Laute. Zu Avicennas Schülern auf dem Gebiet der Musiktheorie gehört Abu Mansur al-Ḥusain ibn Zaila († 1067), der Verfasser des die damalige Musikpraxis darstellenden Buches "Das Genügende in der Musik" ("Kitāb al-kāfī fi ʼl-mūsīqī").

Avicennas Werk wurde sowohl von hellenistischen Denkern wie Aristoteles und Claudius Ptolemäus als auch von arabischsprachigen muslimischen Geistes- und Naturwissenschaftlern wie al-Farabi und al-Biruni beeinflusst. Das Werk, insbesondere die Metaphysik Avicennas, stieß allerdings nicht überall in der islamischen Welt auf ungeteilte Zustimmung und es wurde, wie der Philosoph Ernst Bloch schreibt, immer wieder auch als ketzerisch verfolgt: „Avicennas philosophische Enzyklopädie wurde 1150 auf Befehl des Kalifen von Bagdad verbrannt; auch später wurde jedes erreichbare Exemplar vernichtet, vom Urtext gibt es nur Bruchstücke“. Im 12. Jahrhundert entstand an der Übersetzerschule von Toledo auch eine lateinische (Teil-)Übersetzung des Werkes, nämlich der "Naturkunde" ("Assepha" bzw. "Sufficientia" oder "Liber sufficientiae", deutsch: "Das Genügende") durch Abraham ibn Daud und Dominicus Gundisalvi. Die von Gundisalvi übriggelassen Teile des Buches übersetzte Adelard von Bath. Im Jahr 1215 wurde das Lesen des metaphysischen Teils des Buches durch einen Erlass des Robert von Courson in Paris verboten.

Das "Buch der Heilung" besteht aus vier größeren Traktaten, von denen der erste sich mit der Logik (gegliedert in acht Teile mit einer Einleitung folgenden logischen, dialektischen, rhetorischen und poetologischen Ausführungen) beschäftigt, der zweite vor allem mit Physik und anderen Naturwissenschaften, der dritte mit Mathematik (Geometrie, Astronomie, Arithmetik und Musik) und der vierte Traktat mit der Metaphysik.

Avicennas zweites Werk war das von ihm in persischer Sprache verfasste "Buch des Wissens für ‘Alā’ ad-Daula" (, mit vollem Namen auch "Ala ad-Daula Abu Dschafar Muhammad ibn Rustam Duschmanziyar"), in dem er seinem kakuyidischen Gönner in Isfahan eine Zusammenfassung seiner Philosophie auf der Grundlage des "Buchs der Heilung" bietet. Ein Teil dieser Arbeit erschien 1490 in Pavia.

Eine im Vergleich zum "Buch der Genesung" kleinere arabische Enzyklopädie stellt seine Schrift "Kitāb an-Nadschāt" („Buch der Rettung“) dar, welche in die Themengebiete Logik, Physik und Metaphysik gegliedert ist. Avicennas Schüler, Biograf und Redakteur al-Dschuzdschani, der sich selber mit Astronomie und Mathematik beschäftigte, trug seinem Interesse entsprechende Teile in Anlehnung an das "Buch der Genesung" zum "Buch der Rettung" bei.

Ein weiteres Werk, ein in Isfahan verfasster ausführlicher Kommentar zu Aristoteles, ist "Das Urteil" bzw. "Buch des ausgewogenen Urteils" (), das sich von den anderen Arbeiten durch seine Radikalität und seine Vermischung von aristotelischem Gedankengut und Neuplatonismus unterscheidet. Somit löste sich Avicenna damit von den antiken Autoritäten und stellte den Ansichten Aristoteles‘ seine eigenen Überlegungen gegenüber. Das 1034 durch die Truppen des Sultans Masud von Ghazna bei der Plünderung Isfahans (und des Quartiers von Avicenna) erbeutete Buch und 1151 in der Palastbibliothek von Ghazna, vermutlich mit weiteren Schriften, einem Brand zum Opfer gefallene Buch ist nur fragmentarisch in einer Parallelüberlieferung erhalten. Bei den erhaltenen Teilen handelt es sich um einen Abschnitt zu "Buch Lambda" der aristotelischen Metaphysik sowie um einen Kommentar zur auch im "Buch des ausgewogenen Urteils" kommentierten "Theologia Aristotelis" Plotins. Im "Buch des ausgewogenen Urteils" untersuchte Avicenna die unterschiedliche Behandlung philosophischer Fragestellungen durch die „Westlichen“ und „Östlichen“.

Sein letztes größeres Werk ist seine Schrift "Die östliche Philosophie", auch "Die Östlichen" genannt (), die er in den späten 1020ern schrieb. Die Schrift ist im inhaltlichen Aufbau an das "Buch der Heilung" angelehnt, ging aber über die Ansichten der gewöhnlichen aristotelischen Schule der Peripatetiker hinaus; ähnlich wie das "Buch des ausgewogenen Urteils" unterschied sie sich im Denkansatz von Aristoteles und den griechischen und arabischen Kommentatoren. Sie enthielt einen Überblick über Logik, eine Abhandlung über Metaphysik und Ausführungen zu Physik und Ethik. Das Werk ist weitgehend verloren, erhalten ist seine Einleitung und fragmentarisch der Abschnitt über die Logik.

Avicenna verfasste außerdem noch das kurze "Buch der Ratschläge und Erinnerungen" oder "Hinweise und Ermahnungen" (), ein bedeutendes Werk, das sein Denken über eine Vielzahl von logischen und metaphysischen Themen vorstellt.

Zu seinen politischen und ökonomischen Schriften zählt der Traktat "Über die Leitung" (des privaten Haushalts), der unter anderem die Knabenerziehung behandelt und die Unterordnung der Ehefrau sowie die Notwendigkeit der Sklavenhaltung begründet.

Die frühe islamische Philosophie, die sich noch eng am Koran orientierte, unterschied klarer als Aristoteles zwischen Wesen und Existenz. Avicenna entwickelte eine umfassende metaphysische Weltbeschreibung, indem er neuplatonisches Gedankengut mit aristotelischen Lehren verband. Das Verhältnis von Stoff und Form verstand er so, dass im "Stoff" (materia) die Möglichkeiten der "Formen" (essentiae) bereits enthalten sind. Gott sei notwendig an sich, alles andere Sein notwendig durch anderes. "«Gott ist das einzige Sein, bei dem Essenz (Wesen) und Existenz (Dasein) nicht zu trennen sind und das daher notwendig an sich ist.»" Alles andere Sein sei bedingt notwendig und lasse sich in Ewiges und Vergängliches unterteilen. Gott schuf durch seine geistige Tätigkeit die Welt. Der Intellekt des Menschen habe die Aufgabe, den Menschen zu erleuchten.

Avicenna über Aristoteles:
In der Frage der Ideen oder Allgemeinbegriffe vertrat Avicenna die auf Platon aufbauende These, dass diese "ante rem" (also vor der Erschaffung der Welt) bereits im Verstand Gottes, "in re" effektiv in der Natur und "post rem" auch in der menschlichen Erkenntnis zu finden seien. Mit dieser Unterscheidung zwischen "ante rem", "in re" und "post rem" wurde Avicenna für den abendländischen Universalienstreit von großer Bedeutung. Avicenna bestritt Gottes Interesse an Einzelereignissen sowie eine Erschaffung der Welt in der Zeit. In Berufung auf den Koran lehnte er auch die Idee einer vorgeburtlichen Existenz der menschlichen Seele ab, führte aber mit philosophischen Argumenten die Unsterblichkeit der menschlichen Seele ein. Diese Auslegung wurde bereits zu Lebzeiten von seinen orthodoxen Gegnern kritisiert, weil sich nach dieser Auffassung eine unendliche Menge menschlicher Seelen ansammeln müsse.

An der Metaphysik und der Seelenlehre Avicennas zeigte von seinen Schülern besonders der aserbaidschanische Zoroastrier Bahmanyār ibn Marzubān (gestorben 1066) Interesse. Auch Avendauth, der eine Schrift Avicennas über die Seele übersetzt hat, beruht in seiner Philosophie auf Avicenna.

Drei lateinische Fassungen der Metaphysik wurden 1493, 1495 und 1546 in Venedig gedruckt.

Bereits in der von al-Dschuzdschani notieren Autobiografie Avicennas ist ein "Kleines Kompendium der Logik" erwähnt, das in den ersten Band von Avicennas "Buch der Heilung" Eingang fand. In seinem philosophischen Hauptwerk "Buch der Genesung" nimmt die Logik mehr als ein Drittel des Umfangs ein (eine Bearbeitung dieses Werkteils nahm der im letzten Viertel des 13. Jahrhunderts gestorbene Astronom Nağmaddīn ʿAlī ʿUmar al-Qazwīnī al-Kātibī vor).

Avicenna widmete sich der Logik sowohl in islamischer Philosophie als auch in der Medizin mit großer Hingabe und entwickelte sogar ein eigenes logisches System, das auch als „Avicennische Logik“ bezeichnet wird. So war Avicenna wohl einer der ersten, die es wagten, Aristoteles zu kritisieren und von ihm unabhängige, sich stoischen Lehrsätzen annähernde Abhandlungen zu verfassen. Besondere Kritik erhielt die Schule von Bagdad von ihm, da sie sich zu sehr auf Aristoteles begründete. Eine grundlegende Rolle in Avicennas Logik dürfte das philosophische Werk Galens "Über den Beweis" gespielt haben.

Avicenna untersuchte die Theorien von Definition und Klassifikation sowie die Quantifikation von Prädikaten und kategorische logische Aussagen. Den Syllogismen, insbesondere den logischen Schlüssen, bestehend aus zwei Prämissen und einer Konklusion (Beispiel: Alle Menschen sind sterblich. Sokrates ist ein Mensch. Daher ist Sokrates sterblich.), gab er Veränderungsformen wie „immer“, „meistens“ oder „manchmal“ bei. In der Frage der Induktion bzw. Deduktion war Avicenna gewissermaßen gespalten. Während er in der Philosophie sich auf die Deduktion verließ, d. h. von einem allgemein gültigen Satz auf Spezialformen schloss (z. B. Alle Menschen sind sterblich – daher ist auch Sokrates sterblich), wendete er in der Medizin als einer der ersten die Methode der Induktion an.

Avicenna hatte in Buchara einen Großteil seiner Ausbildung für den Koran und die islamische Religion verwendet. Es heißt, er habe bereits mit 10 Jahren den Koran beherrscht. Er war bis zu seinem Tod ein gläubiger Faqī und nahm auch das islamische fünfmalige tägliche Gebet ernst. Er verfasste fünf Abhandlungen über verschiedene Suren, die generell voll Respekt sind. Nur seine philosophischen Tätigkeiten brachten ihn manchmal in Konflikt mit der islamischen Orthodoxie: Ausgehend von der Seelenlehre des Aristoteles differenzierte er die drei Seelenvermögen weiter aus und ordnete sie der Weltseele unter. Damit widersprach er zentralen Glaubensinhalten, was ihm die Feindschaft sunnitischer Theologen einbrachte. Wie die christlichen Scholastiker nach ihm versuchte Avicenna, die griechische Philosophie mit seiner Religion, die Vernunft mit dem Glauben zu verbinden. So benutzte er philosophische Lehren, um die islamischen Glaubenssätze wissenschaftlich zu unterlegen. In seiner Schrift "Über die Bestätigung des Prophetentums" geht er nicht auf alle Fragen der islamischen Prophetenlehre ein. Obwohl er sowohl Religion als auch Philosophie als zwei notwendige Teile der ganzen Wahrheit auffasste, argumentierte er, dass die islamischen Propheten mehr Bedeutung als die antiken Philosophen haben sollten.
Ein zentrales Problem seiner Theologie ist die Theodizee, die Frage nach der Existenz des Bösen in der ursprünglich von einem gütigen und allmächtigen Gott geschaffenen Welt. Da Gott ewig ist, der Mensch jedoch nur eine begrenzte Lebenszeit zur Verfügung hat, ist die sittliche Verantwortung des Menschen eine große Verantwortung, in der seine Würde liegt.

Der Kanon wurde um die Mitte des 12. Jahrhunderts von Gerhard von Cremona in Toledo ins Lateinische übersetzt. Indem Gerhard den Namenszusatz und ursprünglich seiner Regierungsfunktion entsprechende Ehrentitel "aš-šaiḫ ar-raʾīs" (auch "rajīs" und "al-raïs"), („oberster Scheich“, „seine Eminenz, der Minister“, „der Ehrwürdige“, „der Erhabene“, „der Fürst“) mit "princeps" („Fürst“) und im Explicit des Kanons mit "rex" („König“) übersetzte, trug er zu der besonders in Italien seit dem 14. Jahrhundert verbreiteten Legende bei, dass Avicenna ein „Fürst von Córdoba“ oder von Sevilla gewesen sei. Daher erscheint Avicenna in bildlichen Darstellungen oft mit Krone und Zepter und wurde auch in der islamischen Welt oft als „fürstlicher Meister“ (türkisch "Scheikü'r-Reis") dargestellt. Im Abendland wurde er auch lateinisch als "princeps medicorum" bezeichnet.

Zu Beginn des 14. Jahrhunderts übertrug Armengaud Blasius in Montpellier ein medizinisches Vershandbuch Avicennas in lateinische Prosa. Blasius’ Onkel, Arnald von Villanova (Dozent an der Universität Montpellier und päpstlicher Leibarzt) übersetzte 1306 den von Avicenna verfassten psychiatrischen Traktat "De viribus cordis".

Etwas früher noch als Gerhards Kanon-Übersetzung entstand in der Übersetzerschule von Toledo eine dem Erzbischof Johannes von Toledo (1151–1166) gewidmete Übersetzung des "Kitāb asch-Schifā", die zunächst durch den jüdischen Philosophen Abraham ibn Daud bzw. Avendauth "(Avendarith israelita philosophus)" aus dem Arabischen ins Spanische und dann durch Dominicus Gundisalvi aus dem Spanischen ins Lateinische übertragen wurde. Aus dieser Übersetzung hat besonders das sechste Buch über die Seele unter dem Titel "Liber sextus naturalium" die philosophischen Debatten der Scholastik seit der zweiten Hälfte des 13. Jahrhunderts nachhaltig geprägt. Eine selbständige Übersetzung speziell des achten Buches über die Tiere wurde in der Zeit nach 1220 von Michael Scotus in Italien angefertigt und Friedrich II. gewidmet: ein in Melfi entstandenes, kaiserlich autorisiertes Exemplar ist im Kolophon auf den 9. August 1232 datiert.

Avicennas Kompendium "Dāneschnāme-ye ʿAlā’ī" wurde zwar nicht direkt ins Lateinische übersetzt, wurde jedoch indirekt einflussreich für die lateinische Tradition, nämlich dank der Verwendung durch al-Ghazālī als Vorlage für dessen Schrift "Maqāṣid al-falāsifa" (Die Absichten der Philosophen, 1094), in der dieser seinem Angriff auf die Lehren Avicenna, al-Farābīs und anderer „Philosophen“ ("Tahāfut al-falāsifa", Die Inkohärenz der Philosophen, 1095, lat. Destructio philosophorum) zunächst eine Darstellung von Grundbegriffen der Logik, Metaphysik, Theologie und Physik aus den Lehren dieser Philosophen vorhergeschickt hatte. "Maqāṣid al-falāsifa" wurde bereits in der ersten Hälfte des 12. Jahrhunderts in Toledo ins Lateinische übersetzt, wohl von Dominicus Gundisalvi, und kursierte dann in einer der Handschriften unter dem Titel "Liber Algazelis de summa theoricae philosophiae". Die lateinischen Leser kannten die Abhängigkeit von Avicennas "Dāneschnāme-ye ʿAlā’ī" nicht, sondern hielten das Buch für eine Darlegung genuiner Lehre al-Ghazālīs, was dann dazu führte, dass der letztere auch von solchen Autoren besondere Wertschätzung erfuhr, die mit der von ihm bekämpften Traditionslinie sympathisierten.

Avicenna unrichtig zugeschrieben wurde eine unter dem Titel "Liber Avicennae in primis et secundis substantiis et de fluxu entis" oder auch "De intelligentiis" verbreitete, platonisierende Schrift des 12. Jahrhunderts, die unter anderem aus Pseudo-Dionysius Areopagita, Augustinus und Avicenna schöpft und jedenfalls von einem christlichen lateinischen Autor stammt, wahrscheinlich von Dominicus Gundisalvi. Avicenna zugeschrieben wurde ferner auch ein "Liber de causis primis et secundis", der in der Nachfolge des pseudo-aristotelischen "Liber de causis" steht und ebenfalls im 12. Jahrhundert in Toledo entstand.

In der lateinischen Scholastik wurde Avicenna zu dem – nach Averroes – angesehensten Vertreter der islamischen Philosophie und Vermittler der aristotelischen Philosophie und Naturkunde. Seine Werke wurden nicht nur an den Artistenfakultäten und von Theologen wie Thomas von Aquin (etwa in "De ente et essentia", deutsch: "Über das Sein und das Wesen") und Johannes Duns Scotus, sondern seit dem ausgehenden 13. Jahrhundert auch und besonders an den medizinischen Fakultäten, und dort dann sowohl unter medizinischen wie auch philosophischen Fragestellungen rezipiert, wobei besonders Montpellier in Frankreich und Bologna in Italien eine Schlüsselrolle spielten. In Montpellier gehörte der Kanon seit 1309 (und bis 1557) zum medizinischen Pflichtprogramm. In Bologna wurde die Rezeption maßgeblich von Taddeo Alderotti († 1295), Professor seit 1260, initiiert, dessen Schüler Dino del Garbo die Ansätze in Bologna, Siena, Padua und Florenz weiterführte. Dinos Schüler Gentile da Foligno wiederum, der vornehmlich in Siena und Perugia wirkte, verfasste den ersten annähernd vollständigen lateinischen Kommentar des Kanon, ein Unterrichtswerk, das dann bis ins 16. Jahrhundert große Wirkung entfaltete.

Neue lateinische Übersetzungen des Kanon und weiterer, bis dahin zum Teil unübersetzter Schriften Avicennas fertigte Andrea Alpago († 1521 oder 1522) aus Belluno an. Alpago war rund dreißig Jahre lang als Arzt an der venezianischen Gesandtschaft in Damaskus tätig und studierte dort arabische Handschriften der Werke von Avicenna und Averroes und ihrer arabischen Kommentatoren. Seine Bearbeitung des Kanon, die 1527 erstmals im Druck erschien, entstand als kritische Revision und Glossierung der etablierten Übersetzung von Gerhard von Cremona. Sie wurde seit der Erstausgabe in mehr als 30 Neuauflagen und Neuausgaben gedruckt. Der Kanon blieb bis ins 17. Jahrhundert eines der Hauptwerke der medizinischen Wissenschaft.

Dante Alighieri (1265–1321), der bereits im "Gastmahl" Avicenna zu Wort kommen ließ, versetzt in seiner (von Avicennas oben erwähntem Werk "Ḥayy ibn Yaqẓān" bzw. der hebräischen Version "Chaj ben Mekitz" nicht unabhängigen) "Göttlichen Komödie" ("Inferno" 4,143) Avicenna zusammen mit seinen beiden muslimischen Glaubensbrüdern Averroes und Saladin in das „edle Schloss“ "(nobile castello)" im Limbus der Hölle, wo ansonsten nur Personen der vorchristlichen heidnischen Antike, insbesondere Philosophen und Dichter der griechischen und römischen Welt, angesiedelt sind: er teilt dort mit ihnen das Schicksal, durch eine tugendhafte Lebensführung zwar der ewigen Verdammnis entgangen zu sein, da er sonst in einem der tieferen Kreise der eigentlichen Hölle zu strafen wäre, zugleich aber mangels Teilhabe am Sakrament der Taufe von der Erlösung ins "Paradiso" ausgeschlossen zu sein und deshalb einen Zustand ohne Strafe, aber in ewiger Gottesferne, erleiden zu müssen. Dass er und seine beiden Glaubensbrüder im Unterschied zu ihren heidnischen Leidensgenossen der vorchristlichen Zeit die christliche Lehre bereits kannten und sich zur Taufe hätten entscheiden können, ihr Beharren in einem anderen Glauben folglich auf eigener Wahl beruhte und sie trotzdem nicht mit ihren übrigen Glaubensbrüdern zur Strafe in einem tiefer gelegenen Kreis der Hölle verdammt sind, bringt die besondere Wertschätzung zum Ausdruck, die Dante ihnen entgegenbrachte.

Phantasieporträts Avicennas befinden sich unter anderem in der Halle der medizinischen Fakultät der Sorbonne, auf dem tadschikischen 20-Somoni-Geldschein und im Mailänder Dom in einem Kirchenfenster, gestiftet 1479 von der Apothekerzunft Mailands.

Weiters befinden sich Statuen Avicennas im tadschikischen Duschanbe und an seinem Geburtsort in Afschana bei Buchara im heutigen Usbekistan.
Usbekische Anthropologen hatten anhand zweier Fotografien des Schädels von Avicenna (siehe oben) seinen Kopf in Form einer Büste rekonstruiert.

Bereits Niẓāmī ʿArūḍī, ein persischer Dichter aus Samarkand, verherrlichte im 12. Jahrhundert in anekdotenhaften Geschichten die medizinischen Fähigkeiten Avicennas. Legendäre Erzählungen ranken sich in der Volksliteratur um den berühmten Arzt, dem auch magische Kräfte angedichtet wurden, und Inhalte von Avicennas "Kanon" werden sogar in den Märchen aus "Tausendundeiner Nacht" (so in der 134. und der die Erzählung von Sklavin Tawaddud enthaltenden 449. Nacht) erwähnt. In einem türkischen Volksroman werden wundersame Abenteuer, die Avicenna mit einem angeblichen Zwillingsbruder Abu l-Ḥāriṯ erlebt, geschildert. In den englischen Canterbury Tales aus dem 14. Jahrhundert gehört „Avicen“ ebenso zur Standardliteratur für Ärzte wie der „Galien“. Der tatarische Pädagoge und Schriftsteller Kajum Nasyri (1824–1904) überlieferte in russischer Übersetzung eine volkstümliche türkische Erzählung über Avicenna. Auch in der Moderne wird Avicenna in der Belletristik rezipiert. So studiert in Noah Gordons Bestseller "Der Medicus" der Protagonist des Romans bei Avicenna Medizin. Im historischen Roman "Die Straße nach Isfahan" von Gilbert Sinoué ist Avicenna die Hauptfigur; geschildert wird sein gesamter Lebensweg.

Seit Beginn des Wintersemesters 2014/15 fördert das Avicenna-Studienwerk muslimische Studenten mit einem staatlichen Stipendium. Es ist damit das 13. Begabtenförderungswerk in Deutschland und unter diesen, neben dem katholischen Cusanuswerk, dem Evangelischen Studienwerk Villigst und dem jüdischen Ernst Ludwig Ehrlich Studienwerk, das vierte konfessionell geprägte.

Im Jahr 2005 wurde in Deutschland der Avicenna-Preis-Verein von Mitgliedern aus Wissenschaft, Politik und Gesellschaft unter der Initiative von Yaşar Bilgin, dem Vorsitzenden der Türkisch-Deutschen Gesundheitsstiftung, gegründet. Der Preis wird in regelmäßigen Abständen verliehen und zeichnet Initiativen von Personen oder Institutionen zur interkulturellen Verständigung aus. Mitglieder der Jury sind hochrangige Mitglieder aus Politik und Gesellschaft. Im Jahr 2012 gehörte der Jury unter anderem Rita Süssmuth, die ehemalige Präsidentin des Deutschen Bundestages, an.

Erstmals wurde der Preis im Jahr 2009 an die Allianz der Zivilisationen, engl. Alliance of Civilizations (AoC), verliehen, die 2005 auf Initiative der spanischen und türkischen Regierungen unter der Schirmherrschaft der Vereinten Nationen gegründet wurde. Stellvertretend nahm ihn der frühere portugiesische Staatschef Jorge Sampaio entgegen. 2012 wurde der Preis an die Friedensnobelpreisträgerin aus dem Jahr 2003, die iranische Rechtsanwältin und Menschenrechtsaktivistin Shirin Ebadi, in der Frankfurter Paulskirche verliehen.

Carl von Linné benannte ihm zu Ehren die Gattung "Avicennia" aus der Pflanzenfamilie der Akanthusgewächse (Acanthaceae). Auch die Avicenna Bay in der Antarktis trägt seinen Namen.

Der Mondkrater Avicenna und der Asteroid des äußeren Hauptgürtels (2755) Avicenna sind ebenfalls nach Avicenna benannt.

lateinisch (Renaissance)

lateinisch (modern)

deutsch

arabisch

französisch

italienisch

englisch






</doc>
<doc id="80196" url="https://de.wikipedia.org/wiki?curid=80196" title="Molekularstrahlepitaxie">
Molekularstrahlepitaxie

Molekularstrahlepitaxie (, MBE) ist ein Verfahren der physikalischen Gasphasenabscheidung (PVD), um kristalline dünne Schichten (bzw. Schichtsysteme) herzustellen.

Das Verfahren wird vor allem in der Halbleitertechnik verwendet, unter anderem um einkristalline Strukturen aus Halbleiterverbindungen wie Galliumarsenid (GaAs), Indiumphosphid (InP), GaInNAs, Galliumantimonid (GaSb) auf einem Substrat zu erzeugen.

Sie wurde Ende der 1960er Jahre an den Bell Laboratories durch Alfred Y. Cho und John R. Arthur entwickelt.

"Epitaxie" bedeutet, dass die Kristallstruktur der aufwachsenden Schicht sich der des Substrates anpasst, solange die physikalischen Eigenschaften (insbesondere die Gitterparameter) der beiden Substanzen nicht zu stark voneinander abweichen. Man spricht von "Homoepitaxie", wenn Substrat und Schicht aus der gleichen Verbindung bestehen, ansonsten von "Heteroepitaxie". Bei der Heteroepitaxie kommt es wegen der im Allgemeinen unterschiedlichen Gitterparameter zu Verspannungen in der aufgewachsenen Schicht. Ab einer kritischen Schichtdicke bilden sich Versetzungen (Defekte) und die Verspannung klingt exponentiell ab.

MBE setzt ein Ultrahochvakuum voraus, um Verunreinigungen durch Restgasatome zu vermeiden. Während des Wachstumsprozesses steigt der Druck aber bedingt durch die Effusion in den Hochvakuumbereich. Die Stoffe, aus denen die Schicht bestehen soll, werden in Evaporationstiegeln "(Effusionszellen)" erhitzt und gelangen als gerichteter Molekularstrahl (ohne Stöße mit dem Hintergrundgas) zum Substrat. Dieses wird ebenfalls geheizt und erlaubt so ein geordnetes Anwachsen der Schicht.

Durch Steuerung der Tiegeltemperaturen und kontrolliertem Öffnen und Blockieren des Molekularstrahls einzelner Quellen können komplizierte Mehrschichtstrukturen mit wechselnden Zusammensetzungen und Dotierungen hergestellt werden. Die Schichtdicken können wenige Atomlagen (also weniger als ein Nanometer) bis Mikrometer betragen.

Der MBE-Prozess kann durch geeignete In-situ-Verfahren (RHEED, Ellipsometrie), welche den Wachstumsprozess nicht beeinflussen, kontrolliert und gesteuert werden.

 

Die Molekularstrahlepitaxie findet vor allem bei der Herstellung optoelektronischer Bauelemente Verwendung, unter anderem bei Laserdioden, dielektrischen Spiegeln oder Quantenkaskadenlasern.

Durch die genaue Schichtdickenkontrolle lassen sich außerdem Strukturen mit sehr kleinen räumlichen Abmessungen verwirklichen, wie sie für die Nanotechnologie typisch sind. Diese haben neuartige Eigenschaften, die auf Quantenphänomenen basieren. Dabei werden häufig natürliche Rauhigkeiten oder Selbstorganisation innerhalb der Grenzschichten bei Heteroepitaxie ausgenutzt. Besonders die Verspannung epitaktisch gewachsener Heterostrukturen führt zu null-, ein- und höherdimensionalen Strukturen wie den schon erwähnten

In der Grundlagenforschung wird MBE deshalb auch zum Wachstum von verspanntem Si/SiGe eingesetzt. Mit dieser Technologie sollte es in Zukunft möglich sein, sogenannte HEMTs in Si/SiGe-Technik (MODFETs genannt) zu realisieren und Kosten durch die Verwendung von Materialien wie zum Beispiel GaAs einzusparen.

Durch Verdampfen können auch wohlgeordnete Schichten organischer Moleküle auf atomar ebenen anorganischen Oberflächen hergestellt werden. Dieses Verfahren wird auch als "organische Molekularstrahlepitaxie" (engl. "", OMBE) bezeichnet.

In der anorganischen Chemie wird die Tieftemperatur-Molekularstrahlexpitaxie zur Synthese thermodynamisch instabiler, aber kinetisch gehemmter Stoffe eingesetzt.

Ein spezielles MBE-Verfahren ist die Allotaxie, mit deren Hilfe sich zum Beispiel vergrabene
Cobaltdisilicid-Schichten in monokristallinem Silicium herstellen lassen.





</doc>
<doc id="88990" url="https://de.wikipedia.org/wiki?curid=88990" title="Kristallmorphologie">
Kristallmorphologie

Die Kristallmorphologie ist ein Begriff aus der Kristallographie und der Mineralogie und beschreibt die Form eines Kristalls, der aus geometrisch bestimmten Flächen, Kanten und Ecken besteht. Zwei aneinander stoßende Kristallflächen bilden dabei eine Kristallkante und mindestens drei Kanten bilden eine Kristallecke. Je nach Kristallsystem und Kristallklasse schließen die Kanten dabei bestimmte, für die betreffende Kristallklasse charakteristische Winkel ein.

Um die Lage von Kristallflächen und -kanten im Raum mathematisch zu beschreiben, bedient sich der Mineraloge verschiedener Indizes. Mit den millerschen Indizes "(hkl)" wird dabei die Lage der Flächen in Bezug auf das Achsensystem des Kristalls beschrieben, mit den Richtungsindizes "[uvw]" die Richtung der Kanten.

Die Kristallflächen bilden die äußere Begrenzung des Kristallkörpers und liegen parallel zu den Gitter- beziehungsweise Netzebenen der dem Kristall innewohnenden Kristallstruktur, die wiederum von seiner chemischen Zusammensetzung abhängt. Durch Symmetrieoperationen ineinander überführbare kristallographische Flächen heißen "Form" (siehe unten: Abschnitt "Kristallform"). Die Kombination der ausgebildeten Formen heißt Tracht. Dieser Begriff ist nicht zu verwechseln mit dem Habitus, der die geometrische Ausdehnung eines Kristalls beschreibt. Der Habitus kann z. B. stängelig, faserig, plattig oder isometrisch sein.

Kristallflächen bilden sich bevorzugt an den Netzebenen, die die dichteste Packung (größte Anzahl von Atomen) und gleichzeitig möglichst wenig freie, offene Valenzen aufweisen (chemische Neutralität). Bei idealen Kristallen besitzen die Flächen eine klare Geometrie (Dreieck, Viereck, Sechseck) und bilden zusammen regelmäßige Körper (siehe platonischer Körper, catalanischer Körper). Bei Kristallen, die ihre Eigengestalt störungsfrei voll entwickeln konnten, spricht man auch von "idiomorphen" Kristallen. Welche Gestalt dies ist, bestimmt die jeweilige Kristallklasse, der der Kristall angehört. Ein idiomorpher Kristall ist daher nicht gleichzusetzen mit einem holoedrischen Kristall.

Natürliche Kristalle bilden sich aber nur selten in der idealen Form, bedingt durch Störungen der Stoffzufuhr und das anisotrope Verhalten beim Wachsen des Kristalls während der Kristallisation oder durch gegenseitige Behinderung bei einer Vielzahl gleichzeitig entstehender Kristalle, beispielsweise bei schneller Abkühlung. Kristallflächen können somit verzerrt sein oder sogar ganz fehlen. Sie stehen jedoch zum einen immer konvex zueinander, was bedeutet, dass sie sich vom Kristallmittelpunkt „wegbeugen“ und zum anderen bleiben trotz aller Verformungen die Winkel zwischen den Flächen immer konstant. Bei Kristallen, die ihre Eigengestalt aufgrund der genannten Störungen nur teilweise entwickeln konnten, spricht man von "hypidiomorphen" Kristallen.

Wird ein Kristall während des Wachstums so sehr gestört, dass er seine Eigengestalt gar nicht entwickeln konnte, spricht man von "xenomorphen" Kristallen.

In der Kristallographie bezeichnet man als Form (seltener "Kristallform" oder "Flächenform"; engl. "form", "crystal form", "face form", frz. "forme") die Gesamtheit aller zueinander symmetrieäquivalenten Kristallflächen. Eine Kristallform wird mit dem Symbol {hkl}, also den millerschen Indizes (hkl) einer der Flächen in geschweiften Klammern, bezeichnet. Man beachte, dass sich die kristallographische Bedeutung des Begriffs Kristallform deutlich von der umgangssprachlichen Bedeutung unterscheidet: die „Form“ eines Kristalls im umgangssprachlichen Sinn wird eher durch die Begriffe Tracht und Habitus beschrieben. Ein einzelner Kristall hat genau eine Tracht und einen Habitus, aber in der Regel mehrere (kristallographische) Formen.

Kristallformen können geschlossene Körper (Polyeder), wie Würfel oder Oktaeder, bilden. Daneben gibt es aber auch offene Formen, wie das Pinakoid, aber auch Prismen und Pyramiden. Solche offenen Formen müssen an einem Kristall notwendig mit anderen Formen kombiniert sein. Man beachte, dass die Basisflächen von Prismen oder Pyramiden – anders als in der Schulgeometrie – "nicht" zur Form dazugehören; sie sind nicht symmetrieäquivalent zu den eigentlichen Prismen- oder Pyramidenflächen. Ein tetragonales (quadratisches) Prisma hat in der Kristallographie also vier, nicht sechs Flächen.

Insgesamt gibt es 17 offene (18, wenn man das Dieder in Doma und Sphenoid unterteilt) und 30 geschlossene Sorten kristallographischer Formen.

Man unterscheidet daneben "allgemeine Formen" und "spezielle Formen", außerdem "Grenzformen". Die allgemeine Form {hkl} geht aus einer „Fläche allgemeiner Lage“ (hkl) hervor, das heißt die Fläche liegt nicht parallel oder senkrecht zu einer Spiegelebene oder senkrecht zu einer Drehachse; die Indizes h, k, l sind im Allgemeinen nicht null und paarweise verschieden. Andernfalls handelt es sich um spezielle Formen. Grenzformen nehmen eine Zwischenstellung ein: Sie haben die gleiche Flächenzahl und Symmetrie wie die allgemeine Form. Zum Beispiel ist die allgemeine Form der Kristallklasse "3" die trigonale Pyramide {hkil}; wird der Index l immer kleiner, die Flächen also immer steiler, so ergibt sich als Grenzform das trigonale Prisma {hki0}. Eine spezielle Form in dieser Kristallklasse wäre die Basisfläche (das Pedion) (0001).

Die folgenden Tabellen geben einen Überblick über alle Formen der 32 Kristallklassen. Nebeneinander stehen zunächst die allgemeine Form {hkl}, dann die speziellen und Grenzformen. Angegeben ist jeweils der Name der Form mit Synonymen sowie die Zahl der Flächen.

Das Gesetz der Winkelkonstanz besagt:

"Alle zur selben Kristallart gehörenden Einzelkristalle schließen zwischen analogen Flächen – gleichen Druck, gleiche Temperatur und chemische Zusammensetzung vorausgesetzt – stets gleiche Winkel ein."

Der Däne Niels Stensen (lat. Nicolaus Steno) bemerkte um 1669 bei der Untersuchung von Quarz, dass die Flächen der Kristalle – unabhängig von ihrer Größe und Form – immer gleiche Winkel bilden. Er vermutete, dass dies eine Eigenschaft aller Mineralkristalle sei. Bestätigt wurde diese Vermutung nach weiteren Vorarbeiten durch Torbern Olof Bergman schließlich von Jean-Baptiste Romé de L’Isle. Romé de L’Isle und sein Assistent Arnould Carangeot vermaßen systematisch Kristalle mit dem von Carangeot entwickelten Anlegegoniometer. 1783 veröffentlichte Romé de L’Isle eine detaillierte Beschreibung von 500 Kristallarten, die auf diesen Messungen basierte. Dabei konnte er empirisch bestätigen, dass das Gesetz der Winkelkonstanz – wie von Steno vermutet – für jede Kristallart gilt. Romé de L’Isles systematische Messungen und der daraus folgende induktive Beweis des Gesetzes der Winkelkonstanz sind das erste Beispiel für wissenschaftliches (methodisch-empirisches) Vorgehen in der Kristallographie. Insofern kann Romé de L’Isle als Begründer der wissenschaftlichen Kristallographie gelten.

Wenn man bedenkt, auf welche Weise Kristalle wachsen, ist dieses Gesetz nur logisch. Die chemische Zusammensetzung und die Bindungsart der Grundbausteine eines Minerals bestimmt die Ausbildung des Kristallsystems mit den entsprechenden Atomen und Molekülen an den Kreuzungspunkten des Raumgitters. Weitere Atome werden immer parallel zu den einzelnen Ebenen des Raumgitters eingebaut. Durch Konvektionsströme innerhalb der mineralischen Lösung kommt es zur unregelmäßigen Verteilung der aufbauenden Atome und damit zur Bevorzugung oder Benachteiligung einzelner Flächen. Dennoch bleiben die Winkel zwischen den Ebenen des Raumgitters durch das vorbestimmte Kristallsystem zwingend erhalten.

Das Rationalitätsgesetz (auch Rationalitätsprinzip, Gesetz der rationalen Verhältnisse oder Gesetz der rationalen Indizes) besagt, dass sich alle Kristallflächen und alle Kanten durch rationale Indizes darstellen lassen. Die Indizes sind immer kleine ganze Zahlen. Das gilt sowohl für die Weiss'schen Indizes m:n:p als auch für deren Kehrwerte, die später eingeführten Millerschen Indizes (hkl). Das Rationalitätsgesetz in dieser Formulierung wurde 1809 von Christian Samuel Weiss eingeführt.

Im Ansatz findet sich dieses Gesetz bereits im "Dekreszenzgesetz" („loi de décrescence“) von René-Just Haüy (1801). Es besagt: Bei der sukzessiven Aufschichtung der kleinsten Baueinheiten tritt jede folgende Schicht, bzw. eine Stufe aus m Schichten parallel einer Kante oder Flächendiagonale um eine feste Anzahl von n = 1, 2, 3, 6 Reihen subtraktiver Moleküle zurück. Haüy bemerkte selbst, dass es unmöglich ist, das reguläre Dodekaeder aus würfelförmigen Baueinheiten zu erzeugen. Er berechnete für das reguläre Dodekaeder ein irrationales Verhältnis entsprechend dem Goldenen Schnitt.

Mit Auguste Bravais begannen Versuche, Gesetze zu finden, mit denen sich die Kristallmorphologie aus der inneren Kristallstruktur vorhersagen lässt (und umgekehrt). Bravais sagte um 1848 voraus, dass die „relative Wichtigkeit“ einer Kristallfläche proportional ist zu ihrer Besetzungsdichte, das heißt, dass Formen umso wahrscheinlicher am Kristall auftreten, je mehr Gitterpunkte je Flächeneinheit auf der entsprechenden Gitterebene liegen. Gleichbedeutend damit ist, dass die morphologische Wichtigkeit einer Fläche umgekehrt proportional zum Netzebenenabstand ist.

Im 20. Jahrhundert wurde das Bravaissche Gesetz (auch Bravaissches Prinzip, frz. „loi de Bravais“, engl. „Bravais rule“) von Georges Friedel sowie von Joseph D. H. Donnay und David Harker wieder aufgegriffen. Während Bravais bei seinen Überlegungen nur Zentrierungen (die Bravais-Gitter) berücksichtigte, bezogen Donnay und Harker auch andere Symmetrieelemente (Gleitspiegelebenen und Schraubenachsen) ein, die zu veränderten Besetzungsdichten von Gitterebenen führen. Sie konnten so jeder Raumgruppen eine Flächenrangfolge zuordnen, die sie "morphologischer Aspekt" nannten.

Victor Mordechai Goldschmidt stellte 1897 das Komplikationsgesetz (Komplikationsregel) auf, welches besagt, dass sich aus zwei Kristallflächen, z. B. (100) und (010), durch wiederholte Addition oder Subtraktion ihrer millerschen Indizes alle weiteren Flächen dieser Zone ableiten lassen. Durch umfangreiche statistische Untersuchungen konnte Goldschmidt zeigen, dass Flächen im Allgemeinen umso seltener auftreten, je „komplizierter“ diese Ableitung ist, also je größer ihre Indizes werden.

Die Periodic-Bond-Chain-Theorie oder Hartman-Perdok-Theorie leitet die Kristallmorphologie von den intermolekularen Bindungen zwischen den Kristallbausteinen ab. Diese Theorie wurde ab 1955 von Hartman und Perdok eingeführt.




</doc>
<doc id="104701" url="https://de.wikipedia.org/wiki?curid=104701" title="James Prescott Joule">
James Prescott Joule

James Prescott Joule [] (* 24. Dezember 1818 in Salford bei Manchester; † 11. Oktober 1889 in Sale) war ein britischer Brauer, der als Physiker zu größten Ehren kam. Als Spross einer Brauerfamilie war er selbst Besitzer einer Bierbrauerei und forschte, ausgehend von technischen Fragen des Maschinenbaus und des Brauereiwesens, zu naturwissenschaftlichen Fragen. Er war „vielleicht der letzte Autodidakt, der einen wesentlichen Beitrag zur Weiterentwicklung der Wissenschaften geleistet hat.“ Die SI-Einheit der Energie wurde ihm zu Ehren „Joule“ genannt.

James Prescott Joule war der zweite Sohn des Brauereibesitzers Benjamin Joule (1784–1858) und seiner Frau Alice Prescott. Neben dem älteren Bruder Benjamin (1817–1895) hatte Joule noch zwei jüngere Schwestern (Alice und Mary) sowie einen jüngeren Bruder John Arthur.

Am 18. August 1847 heiratete James Joule Amelie Grimes. Der erste gemeinsame Sohn war Benjamin Arthur Joule (1850–1922), die gemeinsame Tochter hieß Alice Amelia (1852–1899). Amelie verstarb bereits 1854, wenige Monate nachdem im Alter von knapp drei Wochen der gemeinsame Sohn Henry starb.

Nach dem Tod seiner Frau zog James Joule von der Acton Square in die Whalley Range in Manchester zu seinem Vater. 1858 kaufte James Immobilien in Old Trafford. Beschwerden der Nachbarn über seine Experimente erzwangen seinen Wegzug. Nach neun Jahren in Cliff Point, Higher Broughton, Salford, zog er letztlich in die Wardle Road 12 in Sale.

Von 1872 an plagten Joule gesundheitliche Probleme. Als er in späteren Jahren in finanzielle Schwierigkeiten geriet, gewährte ihm die britische Königin Victoria ab 1878 eine Pension von jährlich 200 Pfund, zu dieser Zeit schwanden seine geistigen Kräfte. Er verstarb nach langer Krankheit in seinem Haus in Sale am 11. Oktober 1889 und wurde auf dem Friedhof in Brooklands beigesetzt. Auf seinem Grabstein ist der Zahlenwert 772,55 des Wärmeäquivalents zu sehen, der von James Joule 1878 gefundene Energiewert der notwendig ist, um ein Pfund Wasser um ein Grad zu erhitzen.

James Joule übernahm und betrieb zusammen mit seinem Bruder Benjamin die in Salford gelegene Brauerei, die gut 30 Jahre vor seiner Geburt von seinem Großvater Wiliam gegründet worden war. Während einige Autoren davon ausgehen, dass James Joule wenig mit dem Geschäftsbetrieb der Brauerei zu tun hatte, gehen andere ganz im Gegenteil von einem großen Interesse Joules gerade auch an der Verbesserung der technisch-physikalisch-chemischen Abläufe der väterlichen Brauerei aus und führen dazu seinen Briefwechsel mit William Thomson, 1. Baron Kelvin als Beleg an. Demnach sei die Gasproduktion bei der Bierherstellung der Ausgangspunkt weitergehender Betrachtungen gewesen, die schließlich zur Beschreibung des Joule-Thomson-Effekts führten. Durch die Erkrankung seines Vaters und den Rückzug seines Bruders aus dem Brauereibetrieb musste James Joule sich mit der technischen und der kaufmännischen Seite der Bierherstellung auseinandersetzen. Die Brauerei wurde im Jahr 1855, im Zusammenhang mit Krankheit und Tod des Vaters, verkauft, damit sich Joule ganz seinen naturwissenschaftlichen Experimenten widmen konnte.

Weil er seit seiner Kindheit an einer Wirbelsäulenerkrankung litt, die auch seine sozialen Fähigkeiten beeinträchtigte, und er als anfällig galt erhielt der naturwissenschaftlich begabte Joule gemeinsam mit Benjamin Privatunterricht. Dieser begann im Hause seines Vaters in Broomhill, Pendlebury, durch die Halbschwester seiner Mutter. Einer ihrer Lehrer war ab 1834 der Chemiker und Physiker John Dalton. Dalton unterrichtete die Joules zweimal die Woche. Joule war in seiner Jugendzeit sehr experimentierfreudig. So verlor Joule seine Augenbrauen durch ein Experiment mit Sprengstoffen. Er ließ Drachen bei Gewitter steigen oder steigerte Elektroschocks bei einem Dienstmädchen, bis sie bewusstlos wurde. 1837 richtete Joule sich im Keller der Brauerei ein Labor ein.

1838 begann Joule auf der Grundlage der Arbeiten von William Sturgeon mit elektromagnetischen Experimenten. Sein Ziel war die genauere experimentelle Untersuchung, welchen Wirkungsgrad ein Elektromotor erreichen kann. Dazu experimentierte er mit verschiedenen Eisensorten für Elektromagnete und deren magnetische Anziehung. Ein weiterer Aspekt war die quantitative Untersuchung der Wärmewirkungen des Stroms, diese Erwärmung des stromdurchflossenen Leiters wird Joulesche Wärme genannt. 1840 formulierte er das erste Joulesche Gesetz, nach dem die Wärme proportional dem Produkt aus dem Quadrat der Stromstärke und dem Widerstand des Stromkreises ist. Joule bewies, dass die von einer Batterie erzeugte Wärmemenge direkt mit der Menge des chemisch umgesetzten Metalls in der Batterie zusammenhängt. Joules Fazit seiner Bemühungen um einen effizienten Elektromotor war ernüchternd: Der bestmögliche erreichbare Wirkungsgrad betrug nur 20 % des Wertes der optimalen kohlebefeuerten Dampfmaschinen seiner Zeit, wobei die Kosten für den Batteriestrom zusätzlich erheblich über denen für Kohle lagen. Die Experimente zum Wirkungsgrad des belasteten Elektromotors führten Joule zu Fragestellungen der Thermodynamik. Während die vom elektrischen Strom erzeugte Wärmemenge genau mit dem chemischen „Verbrennungsvorgang“ in der Batterie übereinstimmte, war die Wärmeerzeugung durch einen Magnetzünder dadurch nicht zu erklären. Gemäß der Theorie vom unzerstörbaren Wärmestoff („Caloricum“) müsste sich – da ein chemischer Vorgang zur Erklärung ausschied – das Materials des Zünders abkühlen, was es aber nicht tat. Joules Schlussfolgerung war, dass die auftretende Wärme gleich der zum Betrieb des Magnetzünders aufgewendete Arbeit (er verwendet den Terminus vis viva) sein muss. Aufgrund der von ihm vermuteten Äquivalenz von mechanischer Arbeit und Wärme muss es eine konstante Umwandlungsrate von mechanischer Energie und Wärme geben. Dieses Wärmeäquivalent bestimmte er in zahlreichen unterschiedlichen Versuchen in den Jahren 1843 bis 1847, beispielsweise mit Kompression von Luft oder der Reibung zwischen Eisenscheiben und Flüssigkeiten. Der klassische, nach ihm benannte Versuch, wurde 1843 von ihm durchgeführt: Einer thermisch isolierten Wassermenge wurde eine definierte Menge mechanischer Energie zugeführt und anschließend die Temperaturerhöhung gemessen. Ergebnis seiner Untersuchungen war, dass eine British thermal unit, also die Wärmeenergie, die benötigt wird, um ein britisches Pfund Wasser um ein Grad Fahrenheit zu erwärmen, dem Gewicht von 772,55 Pfund, das aus einer Höhe von einem Fuß fällt, entspricht. In moderner Ausdrucksweise ergibt sich ein Zahlenwert von etwa 4,15 Joule pro Kalorie – in guter Übereinstimmung mit dem heutigen Wert von 4,1868.

Den Nachweis des Wärmeäquivalents erbrachte 1841 bereits der Heilbronner Arzt Robert Mayer, der damit allerdings ebenfalls zunächst keine Anerkennung fand. Als Hermann von Helmholtz den Energieerhaltungssatz 1847 endgültig ausformulierte, würdigte er sowohl Mayer als auch Joule.

1847 sprach Joule zum wiederholten Mal in Oxford vor der British Association for the Advancement of Science. Diesmal konnte er seine Zuhörer mehr überzeugen als zuvor, darunter George Gabriel Stokes und Michael Faraday. Nach anfänglichen Zweifeln bedeutete dieses Treffen auch den Beginn der Zusammenarbeit mit William Thomson, 1. Baron Kelvin. Ab 1852 arbeitete Joule gemeinsam mit Thomson an Experimenten zur Bestätigung thermodynamischer Theorien. 1852 zeigten die beiden Forscher, dass ein Gas, das sich ungestört ausdehnen kann, sich abkühlt. Dieser Joule-Thomson-Effekt war ein Beweis für die Annahme, dass zwischen den Gasmolekülen schwache Kräfte wirksam sind. Anwendung findet der Satz bei der Gasverflüssigung und in der Kältetechnik. Außerdem konzipierte Joule den idealen Kreisprozess der Heißluftmaschine (Joule-Prozess).

Zu vielen weiteren Phänomenen veröffentlichte Joule. 1846 entdeckte Joule mit der Längenänderung magnetisierter ferromagnetischer Stoffe die Magnetostriktion (was – wie andere Effekte auch – „Joule-Effekt“ genannt wird), die bei der Erzeugung von Ultraschallwellen Anwendung fand. 1847 formulierte er die Ansicht, dass die Sternschnuppen sehr schnell in die Erdatmosphäre eintauchende Körper sind, bei denen die umgewandelte Bewegungsenergie zum Aufglühen führt. Er behandelte 1869 auch das Phänomen des „grünen Blitzes“ oder analysierte Photographien der Sonne.

Die Tatsache, dass Joule keine akademische Ausbildung in den Naturwissenschaften hatte, machte ihn zum Außenseiter und verzögerte die Anerkennung seiner Leistungen deutlich.

Erst um 1850 herum wurden seine Leistungen als Physiker gewürdigt, er wird trotz fehlendem Universitätsstudium dementsprechend von der Royal Society und renommierten Nachschlagewerken wie der Encyclopædia Britannica, dem Larousse, dem Brockhaus oder dem Duden als „Physiker“ klassifiziert.

Joule war ab 1842 Mitglied der Manchester Literary and Philosophical Society und ab 1860 ihr Präsident.

Ab 1850 war Joule Mitglied der Royal Society, die ihn 1852 mit der Royal Medal (‘For his paper on the mechanical equivalent of heat, printed in the Philosophical Transactions for 1850’), 1870 mit der Copley Medal (‘For his experimental researches on the dynamical theory of heat’) auszeichnete.

Ehrendoktorwürden wurden ihm verliehen 1857 (Trinity College Dublin), 1860 (University of Oxford), 1871 (University of Edinburgh).

1867 wurde er Ehrenmitglied der Royal Society of Edinburgh. 1870 wurde er in die Académie des sciences in Paris, 1874 in die American Academy of Arts and Sciences und 1887 in die National Academy of Sciences gewählt.

Er war Präsident der British Association for the Advancement of Science und ab 1857 Ehrenmitglied der Institution of Engineers and Shipbuilders in Scotland

1880 erhielt er die Albert Medal der Royal Society of Arts (‘For having established, after most laborious research, the true relation between heat, electricity and mechanical work, thus affording to the engineer a sure guide in the application of science to industrial pursuits’).

Eine Gedenkstelle für Joule befindet sich im Nordchor der Westminster Abbey, eine Statue befindet sich im Rathaus von Manchester (gegenüber von der für Dalton).

Zu seinen Ehren heißt die SI-Einheit der Energie, Arbeit und Wärmemenge „Joule“ (Einheitenzeichen J). Diese Auszeichnung, die nur wenigen Wissenschaftlern zuteil wurde, wurde wenige Wochen vor seinem Tod auf dem zweiten Internationalen Elektrizitätskongress in Paris beschlossen.

Ein Mondkrater wurde nach Joule benannt.





</doc>
<doc id="241105" url="https://de.wikipedia.org/wiki?curid=241105" title="Beschleunigungsspannung">
Beschleunigungsspannung

In der Elektronen- und Ionenoptik wird eine Beschleunigungsspannung zwischen Elektroden angelegt, um elektrisch geladenen Teilchen kinetische Energie zu geben. Die elektrische Feldstärke zwischen den Elektroden und damit die Beschleunigung der Teilchen hängen vom Abstand der Elektroden ab, der Energiezuwachs am Ende der Beschleunigungsstrecke aber nicht, siehe Potential und Spannung, daher die Bedeutung der Beschleunigungsspannung und die Angabe von Teilchenenergien in der Einheit Elektronenvolt. Mit leicht handhabbaren Spannungen werden relativistische Geschwindigkeiten erreicht.

Teilchen der Ladung formula_1 erhalten durch eine Beschleunigungsspannung formula_2 die elektrische Energie 

Für nichtrelativistische Geschwindigkeiten (formula_4) beträgt die kinetische Energie eines Teilchens der Masse formula_5 näherungsweise

Gleichsetzen ergibt


Für den exakten Zusammenhang zwischen Geschwindigkeit und Energie siehe kinetische Energie in der relativistischen Mechanik.

Die Gründe für die Wahl der Beschleunigungsspannung können sich auf eine Reihe von Parametern beziehen:


</doc>
<doc id="281301" url="https://de.wikipedia.org/wiki?curid=281301" title="Menyhért Palágyi">
Menyhért Palágyi

Menyhért Palágyi (ursprünglicher Name: Silberstein) (* 16. oder 26. Dezember 1859 in Paks, Ungarn; † 14. Juli 1924 in Darmstadt), in deutschsprachigen Publikationen Melchior Palágyi, war ein ungarisch-jüdischer Philosoph, Mathematiker, Physiker, Literatur- und Erkenntnistheoretiker und betrieb Forschungen zur Logik, Erkenntnistheorie und Naturphilosophie. Er war der ältere Bruder des ungarischen Dichters Ludwig Palágy.

Palágyi präsentierte in seinem Werk "Neue Theorie des Raumes und der Zeit" (1901), eine „Raumzeitlehre“, welche eine gewisse äußerliche Ähnlichkeit mit dem Raumzeitformalismus von Henri Poincaré und Hermann Minkowski im Rahmen der speziellen Relativitätstheorie hatte (z. B. die imaginäre Zeitkoordinate "it" als vierte Dimension). Um 1914 warf er Albert Einstein und Minkowski gar Plagiat vor, jedoch drückte er gleichzeitig seine Ablehnung der RT aus, was er damit begründete, dass sie seine Theorien völlig missverstanden hätten. Wie Max Born oder Klaus Hentschel jedoch ausführen, hatte seine Philosophie mit der Physik der Relativitätstheorie nichts gemein, sondern ist bloß eine Neuformulierung der klassischen Zusammenhänge. Palagyi wird auch mit seinem Konzept einer vitalen Phantasie als einer der Ahnherren der kybernetischen Anthropologie gehandelt. Seine Theorie der virtuellen Bewegung bildet die Basis für verschiedene bewegungstherapeutische Konzepte.

Menyhért Palágyi (Melchior Palágyi) wurde auf dem Waldfriedhof Darmstadt (Grabstelle: R 14 IIc 109) bestattet.






</doc>
<doc id="495364" url="https://de.wikipedia.org/wiki?curid=495364" title="Jahn-Teller-Effekt">
Jahn-Teller-Effekt

Der Jahn-Teller-Effekt, entdeckt 1937, geht auf die Wissenschaftler Edward Teller (1908–2003) und Hermann Arthur Jahn (1907–1979) zurück. Das Jahn-Teller-Theorem erklärt die Verzerrung in der Geometrie des Ligandenfelds einiger oktaedrischer Komplexverbindungen entlang einer Raumachse.

Das Theorem besagt:

Der Jahn-Teller-Effekt lässt sich auch bei Komplexen mit oktaedrischen Struktur beobachten, führt dabei aber zu keinen strukturellen Veränderungen. Insbesondere Komplexe mit schwachen Ligandenfeldern zeigen sich anfällig für Jahn-Teller-Verzerrungen, sodass man im starken Ligandenfeld nur bei oktaedrischen d-low-spin-Komplexen (te) eine starke Jahn-Teller-Verzerrung beobachten kann.

Zwei der Liganden nehmen durch eine Verzerrung (Jahn-Teller-Verzerrung) einen größeren oder geringeren Abstand zum Zentralatom ein als die Liganden in der Äquatorialebene der Komplexverbindung. Die quadratisch planare Koordination kann als Extremfall einer Jahn-Teller-Verzerrung aufgefasst werden, wenn die Liganden entlang der z-Achse unendlich weit vom Zentralatom entfernt sind.

Die Verzerrung erweist sich bei den betroffenen Komplexen als energetisch günstig, da die besetzten Molekülorbitale energetisch abgesenkt werden. Unbesetzte Molekülorbitale werden energetisch angehoben. Durch die energetische Aufspaltung der Molekülorbitale geht die Entartung verloren.

Eine Jahn-Teller-Verzerrung (Streckung oder Stauchung) tritt ein, wenn es aufgrund der Ligandenfeldaufspaltung Gruppen von d-Zuständen gibt, so dass es mehr als eine Möglichkeit der Elektronenanordnung gibt und diese Elektronenanordnung energetisch entartet ist. Da entartete Zustände instabil sind, wird durch die Verzerrung die Symmetrie der Struktur aufgehoben; aus den teilweise besetzten Orbitalen gleicher Energie werden dabei Orbitale unterschiedlicher Energie. Dies bedeutet einen Energiegewinn, weil nur die Orbitale niedrigerer Energie besetzt werden. Eine Verzerrung ist z. B. für die Elektronenanordnungen d-high-spin, sowie bei d-low-spin und d zu erwarten.

Liegt nun ein High-spin-Komplex vor, beispielsweise d(te), so kann das vierte d-Elektron entweder das d- oder das energiegleiche d-Orbital besetzen. Wird das d-Orbital besetzt, so werden die 4 äquatorial angeordneten Liganden abgestoßen, was eine Stauchung des Oktaeders zur Folge hat. Wird dagegen das d-Orbital besetzt, so werden nur die beiden axial angeordneten Liganden abgestoßen, was zu einer Streckung des Oktaeders in z-Richtung führt. In beiden Fällen führt die Besetzung des abgesenkten Orbitals zu einem wenn auch nicht sehr großen Energiegewinn, welcher als Jahn-Teller-Stabilisierungsenergie bezeichnet wird.
Ob es nun zu einer Streckung oder Stauchung kommt, hängt unter anderem auch vom Gegenion ab.
Beispielsweise findet man einen [Cu(NO)]-Komplex je nach Gegenion teils als oktaedrisch-gestauchten oder als oktaedrisch-gestrecken Komplex wieder. Andere wie zum Beispiel [Cu(py')] mit py' = Pyridinoxid haben sogar einen zwischen beiden Formen fluktuierenden Bau.

Ein sehr ähnlicher Mechanismus tritt auch in eindimensionalen Ketten von Atomen und in Festkörpern auf, er wird dann als Peierls-Verzerrung bezeichnet.

Als Beispiel für eine derartige Verzerrung in der organischen Chemie lässt sich der Antiaromat Cyclobutadien heranziehen. (Vgl. Frost-Musulin-Diagramm)

Zwischen den durch den Jahn-Teller-Effekt aufgespalteten Energieniveaus kann es durch Lichteinfall zu Elektronenübergängen kommen, wobei Licht der entsprechenden Wellenlänge absorbiert wird. Dieser Mechanismus ist hauptverantwortlich für die Farbe verschiedener Minerale, z. B. der Grünfärbung des Malachits und der Blaufärbung des Azurits.




</doc>
<doc id="928377" url="https://de.wikipedia.org/wiki?curid=928377" title="Micro Black Hole">
Micro Black Hole

Das Micro Black Hole ( für „Mikro-Schwarzes Loch“) ist ein hypothetisches, sehr kleines und leichtes Schwarzes Loch.

Mitte der 1970er Jahre stellte Roger Penrose die Vermutung auf, Schwarze Löcher könnten auch im Labor erzeugt werden. Es gibt Theorien, nach denen es möglich ist, mit dem Large Hadron Collider (LHC), der am 10. September 2008 in Betrieb genommen wurde, solche Schwarzen Löcher bis zu einmal pro Sekunde zu erzeugen. Dies setzt jedoch die Existenz zusätzlicher kompakter Raumdimensionen voraus, welche u. a. von bestimmten Modellen der Stringtheorie vorhergesagt werden.

Solche Schwarzen Löcher wären nicht vergleichbar mit stellaren Schwarzen Löchern, die kosmologisch beobachtet werden. Ihre Ausmaße lägen in der Größenordnung von Elementarteilchen. Auf Grund von Quanteneffekten (Hawking-Strahlung) würden sie höchstwahrscheinlich sehr kurze Zeit nach ihrer Entstehung schon wieder zerstrahlen. Die dabei entstehenden Elementarteilchen könnten mittels Teilchendetektoren nachgewiesen werden. Gemäß dem aktuellen Stand der Forschung auf diesem Gebiet wären die entstehenden Teilchenschauer () isotroper verteilt als diejenigen, die beim Zusammenstoß hochenergetischer Teilchen entstehen, und daher von ihnen zu unterscheiden.

Alle Größen sind in Natürlichen Einheiten angegeben.

Entsprechend der Theorie Schwarzer Löcher sind Schwarzschild-Radius und Masse eines Schwarzen Lochs proportional zueinander. Da man davon ausgeht, dass unterhalb der Planck-Länge Quanteneffekte dominant werden und keine stabilen Schwarzen Löcher mehr existieren können, gibt es damit auch eine untere Grenze für die Masse eines Schwarzen Loches, welche ca. 1,22 × 10 TeV/c (ca. 22 Mikrogramm) beträgt. Dies lässt die Erzeugung Schwarzer Löcher im Labor erst einmal unmöglich erscheinen, da die maximal erreichbare Energie im größten Teilchenbeschleuniger (dem LHC) nur einige TeV beträgt, also 16 Größenordnungen zu wenig.

Dieses Bild verändert sich allerdings, wenn man die Theorie um so genannte (dt. „große zusätzliche Dimensionen“) erweitert. Darunter versteht man kompakte zusätzliche Raumdimensionen, wobei das „groß“ (deutlich unter einem Millimeter, noch größere sind bereits durch Beobachtungen ausgeschlossen) hier in Relation zu anderen Theorien zu verstehen ist. Solche Zusatzdimensionen ergeben sich natürlicherweise in sehr vielen Modellen der Stringtheorie.

Unter dieser Voraussetzung verändert sich das Gravitationsgesetz, sobald man Energien erreicht, welche dem Radius dieser Extradimensionen entsprechen. Dies verändert auch die Massenskala, oberhalb derer die Existenz Schwarzer Löcher möglich ist:
wobei formula_2 die Planck-Masse ist, formula_3 die Anzahl der zusätzlichen Dimensionen, formula_4 die neue fundamentale Massenskala und formula_5 der Radius der Extradimensionen.

Nimmt man z. B. an, dass drei Zusatzdimensionen mit einem Radius von ca. 1 eV existieren, ergibt sich eine Masse von ca. 0,16 TeV für die effektive Planckmasse und damit die Möglichkeit, Schwarze Löcher im Labor herzustellen.

Eine weitere Möglichkeit besteht darin, dass Schwarze Löcher bei der Kollision kosmischer Strahlung mit Bestandteilen der Erdatmosphäre entstehen. Dies konnte bislang nicht nachgewiesen werden, Fortschritte lassen sich zukünftig vielleicht durch das 2004 in Betrieb gegangene Pierre-Auger-Observatorium erzielen.

Die Lebenszeit solcher kleinen Schwarzen Löcher wäre vermutlich sehr kurz, weil sie, wie wahrscheinlich alle Schwarzen Löcher, durch die Hawking-Strahlung an Masse verlieren und schließlich verdampfen sollten. Da die Lebenszeit proportional zur dritten Potenz der Masse ist, ergibt sich bei kleinen Schwarzen Löchern eine nicht beobachtbar kurze Lebenszeit. Nachweisbar wären sie potenziell durch die bei ihrem Zerfall entstehenden Elementarteilchen. Allerdings ist nicht eindeutig geklärt, ob der Hawking-Effekt ohne Modifikation auch in diesem Fall anwendbar ist, da seine Herleitung auf einer vernachlässigbaren Krümmung des Ereignishorizontes beruht, d. h. auf „hinreichend“ großer Masse.

Man geht davon aus, dass der Zerfall in mehreren Phasen stattfindet. Wie dies genau vor sich geht und ob es ein „Relikt“ gibt oder der Zerfall vollständig stattfindet, ist Gegenstand aktueller Forschung und nicht abschließend geklärt. So wird zurzeit im LHC die mögliche Entstehung und der Zerfall untersucht.



Zur Hawkingstrahlung:

Zur möglichen Produktion in Beschleunigern:

Zur möglichen Produktion durch kosmische Strahlung:


</doc>
<doc id="1051159" url="https://de.wikipedia.org/wiki?curid=1051159" title="Alexander Alexandrowitsch Andronow">
Alexander Alexandrowitsch Andronow

Alexander Alexandrowitsch Andronow (, wiss. Transliteration ""; * in Moskau; † 31. Oktober 1952 in Gorki) war ein sowjetischer Physiker und Mitglied der sowjetischen Akademie der Wissenschaften.

Andronow studierte ab 1920 Elektrotechnik an der Technischen Hochschule Moskau und 1923 bis 1925 theoretische Physik an der Lomonossow-Universität. 1926–29 setzte er sein Studium bei Leonid Isaakowitsch Mandelstam fort, während er gleichzeitig an der Zweiten Moskauer Universität unterrichtete. 1930 wurde er promoviert (Kandidat). Ab 1931 war er Professor in Nischni Nowgorod (damals Gorki). Gleichzeitig war er am Institut für Automatisierung der sowjetischen Akademie der Wissenschaften.

Andronow arbeitete über quantenmechanische Statistik und vor allem über nichtlineare Schwingungen. Sein 1937 verfasstes Buch darüber mit Alexander Adolfowitsch Witt und Semen Emmanuilowitsch Chaikin wurde ein Standardwerk zur Störungstheorie nichtlinearer Systeme. Außerdem untersuchte er Bifurkationen, Stabilität und Topologie der Lösungen dynamischer Systeme, teilweise mit Lew Pontrjagin (1937). Er begründete eine eigene wissenschaftliche Schule auf dem Gebiet dynamischer Systeme und deren Anwendung in einer Vielzahl von Gebieten (Mechanik, Regelungstechnik, Ökonomie, Biologie usw.). Die Hopf-Bifurkation ist manchmal auch zusätzlich nach ihm benannt. Die Wurzeln der Theorie der Hopf-Bifurkation gehen auf Henri Poincaré (um 1892) zurück, sie wurde ausführlich in den Arbeiten Andronow, Witt und Chaikin in den 1930er Jahren behandelt und von Eberhard Hopf 1942.

Der Andronov-Krater auf dem Mond ist nach ihm benannt.

Er arbeitete auch mit der Mathematikerin Ewgenija Alexandrowna Leontowitsch (später Leontowitsch-Andronowa) zusammen, mit der er verheiratet war.

Zu seinen Doktoranden gehört der Mathematiker Dmitri Andrejewitsch Gudkow.

Er wurde 1947 in den Obersten Sowjet der russischen Sowjetrepublik und 1950 der UdSSR gewählt und war auch im Präsidium des Obersten Sowjet der russischen Teilrepublik. 1952 starb er an den Folgen von Bluthochdruck.




</doc>
<doc id="1121850" url="https://de.wikipedia.org/wiki?curid=1121850" title="Magnet">
Magnet

Ein Magnet (Plural "Magneten"), von , (vgl. das Mineral Magnetit) ist ein Körper, der bestimmte andere Körper magnetisch anzieht oder abstößt. Magnetische Anziehung oder Abstoßung ist ein grundlegendes Naturphänomen – siehe dazu den Artikel Magnetismus.

Die Richtung und Stärke magnetischer Kräfte kann man durch Feldlinien anschaulich darstellen. Ein Magnet ruft ein Magnetfeld hervor und wird von diesem durchströmt. Die Oberflächenbereiche, die vom überwiegenden Teil des Magnetfeldes durchflossen werden, heißen die Pole des Magneten; nach gängiger Konvention treten die Feldlinien am „Südpol“ (meist grün dargestellt) in den Magneten ein und am „Nordpol“ (rot) aus. Die Magnetfeldrichtung ist durch die Kraftwirkung auf einen Probemagneten definiert.

"Magnetische Monopole", also einzelne Nord- oder Südpole ohne ihren Widerpart, sind spekulativer Natur und konnten bisher nicht experimentell nachgewiesen werden. Zwar haben neuere Experimente monopolähnliche Strukturen in bestimmten Festkörpern nachgewiesen; diese treten aber nur paarweise auf und können zwar als Quellen der Magnetisierung, aber nicht des Magnetfelds selbst angesehen werden (siehe Magnetischer Monopol).

Auch viele Gesteine haben magnetische Eigenschaften. Das Erdmagnetfeld, nach dem sich Kompassnadeln ausrichten, entsteht jedoch nur zu einem geringen Teil durch solche magnetisierten Gesteine in der Erdkruste, sondern durch tieferliegende Strömungen von "elektrisch" leitender Materie, also konkreten, makroskopischen Strömen.

Man unterscheidet folgende Arten des Magnetismus:

Bevor der Zusammenhang von Magnetismus und Elektrizität bekannt wurde, waren magnetische Phänomene und Nutzungen nur unter Zuhilfenahme natürlicher Magneteisensteine zu beobachten und zu verwenden. Die praktische Anwendung galt ausschließlich dem Kompass. Dessen Prinzip war schon im vorchristlichen China und in der griechischen Antike bekannt. Nach dem römischen Dichter Lukrez (De rerum natura) wurden die Magneteisensteine nach der Landschaft Magnesia in Griechenland benannt, wo diese Steine schon sehr früh gefunden wurden.

Bei Augustinus wird das Bild vom Magneten noch allegorisch verwendet. Der mittelalterliche englische Theologe und Naturforscher Alexander Neckam veröffentlichte gegen 1200 die frühesten europäischen Aufzeichnungen über die Magnetisierung von Kompassnadeln, und Petrus Peregrinus de Maricourt beschrieb 1269 erstmals die Polarität von Magneten. Grundlegendes zum Magnetismus, z. B. die Kenntnis von der Magneteigenschaft der Erdkugel trug William Gilbert bei, indem er systematisch und experimentierend vorging, nach seinem Vorschlag konzentrierte man die Kraftlinien an den Polen der Magnetsteine mit kleinen Eisenkappen. Die ungebrochene Faszination des auch im 18. Jahrhundert in seinen Ursachen noch unklaren Magnetismus spiegelt der lange Artikel "Magnet" in der "Oekonomischen Encyclopädie" von Johann Georg Krünitz.
1820 entdeckte Hans Christian Ørsted die Zusammenhänge zwischen elektrischem Strom und Magnetismus. Erst dies war die Voraussetzung für die Entwicklung der Elektrotechnik.

Dauermagneten (auch Permanentmagneten genannt) behalten nach einer Magnetisierung diese über lange Zeit bei. Zur Herstellung dienen heute metallische Legierungen aus Eisen, Nickel und Aluminium mit Zusätzen aus Cobalt, Mangan und Kupfer oder auch keramische Werkstoffe (Barium- bzw. Strontiumhexaferrit). Besonders starke Magneten werden im Sinterverfahren aus seltenen Erden hergestellt, wie zum Beispiel Samarium-Cobalt oder Neodym-Eisen-Bor. Verwendung finden Dauermagneten in Kompassen als Magnetnadel, in Elektromotoren, in elektrischen Messinstrumenten, zum Beispiel Drehspulinstrumenten, in Lautsprechern, Kopfhörern, Mikrofonen und Gitarrentonabnehmern sowie in vielen anderen modernen Geräten wie Druckköpfen von Nadeldruckern, Festplattenlaufwerken, Aktoren und Sensoren und Metall-Abscheidern. Die einfachste Anwendung als Haltemagnet auf Eisen hält Möbeltüren, Handtaschen, den Deckel einer extravaganten Kartonverpackung geschlossen, Dekoration oder Infotafeln an Blechstreben einer abgehängten Zwischendecke, Notizen auf einer „Magnettafel“ (eigentlich mittels Magnetknopf auf Blech), ein mit Stahlblech versehenes Smartphone auf einer Magnethalterung, eine Warn- oder Arbeitsleuchte auf Autoblech, Pseudopiercings etwa an der Wange.

Mit Hilfe eines von einem anderen magnetischen Körper oder durch elektrischen Strom erzeugten Magnetfeldes können ferromagnetische Stoffe vorübergehend (sogenannter induzierter Magnetismus) oder dauerhaft durch Ausrichtung der Weiss-Bezirke selbst zu Magneten werden.

Auf diese Weise werden übliche Dauermagneten hergestellt.

Elektromagneten bestehen im Allgemeinen aus einer oder zwei stromdurchflossenen Spulen, meistens mit einem Kern aus einem weichmagnetischen Werkstoff, im einfachsten Fall aus Weicheisen. Diese Anordnung führt zu einem starken Magnetfeld, siehe hierzu Elektromagnetismus. Man verwendet Elektromagneten für zahlreiche kleine und große technische Einrichtungen, z. B. fremderregte Elektromotoren und Generatoren, Relais, Schütze, Zug-, Hub- und Stoßmagneten, elektrischer Türöffner.

Wechselstrom-Elektromagneten finden sich in Membranpumpen (z. B. zur Aquarium-Belüftung) und Schwingförderern.

Als Sonderfall weisen Ablenkspulen beispielsweise in einer Kathodenstrahlröhre keinen Kern auf und wirken so, als Luftspule, ebenfalls als Elektromagnet.

Mit Elektro-Magnetfiltern können ferromagnetische Feststoffe aus Flüssigkeiten abgetrennt werden. Diese Feststoffe bestehen überwiegend aus Eisenoxiden. Diese werden beispielsweise aus den Umlaufkondensaten von Kraftwerken und den Umlaufwässern von Fernheiznetzen abfiltriert.

Ab einer magnetischen Flussdichte von etwa 2 T (Sättigungsfeldstärke) sind die üblichen ferromagnetischen Werkstoffe für den Kern eines Elektromagneten in der Sättigung und können nicht mehr zur Verstärkung des Feldes beitragen. Ohne die Unterstützung durch den Kern können z. B. wie in einer Luftspule auch bedeutend größere Flussdichten erreicht werden, allerdings mit viel höherem Energieaufwand.

Bei Verwendung von supraleitenden Werkstoffen zur Wicklung eines Elektromagneten ist es möglich, magnetische Flussdichten bis ca. 20 Tesla im Dauerbetrieb zu erreichen. Da die Sprungtemperatur der Supraleitung bei solchen Magnetfeldern und Stromdichten stark absinkt, müssen die Spulen dazu in mit flüssigem Helium gefüllten Kryostaten durch Siedekühlung bei Unterdruck auf deutlich unter 4 K gekühlt werden.

Solche Magneten sind z. B. für Kernspinresonanzspektroskopie (NMR), Kernspintomografen oder kontinuierlich arbeitende Kernfusionsreaktoren erforderlich. Im Jahr 2009 besaß der stärkste kommerziell erhältliche NMR-Magnet eine Flussdichte von 23,5 T (Bruker).

Ein Ablenkmagnet ist ein Magnet (fast immer Elektromagnet), der in einem technischen Gerät eingesetzt wird, um einen Strahl aus geladenen Teilchen (z. B. Elektronen) in eine andere Richtung abzulenken. In diesem Fall wird auch eine kernlose Spule als Magnet bezeichnet.

Ablenkmagneten nutzen die Lorentzkraft, die bewegte elektrische Ladungen in einem magnetischen Feld zu einer Richtungsänderung zwingt. Ist das Feld homogen, durchfliegen die Teilchen dabei einen Kreisbogen quer zur Magnetfeldrichtung. Das Magnetfeld kann permanent oder induktiv erzeugt werden. Letztere Variante erlaubt schnelle Änderungen der Feldstärke.

Die Richtungsänderung dient meist der Fokussierung oder Lenkung eines Strahls. Zur Ablenkung von Elektronen sind Ablenkmagneten Bestandteile von:

Auch in Teilchenbeschleunigern und ihren Strahlführungen werden geladene Teilchen mit Dipolmagneten auf bestimmte Bahnen gelenkt.

Umgekehrt erlaubt der Winkel der Richtungsänderung bei bekannter Ladung Rückschlüsse auf die Masse der abgelenkten Teilchen. Dies ist die Grundlage der Massenspektrometrie.

Kommt ein magnetisch aufzeichnender Datenträger (Festplatte, Magnetstreifen einer Kreditkarte, Tonbandspulen o. ä.), in die Nähe eines stärkeren Magneten, kann das einwirkende Magnetfeld zu Datenverlusten durch "Überschreiben" der magnetischen Informationen des Datenträgers führen. Ein bekanntes Beispiel dafür sind die Magnethalterungen von Klapptischen in Interregio-Zügen der Deutschen Bahn AG, die nicht an der Arretierposition (Lehne), sondern im Tisch angebracht, also in der Tischauflagefläche eingearbeitet waren. Die Festplatten aufliegender Laptops wurden durch diese Magnethalterungen nicht nur gelöscht, sondern beschädigt, die Datenverluste konnten nicht rückgängig gemacht werden. Oft kommt es auch an Ladenkassen zur Zerstörung von EC-/Kreditkarten, weil dort manche Waren-Diebstahlsicherungen mittels eines starken Magneten entfernt werden.




</doc>
<doc id="1268091" url="https://de.wikipedia.org/wiki?curid=1268091" title="Saha-Gleichung">
Saha-Gleichung

Die Saha-Gleichung (auch "Saha-Ionisierungs-Gleichung" oder "Eggert-Saha-Gleichung") beschreibt im thermodynamischen Gleichgewicht die Abhängigkeit des Ionisationsgrades eines Gases von der Temperatur; erreicht der Ionisationsgrad eine nennenswerte Größenordnung, spricht man nicht mehr von einem Gas, sondern von einem Plasma. 

Die Gleichung wurde 1920 von dem damals 27-jährigen indischen Astrophysiker Meghnad Saha aus der Boltzmann-Statistik abgeleitet und ist bedeutend für die Physik der Sterne. John Eggert lieferte Vorarbeiten 1919 durch eine Veröffentlichung in der Physikalischen Zeitschrift. Saha las diese Arbeit in Indien und konnte sie entscheidend verbessern.

Man kann die Saha-Gleichung so lesen, dass gerade diejenigen Atome ionisiert sind, bei denen die thermische Energie der Elektronen gemäß der Boltzmann-Verteilung über der Ionisierungsenergie liegt.

Für reine Gase lautet die Saha-Gleichung

mit:



</doc>
<doc id="1387739" url="https://de.wikipedia.org/wiki?curid=1387739" title="Standfestigkeit">
Standfestigkeit

Standfestigkeit beschreibt die Fähigkeit eines Körpers oder Gerätes, eine vorgegebene Position, Eigenschaft oder Leistung über eine gegebene Zeit beizubehalten. <br> Nicht mit Stehvermögen zu verwechseln.

Im Maschinenbau wird mit Standfestigkeit in der Regel die Fähigkeit von Maschinen verstanden, eine gewünschte Nennleistung über längere Zeiträume ohne Defekt oder besondere Wartungsmaßnahmen bereitstellen zu können. Insbesondere bei leistungsstarken Verbrennungsmotoren ist hiermit gemeint, die Maximalleistung nachhaltig abfordern zu können.

Entscheidend für die Standfestigkeit eines Körpers ist die Lage seines Schwerpunktes bezüglich der Auflagefläche. Ein Körper bleibt nur dann auf seiner Auflagefläche stehen, wenn das Schwerpunktlot sie trifft, das heißt, wenn die Auflagefläche sich unter dem Massenmittelpunkt befindet. Ist dies nicht der Fall, so kippt der Körper um. 

Von Bedeutung ist die Standfestigkeit in der Baustatik zum Beispiel für Gebäude, Türme, Masten, Kräne oder Regale. Bei allen diesen Beispielen muss ein stabiles Gleichgewicht gewährleistet sein.




Ein Maß für die Standfestigkeit ist der Betrag "M" des Drehmoments, das auf den Körper wirken muss, um ihn umzuwerfen:

Bedeutung der Formelzeichen:

formula_2 - Betrag des Drehmoments, das wirken muss, um den Körper umzuwerfen,<br>
formula_3 - Gewichtskraft des Körpers,<br>
formula_4 - Entfernung des Massenschwerpunktes von der Kippkante. 

Aus dieser Formel lässt sich schließen, dass die Standfestigkeit umso größer ist, je tiefer der Massenschwerpunkt des Körpers liegt, je größer seine Gewichtskraft ist und je größer seine Standfläche ist.



</doc>
<doc id="2976157" url="https://de.wikipedia.org/wiki?curid=2976157" title="Rudolph Franz">
Rudolph Franz

Rudolph Franz (* 16. Dezember 1826 in Berlin; † 31. Dezember 1902 ebenda) war ein deutscher Physiker.

Franz studierte an der Universität Bonn Mathematik und Naturwissenschaften und konnte dieses Studium 1850 erfolgreich mit einer Promotion abschließen. Im gleichen Jahr begann er, als Lehrer am Gymnasium zum Grauen Kloster in Berlin zu arbeiten.

Bereits in diesen Jahren begann er mit seinen Forschungen, welche dann 1857 mit seiner Habilitation an der Berliner Universität endeten. Im Anschluss daran betraute man ihn mit einem Lehrauftrag der "physikalischen Fächer" (vor allem der Wärmelehre), dem er bis 1865 nachkam.

Bekannt geworden ist er durch seine Zusammenarbeit mit Gustav Heinrich Wiedemann, mit dem er 1853 das Wiedemann-Franzsche Gesetz entwickelte. Dieses Gesetz besagt, dass das Verhältnis von elektrischer Leitfähigkeit und Wärmeleitfähigkeit bei allen reinen Metallen bei konstanter Temperatur nahezu gleich ist.


</doc>
<doc id="3495192" url="https://de.wikipedia.org/wiki?curid=3495192" title="Mischbarkeit">
Mischbarkeit

Unter Mischbarkeit versteht man die qualitative Aussage, dass bei der Vermengung von mindestens zwei verschiedenen Flüssigkeiten sie sich vollständig unter Bildung einer einzigen homogenen Phase mischen.

Zum Beispiel lassen sich Wasser und niedere Alkohole in jedem Verhältnis mischen, Wasser und Benzol bilden hingegen zwei Phasen, eine wasserreiche und eine benzolreiche. Auch in der benzolreichen Phase ist allerdings Wasser enthalten (ca. 0,25 Molprozent bei 25 °C) und in der wasserreichen etwas Benzol (0,04 Molprozent bei 25 °C). Damit kann selbst dieses System in sehr begrenztem Umfang als mischbar betrachtet werden.

Die Mischbarkeit hängt bei einigen Stoffgemischen deutlich von der Temperatur ab. Ein Beispiel dafür ist das Gemisch aus Methylvinylketon (3-Buten-2-on) und Wasser, das unterhalb 28 °C und oberhalb 84 °C mischbar ist, jedoch nicht zwischen diesen Grenzen (Mischungslücke, vgl. Abbildung).

Einen großen Einfluss auf die Mischbarkeit zweier Flüssigkeiten hat ihre Polarität; so lassen sich vor allem Flüssigkeiten miteinander mischen, deren Polarität zueinander passt.

Über die Mischbarkeit verschiedener Flüssigkeiten gibt eine Mischbarkeitstabelle Aufschluss.




</doc>
<doc id="3897376" url="https://de.wikipedia.org/wiki?curid=3897376" title="American Association of Physics Teachers">
American Association of Physics Teachers

Die American Association of Physics Teachers (AAPT) ist eine US-amerikanische Gesellschaft mit Physik-didaktischen Zielen. Nach ihren offiziellen Statuten dient sie der „Verbreitung des Wissens über Physik, speziell durch die Lehre“ („dissemination of knowledge of physics, particularly by way of teaching“). Sie hat über 11.000 Mitglieder in über 30 Ländern (2007). Die AAPT gibt die Zeitschriften (beide Peer-reviewed) „American Journal of Physics“ und „The Physics Teacher“ heraus. Es gibt im Jahr zwei nationale Konferenzen im Winter und Sommer sowie Konferenzen der regionalen Teilgesellschaften. Sie organisieren eine Reihe von Wettbewerben für Studenten (u. a. des US Physics Teams für die internationale Physik-Olympiade) und verleiht Preise – ihre höchste Auszeichnung ist die Oersted Medal.

Die Gesellschaft wurde am 31. Dezember 1930 bei einem speziell dafür organisierten Treffen von 45 Physikern während der jährlichen gemeinsamen APS – AAAS Konferenzen gegründet. Die AAPT war eines der Gründungsmitglieder des American Institute of Physics.



</doc>
<doc id="3914850" url="https://de.wikipedia.org/wiki?curid=3914850" title="Carlo Becchi">
Carlo Becchi

Carlo Maria Becchi (* 20. Oktober 1939 in Turin) ist ein italienischer theoretischer Physiker.

Becchi studierte an der Universität Genua, wo er 1962 seinen Abschluss machte. Er ist seit 1976 Professor für theoretische Physik an der Universität Genua. Zweimal (zuerst 1983) war er dort Vorstand der Physik-Fakultät. 1997 bis 2003 war er Vorsitzender des Theorie-Komitees im nationalen italienischen Kernforschungsinstitut INFN.

Becchi begann mit Untersuchungen über den Photoeffekt in der Kernphysik (Thema seiner Dissertation). In den 1960er Jahren befasste er sich mit Quarks und den zugehörigen unitären Symmetrien. Seit 1971 befasste er sich mit Renormalisierungstheorie. Er wurde dabei für seine Entwicklung des BRST-Formalismus mit Raymond Stora und Alain Rouet um 1975 bekannt, einer Methode zur Quantisierung von Systemen mit Nebenbedingungen wie Eichtheorien. 2009 erhielt er dafür den Dannie-Heineman-Preis für mathematische Physik mit Stora, Rouet und Igor Tyutin. Becchi ist seit Anfang der 1990er Jahre Mitherausgeber der Zeitschrift Nuclear Physics B. 

Seit 1991 ist er Herausgeber von Nuclear Physics B.




</doc>
<doc id="3978917" url="https://de.wikipedia.org/wiki?curid=3978917" title="Markus Arndt">
Markus Arndt

Markus Arndt (* 14. September 1965 in Unkel) ist ein deutscher Physiker und Hochschullehrer. Er lehrt Quantennanophysik an der Universität Wien.

Markus Arndt studierte von 1985 bis 1991 Physik an der Universität Bonn und an der Ludwig-Maximilians-Universität München. Im Rahmen seines Doktorats von 1991 bis November 1994 am Max-Planck-Institut für Quantenoptik in Garching arbeitete er mit Antoine Weis in der Arbeitsgruppe von Theodor Hänsch zur optischen und magnetooptischen Spektroskopie an Metallatomen in flüssigem und festem Helium. Von 1994 bis Februar 1995 war er dort bis zum Wechsel nach Paris weiterhin wissenschaftlicher Mitarbeiter. Von 1995 bis 1997 arbeitete er als Feodor Lynen- und DFG- Stipdendiat bei Jean Dalibard an der Ecole Normale Supérieure in Paris über Atomoptik, Atominterferomterie und kalte Atomstöße. Zwischen 1997 und 1999 war er Postdoc bei Anton Zeilinger an der Universität Innsbruck und wechselte von 1999 bis 2002 als Universitätsassistent von Anton Zeilinger ans Institut für Experimentalphysik der Universität Wien, wo die ersten Experimente zur Beugung und Interferometrie des Fullerens C60 realisiert wurden. Arndt habilitierte 2002 über Themen der Atom- und Molekülinterferometrie und war von 2002 bis 2004 außerordentlicher Professor an der Universität Wien. Ab September 2004 war er Vertragsprofessor für Quantennanophysik an der Universität Wien. 2008 wurde er Universitätsprofessor für Quantennanophysik an der Fakultät für Physik der Universität Wien. Seine Forschungsgruppe beschäftigt sich mit der Untersuchung der Materiewelleninterferenz komplexer Moleküle und Nanoteilchen sowie neuen Methoden zur Präparation und Detektion dieser Objekte.

Markus Arndt ist verheiratet und hat zwei Söhne.

Im Jahr 2000 erhielt er den Erich-Schmid-Preis der Österreichischen Akademie der Wissenschaften (ÖAW), gemeinsam mit G. Springholz, sowie den Fritz-Kohlrausch Preis der Österreichischen Physikalischen Gesellschaft (ÖPG).
Im Jahr 2001 wurde er mit dem START-Preis des Fonds zur Förderung der wissenschaftlichen Forschung (FWF) ausgezeichnet, 2008 mit dem FWF Wittgenstein-Preis. 2012 warb er einen "Advanced Grant" des Europäischen Forschungsrats (ERC) ein. Im Jahr 2013 wurde ihm der Preis der Stadt Wien für Naturwissenschaften zuerkannt. 2014 wurde er zum korrespondierenden Mitglied im Inland der mathematisch-naturwissenschaftlichen Klasse der Österreichischen Akademie der Wissenschaften gewählt. Für 2019 wurde ihm der Robert-Wichard-Pohl-Preis der Deutschen Physikalischen Gesellschaft zugesprochen.



</doc>
<doc id="4173964" url="https://de.wikipedia.org/wiki?curid=4173964" title="Forschungsreaktor Hannover">
Forschungsreaktor Hannover

Der Forschungsreaktor Hannover (FRH) war ein Forschungsreaktor mit einer Leistung von 250 kW, der von 1973 bis 1997 an der Medizinischen Hochschule Hannover zu Forschungszwecken betrieben wurde. Bis zum Jahr 2007 wurde die Reaktoranlage zurückgebaut und ist seither aus dem Gültigkeitsbereich des Atomgesetzes entlassen.

Der Baubeginn für den Forschungsreaktor Hannover war am 2. Januar 1969. Nach vierjähriger Bauzeit konnte der Schwimmbadreaktor vom Typ TRIGA-Mark I schließlich am 31. Januar 1973 in Betrieb genommen werden. Er hatte eine Nennleistung von 250 kW, der maximale thermische Neutronenfluss betrug 8 × 10 n/cm s. Der Forschungsreaktor wurde zur Herstellung von Radioaktivität für die Nuklearmedizin, sowie in Zusammenarbeit mit der Universität Hannover für Aktivierungsanalysen in der Werkstoffprüfung verwendet.

Am 18. Dezember 1996 wurde der Reaktor zur Vorbereitung der Stilllegung in den Stillstandsbetrieb versetzt. Im Jahr 1999 wurden alle 76 Uran-Brennelemente aus der Anlage entfernt und in den USA endgelagert. Die Medizinische Hochschule Hannover beantragte am 22. Februar 2002 die Stilllegung der Reaktoranlage beim Niedersächsischen Ministerium für Umwelt, Energie und Klimaschutz. Die Genehmigung zur Stilllegung wurde vom Ministerium schließlich am 8. Mai 2006 erteilt.

Der Forschungsreaktor wurde daraufhin durch die Firma Babcock Noell, einem Tochterunternehmen von Bilfinger Berger, zurückgebaut. Der Reaktorkern wurde im Juni 2006 demontiert. Insgesamt wurden dabei etwa 7 Tonnen radioaktiver Abfall abgebaut und verpackt sowie etwa 40 Tonnen Reststoffe entsorgt. Der radioaktive Abfall wurde an die Landessammelstelle am Forschungszentrum Jülich übergeben. Im Jahr 2007 konnte die Anlage aus dem Gültigkeitsbereich des Atomgesetzes entlassen werden, die endgültige Stilllegung war am 13. März 2008. Die Stilllegung kostete insgesamt knapp 14 Millionen Euro.

In den Räumlichkeiten will die Medizinische Hochschule Hannover nun ein Zyklotron zur Produktion von Radionukliden in der Positronen-Emissions-Tomographie errichten.




</doc>
<doc id="4421553" url="https://de.wikipedia.org/wiki?curid=4421553" title="David C. Cassidy">
David C. Cassidy

David Charles Cassidy (* 1945) ist ein US-amerikanischer Wissenschaftshistoriker, bekannt für seine Biographie über Werner Heisenberg.

Cassidy studierte Physik an der Rutgers University und promovierte 1976 in Wissenschaftsgeschichte an der Purdue University ("Werner Heisenberg and the crisis in quantum theory 1920–1925"). Danach war er an der University of California, Berkeley und als Humboldt-Fellow an der Universität Stuttgart bei Armin Hermann. Er war Dozent an der Universität Regensburg. Seit 1990 ist er Professor an der Hofstra University in Hempstead. 1992 wurde er Fellow der American Physical Society.

Cassidy war sieben Jahre lang Mitherausgeber der gesammelten Werke von Albert Einstein bei Princeton University Press und war Berater bei den Werkausgaben von Heisenberg, Wolfgang Pauli, Niels Bohr.

Für seine Heisenberg-Biographie erhielt er den Pfizer-Preis. Außerdem erhielt er den Science Writing Award des American Institute of Physics. Er schrieb neben seiner Heisenberg-Biographie auch den Artikel über Heisenberg im neuen "Dictionary of Scientific Biography". Cassidy war Vorsitzender der Sektion Wissenschaftsgeschichte und -philosophie der New York Academy of Sciences. Für 2014 wurde ihm der Abraham-Pais-Preis zugesprochen.




</doc>
<doc id="4664589" url="https://de.wikipedia.org/wiki?curid=4664589" title="Seitwärtsdämpfung">
Seitwärtsdämpfung

Seitwärtsdämpfung ist ein Begriff, der bei Richtmikrofonen in den Mikrofondaten zu finden ist. Die Angabe ist die Dämpfung, die von der Seite des Mikrofons, also aus der 90°-Schalleinfallsrichtung gemessen und in Dezibel (dB) als Schallpegeldämpfung angegeben wird. Eine hohe Seitwärtsdämpfung sorgt auch für gute Rückkopplungsunterdrückung bei Lautsprecherbeschallung aus dieser Einfallsrichtung. Üblich ist bei Großmembranmikrofonen und Kleinmembranmikrofonen der Richtcharakteristik Superniere bei 1 kHz eine Seitwärtsdämpfung von größer 8,7 dB. Die Seitwärtsdämpfung ist immer frequenzabhängig. Bei einer breiten Niere ist die Seitwärtsdämpfung etwa 3,5 dB. 

Hierzu einordnen lässt sich auch der Begriff Rückwärtsdämpfung beim Mikrofon. Das ist die Schallpegeldämpfung 180° zur Schalleinfallsrichtung.

Auch bei Richtantennen gibt es die Seitwärtsdämpfung, die neben dem Antennengewinn einen wichtigen Parameter in der Antennentechnik darstellt.




</doc>
<doc id="5018935" url="https://de.wikipedia.org/wiki?curid=5018935" title="Kalorimetrie">
Kalorimetrie

Die Kalorimetrie bezeichnet die Messung von Wärmemengen, die an biologische, chemische oder physikalische Vorgänge gekoppelt sind und sowohl exotherm als auch endotherm sein können. Für die Bestimmung wird ein Kalorimeter verwendet. Unterschieden wird zwischen direkter und indirekter Kalorimetrie. Begründet wurde diese Wissenschaft im Jahr 1756 von Joseph Black, Professor für Physik an der Universität von Glasgow und Schüler von William Cullen, dem Erfinder der ersten, noch rein experimentellen Eismaschine.

Bei der direkten Kalorimetrie werden die Wärmemengen mittels eines Kalorimeters bestimmt. Diese Methode ist auch zur Messung des Energieumsatzes eines Organismus geeignet.

Das erste derartige Tierkalorimeter wurde von Antoine Laurent de Lavoisier gebaut (siehe Abb.). Das für ein Meerschweinchen ausgelegte Kalorimetergefäß stellt den Kern einer Kugel dar, die von zwei doppelwandigen Schalen umgeben war. Beide Schalen wurden mit Eisstücken gefüllt. Die äußere Schale diente dazu, das Gerät zu isolieren und das Innere zu einem geschlossenen System zu machen. Von außen in das Gerät eintretende Wärme wird dabei zum Schmelzen des Eises in der äußeren Schale benötigt. So bleibt die Trennwand zwischen äußerer und innerer Schale konstant auf 0 °C, solange noch Eis in der äußeren Schale vorhanden ist. Das Eis in der inneren Schale dient dann dazu, die Wärmebildung des Tieres zu messen. Die vom Tier gebildete Wärmemenge wird zum Schmelzen des Eises benötigt. Das Schmelzwasser fließt bei 0 °C ab. Die ausgeflossene Schmelzwassermenge multipliziert mit der Schmelzwärme dient als Maß der vom Tier abgegebenen Wärme. Vorausgesetzt wird hier, dass die Körperwärme des Tieres konstant bleibt.

Bei der indirekten Kalorimetrie wird die frei werdende Wärmemenge indirekt über den gemessenen Sauerstoffverbrauch berechnet. Aus der Menge an Sauerstoff, die z. B. eine Reaktion oder ein Organismus verbraucht, und dem Äquivalent (z. B. oxykalorisches Äquivalent bei metabolischen Reaktionen) lässt sich auf die freigesetzte Wärme zurückrechnen. Dies bietet sich vor allem bei großen Lebewesen wie z. B. dem Menschen an.

Die Gaskalorimetrie ist eine Methode zur Bestimmung des Brennwertes eines Gases mittels Verbrennung einer Gasprobe.

Die Gaskalorimeter werden entsprechend der angewandten Verfahren in folgende Klassen eingeteilt:



</doc>
<doc id="5066871" url="https://de.wikipedia.org/wiki?curid=5066871" title="Tuningkurve">
Tuningkurve

In der Neurowissenschaft ist die Tuningkurve (englisch "tuning curve") eine mathematische Beschreibung zur Charakterisierung des Antwortverhaltens einer sensorischen Nervenzelle auf einen bestimmten externen Reiz. In Abhängigkeit eines bestimmten Reizparameters (Stimulus) antwortet die Nervenzelle mit einer durchschnittlichen Feuerrate (Response).

Sensorische Nervenzellen haben typischerweise die Eigenschaft, auf verschiedene Reizarten zu reagieren, indem sie Aktionspotentiale (auch "Spikes" genannt) erzeugen. Die Tuningkurve formula_1 beschreibt die durchschnittliche Anzahl an Aktionspotentialen (also die Feuerrate formula_2) einer Nervenzelle als Funktion genau eines bestimmten Reizparameters (formula_3). Aus dieser Definition ergibt sich, dass eine Nervenzelle in der Regel mehrere Tuningkurven besitzen kann, und dass eine Tuningkurve allein nur unzureichend dafür geeignet ist, das Verhalten einer Nervenzelle zu charakterisieren.


</doc>
<doc id="5410928" url="https://de.wikipedia.org/wiki?curid=5410928" title="Georg Ludwig Alefeld">
Georg Ludwig Alefeld

Georg Ludwig Alefeld (* 1. November 1732 in Gießen; † 20. November 1774 ebenda) war ein deutscher Mediziner und Physiker.

Der Sohn des Johann Ludwig Alefeld (1695–1760) hatte von 1748 bis 1756 ein Studium der Medizin an der Universität Gießen und an der Universität Straßburg absolviert. In Gießen promovierte er am 7. Oktober 1756 zum Lizentiaten und Doktor der Medizin. Am 11. November wurde er in Gießen Privatdozent, übernahm 1760 eine außerordentliche Professur an der medizinischen Fakultät, erwarb am 29. Dezember 1761 den philosophischen Magistergrad, wurde am 26. Mai 1762 ordentlicher Professor der Medizin und daneben vom 15. Juli 1762 bis 1766 Professor der Physik. 1765 wurde er Mitglied der Kur-Maizer Akademie der Wissenschaften in Erfurt.




</doc>
<doc id="6276997" url="https://de.wikipedia.org/wiki?curid=6276997" title="Carl Holtzmann">
Carl Holtzmann

Carl Alexander Holtzmann, auch Karl Holtzmann, (* 23. Oktober 1811 in Karlsruhe; † 25. April 1865 in Stuttgart) war ein deutscher Physiker.

Carl Holtzmann wurde 1811 in Karlsruhe geboren, wo sein Vater Joh. Michael Holtzmann am Lyzeum Philosophie und klassische Sprachen unterrichtete. Seine Brüder sind der Theologe Karl Julius Holtzmann und der Germanist Adolf Holtzmann. Holtzmann studierte an der damals neu gegründeten polytechnischen Schule in Karlsruhe, wo er sich dem Berg- und Hüttenwesen zuwandte. Er war verheiratet und zu seinen Söhnen gehörte der Theologe Oskar Holtzmann. Ein Enkel war der Darmstadter Bürgermeister und Landtagsabgeordnete Ernst Holtzmann.

Als Lehrer unterrichtete Holtzmann am Karlsruher Polytechnikum und am Mannheimer Lyzeum Mathematik und Physik. 1845 wurde er Hüttenverwalter des Großherzoglichen Hüttenwerks Albbruck bei Waldshut.

Im Jahr 1851 erhielt er die Professur für Physik und Mechanik am Stuttgarter Polytechnikum. Mehrfach war er dort Direktor und galt als „Bahnbrecher im Stuttgarter Hochschulwesen“. Als Wissenschaftler hat er mehrere Lehrbücher verfasst und im Bereich der Wärmelehre und der Mechanik Bedeutendes geleistet (Stephen Brush zählt ihn zu den etwa ein Dutzend Wissenschaftlern, die im 19. Jahrhundert einen mehr oder weniger allgemeinen Energieerhaltungssatz formulierten). In Stuttgart ist er am 25. April 1865 gestorben.




</doc>
<doc id="7062762" url="https://de.wikipedia.org/wiki?curid=7062762" title="Biochemical and Biophysical Research Communications">
Biochemical and Biophysical Research Communications

Biochemical and Biophysical Research Communications ist eine wissenschaftliche Fachzeitschrift, die oft auch "BBRC" oder "Biochem. Biophys. Res. Com." abgekürzt wird. Die Erstausgabe erschien im Juli 1959. Folgende Bereiche werden von der Zeitschrift abgedeckt: Biochemie, Biophysik, Zellbiologie, Entwicklungsbiologie, Immunologie, Molekularbiologie, Neurobiologie, Botanik und Proteomik.

Der Impact Factor lag im Jahr 2014 bei 2,297. Nach der Statistik des ISI Web of Knowledge wird das Journal mit diesem Impact Factor in der Kategorie "Biochemie und Molekularbiologie" an 180. Stelle von 289 Zeitschriften und in der Kategorie "Biophysik" an 41. Stelle von 73 Zeitschriften geführt.

Chefherausgeber ist Ernesto Carafoli von der Universität Padua.



</doc>
<doc id="7270649" url="https://de.wikipedia.org/wiki?curid=7270649" title="Periodenlücke">
Periodenlücke

Die Periodenlücke () beschreibt in der Astronomie, dass bei den nicht-magnetischen kataklysmischen Veränderlichen nur wenige mit Umlaufdauern zwischen zwei und drei Stunden zu beobachten sind.

Die kataklysmischen Veränderlichen bestehen aus einem Roten Zwerg und einem Weißen Zwerg. Der Rote Zwerg füllt sein Roche-Grenzvolumen aus und transferiert Materie an seinen Begleiter, den Weißen Zwerg. Aufgrund des Erhalts des Drehimpulses bildet sich um den Weißen Zwerg eine Akkretionsscheibe, durch die Materie auf den Weißen Zwerg fließt. Die Periodenlücke wird verursacht durch ein Schrumpfen des Durchmessers des Roten Zwergs aufgrund einer Änderung des inneren Aufbaus, wenn der Energietransport in dem Stern vollständig konvektiv wird.

Alle diese nicht-magnetischen kataklysmischen Veränderlichen zeigen eine starke Unterhäufigkeit bei Bahnumlaufdauern zwischen 2,15 und 3,18 Stunden. Die Anzahl der beobachteten kataklysmischen Veränderlichen liegt innerhalb der Periodenlücke um einen Faktor 20 niedriger als ober- und unterhalb der Periodenlücke. Wenn der Abstand zwischen den Sternen zu einem Wert von 3,18 Stunden führt, verfügt der Rote Zwerg über eine Masse von circa 0,4 Sonnenmassen, bei welcher der Energietransport im gesamten Stern ausschließlich mittels Konvektion erfolgt. In der Folge schrumpft der Begleiter aufgrund seines geänderten Aufbaus unter die Roche-Grenzfläche, woraufhin der Materiefluss abreißt und die kataklysmische Aktivität ausklingt. Innerhalb der Periodenlücke kommt es zu einem langsamen Drehimpulsverlust aufgrund der Abstrahlung von Gravitationswellen, wobei dieser Mechanismus bis zu einer Milliarde Jahre braucht, um das Doppelsternsystem bei einer Umlaufdauer von 2,15 Stunden wieder in Kontakt zu bringen. Es gibt einige aktive kataklysmische Veränderliche innerhalb der Periodenlücke, wobei diese wahrscheinlich innerhalb der Periodenlücke erstmals die Roche-Grenzfläche ausfüllten und der Materietransfer einsetzte. Daneben sind auch Doppelsterne aus prä-kataklysmischen Veränderlichen wie GD 448 und SDSS 1355+0856 beobachtet worden, die direkt aus der Common-Envelope-Phase in die Periodenlücke gewandert sind. Diese Doppelsterne sind noch getrennt und mangels Massentransfers nur schwer nachzuweisen,

Die Periodenlücke kann bei den nicht-magnetischen kataklysmischen Veränderlichen nachgewiesen werden, wobei die Unterteilung anhand ihrer Lichtkurve erfolgt:
Bei den magnetischen kataklysmischen Veränderlichen erfolgt der Massentransfer auf den Weißen Zwerg nicht über eine Akkretionsscheibe, sondern der Materiefluß geschieht entlang der Magnetfeldlinien des kompakten Sterns. Da es kaum AM-Herculis-Sterne und DQ-Herculis-Sterne mit Bahnumlaufdauern unterhalb von drei Stunden gibt, wird bei den magnetischen kataklysmischen Veränderlichen nicht von einer Periodenlücke gesprochen.

Das oben beschrieben Modell würde eine große Anzahl an alten magnetisch aktiven Roten Zwergen mit Rotationsperioden zwischen zwei und drei Stunden erwarten lassen in Form von BY-Draconis-Sternen durch Sternflecken oder durch Flares bei UV-Ceti-Sternen. Es gibt bei jungen Roten Zwergen, die ebenfalls noch schnell rotieren, eine große Anzahl dieser Veränderlichen mit einem Spektraltyp zwischen M4 und M6, entsprechend dem Spektraltyp der kataklysmischen Veränderlichen innerhalb der Periodenlücke. Aber diese kinematisch alten magnetisch aktiven Rote Zwerge in kataklysmischen Veränderlichen sind nie beobachtet worden. Eventuell rotieren die Rote Zwerge in inaktiven kataklysmischen Veränderlichen nicht differentiell und sind deshalb nicht magnetisch aktiv.


</doc>
<doc id="7314923" url="https://de.wikipedia.org/wiki?curid=7314923" title="Theodorus Moretus">
Theodorus Moretus

Theodorus Moretus, auch bekannt als Theodor oder Theodore Moretus (* 9. Februar 1602 in Antwerpen; † 6. November 1667 in Breslau) war ein flämischer Mathematiker, Physiker und Astronom und gehörte dem Jesuiten-Orden an. Er war Professor an den Jesuiten-Hochschulen in Prag, Olmütz und Breslau.

Moretus wurde als Sohn von Peter und Henriette Moerentorf (latinisiert Moretus) geboren. Er stammt aus einer berühmten Drucker- und Verleger-Familie: Seine Mutter war die jüngste Tochter von Christoph Plantin (um 1520–1589), und Theodorus war ein Neffe von Jan I. Moretus und Vetter von Balthasar I. Moretus.

Im Jahre 1618 trat er in Mechelen in den Jesuiten-Orden ein. Als Gehilfe von Gregor von St. Vincent kam er 1629 nach Prag, übernahm dann ab 1632 die Physik-Professur in Olmütz, kehrte aber 1634 wieder nach Prag zurück. In den folgenden Jahren war er an verschiedenen Jesuiten-Kollegien in Böhmen und Mähren tätig. Schließlich wurde er 1659 mit der Mathematik-Professur in Breslau betraut, die er bis zu seinem Tode wahrnahm.

Theodorus Moretus war ein Mathematiker mit außerordentlich breit gefächerten Interessen. Moretus’ mathematische Werke behandeln vorwiegend praktische Anwendungen dieser Wissenschaft. Aus heutiger Sicht würde man sie zum überwiegenden Teil in die Physik einordnen. Sie behandeln beispielsweise Fragen der Akustik und der Hydrostatik. Aufgrund seiner astronomischen Tätigkeit wurde der Mondkrater Moretus nach ihm benannt.




</doc>
<doc id="7339428" url="https://de.wikipedia.org/wiki?curid=7339428" title="Cherenkov Telescope Array">
Cherenkov Telescope Array

Das Cherenkov Telescope Array (CTA) ist ein 2010 gestartetes Projekt eines internationalen Konsortiums zur erdbasierten Gammastrahlen-Astronomie. Dabei werden durch die Beobachtung von Tscherenkow-Blitzen in der Erdatmosphäre Rückschlüsse auf astronomische Gammastrahlenquellen wie Galaxien und Supernovae gezogen. Das Projekt befindet sich seit 2010 in der Planungs- beziehungsweise Prototypenphase.

Angestrebt wird die Errichtung zweier erweiterbarer Raster aus einigen zehn Tscherenkow-Teleskopen (engl. IACTs, Imaging Atmospheric Cherenkov Telescopes). Für die Einzelteleskope sind standardisierte Modelle mit 6, 12 und 24 Metern Reflektordurchmesser vorgesehen, die sich zurzeit in der Entwicklung bzw. Erprobung befinden. Das erste Raster – in der nördlichen Hemisphäre – soll hauptsächlich der Beobachtung extragalaktischer Objekte geringer Energie dienen, während das zweite Raster – auf der südlichen Hemisphäre – den gesamten Energiebereich abdecken und sich auf Quellen in der Milchstraße konzentrieren soll.

Mit einem Messbereich von einigen zehn GeV bis über 100 TeV soll das CTA die Empfindlichkeit der aktuellen Generation von Tscherenkow-Teleskopen wie MAGIC, HESS und VERITAS um eine Größenordnung übertreffen. Die wissenschaftlichen Anwendungen des CTA reichen von Hochenergie-Astrophysik über Grundlagenphysik bis hin zu Kosmologie.

Das CTA wird in internationaler Zusammenarbeit von Wissenschaftlern geplant und errichtet, mit einer starken Beteiligung europäischer Institutionen wie dem Europäischen Strategieforum für Forschungsinfrastrukturen ESFRI, dem Europäischen Astroteilchenphysik-Netzwerks ASPERA und dem europäischen Astrophysiknetzwerk ASTRONET. Das administrative Projektbüro hat seinen Sitz an der Landessternwarte Heidelberg-Königstuhl. Während der Vorbereitungsphase des CTA ist es ein Schlüsselziel, das Projekt Studenten, Schülern und der Allgemeinheit näherzubringen. Dazu wurden diverse Materialien herausgegeben.

Die Vorbereitungsphase wurde im August 2014 abgeschlossen. Der Baustandort auf der südlichen Hemisphäre – es standen die Atacamawüste in Chile und die Farm Aar in Namibia zur Auswahl – wurde am 15. und 16. Juli 2015 bekanntgegeben. Das CTA-Konsortium entschied sich für Chile.



</doc>
<doc id="7360050" url="https://de.wikipedia.org/wiki?curid=7360050" title="American Journal of Physics">
American Journal of Physics

Der American Journal of Physics ist eine monatlich erscheinende Zeitschrift der American Association of Physics Teachers und des American Institute of Physics. Sie hat Peer Review und veröffentlicht überwiegend Artikel zur Physik-Didaktik. Es werden aber auch historische und kulturelle Aspekte der Physik behandelt.

In einer regelmäßigen Rubrik werden Resource Letters zu verschiedenen physikalischen Themen veröffentlicht.

Die Zeitschrift erschien zuerst ab 1933 als "American Physics Teacher" (der vierteljährlich und später alle zwei Monate erschien). Seinen heutigen Namen hat die Zeitschrift seit 1940.

Die ISSN ist 0002-9505.


</doc>
<doc id="7451924" url="https://de.wikipedia.org/wiki?curid=7451924" title="Martin Weisskopf">
Martin Weisskopf

Martin Charles Weisskopf (* 21. April 1942 in Omaha, Nebraska) ist ein US-amerikanischer Astrophysiker, der sich mit Röntgenastronomie befasst.

Weisskopf studierte am Oberlin College mit dem Bachelor-Abschluss 1964 und wurde 1969 an der Brandeis University promoviert. Als Post-Doktorand forschte er an der Columbia University, an der er 1971 Assistant Professor wurde. In den 1970er Jahren war er an Pionierexperimenten in der Röntgenastronomie beteiligt. 1977 ging er als "Senior Astronomer" ans Marshall Space Flight Center der NASA, wo er Projektwissenschaftler am Chandra-Röntgenteleskop war. 

Er ist am International X-Ray Imaging Experiment (IBIS) der ESA beteiligt und an einem 1978 von ihm begründeten großen Forschungsprogramm zur Entwicklung von Röntgenoptiken. 

2004 erhielt er den Bruno-Rossi-Preis mit Harvey Tananbaum für ihre Führungsrolle beim Chandra Teleskop, 2006 erhielt er den George W. Goddard Award. Er erhielt verschiedene Preise der NASA, unter anderem die Medals for Exceptional Service 1992 und für Scientific Achievement 1999. Er wurde 1994 Fellow der American Physical Society für "Pionierleistungen in Röntgen-Polarimetrie und Studien zur Zeitveränderlichkeit kosmischer Röntgenquellen und seine Führungsrolle als Projektwissenschaftler der Advanced X-ray Astrophysics Facility". 2001 wurde er Fellow der International Society for Optical Engineering (SPIE); er ist Fellow der American Association for the Advancement of Science.



</doc>
<doc id="7464156" url="https://de.wikipedia.org/wiki?curid=7464156" title="Raymond G. Arnold">
Raymond G. Arnold

Raymond George Arnold (* 1942) ist ein US-amerikanischer experimenteller Kernphysiker am Stanford Linear Accelerator Center (SLAC).

Arnold studierte an der Alaska Methodist University in Anchorage (heute Alaska Pacific University) mit dem Bachelor-Abschluss 1964 und wurde 1972 an der Boston University bei Edward Booth promoviert. Als Post-Doktorand war er bei Benson Chertok an der American University in Washington D. C. und forschte am SLAC, wo er die Struktur von leichten Kernen wie Helium und dem Deuteron und schwereren Kernen mit Elektronenstreuung studierte. In den 1980er und 1990er Jahren untersuchte er die Quarkstruktur von Nukleonen und Kernen mit Elektronenstreuung am SLAC, wobei er Gruppenleiter oder Sprecher der Experimente war. Zum Beispiel bestimmte er die tiefinelastischen Spin-Strukturfunktionen von Proton und Neutron mit der Streuung von polarisierten Elektronen und Targets.

Für seine Experimente, die eine Verbindung von Kernphysik zur Quantenchromodynamik und dem Quarkbild schlugen, erhielt Arnold 2000 den Tom-W.-Bonner-Preis für Kernphysik. Er ist Fellow der American Physical Society. Er war auch Berater beim Jefferson Lab.


Arnold veröffentlichte mehr als 40 Aufsätze in der Physical Review, darunter:


</doc>
<doc id="7811102" url="https://de.wikipedia.org/wiki?curid=7811102" title="Tidal Disruption Event">
Tidal Disruption Event

Der Begriff Tidal Disruption Event ( für "Gezeiten-Sternzerrissereignis") beschreibt in der Astronomie die nahe Begegnung eines Sterns mit einem supermassiven Schwarzen Loch im Kern einer Galaxie, wodurch ein Teil der Materie des Sterns sein Gravitationsfeld verlässt und eine Akkretionsscheibe um das Schwarze Loch bildet. Dieser Vorgang sollte ruhige galaktische Kerne wieder in aktive galaktische Kerne umwandeln und mit einer Rate von 0,0001 pro Galaxie und Jahr ein vergleichsweise häufiges Ereignis sein.

Um das Zentrum der Milchstraße Sagittarius A* sind die S-Sterne gefunden worden. Die S-Sterne sind frühe massereiche Blaue Riesen mit einer Umlaufdauer von wenigen Jahren. Durch Streuung aufgrund naher Begegnungen wird alle 100 bis 10.000 Jahre ein Stern dem supermassiven Schwarzen Loch so nahe kommen, dass sich Teile des Sterns innerhalb der Roche-Grenze des Schwarzen Lochs befinden. Diese Materie verlässt den gravitativen Einfluss des Sterns und bildet eine Akkretionsscheibe um das Schwarze Loch. Neben dem Einfangen von Gaswolken stellt dies eine zweite Variante dar, dass Materie in ein Schwarzes Loch im Zentrum einer Galaxie befördert wird und dieses damit in einen aktiven galaktischen Kern umwandelt. Ein Tidal Disruption Event führt aufgrund der Viskosität der Materie in der Akkretionsscheibe zu einem Ausbruch im Ultraviolett und im Röntgenbereich, der als "Tidal Flare" bezeichnet wird.

Ein Tidal Flare sollte Schwarzkörpertemperaturen zwischen 0,04 und 0,12 keV haben und in der Größenordnung von Monaten bis Jahren andauern. Die Leuchtkraft kann bis zu 10 erg/s erreichen. Der Anstieg der Leuchtkraft liegt in der Größenordnung von Tagen bis Wochen in Abhängigkeit von der Masse des Schwarzen Lochs, wobei geringere Massen zu einem steileren Anstieg in der Lichtkurve führen. Neben der Masse kann mit Hilfe der Tidal Flares auch die Rotationsperiode der Schwarzen Löcher untersucht werden. Der Helligkeitsabfall sollte mit einem Power law von −5/3 abnehmen und dieser Abfall sollte zur Identifizierung von Tidal Disruption Event genutzt werden können, da kein anderes bekanntes Ereignis zu einem derartigen Rückgang der Leuchtkraft führt.

Kandidaten für Tidal Disruption Events bzw. Tidal Flares sind:


</doc>
<doc id="8038754" url="https://de.wikipedia.org/wiki?curid=8038754" title="Vladimir Jarmolenko">
Vladimir Jarmolenko

Vladimir Jarmolenko (* 15. Mai 1948 in Astrachan) ist ein litauischer Politiker.

Nach dem Abitur 1966 in Kaunas absolvierte er 1972 das Diplomstudium der Biophysik am Kauno medicinos institutas (KMI). Ab 1971 arbeitete er in der litauischen Filiale des sowjetischen Forschungsinstituts für Butter- und Käse-Industrie als Techniker, Ingenieur, Physiker, wissenschaftlicher Mitarbeiter. Von 1977 bis 1980 absolvierte er die Aspirantur am Institut für Biophysik Pusczin. Ab 1983 arbeitete er als Labor-Leiter und ab 1986 lehrte im Lehrstuhl für biologische und bioorganische Chemie am KMI.
Von 1992 bis 2000 war er Mitglied im Seimas.

Seit 1993 ist er Mitglied der Tėvynės Sąjunga.

1999 arbeitete er im Außenministerium Litauens, ab 2006 als Charge d'affaires in Rumänien.



</doc>
<doc id="8790453" url="https://de.wikipedia.org/wiki?curid=8790453" title="Festkörperionik">
Festkörperionik

Die Festkörperionik () ist ein wissenschaftliches und technisches Fachgebiet, das aus Teilgebieten der Festkörperphysik und der Elektrochemie besteht.

Die Festkörperionik behandelt Feststoffe, die elektrisch leitend sind und bei denen diese Leitfähigkeit vor allem auf der Beweglichkeit von Ionen im Feststoff beruht. Diese festen Ionenleiter nennt man auch Festelektrolyte; daher kann die Festkörperionik auch als die Wissenschaft von den Festelektrolyten bezeichnet werden. Die Festkörperionik hat wichtige Anwendungen, beispielsweise in der Sensorik und bei Hochtemperatur-Brennstoffzellen (Festoxidbrennstoffzelle); diese Anwendungen werden unter Anwendungsbeispiele weiter ausgeführt.

Die elektrische Leitfähigkeit kommt dadurch zustande, dass zumeist eine Ionensorte im Festkörper beweglich ist. Die Ionen hüpfen von einem freien Gitterplatz zum nächsten, wobei für den Sprung eine Aktivierungsenergie "E" überwunden werden muss. Dann gibt das einfache Modell für die Leitfähigkeit σ eine der Arrheniusgleichung entsprechende Abhängigkeit, d. h.

mit
Für experimentell ermittelter Leitfähigkeiten σ werden oft die Logarithmen ln σ gegen den Kehrwert der Temperatur (1/"T") aufgetragen. Zumeist wird die angegebene Gleichung in guter Näherung erfüllt und es ergeben sich Geraden, die mit einer Steigung −"E"/"R" abfallen.

Bereits 1854 hatte Heinrich Buff festgestellt, dass der Widerstand von Glas mit steigender Temperatur abnimmt. Im Gegensatz zum üblichen Verhalten bei Metallen, deren Leitfähigkeit mit steigender Temperatur sinkt, nimmt sie demgemäß bei Festelektrolyten mit steigender Temperatur zu. 1884 zeigte Emil Warburg, dass bei der Stromleitung eine Elektrolyse des Glases stattfindet, die zu einer schlecht leitenden natriumarmen Kieselsäureschicht an der Anode führt. Die Bildung dieser Schicht kann verhindert werden, wenn an der Anodenseite Natriumamalgam verwendet wird. Er schloss daraus, dass die Anionen des Glases stehenbleiben und dass allein das Kation, d. h. das Natriumion, durch das Glas hindurchwandert. Walther Nernst patentierte 1897 eine erste technische Anwendung fester Elektrolyte, die Nernstlampe. Die AEG kaufte das Patent und erleuchtete 1900 bei der Weltausstellung in Paris einen Pavillon mit 800 Nernstlampen. 1960 prägte der Japaner Takehiko Takahashi den Begriff „solid state ionics“, und der anschließende Aufschwung in der Festkörperionik trug dazu bei, sie als Fachgebiet zu etablieren. Seit 1980 hat es mit der Zeitschrift „Solid State Ionics“ ein eigenes regelmäßig erscheinendes Mitteilungsorgan.

An vielen Universitäten wird zur Festkörperionik geforscht, in Deutschland z. B. in Gießen, Kiel und Marburg, in Österreich z. B. in Wien. Außereuropäische Forschung zur Festkörperionik findet z. B. in Pasadena oder Sendai statt.


</doc>
<doc id="9061620" url="https://de.wikipedia.org/wiki?curid=9061620" title="Joaquín Drut">
Joaquín Drut

Joaquín Emiliano Drut (* 1978 in La Plata) ist ein argentinischer theoretischer Physiker, der sich mit quantenmechanischer Vielteilchentheorie in der Festkörperphysik und Kernphysik befasst.

Drut wuchs in La Plata auf und studierte 2003 bis zur Promotion 2008 an der University of Washington Physik. Als Post-Doktorand war er in der Gruppe für theoretische Kernphysik der Ohio State University und 2010 bis 2012 in der Theorieabteilung des Los Alamos National Laboratory. 2012 wurde er Assistant Professor und Melchor Fellow an der University of North Carolina at Chapel Hill.

Er entwickelte Quanten-Monte-Carlo-Simulationsverfahren auf dem Gitter für Vielteilchensysteme in der Festkörperphysik, unter anderem für Graphen (wobei sie sich für einen Isolator als Grundzustand aussprechen) und einen Übergang von BCS-Supraleiter zu Bose-Einstein-Kondensat in einem Fermionengas bei tiefen Temperaturen, den sie als Supraflüssigkeit neuer Art einstuften.

2009 erhielt er den Hermann Kümmel Award in Vielteilchentheorie, für "die Bestimmung der thermodynamischen und Pairing-Eigenschaften eines verdünnten wechselwirkenden Spin 1/2 Fermigases (drei Dimensionen) im unitären Regime unter Verwendung von Quanten-Monte-Carlo- und feldtheoretischen Methoden".




</doc>
<doc id="9525703" url="https://de.wikipedia.org/wiki?curid=9525703" title="Kreiselspiel">
Kreiselspiel

Kreiselspiel ist die Gattungsbezeichnung, unter der in der Spielwissenschaft die Vielzahl verwandter Spielformen mit dem physikalischen Spielzeug Kreisel gesammelt und systematisiert wird.

Kreiselspiele sind in fast allen Ländern der Welt in bunter Vielfalt verbreitet. Sie können dort oft schon eine lange Tradition vorweisen und haben in ihrer Geschichte zahlreiche Spielzeugvarianten, Spielformen und Namensgebungen hervorgebracht.

Im europäischen Kulturraum finden sich Kreiselspiele schon früh in den Niederlanden auf dem Gemälde „Die Kinderspiele“ des Bauernmalers Pieter Brueghel der Ältere aus dem Jahr 1560 dokumentiert und bildlich dargestellt.

Kreiselspiele waren zweihundert Jahre später nachweislich auch dem Philanthropen Johann Bernhard Basedow bekannt. Sie wurden in seinem Dessauer Philanthropinum gelehrt und praktiziert und finden sich in seinem „Elementarwerk“ von 1774 publiziert. Der Grafiker und Illustrator Daniel Chodowiecki hat sie in demselben Buch ins Bild gesetzt. Ein weiterer führender Kopf der reformpädagogischen Bewegung der Zeit der Aufklärung, Johann Christoph Friedrich GutsMuths, hat die Spiele in seine berühmte Spielesammlung von 1796 aufgenommen.

Das Hand- oder Tischkreiseln ist ein beschauliches Spiel, bei dem unterschiedliche Kreiseltypen von Hand, etwa durch Reiben zwischen den beiden Handflächen oder durch eine Zwitschbewegung von Daumen, Zeige- und Mittelfinger, in eine Drehbewegung versetzt werden. Spiele dieser Art waren schon zur Goethezeit bei Kindern wie Erwachsenen als Gesellschaftsspiel zur Pfandeinforderung bekannt und beliebt.

Das Sakai-Kreiselspiel bedient sich einer Drahtkonstruktion, etwa aus einer Büroklammer, die so gebogen wird, dass sie sich als Kreisel auf einer waagerechten Unterlage bewegen lässt. Im Jahre 1986 ursprünglich von einem japanischen Physikprofessor erdacht, um seinen Studenten das Prinzip des Kreiselspiels zu vermitteln, entwickelte es sich zu einem einfach herzustellenden Tischkreisel, der dem Spieltrieb Gelegenheit des Auslebens verschafft.

Das Nimmgibspiel (in England auch als „Put and Take“ bekannt) datiert aus dem ersten Drittel des 20. Jahrhunderts. Das bei Kindern beliebte Spielzeug besteht aus einem winzigen Kreisel, der auf seinen sechs abgeflachten Seiten verschiedene Spielaufforderungen enthält wie „Nimm zwei“, „Gib eins“ oder „Nimm alles“. Dazu wird der Kreisel per Hand an seinem Griff gedreht, um eine Antwort des würfelähnlichen Spielzeugs zu provozieren. Der Nimmgib wandert reihum in der Spielerrunde und verspricht dem einzelnen Mitspieler Gewinn oder Verlust von kleinen Einsätzen wie Süßigkeiten oder Murmeln.

Das Dreidelspiel, auch „Toton“ genannt, wird mit einem Kreisel gespielt, der vier Seiten aufweist, von denen jede eine andere Beschriftung trägt. Nach dem Drehen fungiert er wie ein Würfel, der nach dem Niederfallen dem jeweiligen Spieler signalisiert, was er tun soll oder was er gewonnen hat, dass er etwa ein Pfand hinterlegen muss, dass ihm der Spielerpott zufällt oder auch nichts passiert. Der Bauernmaler Pieter Brueghel der Ältere hat auch dieses Spiel auf seinem Gemälde Die Kinderspiele aus dem Jahr 1560 bereits dokumentiert. In Israel wird der Dreidel noch heute beim Lichterfest Chanukka von den Kindern gedreht, wobei die aufscheinende Oberseite den Gewinn, etwa eine Süßigkeit, verkündet.

Das Wackelsteinspiel reicht angeblich bis in keltische Zeiten zurück und wurde damals von Priestern zu rituellen Zwecken und Wahrsagungen eingesetzt. Das namengebende ellipsoidförmige, schiffchenartige Spielzeug ist aus unterschiedlichen Materialien wie Holz, Stein oder Plastik gefertigt und fungiert als Kreisel. Wenn eines seiner aufwärts gebogenen Enden leicht gedrückt wird, entsteht eine Unwucht, die den Kreisel in eine Wackel-, Schwing- und Drehbewegung versetzt, die sich aus der anfänglichen Richtung auch wieder rückläufig orientieren kann.

Das Brummkreiselspiel erhielt seinen Namen nach dem von ihm beim Kreisen erzeugten Geräusch: Durch kleine Öffnungen im Kreiselkörper, der in der Regel aus Blech besteht, lassen sich durch das Ausströmen von Luft aus dem Innern durch die Fliehkräfte der Drehbewegung unterschiedliche Töne hervorbringen. Durch das Vibrieren kleiner Metallzungen entsteht ein Summton, der je nach Drehgeschwindigkeit des Kreisels eine unterschiedliche Tonhöhe erreicht. Komplizierter gebaute sogenannte „Choralkreisel“ können sogar einfache Kinderlieder abspielen.

Der Wende- oder Stehaufkreisel ähnelt liegend einem leicht geöffneten Parasolpilz. Wird er an seinem Stiel in eine rotierende Bewegung versetzt, richtet er sich aufgrund der Schwerkraftverhältnisse auf, um sich nach Beendigung der Drehbewegung selbsttätig wieder auf den Rücken zu legen. Das Spiel mit ihm ist ein Spiel mit überraschenden physikalischen Gesetzmäßigkeiten.

Bodenkreiselspiele zeichnen sich durch eine größere Heftigkeit der Bewegung und raumgreifendes Spielen aus. Sie benötigen daher als Spielfeld in der Regel mehr Räumlichkeit.

Das Wurfkreisel- oder Preckelspiel ist ein dynamisches Spiel, das nach der Bildanalyse von Warwitz/Rudolf im Rückblick von heutigen Zeitzeugnissen schon zur Zeit Pieter Brueghels auch als Kriegsspiel ausgetragen wurde. Es hat über die Jahrhunderte in vielen Ländern der Welt zahlreiche Varianten entstehen lassen und wurde vor allem von Knaben bevorzugt: Ein mit einer Schnur umwickelter schwerer Hartholzkreisel wird heftig auf den Boden geschleudert, wobei ihn die sich abwickelnde Schnur in eine rotierende Bewegung versetzt. Ein Mitspieler kann daraufhin versuchen, den sich drehenden Preckel am Boden mit seinem eigenen Preckel zu treffen und damit außer Gefecht zu setzen und gefangen zu nehmen.

Im Unterschied zum Preckelspiel wird das Spiel mit dem Peitschenkreisel mehr als Geschicklichkeitsspiel ausgetragen und von den Mädchen bevorzugt. Nach Abziehen der Peitschenschnur von dem umwickelten Kreisel kann dieser durch dosierte Peitschenschläge beliebig lange in Bewegung gehalten werden. Es lassen sich auch Wettkämpfe austragen, wer als erster mit seinem Kreisel eine bestimmte Linie erreicht oder wem es gelingt, seinen Kreisel eine schräge Fläche hinauf oder um eine Wendemarke herum zu treiben.

Das Beybladespiel ist eine japanische Erfindung aus dem Jahr 1999. Das in eigenen Arenen als Wettkampf ausgetragene Spiel verbreitete sich ab dem Jahr 2000, unterstützt durch eine Fernsehserie, sehr schnell weltweit zu einem der beliebtesten Spiele mit Verkaufszahlen von inzwischen mehreren 100 Millionen Stück. Es geht darum, seinen Kreisel in einem schalenförmig angelegten sogenannten „Beystadium“ gegen die Konkurrenz anderer Spieler möglichst lange in der Drehbewegung zu halten.

Im Gegensatz zu Ländern wie Vietnam, Indien oder Südafrika sind die noch bis in die 1970er Jahre allenthalben praktizierten Kreiselspiele wie fast alle Spiele im heutigen Straßenbild der europäischen Städte nahezu verschwunden. Das elektronische Spielzeug hat sie an Attraktivität überflügelt. Aber auch der verdichtete Verkehr hat dem Spiel im öffentlichen Gelände weitestgehend den Raum entzogen. So haben sich die vergessenen Kinderspiele in den pädagogischen Bereich und auf die relativ beengten Spielflächen in Reservaten wie Schulplätze, Sportplätze und Klassenzimmer zurückgezogen. Sie sind Teil der Lehrerbildung und Spielpädagogik geworden und müssen in diesem Rahmen von den Studenten, Lehramtsanwärtern und Kindern als wertvolles Kulturgut erst wiederentdeckt werden.




</doc>
<doc id="9541473" url="https://de.wikipedia.org/wiki?curid=9541473" title="Geschichte der Klassischen Mechanik">
Geschichte der Klassischen Mechanik

Dieser Artikel behandelt die Entwicklung der klassischen Mechanik als Teil der Geschichte der Physik, von ihren Vorläufern im Altertum bis zur Gegenwart. Sie wurde im 17. Jahrhundert im Wesentlichen durch die Arbeiten von Isaac Newton begründet und war damit die erste Naturwissenschaft im modernen Sinn. Die Darstellung erfolgt unter historischem Aspekt und verzichtet dabei fast vollständig auf mathematische Formeln. Für genauere Erklärungen der einzelnen Begriffe und Methoden wird auf die entsprechenden Artikel verwiesen.

Bis zu Beginn des 17. Jahrhunderts war die Mechanik auf die Statik beschränkt. Als deren Begründer gilt Archimedes mit seinen beiden im 3. Jahrhundert v. u. Z. formulierten Gesetzen vom Gleichgewicht am Hebel und vom archimedischen Prinzip beim hydrostatischen Auftrieb. Erst im 16. Jahrhundert n. u. Z. wurden diese Gesetze durch Simon Stevin auf das Gleichgewicht von mehr als zwei Kräften erweitert, indem er das geschlossene Krafteck als Gleichgewichtsbedingung angab. Stevin leitete auch als Erster die Gleichgewichtsbedingung für die auf der schiefen Ebene wirkenden Hangabtriebskraft her. Kraft galt ganz allgemein als eine dem einzelnen Körper innewohnende Fähigkeit, eine bestimmte Wirkung zu erzielen (oder zu verhindern, z. B. das Herunterfallen eines anderen Gegenstandes). Um eine Wirkung im Sinne der Mechanik auszuüben, musste ein Körper nach damaliger Ansicht einen anderen Körper berühren, um ihn schieben, ziehen, heben, drücken oder stoßen zu können, wobei letzteres abrupt seine Geschwindigkeit verändert. Zwischen Masse und Gewicht wurde nicht richtig unterschieden.

Bewegung galt von der Zeit des Aristoteles bis ins 16. Jahrhundert als etwas absolut Gegensätzliches zu Ruhe. Daher stellte z. B die Erklärung eines allmählichen (stetigen) Übergangs von Ruhe zu Bewegung ein philosophisch nicht lösbares Problem dar. Für den freien Fall der irdischen Körper wurde nicht eine äußere Einwirkung verantwortlich gemacht, sondern ein natürliches, jedem schweren Körper innewohnendes Streben nach unten. Für die Himmelskörper wurde angenommen, dass sie ewig die natürliche, von den oben genannten mechanischen Einwirkungen freie Bewegung in Form einer gleichförmigen Kreisbewegung ausführen.

Die Abkehr von solchen jahrhundertelang verbreiteten Ansichten markierte im 17. Jahrhundert den Beginn der klassischen Mechanik und der neuzeitlichen Naturwissenschaft überhaupt. Sie ist vor allem Galileo Galilei zu verdanken. Er setzte der bis dahin in der Naturphilosophie vorherrschenden philosophisch-theologischen Spekulation seine neuartige experimentelle Methode entgegen, die von Beobachtungen, wenn möglich Messungen, ausgeht, und diese mit mathematischer Strenge analysiert. Für die auf diese Weise gewonnenen Erkenntnisse forderte er zusätzlich, dass sie im Zweifelsfall Vorrang vor den aus der Spekulation gewonnenen haben sollten.

Zur gleichen Zeit formulierte Johannes Kepler, allein auf die beobachteten Positionen der Planeten gestützt, die keplerschen Gesetze, nach denen die Planeten sich mit variabler Geschwindigkeit auf Ellipsenbahnen um die Sonne bewegen.

Statt die Form der natürlichen Bewegung wie bisher aus allgemeingültigen philosophischen Grundsätzen erschließen zu wollen, nahm Galilei die Beobachtung wirklicher konkreter Bewegungsabläufe zum Ausgangspunkt. Als Erster führte er gezielt geplante Experimente durch, um diese Vorgänge direkt zu beobachten, so genau wie möglich zu vermessen und die Messwerte anschließend mithilfe von mathematischen Verfahren zu analysieren. Daraus gewann er durch Verallgemeinerung und Idealisierung grundlegende allgemeine Einsichten und neue, noch heute gültige Begriffe. Zu nennen sind etwa die gleichmäßig beschleunigte Bewegung, bei der nach den Fallgesetzen die Momentangeschwindigkeit proportional zur Fallzeit zunimmt, die Wurfparabel, die durch Zusammensetzung einer horizontalen unbeschleunigten mit einer vertikalen beschleunigten Bewegung erklärt wird, und die Pendelgesetze, nach denen bei kleinen Auslenkungen die Schwingungsdauer von nichts anderem als der Länge des Aufhängefadens bestimmt wird. Sehr nahe kam Galilei auch dem Trägheitsprinzip, dem zufolge ein unbeeinflusster Körper keine Kreisbahn, sondern eine gleichförmige Bewegung in gerader Linie vollführt, und dem Relativitätsprinzip, dem zufolge es keinen grundlegenden Unterschied zwischen Ruhe und Bewegung gibt. Klar ausgesprochen wurden diese beiden auch heute gültigen Prinzipien noch nicht von Galilei, aber kurz danach von René Descartes. bzw. Christiaan Huygens.

Den Entwicklungsschritt zu einer voll entfalteten Mechanik leistete Isaac Newton in seinem 1687 erschienenen Hauptwerk "Mathematische Prinzipien der Philosophie der Natur". Darin schrieb Newton zunächst jedem Körper eine unveränderliche Masse zu und weiter eine Bewegungsgröße, die das Produkt aus Masse und momentaner Geschwindigkeit ist (heutiger Name: Impuls). Indem er ohne Ausnahme eine von außen „eingeprägte Kraft“ dafür verantwortlich machte, wenn die Bewegungsgröße eines Körpers sich ändert, gab er der Kraft ihre endgültige zentrale Bedeutung in der Mechanik. Ohne über das Wesen, die Ursache oder die Wirkungsweise der Kraft nach weitergehenden Erklärungen zu suchen ("„Hypotheses non fingo“" – "Hypothesen mache ich nicht"; gemeint sind die alten philosophischen Spekulationen), definierte er als Maß einer bestimmten eingeprägten Kraft die von ihr bewirkte Änderung der Bewegungsgröße selbst, sowohl nach ihrem Betrag als auch nach ihrer Richtung. Mit diesen Konzepten und vielen eigenen Experimenten gelang es Newton, die Bewegungen der Körper umfassend zu beschreiben. Eine konstante Kraft z. B., die in Richtung der Geschwindigkeit wirkt, ändert nur den Betrag der Bewegungsgröße und führt zu einer Bewegung wie im freien Fall; steht die Kraft bei konstantem Betrag aber stets auf der Geschwindigkeit senkrecht, verändert sie nur die Richtung der Bewegungsgröße und führt damit zur gleichförmigen Kreisbewegung. Insbesondere konnte Newton auch die realen Bewegungen analysieren, die z. B. durch Reibung von ihrer idealen, vorher als „natürlich“ angesehenen Form abweichen. Es gelang ihm, die Bewegung der Erde, der Planeten und ihrer Monde, einschließlich ihrer schon bekannten Unregelmäßigkeiten, sowie die Gezeiten und die Bewegungen fallender und pendelnder Körper auf das Wirken einer einzigen Art von Kraft zurückzuführen, für die er ein einfaches universelles Gesetz angeben konnte, das newtonsche Gravitationsgesetz. Allerdings blieb Newton in seiner Mechanik im Wesentlichen auf die Bewegungen einzelner Körper oder Massenpunkte beschränkt, auch wenn er damit bereits Strömung, Reibung und Schall in flüssigen oder gasförmigen Stoffen behandelte. Die Eigentümlichkeiten der Mechanik ausgedehnter Körper, wo auch Drehbewegung und Verformung möglich sind, blieben noch weitgehend außerhalb der Betrachtung.

Zur Zeit Newtons arbeiteten auch andere Wissenschaftler intensiv an Problemen der Mechanik und steuerten wesentliche Erkenntnisse bei. Zu nennen sind unter anderem Christiaan Huygens (Relativitätsprinzip, Stoßgesetze, Physikalisches Pendel), Robert Hooke (Hookesches Gesetz der Elastizität, Federschwingungen), Gottfried Wilhelm Leibniz (Vorbereitung des Begriffs der kinetischen Energie), Johann I Bernoulli (Modellvorstellung, dass in einem beschleunigten festen Körper jeder Teil relativ zum Körper deshalb an seinem Platz bleibt, weil er durch geeignete Kräfte, die von den angrenzenden Teilen auf ihn ausgeübt werden, entsprechend beschleunigt wird).
Zusammen mit den weiteren Entwicklungen, die im Laufe des 18. und 19. Jahrhunderts durch Leonhard Euler, Jean-Baptiste le Rond d’Alembert, Joseph-Louis de Lagrange, Pierre-Simon Laplace, William Rowan Hamilton (und anderen) erzielt wurden, wurde diese ganze Theorie als "Newtonsche Mechanik" oder "Rationale Mechanik" bezeichnet und erhielt ab 1900 auch den Namen "klassische Mechanik".

Ein wichtiger Schritt zur klassischen Mechanik hin war die präzise Formulierung von Newtons Mechanik durch Leonhard Euler (1736) in Form der von Leibniz erfundenen Differentialrechnung (Newton selbst hatte in seinen Schriften ausschließlich geometrische Beweise vorgestellt). Hier findet sich erstmals die heute als „2. newtonsches Gesetz“ bekannte Differentialgleichung (in heutiger Notation formula_1), in der die Änderungsgeschwindigkeit der Bewegungsgröße vollständig mit der wirkenden Kraft identifiziert wird, statt sie nur als deren Maß zu betrachten. Ferner unterschied Euler erstmals richtig zwischen dem Massenpunkt in Newtons Mechanik und dem starren Körper und gab die Formeln für dessen Dynamik an. Besonders die von ihm formulierten Gesetze der Drehbewegung mit den Begriffen Winkelgeschwindigkeit, Drehmoment, Trägheitsmoment, Hauptträgheitsachsen waren für die Weiterentwicklung der newtonschen Mechanik und ihre technische Anwendung im beginnenden Maschinenzeitalter bedeutsam.

Jean-Baptiste le Rond d’Alembert veröffentlichte 1743 die Methode, die Bewegungen vieler Körper (Massenpunkte), die nicht nur aufeinander einwirken, sondern auch durch Zwangskräfte auf bestimmten Bahnen gehalten werden, zu berechnen.

Mit dem Schnittprinzip legte Euler 1752 die allgemeine Grundlage, um ausgedehnte (auch deformierbare oder flüssige) Körper behandeln zu können: Wird ein beliebiger Teil eines Körpers in Gedanken herausgeschnitten, befolgt er für sich die newtonschen Gesetze der Mechanik, wobei die auf ihn wirkenden Kräfte die sind, die der restliche Körper an den Schnittflächen auf ihn ausübt (zuzüglich eventueller äußerer Kräfte wie Gewichtskraft etc.). Damit konnten auch ausgedehnte Körper als Systeme vieler Massenpunkte aufgefasst und mit den Mitteln von Newtons Punktmechanik behandelt werden.

Pierre Louis Moreau de Maupertuis entdeckte 1744 den Begriff der Wirkung und das Prinzip der kleinsten Wirkung, dem alle mechanische Vorgänge unterliegen.

Auf dieser Grundlage entwickelte Joseph-Louis de Lagrange 1788 den Zweig der klassischen Mechanik, der genauer als Analytische Mechanik bezeichnet wird. Darin können mithilfe verallgemeinerter Koordinaten und der neu eingeführten Lagrangefunktion die Bewegungsgleichungen beliebiger mechanischer Systeme aufgestellt werden. Dies Verfahren ist heute auch als Ausgangspunkt für die Bewegungsgleichungen der Quantenfeldtheorie in Gebrauch.

Pierre-Simon Laplace brachte 1796 die Himmelsmechanik auf einen Stand, der es möglich machte, aus dem unerklärten Rest der Bahnstörungen des Planeten Uranus die Existenz des Planeten Neptun abzuleiten und dessen ungefähre Koordinaten zu berechnen.

William Rowan Hamilton veröffentlichte 1835 eine Weiterentwicklung der Lagrange'schen Methode, die mittels der Hamiltonfunktion nicht nur die Bewegungsgleichungen beliebiger mechanischer Systeme liefert, sondern auch in der Hamilton-Jacobischen Differentialgleichung eine tiefe Parallele der Punktmechanik zur geometrischen Optik erkennen lässt. Hierauf konnte Werner Heisenberg 1925 die Quantenmechanik aufbauen.

Ebenso wichtig war die Ausdehnung der newtonschen Mechanik auf die Dynamik strömender Flüssigkeiten und Gase (Hydrodynamik, Aerodynamik). Diese Anwendungen sind für Strömungen in Kanälen und Rohrsystemen ebenso relevant wie für den Fahrwiderstand von Schiffen. Zuerst war Daniel Bernoulli hier 1738 erfolgreich, dann vor allem Leonhard Euler. Für den endgültigen Durchbruch zur praktischen Anwendbarkeit bei realen, zähen Flüssigkeiten fehlte noch die Berücksichtigung der inneren Reibungsverluste. Dies gelang Claude Louis Marie Henri Navier (1822) und George Gabriel Stokes (1845), ihre Navier-Stokes-Gleichungen sind bis heute die Grundgleichungen für die Berechnungen der Strömungen.

Im Bereich der Statik hatte schon Galilei 1638 die Bruchfestigkeit eines Balkens in Abhängigkeit von Länge, Breite und Höhe zu berechnen versucht und eine – allerdings nicht ganz richtige – Formel abgeleitet. Nachdem Robert Hooke 1678 das Gesetz der Elastizität (Hookesches Gesetz) entdeckt hatte, verbesserten Jakob I Bernoulli, Leibniz, Euler und andere den fehlerhaften Vorfaktor in Galileis Gleichung schrittweise, jedoch wurde die richtige Form erst 1773 von Charles Augustin de Coulomb abgeleitet. Die heutige Differentialgleichung der Balkenbiegung, die auch die Schwingungen von Balken beschreibt, wurde erstmals von Navier 1821 angegeben.

Auch die Betrachtung dynamischer elastischer Phänomene hatte schon einige Jahre vor Newtons "Principia" (1687) bei Robert Hooke begonnen. Er konnte 1678 aus seinem Grundgesetz der Elastizität ableiten, dass die Schwingungsdauer von Federpendeln von der Auslenkung unabhängig ist, was heute als Kennzeichen der harmonischen Schwingung gilt. Hookes Kraftbegriff entsprach damals allerdings nicht dem von Newton, doch dieser fand in den "Principia" das gleiche Ergebnis, als er Bewegungen auf Ellipsenbahnen analysierte, wenn das Kraftzentrum in deren Mittelpunkt liegt. Newton erklärte darin auch den Schall als eine elastische Schwingung in Luft. Die Analyse der Bewegung einer schwingenden Saite wurde 1713 Brook Taylor (bekannt durch die Taylorentwicklung in der Differentialrechnung) begonnen und durch Leibniz, Johann I Bernoulli, d'Alembert, Euler fortgesetzt. Daniel Bernoulli fand schließlich 1753, dass die allgemeine Form der Saitenschwingung immer als eine Überlagerung der Grundschwingung der Saite und ihren Obertönen dargestellt werden kann – eine Vorwegnahme der Fourierzerlegung um 60 Jahre.

Lange Zeit blieb die Anwendung der newtonschen Mechanik auf die inneren Bewegungen von deformierbaren (z. B. elastischen) Körper ein Problem. Der in der heutigen Kontinuumsmechanik zentrale Begriff des Spannungstensors wurde 1822 von Augustin Louis Cauchy richtig eingeführt.

Seit dem Altertum waren die einfachen Maschinen (Rad, Hebel, Seilrolle, schiefe Ebene etc.), die Wasserräder, Kanäle, Dämme, Schleusen und Schiffe, die Tragwerke von Häusern, Schutzmauern und Bergwerken sowie das militärische Arsenal für Angriff und Verteidigung etc. nach tradierten Regeln der Handwerks-, Ingenieurs- und Baukunst konstruiert und auch immer wieder zum Gegenstand der Mechanik gemacht worden. Jedoch begann eine „Technische Mechanik“ als eigenständiger Zweig der klassischen Mechanik sich erst im 19. Jahrhundert zu entwickeln, als die zunehmend komplexer werdenden Maschinen und Bauwerke (z. B. Hängebrücken, Stahltürme) neue Konstruktionsmethoden voraussetzten. Für die mechanischen Probleme, die aufgrund der altbekannte Kräfte wie Gewicht, Wind- und Wasserdruck im Zusammenhang mit Stahl- und Hängebrücken, Lokomotiven und Dampfschiffen auftraten, wurde zunächst die Statik (und Baustatik) erheblich weiter entwickelt. In der zweiten Hälfte des 19. Jahrhunderts kam die dynamische Betrachtungsweise hinzu, weil mit immer schneller laufenden Maschinen zusätzlich auch die "Massenkräfte" bedeutende Größe erreichten, die durch die beschleunigte Bewegung der Maschinenteile verursacht wurden. Ab Anfang des 20. Jahrhunderts wurde in der technischen Mechanik auch berücksichtigt, dass elastische Verformungen durch eine äußere Kraft durch mechanische Resonanz auf ein Vielfaches gesteigert werden können, wenn die Kraft sich periodisch ändert. Mehr als ein halbes Jahrhundert zuvor war erstmals eine Hängebrücke zum Einsturz gebracht worden, indem eine Gruppe marschierender Soldaten sie in Schwingungen versetzt hatte (siehe Broughton Suspension Bridge). Weitere bedeutende Zweige der technischen Mechanik sind die Strömungsmechanik, insbesondere für turbulente Strömung (ab Ende 19. Jahrhundert), und die Finite-Elemente-Methode, die Mitte des 20. Jahrhunderts im Bereich des Flugzeugbaus entstand und inzwischen für eine Vielzahl weiterer Konstruktionen eingesetzt wird.

In der 2. Hälfte des 20. Jahrhunderts entstand im Rahmen der klassischen Mechanik die Chaosforschung. Für zahlreiche Systeme der klassischen Mechanik vom einfachen Doppelpendel bis zur turbulenten Strömung, die den deterministischen klassischen Bewegungsgleichungen folgen, wurde gezeigt, unter welchen Umständen sie sich chaotisch verhalten, d. h. bei kleinsten Unterschieden der Anfangsbedingungen sich völlig unterschiedlich entwickeln. Näheres siehe unter Deterministisches Chaos.

Der Erfolg, den die klassische Mechanik im Laufe des 18. und 19. Jahrhunderts bei der Erklärung zahlloser Phänomene in Natur und Technik hatte, führte zur verbreiteten Ansicht, möglicherweise könne man auf mechanische Weise die ganze Welt verstehen. Alle Vorgänge, von den Himmelserscheinungen über die chemischen Umwandlungen und die Brechung des Lichts bis zum menschlichen Geist, sollten auf die Bewegung von materiellen Körpern unter dem Einfluss gegenseitiger Kräfte zurückzuführen sein. Herausragende Vertreter dieser Entwicklung waren unter anderen Pierre-Simon Laplace, Claude-Louis Berthollet und Jean-Baptiste Biot. Ein mechanistisches Weltbild entstand, wurde schließlich zum Paradigma wissenschaftlicher Rationalität überhaupt und ist es in modifizierter Form bis heute geblieben. Näheres siehe auch unter Laplacescher Dämon, Naturtheorie, Maschinenparadigma. Erbitterte Gegnerschaft fand diese Denkweise hingegen an vielen deutschen Universitäten, wo insbesondere die Philosophischen Fakultäten zuständig für die Physik als Voraussetzung für das Studium der Philosophie waren. Dort sperrte man sich gegen das Eindringen der ursprünglich aus England stammenden "experimentellen Philosophie", die statt echter philosophischer Letzt-Erklärungen auf der Grundlage tiefster Grundsätze nur noch oberflächliche Beschreibungen von Zusammenhängen zwischen beobachtbaren Erscheinungen hervorbringe, diesen aber durch die extensive Nutzung von mathematischen Methoden den falschen Anschein der Wissenschaftlichkeit gebe. In der romantischen Naturphilosophie des deutschen Idealismus mündete diese Kritik Anfang des 19. Jahrhunderts in eine grundsätzliche Ablehnung der modernen Naturwissenschaft in der durch Newton begründeten Art. In der Gestalt einer gegenseitigen Geringschätzung von Natur- und Geisteswissenschaften trifft man diese Ablehnung bis heute an. Einer der letzten großen philosophischen Vertreter dieser Denkweise war Georg Wilhelm Friedrich Hegel noch 1830. Der letzte von ihm inspirierte Lehrstuhlinhaber der Physik war Georg Friedrich Pohl in Breslau, der noch 1845 erfolglos versuchte, die newtonsche Himmelsmechanik entsprechend den Grundsätzen der romantischen Philosophie umzuformulieren.


</doc>
<doc id="10103680" url="https://de.wikipedia.org/wiki?curid=10103680" title="Josef Adam Braun">
Josef Adam Braun

Josef Adam Braun (; * 1712 in Asch; † in St. Petersburg) war ein deutsch-russischer Physiker und Hochschullehrer.

Braun kam nach St. Petersburg und wurde 1746 Mitglied der St. Petersburger Akademie der Wissenschaften. 1748 wurde er Professor der Philosophie an der Akademie-Universität der St. Petersburger Akademie der Wissenschaften. Als Erster erreichte er im Labor die niedrige Temperatur von −42 °C, so dass er bei der Untersuchung der Eigenschaften des Quecksilbers zusammen mit Michail Lomonossow den Gefrierpunkt des Quecksilbers bestimmen konnte. Darauf untersuchte er insbesondere das Verformungsverhalten des festen Quecksilbers. Er erreichte Temperaturen von −65 °C.

Braun arbeitete auch auf dem Gebiet der Meteorologie und Philosophie. Er entwickelte eine eigene Theorie der Schwerkraft.

Braun war verheiratet mit Georg Wilhelm Richmanns Witwe.


</doc>
<doc id="10346437" url="https://de.wikipedia.org/wiki?curid=10346437" title="Əhməd Abdinov">
Əhməd Abdinov

Əhməd Şahvələd oğlu Abdinov (; englische Transkription ";" * 30. Mai 1945 in Behrud, Rayon Ordubad, Nachitschewan, AsSSR, UdSSR) ist ein aserbaidschanischer Physiker, Hochschullehrer und ehemaliger Vizeminister für Bildung (1993–2000).

Abdinov begann sein Studium der Physik 1963 an der Staatlichen Universität Baku und schloss dieses 1968 mit Auszeichnung ab. Anschließend war er wissenschaftlicher Mitarbeiter am Institut für Physik der Akademie der Wissenschaften der AsSSR, bevor er 1969 als Doktorand an das Physikalisch Technisches Institut Joffe der Akademie der Wissenschaften der UdSSR in Sankt Petersburg wechselte. 1972 verteidigte er seine Dissertation über Germanium-Halbleiter und erlang 1979 seinen Doktor der physikalischen und mathematischen Wissenschaften über elektronische Prozesse in Halbleitern des Typs III–VI.

Əhməd Abdinov ist seit 1972 an der Fakultät für Physik der Staatlichen Universität Baku tätig. Anfangs als Assistent (1972–1975), wurde er 1975 zum Dozenten, 1979 zum außerordentlichen Professor und 1981 zum Professor ernannt. In den Jahren 1988 und 1989 war er Dekan der Fakultät für Physik. 1989 wurde Abdinov Leiter und wissenschaftlicher Direktor des Labors für Festkörperelektronik und 1992 zum Leiter der Abteilung für Physikalische Elektronik. Am 2. Dezember 1993 wurde Abdinov zum stellvertretenden Minister für Bildung ernannt und am 4. Januar 2000 von diesem Posten entlassen. Seit 2000 ist er Inhaber des Lehrstuhls für Physikalische Elektronik und unterrichtet über Optoelektronik, Festkörperelektronik und allgemeine Physik. Seine Forschungsinteresse liegt in den Eigenschaften elektronischer Prozesse in ungeordneten Halbleitermaterialien und -strukturen.

1977 wurde Abdinov mit dem Lenin-Komsomol-Preis im Bereich Wissenschaft und Technik ausgezeichnet, war 1993 Laureat der Internationalen Soros Foundation und wurde 1995 zum Mitglied der New York Academy of Sciences ernannt.

Abdinov ist verheiratet und hat drei Kinder. Sein Bruder ist der Physiker Cavad Şahvələd oğlu Abdinov.




</doc>
<doc id="10371583" url="https://de.wikipedia.org/wiki?curid=10371583" title="Albert Abubakirowitsch Galejew">
Albert Abubakirowitsch Galejew

Albert Abubakirowitsch Galejew (; * 19. Oktober 1940 in Ufa) ist ein tatarisch-russischer Astrophysiker und Hochschullehrer.

Galejew begann das Studium 1957 an der Radiotechnik-Fakultät des Moskauer Energie-Instituts. Als Student begeisterte er sich für den Sambo-Kampfsport und wurde Moskauer Meister. 1961 wechselte er an die gerade gegründete Staatliche Universität Nowosibirsk (NGU). Neben dem Studium arbeitete er im Institut für Kernphysik der Sibirischen Abteilung der Akademie der Wissenschaften der UdSSR (AN-SSSR, seit 1991 Russische Akademie der Wissenschaften (RAN)). 1963 schloss er das Studium als erster Absolvent der NGU ab.

Ab 1971 lehrte Galejew am Lehrstuhl für Kosmosphysik des Moskauer Instituts für Physik und Technologie (1980 Ernennung zum Professor). Er wurde zum Doktor der physikalisch-mathematischen Wissenschaften promoviert. Ab 1973 leitete er die Abteilung für Physik des kosmischen Plasmas im Institut für Weltraumforschung der Russischen Akademie der Wissenschaften (IKI-AN-SSSR). 1987 wurde er Korrespondierendes Mitglied der AN-SSSR. 1988 wurde er Direktor des IKI-AN-SSSR. 1992 folgte die Wahl zum Wirklichen Mitglied der RAN. 2002 folgte ihm Lew Matwejewitsch Seljony nach als Direktor des nun IKI-RAN, während Galejew Ehrendirektor des IKI-RAN ist. Er ist Mitglied des Wissenschaftsrats für Physik des Sonnensystems.

Galejew entwickelte die Theorie der schwachen Wechselwirkung der Plasmawellen. Zusammen mit Roald Sinnurowitsch Sagdejew erarbeitete er die neoklassische Transporttheorie für den Tokamak. Galejew entwickelte die Theorie der Rekonnexion in einer Magnetosphäre und zeigte, dass dabei Ionen auf Energien der Größenordnung 1 MeV beschleunigt werden. Er entwickelte die Theorie des Alfvén-Phänomens bei der Ionisation eines verdünnten Gases durch einen magnetisch stabilisierten Plasmastrom. Galejews Beschreibung der Beladung des Sonnenwindes mit Kometenionen zeigte, dass dabei die Umladung der Ionen im Kometenschweif eine wichtige Rolle spielt. Galejew schlug eine Theorie vor, nach der der Sonnenwind aus koronalen Löchern durch Alfvén-Wellen beschleunigt wird. Unabhängig von Donald Lynden-Bell folgerte Galejew 1979, dass es zu Magneteruptionen in Akkretionsscheiben kommen kann. Mit seinen Veröffentlichungen erreichte er einen h-Index von 36.

Galejew ist Auswärtiges Wissenschaftliches Mitglied der Max-Planck-Gesellschaft im Göttinger Max-Planck-Institut für Sonnensystemforschung, Mitglied des Fachbeirats des Max-Planck-Instituts für extraterrestrische Physik, Mitglied der Academia Europaea, Mitglied der International Academy of Astronautics und Mitglied der Moskauer Ziolkowski-Akademie der Kosmonauten.



</doc>
<doc id="10562142" url="https://de.wikipedia.org/wiki?curid=10562142" title="Henryk Niewodniczański">
Henryk Niewodniczański

Henryk Niewodniczański (* 10. Dezember 1900 in Wilna; † 20. Dezember 1968 in Krakau) war ein polnischer Atomphysiker und Gründungsdirektor des Instituts für Kernphysik in Krakau, das heute nach ihm benannt ist.

Niewodniczańskis Vater war der Ingenieur Wiktor Niewodniczański (1872–1929), Direktor des ersten Elektrizitätswerkes in Wilna. Henryk studierte an der "Stefan Batory Universität" in Wilna, wo er 1926 doktorierte. 1927 war er an der Eberhard Karls Universität Tübingen bei Walther Gerlach. Zu der Zeit waren seine Hauptinteressen die Optik der Metalle und molekulare Optik, wobei er in Kooperation mit Jan Blaton Phänomene der Quantenreflexion entdeckte. 1932 erhielt er eine Assistenz-Professur in Wilna und wurde 1934 Fellow der Rockefeller Foundation, woraufhin er nach Cambridge ging, um am "Mond Laboratory" der Royal Society und am Cavendish-Laboratorium zu arbeiten. 1937 kehrte er nach Polen zurück und arbeitete zuerst in Poznań an der Adam-Mickiewicz-Universität und später wieder in Wilna als Professor für Experimental-Physik. Während des Zweiten Weltkrieges engagierte er sich im Widerstand gegen das Besatzungsregime und lehrte an der Untergrund-Universität. 

Nach dem Krieg wurde Niewodniczański an die Jagiellonen-Universität zu Krakau berufen. Dort organisierte er 1952 einen sowjetischen U-120-Zyklotron und gründete 1955 das Institut für Kernphysik.

Verheiratet war Henryk Niewodniczański mit Irena, geborene Prawocheńska, eine Tochter von Roman Prawocheński (1877–1965), Professor für Biologie an der Jagiellonen-Universität.

Seine Söhne Tomasz Niewodniczański und Jerzy Niewodniczański wurden ebenfalls Atomphysiker; Jerzy, der Jüngere der beiden, war von 1992 bis 2009 Präsident der Nationalen Atomenergie-Organisation in Polen. Außerdem gibt es eine Tochter, die an einer polnischen Universität als Professorin tätig war.




</doc>
<doc id="10845040" url="https://de.wikipedia.org/wiki?curid=10845040" title="Gero Vogl">
Gero Vogl

Gero Vogl (* 6. Januar 1941 in Bielitz, Teschener Schlesien) ist ein österreichischer Emeritus für Physik. 

Vogl besuchte das Bundesrealgymnasium in Neunkirchen (Niederösterreich). Nach der Matura studierte er ab 1959 an der Universität Wien Physik. Seine Doktorarbeit schrieb er bei Erich Schmid am II. Physikalischen Institut der Universität Wien. 1965 wurde er in Wien zum Dr. phil. promoviert. 1965/66 diente er im Bundesheer. Er ging an das Physik-Department der Technischen Universität München und war Assistent von Heinz Maier-Leibnitz, Herbert Vonach und Wolfgang Gläser. Ab 1970 leitete er am Forschungsreaktor München eine Arbeitsgruppe in der Tieftemperaturphysik. 1974 habilitierte er sich für Experimentalphysik (Dr. rer. nat. habil.). 1977 ging er für acht Jahre nach West-Berlin als C3-Professor für Experimentalphysik am Institut für Kernphysik der Freien Universität und am Hahn-Meitner-Institut. Die Universität Wien berief ihn 1985 auf den Lehrstuhl für Experimentalphysik am Institut für Festkörperphysik, Forschungsaufenthalte führten ihn an das Oak Ridge National Laboratory, die Kyoto University, das Institut Laue-Langevin in Grenoble und das Indian Institute of Science Education and Research, Kolkata. Er war von 1999 bis 2001 an das Department Struktur und Dynamik des Hahn-Meitner-Instituts beurlaubt und leitete den Bereich Strukturforschung. 2003/04 war er Vorsitzender der Österreichischen Physikalischen Gesellschaft.





</doc>
<doc id="10991367" url="https://de.wikipedia.org/wiki?curid=10991367" title="Isodimorphie">
Isodimorphie

Isodimorphie ist die Bildung von Mischkristallen aus Stoffen mit unterschiedlichen Kristallsystemen bzw. Strukturtypen (Heterotypie).

Isodimorphe Verbindungen verhalten sich chemisch ähnlich und kristallisieren in einem bestimmten Bereich von Mischungsverhältnissen gemeinsam. Die häufigere Komponente zwingt dabei ihr Kristallsystem dem Gesamtkristall auf.

Beispiele für isodimorphe Verbindungen sind unter anderem



</doc>
<doc id="9347" url="https://de.wikipedia.org/wiki?curid=9347" title="Longitudinalwelle">
Longitudinalwelle

Eine Longitudinalwelle, auch Längswelle, ist eine physikalische Welle, die "in Ausbreitungsrichtung" schwingt. Das Standardbeispiel für Longitudinalwellen ist Schall in Gasen oder Flüssigkeiten. Ihr Gegenstück ist die Transversalwelle, deren Amplitude senkrecht zur Ausbreitungsrichtung steht.

Longitudinalwellen sind Druckwellen. Das bedeutet, dass sich in einem Medium Zonen mit Überdruck bzw. Druckspannung (bzw. Unterdruck oder Zugspannung) in der Ausbreitungsrichtung fortpflanzen bzw. verschieben oder ausbreiten.

Die einzelnen Teilchen im Ausbreitungsmedium, Atome oder Moleküle, schwingen hierbei in Richtung der Ausbreitung um den Betrag der Amplitude hin und her. Nach dem Durchlauf der Schwingung bewegen sich die Teilchen wieder an ihre Ruhestellung, die Gleichgewichtslage, zurück. Durch die Ausbreitung der Schwingung geht keine Energie verloren, abgesehen von Reibungsverlusten zwischen den Teilchen.


Die Leistung einer Longitudinalwelle ist proportional zum Quadrat der Amplitude der Auslenkung oder der Druckspannung; siehe auch Schalldruck und Schallschnelle.

Mechanische Longitudinalwellen können sich in jedem Medium, ob fest, flüssig oder gasförmig ausbreiten.

Ein typisches Beispiel einer Longitudinalwelle ist Schall, der in Gasen und Flüssigkeiten ausschließlich als Longitudinalwelle auftreten kann.

Longitudinalwellen haben im gleichen festen Medium eine höhere Geschwindigkeit als Transversalwellen des gleichen Typs bei ansonsten gleichen Parametern.

Longitudinale seismische Wellen heißen P-Wellen. Sie treffen immer zuerst ein und haben bei Erdbeben ein geringeres Zerstörungspotential als Transversalwellen. Dagegen können die (transversalen) Wasserwellen „Tsunamis“ verursachen.

In Plasmen und in anderen elektrischen Leitern gibt es elektromagnetische Longitudinalwellen neben den elektromagnetischen Transversalwellen.



</doc>
<doc id="14854" url="https://de.wikipedia.org/wiki?curid=14854" title="Bewegungsrichtung">
Bewegungsrichtung

Die Bewegungsrichtung beschreibt in der Physik die Richtung, in die sich ein Teilchen bewegt.

Diese spielt vor allen Dingen bei der Impulsübertragung von Massekörpern und Teilchen eine Rolle, bei denen durch Stöße und damit verbundene Impulsübertragung die Bewegungsrichtung der Teilchen verändert wird. Dabei wird die Bewegungsrichtung in Physik und Technik entsprechend dem vorliegen von Linearität bzw. Nichtlinearität in lineare Bewegungen (genannt Translation) und nichtlineare Bewegungen (wie z. B. die Rotation) unterschieden.

Allgemein kann die Bewegungsrichtung durch jede am Teilchen angreifende Kraft geändert werden.

Dargestellt wird die Bewegungsrichtung meistens mit Hilfe von Vektoren. Dabei bedient man sich meistens des sechs-dimensionalen Phasenraumes (hamiltonsche Mechanik), der neben der Ortskoordinaten auch die Impulskoordinaten erfasst, mit denen die Bewegung vollständig charakterisiert ist.

In der Astronomie kennt man lockere Sternassoziationen oder "Bewegungshaufen", deren Mitglieder fast dieselbe, parallele Bewegungsrichtung haben. Daraus kann man ihre gemeinsame Entstehung aus einer riesigen Gaswolke erschließen. Als erste dieser Sterngruppen wurde um die Jahrhundertwende die Bärengruppe entdeckt.

"Siehe auch:"


</doc>
<doc id="59681" url="https://de.wikipedia.org/wiki?curid=59681" title="Brownsche Bewegung">
Brownsche Bewegung

Die brownsche Bewegung ist die vom schottischen Botaniker Robert Brown im Jahr 1827 unter dem Mikroskop entdeckte unregelmäßige und ruckartige Wärmebewegung kleiner Teilchen in Flüssigkeiten und Gasen. Der ebenfalls gebräuchliche Name brownsche Molekularbewegung rührt daher, dass das Wort "Molekül" damals noch generell zur Bezeichnung eines kleinen Körpers gebraucht wurde. Moleküle im heutigen Sinn sind aber noch um Vieles kleiner als die im Mikroskop sichtbaren Teilchen und bleiben hier vollständig unsichtbar. Die Moleküle der umgebenden Materie bringen aber die brownsche Bewegung hervor. Nach der 1905 von Albert Einstein und 1906 von Marian Smoluchowski gegebenen Erklärung wird die im Mikroskop sichtbare Verschiebung der Teilchen dadurch bewirkt, dass die Moleküle aufgrund ihrer ungeordneten Wärmebewegung ständig und aus allen Richtungen in großer Zahl gegen die Teilchen stoßen und dabei rein zufällig mal die eine Richtung, mal die andere Richtung stärker zum Tragen kommt. Diese Vorstellung wurde in den folgenden Jahren durch die Experimente und Messungen von Jean Baptiste Perrin quantitativ bestätigt. Die erfolgreiche Erklärung der brownschen Bewegung gilt als Meilenstein auf dem Weg zum wissenschaftlichen Nachweis der Existenz der Moleküle und damit der Atome.
Brown beobachtete 1827 unter dem Mikroskop, dass längliche, etwa 6–8 Mikrometer große Partikel innerhalb in einem Wassertropfen schwebender Pollenkörner von "Clarkia" existierten, die freigesetzt unregelmäßige ruckartige Bewegungen machten (die Pollenkörner von rund 100 Mikrometern waren zu groß um an diesen selbst Brownsche Bewegung zu entdecken). Heute wissen wir, dass diese Partikel Organellen wie Amyloplasten und Spherosomen sind.
Ursprünglich nahm Brown an, dass dies ein Hinweis auf eine den Pollen innewohnende Lebenskraft sei, wie sie lange Zeit von Wissenschaftlern als existent vermutet wurde (siehe organische Chemie). Jedoch konnte er die gleiche Bewegung dann auch an sicher unbelebten Staubkörnern in Wasser beobachten.

Von einem ganz ähnlichen Phänomen bei Rußteilchen auf Alkohol hatte Jan Ingenhousz bereits 1784 berichtet. Er gab als Ursache die Verdunstung der Flüssigkeit an. Ingenhousz erwähnte dies Phänomen nur nebenbei als Beispiel für vermeidbare Störungen beim Studium von Mikroben, wenn man den Tropfen unter dem Mikroskop nicht mit einem Deckglas abdeckt. Seine Beobachtung blieb dann bis ins 20. Jahrhundert vergessen. Dennoch wird zuweilen Ingenhousz als der eigentliche Entdecker der brownschen Bewegung bezeichnet. Wie David Walker aber bemerkte, waren die in Alkohol suspendierten Kohleteilchen, die Ingenhousz beschrieb, zu groß um an ihnen Brownsche Bewegung zu studieren und die Bewegung außerdem völlig durch Konvektionsbewegungen aufgrund der Verdunstung überlagert, was schon Ingenhousz richtig als wahrscheinliche Quelle der Bewegung vermutete. Ingenhousz beschrieb im selben Buch die Abdeckung der Tropfen mit Glasplättchen, was die Verdunstungsbewegungen auf Randbereiche beschränkte, falls diese nicht versiegelt waren. Auch hier war aber die Brownsche Bewegung nur bei kleinsten Kohleteilchen (mit Durchmessern von rund 5 Mikrometer oder weniger) zu beobachten und dieser Fall war auch nicht von Ingenhousz beschrieben worden. Eine bedeutende Fehlerquelle waren auch Vibrationen, die allein schon durch den Atem des Beobachters ausgelöst wurden.

Nach Browns Veröffentlichung erbrachten detaillierte Experimente, insbesondere durch Christian Wiener 1863, zunehmend die Gewissheit, dass die brownsche Bewegung eine allgemeine und grundsätzliche Erscheinung ist, die durch die Bewegung unsichtbar kleiner Flüssigkeitsteilchen hervorgerufen wird. Damit ergab sich aus der brownschen Bewegung der erste Nachweis der in der molekularen Theorie der Wärme angenommenen allgemeinen Wärmebewegung aller Teilchen (siehe auch Geschichte der Thermodynamik, Phlogiston).

Einstein kam 1905 auf rein theoretischem Weg, ausgehend von der molekularen Theorie der Wärme, zu einer quantitativen „Vorhersage“ der brownschen Bewegung. Er hielt es für „möglich“, dass die theoretisch abgeleitete Bewegung mit der "Brownschen Bewegung" übereinstimmte, befand die ihm zugänglichen Informationen darüber aber als zu „ungenau“, um sich ein „Urteil bilden“ zu können. Nach seiner Formel wächst das "Quadrat" der von einem Teilchen zurückgelegten Strecke im Durchschnitt proportional zur Zeitspanne und zur (absoluten) Temperatur, sowie umgekehrt proportional zum Radius des Teilchens und zur Viskosität der Flüssigkeit. Diese Formel konnte in den folgenden Jahren durch die Experimente von Jean Baptiste Perrin bestätigt werden, der unter anderem hierfür 1926 den Nobelpreis für Physik erhielt. Auch Diffusion, Osmose und Thermophorese basieren auf der Wärmebewegung der Moleküle.

Für Teilchen in einem viskosen Medium, die sich durch unregelmäßige Stöße von ihrem Ausgangspunkt entfernen, konnten Albert Einstein (1905), Marian Smoluchowski (1906) und Paul Langevin (1908) zeigen, dass der mittlere quadratische Abstand von ihrem Ausgangspunkt proportional zur Zeit anwächst. Für Bewegung in einer Dimension gilt
Darin ist formula_2 der Mittelwert der Quadrate der formula_3-Koordinaten, die die Teilchen vom Ort formula_4 ausgehend in der Zeit formula_5 erreichen. formula_6 ist die Boltzmann-Konstante, formula_7 die absolute Temperatur, formula_8 der Radius der Teilchen und formula_9 die Viskosität der Flüssigkeit bzw. des Gases.
Ein wichtiger Aspekt der Formel ist, dass hier die Boltzmann-Konstante formula_10 mit makroskopisch messbaren Größen verknüpft wird. Das ermöglicht die direkte experimentelle Bestimmung dieser Größe und damit der Avogadro-Konstante und weiter der Anzahl, Größe und Masse der wegen ihrer Kleinheit unsichtbaren Moleküle.

Die einfachste Herleitung stammt von Langevin:

Ein Teilchen der Masse formula_11 folgt der Bewegungsgleichung (hier nur in formula_3-Richtung)
wenn neben einer Kraft formula_14 vom Medium eine Reibungskraft formula_15 ausgeübt wird. Nach Multiplikation mit formula_3 kann das umgeformt werden zu
Hiervon wird der Mittelwert über viele Teilchen gebildet (oder über viele wiederholte Beobachtungen am selben Teilchen). Auf der linken Seite der Gleichung wird die Größe formula_2 im 1. Term dann zur mittleren quadratischen Entfernung des Teilchens vom Punkt formula_4, also zur Varianz formula_20 der von vielen Teilchen gebildeten statistischen Verteilung. Der 2. Term auf der linken Seite wird die mittlere kinetische Energie und ist durch den Gleichverteilungssatz gegeben:
Der Durchschnittswert des Terms formula_22 verschwindet, wenn die Kräfte formula_14 von ungeordneten Stößen der Moleküle herrühren, die das Teilchen im Mittel weder nach formula_24 noch nach formula_25 stoßen. Für die Durchschnittswerte bleibt also:
Das ist eine Differentialgleichung, nach der die Varianz formula_27 mit der Zeit anwächst, wobei die Geschwindigkeit einem Sättigungswert zustrebt. Nach Erreichen dieses stationären Zustands verschwindet die linke Seite der Gleichung und es bleibt:
formula_27 wächst dann also proportional zur Zeit. Einsetzen von formula_30 (Gesetz von Stokes) ergibt schließlich die oben angegebene Formel für den mittleren quadratischen Abstand vom Ausgangspunkt.

In der Mathematik ist eine "brownsche Bewegung" formula_31 ein zentrierter Gauß-Prozess mit Kovarianzfunktion formula_32 für alle formula_33. Der resultierende stochastische Prozess ist heute zu Ehren von Norbert Wiener, der die wahrscheinlichkeitstheoretische Existenz desselben 1923 bewies, als Wiener-Prozess bekannt.

Es gibt mehrere Möglichkeiten, eine brownsche Bewegung mathematisch zu konstruieren:
dann ist
eine brownsche Bewegung.

Die brownsche Bewegung spielt auch bei der Simulation von Aktienkursverläufen eine Rolle, außerdem dient sie als Grundlage der Erforschung von Warteschlangen.




</doc>
<doc id="60810" url="https://de.wikipedia.org/wiki?curid=60810" title="Gasgesetze">
Gasgesetze

Als Gasgesetze bezeichnet man physikalische Gesetze, die unter anderem zwischen den Zustandsgrößen Druck formula_1, Volumen formula_2, Temperatur formula_3 und Stoffmenge formula_4 beziehungsweise Masse formula_5 oder Teilchenzahl formula_6 eine Beziehung herstellen und über diese die Eigenschaften beziehungsweise das Verhalten idealer und realer Gase beschreiben. Hierbei bilden die allgemeine Gasgleichung (ideales Gas) und die Van-der-Waals-Gleichung (reales Gas) die wichtigsten hieraus abgeleiteten Gasgesetze.

"Gasgesetze für ideales Gas:"


"Gasgesetze für reale Gase:"


"Weitere Gesetze und wichtige Zusammenhänge:"


"Siehe auch:" Zustand, Zustandsänderung,


</doc>
<doc id="88729" url="https://de.wikipedia.org/wiki?curid=88729" title="Anstellwinkel">
Anstellwinkel

Der Anstellwinkel oder Anströmwinkel ist in der Aerodynamik der Winkel formula_1 zwischen der Richtung des anströmenden Fluids und der Sehne eines Profils. Das Profil kann dabei beispielsweise Teil einer Tragfläche, eines Rotorblatts, eines Segels oder einer Turbinenschaufel sein. Die Größe des Anstellwinkels bestimmt zusammen mit der Anströmgeschwindigkeit die Größe des dynamischen Auftriebs. Er ist daher ein wichtiger Parameter beim Betrieb von Flugzeugen, Windkraftanlagen, Turbinen oder Segelbooten.

Im nebenstehenden Diagramm ist die Beziehung zwischen Anstellwinkel α und dem Koeffizienten des dynamischen Auftriebs C dargestellt. Es ist ersichtlich, dass der Auftrieb bei größer werdendem Anstellwinkel zunächst ebenfalls größer wird, dann ein Maximum erreicht und dann wieder kleiner wird. Der Grund dafür ist, dass beim Erreichen des maximalen Auftriebs die Strömung sich vom Profil zu lösen beginnt. Bei einer weiteren Vergrößerung des Anstellwinkels kommt es zu einem gänzlichen Abreißen der Strömung. Dies vermindert den Auftrieb stark.

Anstell- oder Nickwinkel werden gelegentlich verwechselt. Der Anstellwinkel ist nicht zu verwechseln mit dem Einstellwinkel (dem Winkel zwischen Profilsehne und Flugzeug- oder Propellerlängsachse).

Im stationären Flug wird der Anstellwinkel durch den Schwerpunkt und durch die Stellung des Höhenruders beeinflusst. Bei gleichem Anstellwinkel sind die Auftriebskraft und der Luftwiderstand proportional zum Quadrat der Geschwindigkeit des Flugzeugs (doppelte Geschwindigkeit ergibt vierfache Auftriebskraft) gegenüber der umgebenden Luft. Im Geradeausflug ist der Auftrieb gleich der Gewichtskraft des Flugzeugs. Daher erfordert langsames Fliegen einen besonders großen Anstellwinkel. Da der Auftrieb jenseits einer für das jeweilige Tragflächenprofil charakteristischen Geschwindigkeit wieder abnimmt, bestimmt dies die minimale Geschwindigkeit, mit der ein Flugzeug fliegen kann (v).

Bei einigen wenigen Flugzeugen wird der Anstellwinkel ohne Fluglagenänderung über Verändern des Einstellwinkels der Tragfläche gegenüber dem Flugzeugrumpf gesteuert, z. B. bei der Vought F-8. Der Anstellwinkel ändert sich auch, wenn das Profil durch Ausfahren von Vorflügeln oder Landeklappen verändert wird oder wenn es sich durch Auf- oder Abwinde bewegt. Über den kritischen Anstellwinkel hinaus kann der Auftrieb durch Strakes erhöht werden.

Der Strömungsabriss und die Gleitzahl eines Flugzeuges sind direkt vom Anstellwinkel abhängig und nur indirekt von der Geschwindigkeit (die Mindestgeschwindigkeit und die Geschwindigkeit des besten Gleitens sind von Fluggewicht, Lastvielfachen und weitere Faktoren abhängig, während die zugehörigen Anstellwinkel fest sind.). Deshalb ist eine Messung des Anstellwinkels von Bedeutung. Diese kann mit speziellen Instrumenten erfolgen ("angle of attack indicator"). 
Seitenfäden sind bei Segelflugzeugen an der Haube üblich und bei Hängegleitern an der Unterverspannung. Sie zeigen die Richtung der vorbeiströmenden Luft an.

Bei Hubschraubern wird der Einstellwinkel der Rotorblätter des Hauptrotors gleichförmig oder winkelabhängig über die Taumelscheibe gesteuert, wodurch sich deren Anstellwinkel ändert. Bei Verstellpropellern wird auf gleiche Weise mit der Änderung des Einstellwinkels der Anstellwinkel und damit der Schub verändert.

Propeller und Lüfter haben große Einstellwinkel an der Nabe und kleine an der Spitze, sodass der Anstellwinkel möglichst homogen bleibt.

Moderne Windkraftanlagen nutzen die Veränderung des Anstellwinkels der Rotorblätter zur Leistungsregelung. Dabei wird der aerodynamische Wirkungsgrad des Rotors durch Verringerung des Auftriebs so eingestellt, dass die Nennleistung des Generators nicht überschritten wird.

Im Segelsport ist die Wahl des richtigen Anstellwinkels wichtiger Teil des Segeltrimms. Der Anstellwinkel des Segels muss dabei auf das momentane Segelprofil (Wölbung des Segels) abgestimmt werden, da es im Gegensatz zu herkömmlichen Tragflächen durch Einsatz von Trimmeinrichtungen oder passiv durch zu- oder abnehmenden Winddruck verändert wird.




</doc>
<doc id="110431" url="https://de.wikipedia.org/wiki?curid=110431" title="Bahnstörung">
Bahnstörung

Eine Bahnstörung ist eine Abweichung der tatsächlichen Flugbahn eines Himmelskörpers von der anhand eines Modells berechneten Flugbahn. Neben der Änderung der Bahn wird auch der verursachende Mechanismus bzw. die Störgröße selbst als Bahnstörung bezeichnet.
Grundsätzlich unterscheidet man, neben unregelmäßigen Bahnveränderungen, "periodische" Bahnstörungen, die Schwankungen um einen Mittelwert beschreiben, und säkulare Bahnstörungen, die langfristige monotone Veränderungen der Bahnelemente darstellen.

Welche Einflüsse als Bahnstörungen zu betrachten sind, hängt somit vom zu Grunde liegenden Modell ab. Im einfachsten Fall wird die Bahn mit Hilfe des newtonschen Gravitationsgesetzes in der Näherung als Zweikörperproblem berechnet und die Bahn durch die sechs Bahnelemente der Keplerbahn beschrieben.
Für eine genauere Bahnanalyse beispielsweise von Asteroiden werden dagegen routinemäßig die Gravitationskräfte der Sonne, aller Planeten, des Mondes und weiterer größerer Himmelskörper berücksichtigt, ebenso relativistische Effekte. Durch eine Bahnstörung ändern sich die Zahlenwerte der jeweils sechs Bahnelemente (bzw. Satellitenbahnelemente). Dabei werden sie im Allgemeinen auch zeitabhängig.
Die Präzession der Knotenlinien, die Drehung der Apsiden und die Drift in den Bahnachsen und Umlaufzeiten sind dann typische säkulare Bahnstörungen.
Bei Missionen der Satellitengeodäsie sind im Modell Störgrößen durch das unregelmäßige Erdschwerefeld, durch die Hochatmosphäre, die direkte und indirekte Sonnenstrahlung, sowie der Einfluss von Mond und Sonne (direkt und indirekt über Gezeiten) berücksichtigt. Diese Bahnstörungen sind periodisch.

Die Beobachtung des Sternenhimmels mit der Messung und Berechnung der Positionen astronomischer Objekte und die Sammlung in Positionstabellen gehört zu den ältesten Teilgebieten der Astronomie. Auch heute noch werden Bahnen zahlreicher astronomischer Objekte periodisch in den Ephemeriden von renommierten Observatorien veröffentlicht. Damals wie heute sollen Bahnstörungen möglichst integriert sein. Um für den Beobachter auf der Erde möglichst direkt verwendbar zu sein werden auch Effekte wie die Aberration berücksichtigt.

Genauere Erkenntnisse über die mathematische Darstellung der Bahn eines Himmelskörpers veröffentlichte Johannes Kepler im frühen 17. Jahrhundert in Form der später nach ihm benannten keplerschen Gesetze. Diese Gesetze konnten später von Isaac Newton mit Hilfe seines Gravitationsgesetzes theoretisch hergeleitet und damit begründet werden.

Damit traten elliptische Bahnen an die Stelle der bis dahin angenommenen kreisförmigen Planetenbahnen. Für Himmelskörper höherer Energien gibt es noch andere Keplerbahnen als Lösungen des Problems, wie sich ein Himmelskörper um einen anderen (der in der Regel deutlich massereicher ist) bewegt: Parabeln oder Hyperbeln. Keplers Modell beschrieb die Bahnen deutlich besser als die Theorien zuvor. Aber auch sein Versuch, die Bahnen der Himmelskörper vorhersagbar zu machen, basierte auf bestimmten Vereinfachungen: Das eigentlich vorhandene Mehrkörperproblem wird auf ein Zweikörperproblem reduziert. Gravitations- oder andere Kräfte durch die Anwesenheit anderer Himmelskörper kommen also nicht vor. Die betrachteten Himmelskörper werden als Punktmassen mit Zentralfeld angesehen. Weitere Kraftwirkungen durch elektrische oder Magnetfelder, durch Materieströme, Strahlungsdruck, Luftwiderstand, u. ä. werden nicht betrachtet. Weit später entdeckte relativistische Effekte sind ebenfalls nicht berücksichtigt.

Es gibt verschiedene Arten von Störungen durch Gravitationseinflüsse, die schon auf Grundlage des newtonschen Gravitationsgesetzes verständlich sind. Ursache sind die inhomogene Masseverteilung der Erde oder der gravitative Einfluss anderer Himmelskörper.

Die Beschreibung der Erde als Punktmasse ist stark vereinfachend. In besserer Näherung kann die Erde als ein abgeplatteter Ellipsoid gesehen werden. Somit gilt die Annahme der kugelsymmetrischen Massenverteilung nicht, die bei der Berechnung der Keplerbahn zugrundegelegt wurde. Bis in eine Höhe von mehreren Erdradien werden selbst offene Bahnen (Vorbeiflüge) durch die Erdabplattung messbar gestört. Auch die Abplattung der Sonne, die im Vergleich zur Erde deutlich geringer ausgeprägt ist, trägt zu einem sehr kleinen Teil zu den Periheldrehungen der Planetenbahnen bei.


Niedrig fliegende Satelliten (LEO) spüren Schwereanomalien durch Meere, Gebirge und Unregelmäßigkeiten der Massenverteilung im Erdinnern. Dies nutzt die Satellitengeodäsie aus. Dabei werden selbst die Gezeiten berücksichtigt.

Auf die Bahnebene der Umlaufbahn eines erdnahen Satelliten übt das unregelmäßige Gravitationsfeld der Erde ein Kippmoment aus, dem die Bahnebene durch eine Präzessionsbewegung ausweicht. Diese Ausweichbewegung führt dazu, dass der aufsteigende Knoten bzw. die Knotenlinie nicht feststeht, sondern langsam in der Äquatorebene rotiert und sich damit die Rektaszension des aufsteigenden Knotens formula_1 ständig ändert. Die Bahnebene dreht sich um die z-Achse des astronomischen Koordinatensystems. Gleichzeitig dreht sich die Apsidenlinie in der Bahnebene – ebenfalls durch Schwerkrafteinflüsse – um den Erdmittelpunkt. Damit treten Perigäumsdrehungen auf.

Nur auf vier Positionen hält ein geostationärer Satellit seinen Standort, und nur zwei von ihnen sind stabil: 105°W und 75°O. Kleine Bahnstörungen bewirken einen Drift zu den stabilen Lagen, solche Störungen müssen also ständig korrigiert werden.

Die Auswirkung dieser Bahnstörung kann auch positiv genutzt werden. Es kann durch entsprechende Auswahl der Inklination ein sonnensynchroner Orbit generiert oder das Perigäum über einen festen Erdpunkt gehalten werden. Eine weitere Auswirkung aufgrund der Schwerefeld-Inhomogenitäten der Erde ist die "Ost-West-Drift".

Wenn ein Himmelskörper einer Anziehungskraft ausgesetzt ist, welche streng umgekehrt quadratisch mit der Entfernung vom Zentralkörper abnimmt, so würde er sich in klassischer Näherung auf einer Keplerellipse bewegen, deren Form, Lage und Orientierung im Raum unverändert bliebe. Unter der Apsidendrehung versteht man eine fortschreitende Drehung der ganzen Bahn in der Bahnebene. Im Falle der Planetenbahnen ist der Einfluss der jeweils anderen Planeten die Hauptursache für die Periheldrehungen: Aus der Summe der gravitativen Einflüsse der anderen Planeten und der Auswirkung der allgemeinen Relativitätstheorie (siehe weiter unten) resultiert – in eine bezüglich des Fixsternhintergrunds ruhenden Koordinatensystem – eine rosettenartige Bewegung der Planeten: Die anomalistische Periode entspricht nicht genau der siderischen.

Mit steigender Beobachtungsgenauigkeit konnten Anfang des 19. Jahrhunderts Abweichungen der Umlaufbahn des Planeten Uranus von der Keplerbahn präzise bestimmt werden. Als Ursache wurde ein damals noch unbekannter achter Planet des Sonnensystems angesehen, dessen Position aufgrund der Uranus-Bahnstörungen vorhergesagt werden konnte. 1846 führte das schließlich zur Entdeckung des Neptun. In gleicher Weise wurde 1930 der Zwergplanet Pluto aufgrund der von ihm verursachten Bahnstörungen entdeckt.

Auf lange Sicht bewirkt die Gezeitenreibung nicht nur eine Abbremsung der Erdrotation, sondern eine Vergrößerung der Mondbahn. Die mittlere Entfernung zwischen dem Mond und der Erde wächst jährlich um etwa 3,8 cm. 

Der Einfluss der direkten Kraftwirkung durch andere Himmelskörper auf ein Objekt in der Erdumlaufbahn hängt von den Orbitparametern ab. Die wesentliche Rolle spielen Mond und Sonne. Dieser Einfluss ist im Bereich geosynchroner Bahnen größer als für stärker gebundene Satelliten. Die Gravitationskraft kann zerlegt werden in eine Komponente parallel und eine senkrecht zur Bahnebene. Die senkrechte Komponente bewirkt eine säkulare, die parallele eine periodische Störung. Die säkulare Störung beeinflusst analog der Erdabplattung die Bahnelemente formula_2, formula_3 und formula_4. Für geo"stationäre" Satelliten, also formula_5, verbleibt lediglich eine Auswirkung auf die Inklination. Ohne ausgleichende Korrekturen entsteht mit einer "Nord-Süd-Drift" von

ein .

In der allgemeinen Relativitätstheorie von Albert Einstein wird die Gravitation als Trägheitskraft gedeutet, sie ist demnach eine geometrische Eigenschaft der gekrümmten vierdimensionalen Raumzeit. Zur Bestimmung der an einem Punkt herrschenden Krümmung der Raumzeit dienen die einsteinschen Feldgleichungen. Sie wurden so formuliert, dass sie im Grenzfall schwacher Gravitation mit dem newtonschen Gravitationsgesetz übereinstimmen. Innerhalb des Sonnensystems, wo es sich um schwache Felder bzw. geringe Krümmung der Raumzeit handelt, ergeben sich nur geringe Abweichungen von den Vorhersagen des newtonschen Gravitationsgesetzes und damit von den Keplerbahnen. Bei starker Krümmung, wie sie durch starke Konzentration großer Masse auf kleinem Raum hervorgerufen wird, kommen neuartige Phänomene vor, wie sie sich beispielsweise bei Pulsaren im Orbit um andere Sterne zeigen: Der Hulse-Taylor-Doppelpulsar verändert seine Bahn auch dadurch, weil er Energie in Form von Gravitationswellen abstrahlt.

Bei der Zeitdilatation handelt es sich nicht um eine Bahnstörung im eigentlichen Sinn, sie hat jedoch Auswirkungen auf die technische Anwendung von Satelliten, so dass dieser Effekt für die Analyse von Bahnstörungen wichtig ist. Die Zeitdilatation hängt von zwei Größen ab, zum einen vom Ort im Gravitationsfeld und zum anderen von der Geschwindigkeit des beobachteten Objektes. Das Zeitsignal der GPS-Satelliten muss entsprechend korrigiert werden: In dieser Flughöhe (MEO) hat der gravitative relativistische Effekt (der allgemeinen Relativitätstheorie) eine größere Auswirkung als der geschwindigkeitsbezogene relativistische Effekt (der speziellen Relativitätstheorie), daher laufen die Uhren des Satelliten relativ zu Uhren auf der Erde schneller.

Wegen der allgemeinen Relativitätstheorie weicht das Kraftgesetz für Himmelskörper grundsätzlich vom idealisierten invers-quadratischen Verhalten des newtonschen Gravitationsgesetzes ab (wenn auch nur in sehr geringem Ausmaß), so dass ein weiterer Beitrag zu den Periheldrehungen der Planeten entsteht.

Bekanntestes Beispiel für solche relativistischen Effekte ist die Periheldrehung des Merkur. In der Mitte des 19. Jahrhunderts benutzte Urbain Le Verrier Beobachtungen von Merkurdurchgängen für eine besonders genaue Vermessung der Merkurbahn und stellte anhand der verbesserten Daten fest, dass Merkurs Periheldrehung etwas stärker ausfiel als erwartet. Nach den Berechnungen auf Basis des newtonschen Gravitationsgesetzes sollte sie etwa 530 Bogensekunden (") pro Jahrhundert betragen, wobei circa 280" auf den Einfluss der Venus entfielen, circa 150" auf Störungen durch Jupiter und circa 100" auf die restlichen Planeten. Die beobachtete Periheldrehung (moderner Wert: 571,91" pro Jahrhundert) war jedoch deutlich größer, die Diskrepanz beträgt 43,11". Erst der relativistisch berechnete Anteil von 42,98" zur Periheldrehung stimmt recht gut mit dem beobachteten Überschuss überein.

Ein weiterer relativistischer Effekt ist der Lense-Thirring-Effekt. Anschaulich gesprochen bewirkt eine rotierende Masse durch eine Mitführung der Raumzeit eine Verdrillung der Raumzeit. Wenn ein Himmelskörper auf seiner Bahn rotiert, führt das also zu einer zusätzlichen Präzession. Die Satellitenmission Gravity Probe B bestätigte 2004/05 diesen Effekt. Eine andere Konsequenz des Effekts wäre, dass die Orbitalebene der Sterne, die nahe einem supermassiven Schwarzen Loch kreisen, zur Präzession um die Drehachse des Schwarzen Lochs gebracht würde. Dieser Effekt konnte noch nicht nachgewiesen werden, ein Nachweis wird jedoch in den nächsten Jahren erwartet. Durch einen Vergleich der Präzession von zwei Sternen auf verschiedenen Orbits sollte es prinzipiell möglich sein, das „no-hair-theorem“ der allgemeinen Relativitätstheorie zu bestätigen.

Bei künstlichen Satelliten in niedrigen Orbits ist die Erdatmosphäre nicht zu vernachlässigen. Bei der Wechselwirkung handelt es sich nicht um eine Strömung; die Teilchen treffen einzeln auf. Da sie abprallen und auch Oberflächenatome herausschlagen, tritt je nach Neigung der getroffenen Fläche auch eine Kraft quer zur Flugrichtung auf. Die weit überwiegende parallele Kraftkomponente führt zu einem stetigen Verlust an Bahnenergie, der bei längeren Missionen ausgeglichen werden muss, um einen Absturz zu vermeiden, siehe das Diagramm zur Raumstation Mir in 300 bis 400 km Höhe. Das Hubble-Weltraumteleskop in etwa 600 km Höhe sank dagegen in 19 Jahren um nur rund 80 km.

Das Ausmaß des Effekts ist nur grob vorauszuberechnen, da die Ausdehnung der Hochatmosphäre stark von der Sonnenaktivität abhängt. Eine überschlägige Berechnung erfolgt mit Hilfe der bremsenden Kraft formula_7:

wobei:

Die Störgröße kann auch zu Gunsten einer Raumfahrtmission als Aerobraking-Manöver eingesetzt werden.

Auch der Strahlungsdruck kann zu Bahnstörungen führen. Wenn ein Körper elektromagnetische Strahlung (also auch Licht) absorbiert, reflektiert oder emittiert, dann wirkt sich der entsprechende Impulsübertrag auf seine Bahn aus. Bei Absorption und Emission ist der Strahlungsdruck gleich der Bestrahlungsstärke dividiert durch die Lichtgeschwindigkeit. Bei vollständiger Reflexion ist der Strahlungsdruck doppelt so groß.

Im Bereich der Erdbahn ist formula_11 die (gemittelte) Intensität der Sonnenstrahlung. Mit der Lichtgeschwindigkeit formula_12 berechnet sich der Strahlungsdruck formula_13 wie folgt:

Er ist abhängig von der Größe der bestrahlten Fläche und von deren Reflexionsfaktor formula_15 und wirkt als Kraft, die kontinuierlich von der Strahlungsquelle weg gerichtet ist. Bei einem typischen GPS-Satelliten führt der Strahlungsdruck der Sonne zu einer Beschleunigung in der Größenordnung 10 m/s² und damit einem Positionsfehler von einigen hundert Metern pro Tag.

Neben der direkten Sonnenstrahlung wirkt gegebenenfalls auch das von einem anderen Himmelskörper wie der Erde reflektierte Sonnenlicht.

Beim Poynting-Robertson-Effekt wirkt sich der Strahlungsdruck der Sonne auf die Umlaufbahnen kleiner Teilchen der interplanetaren Materie so aus, dass sich diese immer mehr der Sonne annähern.

Neben Licht aus der Umgebung kann auch die von Satelliten zur Kommunikation ausgesendete elektromagnetische Strahlung zu deutlichen Bahnstörungen führen. Besonders bei Kommunikationssatelliten mit hoher Sendeleistung und Richtantennen (beispielsweise Parabolantenne, Phased-Array-Antenne) macht sich diese Störung mit einer Beschleunigung in der Größenordnung 10 m/s² bemerkbar.

Neben dem direkten Strahlungsdruck macht sich die Strahlungsabsorption auch indirekt bemerkbar. Die daraus resultierende Erwärmung des Objekts führt zu einer zusätzlichen, meist ungleichmäßigen Emission von Wärmestrahlung. Solche Störungen sind insbesondere bei kleineren Körpern mit ihrem hohen Verhältnis von Oberfläche zu Masse relevant.

Die anisotrope Emission von Wärmestrahlung (von der Vorderseite des Flugkörpers) gilt auch als Ursache der Pioneer-Anomalie, der Bahnabweichung der Anfang der 1970er Jahre gestarteten Sonden Pioneer 10 bzw. Pioneer 11.

Beim Jarkowski-Effekt wirkt sich die unterschiedlich starke Erwärmung der sonnenzu- bzw. -abgewandten Seiten eines Asteroiden so aus, dass die verschieden starke Wärmestrahlung in die verschiedenen Raumrichtungen eine je unterschiedliche Kraft auf den Himmelskörper bewirkt. Die Stärke des Effektes hängt dabei auch von der Rotation, der Oberflächenbeschaffenheit (insbesondere der Albedo) und der Wärmeleitfähigkeit des Asteroiden ab. Als Folge werden beispielsweise die Bahnen von Asteroiden, die sich in Richtung ihrer Flugbahn um die eigene Achse drehen (prograd), nach außen gedrückt und die jener, deren Eigendrehung entgegen der Flugbahn (retrograd) verläuft, nach innen verändert. Ein entsprechender Effekt hinsichtlich des Rotationszustandes eines Himmelskörpers ist der YORP-Effekt.

Für elektrisch geladene oder magnetische Objekte spielt die elektromagnetische Wechselwirkung eine besondere Rolle. Auch außerhalb der Magnetosphäre derjeniger Himmelskörper, die selbst ein planetares Magnetfeld besitzen, existiert durch die Sonne ein interplanetares Magnetfeld, welches durch den Sonnenwind verstärkt wird. Aufgrund der Wechselwirkung zwischen den Magnetfeldern bzw. mit den dazu relativ bewegten elektrischen Ladungen der Objekte wirken weitere Kräfte, die eine Bahnstörung hervorrufen können.

Sofern die störenden Kräfte quantitativ bekannt sind, lässt sich eine gestörte Bahn berechnen. Umgekehrt lässt sich durch die genaue Beobachtung einer Bahn auf störende Kräfte schließen. In beiden Fällen wird eine der folgenden Rechenmethoden verwendet.

Falls die Situation näherungsweise ein Zweikörperproblem darstellt, also der betrachtete Körper im Wesentlichen an "einen" dominierenden Massepunkt gebunden ist und Störeinflüsse sich zu wenigen einfachen Termen zusammenfassen lassen, ist eine sogenannte "Störungsrechnung" möglich. Diese geht von der Kepler-Bahn aus und integriert die Störeinflüsse bzw. die relativistischen Korrekturen über einen Umlauf. Es ergeben sich Änderungsraten der Bahnelemente, etwa eine Periheldrehung. Für mehrere Körper, die sich gegenseitig stören, ist das Verfahren ebenfalls geeignet.

Besonders einfach sind Situationen ohne Bahnresonanzen, wenn also die Umlaufperioden zueinander nicht im Verhältnis kleiner ganzer Zahlen stehen. Die bloße Nähe zu einer Resonanzbedingung ist umso unkritischer, je geringer das Niveau der Störung ist, denn dann ändert sich die Phase der Störung, bevor eine wesentliche Wirkung eingetreten ist. Ohne Resonanzen lassen sich die störenden Körper als über ihre jeweilige Bahn verschmierte Masseringe ansetzen. Das oben für nur einen Umlauf erhaltene Ergebnis gilt dann für viele Umläufe.

Falls obige Voraussetzungen nicht erfüllt sind, werden die Bahnen nach der Methode der kleinen Schritte berechnet, z. B. mit einem Runge-Kutta-Verfahren.

Falls sehr viele Massepunkte zu berücksichtigen sind, etwa bei der Begegnung zweier Galaxien, kann unter Umständen eine Finite-Elemente-Methode mit geringerem Rechenaufwand sinnvolle Ergebnisse liefern.





</doc>
<doc id="157288" url="https://de.wikipedia.org/wiki?curid=157288" title="Ortsfrequenz">
Ortsfrequenz

Die Ortsfrequenz (auch Raumfrequenz, Formelzeichen "k" oder "R") ist der Kehrwert der räumlichen Periodenlänge.

Im Allgemeinen wird mit dem Begriff "Periode" die Vorstellung von einer "zeitlich" periodischen Größenänderung verbunden. Doch lässt sich der Begriff der Periode leicht auf beliebige periodische Funktionen erweitern, so auch auf "räumlich" variierende Größen.

Ist die betrachtete physikalische Größe etwa von der "eindimensionalen" Position abhängig, so hat die Periodenlänge die Dimension einer Länge und wird im internationalen Einheitensystem in Meter (m) angegeben. Dementsprechend gibt die Ortsfrequenz die Zahl der Perioden pro Längeneinheit an, hat also die Dimension 1/Länge. So ist für die Ortsfrequenz elektromagnetischer Wellen die Wellenzahl in Perioden pro Zentimeter gebräuchlich und für Frequenzbesen (Abb.) die Einheit Linienpaare pro Millimeter (Lp/mm).

Die Charakterisierung einer örtlich veränderlichen Funktion nach Anteilen mit bestimmen Ortsfrequenzen ist nicht so anschaulich wie die Zerlegung eines Klanges in Grund- und Obertöne, dennoch liefert sie die Grundlage für die Fourieroptik und Bildkompressionsalgorithmen wie z. B. JPEG.

In der Wahrnehmungspsychologie des Sehens wird die Ortsfrequenz auf den Sehwinkel bezogen, Einheit „Perioden pro Grad“. Bilder, in denen niedrige Ortsfrequenzen dominieren, sind unscharf und flächig, Bilder mit hoher Ortsfrequenz sind detailreich und mit gut erkennbaren Umrissen. Im visuellen Kortex gibt es Neuronen, die auf bestimmte Ortsfrequenzen (und Ausrichtung der Kanten) spezialisiert sind. Das Verhältnis des empfundenen zum tatsächlichen Kontrast des Objekts wird als Kontrastempfindlichkeitsfunktion (englisch: Contrast Sensitivity Function (CSF)) angegeben. Die CSF setzt sich als Produkt zusammen aus der rein optischen Modulationstransferfunktion (MTF) vom Objekt auf die Netzhaut und der retinalen Transferfunktion (RTF) vom Bild auf der Netzhaut zu höheren Wahrnehmungsfunktionen. Die MTF fällt ab zehn Perioden pro Grad steil ab. Die RTF ist für schnell veränderliche Reize flach, für statische Bilder fällt sie zu niedrigen Ortsfrequenzen ab, siehe Nachbild.


</doc>
<doc id="192154" url="https://de.wikipedia.org/wiki?curid=192154" title="Nichtionisierende Strahlung">
Nichtionisierende Strahlung

Nichtionisierende Strahlung sind diejenigen elektromagnetischen Wellen, deren Energie nicht ausreicht, um andere Atome zu ionisieren, da die Energiemenge der Photonen unter den meisten Bindungsenergien liegt. Dazu zählen insbesondere technisch genutzte Frequenzen im Bereich der Radiowellen und Mikrowellen sowie der größte Teil des sichtbaren Lichtes.

Die Wirkungen auf biologische und stark wasserhaltige Substanzen sind thermischer Art, die Wirkung nichtionisierender Strahlung ist mit der Erwärmung in einem Mikrowellenherd vergleichbar. Bei elektrisch gut leitfähigen Strukturen, beispielsweise einer Leiterplatte, können bei hohen Strahlungswerten zwischen einzelnen Leiterbahnen hohe Spannungen entstehen und elektronische Geräte in ihrer Funktion gestört werden oder ausfallen. Dieser Umstand ist insbesondere bei lebenswichtigen elektronischen Geräten wie Herzschrittmachern zu beachten.

Photonen mit einer Energie unter rund 3 eV gelten als nichtionisierend, da diese Energie kleiner als die typischen Bindungsenergien sind, welche im Bereich von 3 eV bis 7 eV liegen. Moleküle, die durch Strahlung unter 3 eV zerstört werden, können bei Zimmertemperatur nicht existieren. Sie werden durch die thermische Anregung zerstört.

Zu der nichtionisierenden Strahlung werden elektromagnetische Wellen im Frequenzbereich unter 750 THz oder einer Wellenlänge von mehr als 400 nm gezählt. Der Zusammenhang zwischen Energie und der Frequenz ergibt sich mit dem planckschen Wirkungsquantum.

Zu der nichtionisierenden Strahlung zählt im oberen Bereich Infrarotstrahlung und im Grenzbereich zur ionisierenden Strahlung sichtbares Licht mit Wellenlängen von 400 nm bis 780 nm. Wellenlängen unter 400 nm, welche als UV-Strahlung bezeichnet werden, werden im Regelfall zu der ionisierenden Strahlung gezählt.

Am 29. Juli 2009 hat der Deutsche Bundestag das „Gesetz zum Schutz vor nichtionisierender Strahlung bei der Anwendung am Menschen“ (NiSG) beschlossen, das am 1. März 2010 erstmals in Kraft getreten ist (BGBl. I S. 2433, zuletzt geändert durch Artikel 4 des Gesetzes vom 8. April 2013 (BGBl. I Nr. 17, S. 734)). Eine konkrete Folge daraus ist u. a., dass seit dem 4. August 2009 Minderjährigen die Benutzung von Solarien nach Maßgabe des § 4 NiSG untersagt ist. Eine dagegen gerichtete Verfassungsbeschwerde wurde mit Beschluss des Bundesverfassungsgerichtes vom 21. Dezember 2011, Az. 1 BvR 2007/10, nach § 93a Abs. 2 BVerfGG verworfen.

Die österreichische Verordnung über den Schutz der Arbeitnehmer/innen vor der Einwirkung durch optische Strahlung deckt den Arbeitnehmerschutz bei optischer Strahlung in Österreich und stellt die österreichische Umsetzung der EU-Richtlinie 2006/25/EG dar.


</doc>
<doc id="204839" url="https://de.wikipedia.org/wiki?curid=204839" title="Keplerparabel">
Keplerparabel

Eine Punktmasse (z. B. der Schwerpunkt eines Kometen) bewegt sich auf einer Parabel, der so genannten Keplerparabel, wenn sie bei der Bewegung um die Sonne gerade genügend kinetische Energie hat, um sich beliebig weit entfernen zu können. Die Geschwindigkeit die dazu nötig ist um von einer gebundenen Bahn auf eine Parabelbahn zu wechseln wird als zweite kosmische Geschwindigkeit bezeichnet.

Die Bewegung von Körpern auf Keplerparabeln wird durch die Keplerschen Gesetze beschrieben.


</doc>
<doc id="210457" url="https://de.wikipedia.org/wiki?curid=210457" title="Permeabilität (Geowissenschaften)">
Permeabilität (Geowissenschaften)

Die Permeabilität formula_1 (lat.: "permeare" „durchlassen“, von lat.: "per" „hindurch“, und lat.: "meare" „passieren“) wird in der Geotechnik zur Quantifizierung der Durchlässigkeit von Böden und Fels für Flüssigkeiten oder Gase (z. B. Grundwasser, Erdöl oder Erdgas) benutzt. Mit ihr sehr eng verbunden ist der hier gleichzeitig erläuterte Durchlässigkeitsbeiwert formula_2.

Die Permeabilität ist aus dem Darcyschen Gesetz abgeleitet und definiert als:

Hierbei bedeuten:

Die Permeabilität hängt nur von den Eigenschaften des "durchströmten" Mediums ab (Materialkennwert), denn das Produkt aus Fließrate formula_5 und Viskosität formula_6 bleibt konstant:

Da die Permeabilität nicht von der Dichte beeinflusst wird, die bei Gasen vom Druck abhängt, ist sie gut für Gase geeignet und wird daher häufig in der Erdgas- und Erdölwirtschaft benutzt. Die dynamische Viskosität ist im Bereich der Gültigkeit des Gasgesetzes vom Druck unabhängig, eine Temperaturabhängigkeit ist immer gegeben.

Als SI-Einheit für die Permeabilität ergibt sich m². Eine weitere gebräuchliche Maßeinheit ist das Darcy, benannt nach dem französischen Wissenschaftler Henry Darcy (1803–1858), der 1856 das Fließen von Wasser durch Kiesbettungen untersucht hat:

Da 1 Darcy eine relativ hohe Permeabilität ist, werden in der Geotechnik und im Bergbau oft das Millidarcy (mD) oder die SI-Einheit (µm)² verwendet.

Auch der Durchlässigkeitsbeiwert (bzw. die hydraulische Leitfähigkeit) quantifiziert die Durchlässigkeit von Boden oder Fels, jedoch gehen hier zusätzlich die Dichte und die Viskosität des durchströmenden Fluids ein:

Hierbei bedeuten:

Der Durchlässigkeitsbeiwert wird meist für strömende Flüssigkeiten (Wasser) verwendet, also in den Bereichen Wasserwirtschaft und Wasserbau. Da bei (inkompressiblen) Flüssigkeiten formula_19 vorausgesetzt werden kann, lässt sich der Durchlässigkeitsbeiwert auch vereinfacht schreiben als:

mit der Höhendifferenz formula_21 über die die Strömung erfolgt.

Sofern nicht anders angegeben, beziehen sich die in der Literatur angegebenen Werte für formula_2 üblicherweise auf Wasser. Ist der Durchlässigkeitsbeiwert für ein mit Wasser durchströmtes Medium bekannt, dann lässt sich die Durchlässigkeit dieses Mediums für andere Stoffe berechnen (s. u. "Bestimmung der Permeabilität").

Die Grenze zwischen einem durchlässigen und einem undurchlässigen Boden liegt etwa bei 10 m/s.

Die Durchlässigkeit von Böden hängt in erster Linie von ihrer Porosität ab, die von Fels von seiner Porosität und/oder seiner Klüftigkeit. Die Porosität von Böden wiederum hängt ab von den Korngrößen, ihrer Verteilung und damit vom Porenvolumen des Bodens.

Durchlässigkeit und Durchlässigkeitsbeiwert quantifizieren in ähnlicher Weise die Fließrate formula_5 durch ein durchlässiges Medium in Abhängigkeit von der Druckdifferenz formula_9, unterschiedlich sind nur ihre Einheiten:
Beide Größen können richtungsabhängig sein und werden dann als Tensoren dargestellt.

Außerdem sind beide Größen konstant über die Fließrate formula_5, sofern die folgenden Bedingungen erfüllt sind:

In gesteinsphysikalischen Laboratorien wird die Permeabilität routinemäßig an zylindrischen Proben mit einem Durchmesser von 30 mm und einer Länge von 40 bis 80 mm bestimmt; in den USA sind für Routinemessungen Proben von 1 Zoll ×  Zoll gebräuchlich. Für Untersuchungen, bei denen es auf ein großes Porenvolumen ankommt (beispielsweise relative Permeabilität), sind auch Probendurchmesser von 40 mm üblich. Die Orientierung der Proben ist standardmäßig parallel zur Schichtung.

Ein Sonderfall ist die Bestimmung der Permeabilitätsanisotropie an Würfeln von 30 oder 40 mm Kantenlänge. Diese sind so aus dem Kernmaterial herauszuarbeiten, dass zwei Flächen parallel zur Schichtung orientiert sind und somit Daten parallel und senkrecht zur Schichtung an einer Probe bestimmt werden können.

Mit der skizzierten Messanordnung wird formula_2 für Wasser bestimmt:

formula_1 lässt sich dann über die dynamische Viskosität formula_6 des Wassers und seine Dichte formula_16 berechnen:

Ist der Durchlässigkeitsbeiwert formula_2 für das mit Wasser durchströmte Medium experimentell bestimmt, so kann man aus der eben genannten Beziehung den Durchlässigkeitsbeiwert "dieses" Mediums für andere Fluide, z. B. für Erdöl, durch Einsetzen von deren Dichte und dynamischer Viskosität berechnen:

oder unter Verwendung der kinematischen Viskosität formula_34:

Für Böden besteht die Möglichkeit, den Durchlässigkeitsbeiwert formula_2 für Wasser in m/s aus der Kornverteilungskurve abzuschätzen "(Hazen, 1893)":

Hierbei bedeuten:

Diese Abschätzung gilt nur unter der Voraussetzung, dass der Ungleichförmigkeitsgrad formula_41 ist (gleichförmiger Boden).

Nach Beyer ist

formula_43 ist ein Beiwert, der vom Ungleichförmigkeitsgrad abhängt: formula_44 Für bestimmte formula_45 ist formula_46, so dass die Formel mit der von Hazen übereinstimmt.

Angewendet werden diese Materialparameter dann, wenn Böden oder Fels von Flüssigkeiten oder Gasen durchströmt werden: Grundwasser­strömungen, Trinkwassergewinnung, Gewinnung von Erdöl oder Erdgas, Berechnungen des Wasserandrangs an Bauwerken und Tunneln, Ermittlung der Dichtigkeit von Dämmen und Deichen, auch bei kontaminierten Böden und der Verpressung von Kohlendioxid.

Für die Wirtschaftlichkeit der Förderung von Erdöl und Erdgas ist die Produktionsrate eine wichtige Einflussgröße. Sie hängt u. a. von der Permeabilität der geologischen Formationen ab, aus denen diese Rohstoffe gefördert werden. Da jedoch auch der Weltmarktpreis mit über die Wirtschaftlichkeit entscheidet, lassen sich hier keine dauerhaft gültigen Grenzwerte angeben.

Für die Planung von Injektionen zur Abdichtung und/oder zur Verbesserung der mechanischen Eigenschaften von Lockergesteinen ist die Kenntnis über die hydraulischen Eigenschaften bedeutend.

Die Transmissibilität formula_47 ist definiert als Produkt aus Permeabilität formula_1 und der Mächtigkeit formula_49 der Wasser führenden Boden- oder Gesteinsschicht (Aquifer):

Analog dazu wird die Transmissivität formula_51 definiert als Produkt aus Durchlässigkeitsbeiwert formula_2 und Mächtigkeit:

Besteht der Aquifer aus i Schichten mit unterschiedlichen Durchlässigkeiten und Mächtigkeiten, dann werden die jeweiligen Produkte addiert:

Aus letzter Formel wird der Zweck der beiden Größen deutlich: sie stellen die Integrale der jeweiligen Durchlässigkeitswerte (formula_1 oder formula_2) über die Aquifer-Mächtigkeit dar. Dies berücksichtigt, dass die Durchlässigkeit meist nicht über die gesamte Höhe des Aquifers gleich ist: der Aquifer ist inhomogen bezüglich seiner Durchlässigkeit.

Wegen des gemeinsamen Faktors formula_49 besteht zwischen den beiden Größen die gleiche Beziehung wie zwischen formula_2 und formula_1:

Die gebräuchlichere der beiden Größen ist in Anlehnung an die DIN die Transmissivität, da sie für die Gewinnung von Grundwasser als Trinkwasser eine wichtige Rolle spielt.

Die Transmissivität wird meist über einen Pumpversuch ermittelt. Dabei erhält man jedoch nur die Gesamt-Transmissivität und "keine" Angaben über die o. g. Inhomogenitäten bezüglich der Durchlässigkeit des Aquifers.

Anmerkungen:




</doc>
<doc id="231431" url="https://de.wikipedia.org/wiki?curid=231431" title="Mikrozustand">
Mikrozustand

Ein Mikrozustand ist in der statistischen Physik die vollständige mikroskopische Beschreibung eines thermodynamischen Systems. Ein Mikrozustand entspricht damit einem Punkt im Phasenraum des Systems. Für ein klassisches ideales Gas sind damit Ort und Impuls jedes Teilchens festgelegt.

Im Gegensatz zum Mikrozustand beschreibt der Makrozustand das System durch seine gemittelten Parameter, wie etwa Temperatur, Druck oder Magnetisierung. Ein thermodynamisches System mit gegebenem Makrozustand besetzt nun verschiedene Mikrozustände der Energie formula_1 mit einer gewissen Wahrscheinlichkeit formula_2. Aus diesen Mikrozuständen zusammen mit ihren Wahrscheinlichkeiten lassen sich viele Parameter des Systems berechnen.

Oft sind einige Mikrozustände eines abgeschlossenen Systems nach außen hin nicht unterscheidbar (z. B. weil sie die gleiche Gesamtenergie und den gleichen Gesamtimpuls oder die gleiche Gesamtmagnetisierung haben). Gemäß dem Postulat der gleichen a-priori-Wahrscheinlichkeiten tritt im thermischen Gleichgewicht jeder dieser Mikrozustände mit gleicher Wahrscheinlichkeit auf. Es kann nicht belegt werden, ist aber die einzig plausible Annahme, da jede Auszeichnung eines dieser Zustände durch eine veränderte Wahrscheinlichkeit eine gewisse Willkür bedeuten würde.

Die statistische Physik definiert die thermodynamischen Eigenschaften eines Systems über ein Ensemble von formula_3 Mikrozuständen. Jedem Mikrozustand formula_4 kann eine Energie formula_1 und eine Besetzungswahrscheinlichkeit formula_2 zugeordnet werden, die sich aus den Eigenschaften des Mikrozustandes ergeben. Mit diesen Definitionen können dann Kennzahlen des Systems als Mittelwert der Mikrozustände (Ensemblemittelwert) berechnet werden (siehe auch Ergodenhypothese). Beispiele:

Weitere thermodynamische Größen können über den Formalismus der Zustandssummen berechnet werden. Dabei wird die Anzahl der Mikrozustände für gewisse Randbedingungen gezählt. Die Verteilung der Mikrozustände im Phasenraum wird von der Zustandsdichte angegeben.

In der klassischen Physik wird ein Gas als Menge von formula_3 punktförmigen Teilchen der Masse formula_15 angenommen. Die Teilchen haben die Positionen formula_16 und die Geschwindigkeiten formula_17. Der Index formula_18 nummeriert die Teilchen durch, der Index formula_4 ist die Nummer eines möglichen Mikrozustands; ein Mikrozustand ist dabei die Angabe der Positionen und momentanen Geschwindigkeiten aller Teilchen zu einem bestimmten Zeitpunkt.

Die mittlere Energie des Mikrozustands lässt sich dann aus den kinetischen Energien der Gasteilchen berechnen:

Es gibt viele Zustände mit der Energie formula_1, da nur die Geschwindigkeiten, nicht aber die Positionen der Teilchen zu dieser Größe beitragen. Gemäß dem Postulat der gleichen A-priori-Wahrscheinlichkeiten hat jeder dieser Zustände die gleiche Wahrscheinlichkeit. 

Die Teilchen stoßen elastisch aneinander und an die Wände des Gefäßes. Dadurch stellt sich nach einiger Zeit ein thermisches Gleichgewicht ein, in dem die Verteilung der Geschwindigkeiten der Einzelteilchen der Maxwell-Boltzmann-Verteilung folgt.

Die Wahrscheinlichkeit für einen Zustand mit der Energie formula_1 ist:

Dabei ist

Ein weiteres Beispiel der statistischen Physik ist das Ising-Modell. Bei diesem eindimensionalen System von Spins sind formula_3 Teilchen in einer Reihe angeordnet. Dabei zeigt der Spin jedes Teilchens entweder nach oben formula_28 oder nach unten formula_29. Liegt zusätzlich ein externes Magnetfeld mit der Feldstärke formula_30 an, so lässt sich die Energie eines Mikrozustandes berechnen als:

Dabei ist formula_32 das Bohrsche Magneton. Für den Fall formula_33 kann man die möglichen Mikrozustände und ihre Energie direkt aufschreiben:
Auch in diesem Beispiel kann ein Makrozustand gegebener Energie durch verschiedene Mikrozustände dargestellt werden.

Die meisten Lehrbücher der statistischen Physik, wie etwa:


</doc>
<doc id="280099" url="https://de.wikipedia.org/wiki?curid=280099" title="Galileo Ferraris">
Galileo Ferraris

Galileo Ferraris (* 31. Oktober 1847 in "Livorno Vercellese", ihm zu Ehren Livorno Ferraris genannt; † 7. Februar 1897 in Turin) war ein italienischer Ingenieur und Physiker. 

Ferraris, Sohn eines Apothekers, wurde nach seinem Ingenieurs-Abschluss 1870 Assistent von Giovanni Codazza für Technische Physik am "Reale Museo Industriale Italiano" in Turin, das später Teil des heutigen Politecnico di Torino wurde. 1880 wurde Ferraris selbst ordentlicher Professor der Physik. 1886 bis 1887 richtete er in Turin die erste elektrotechnische Ingenieurschule Italiens ein. Er war bei mehreren Elektrotechnik-Kongressen Vertreter Italiens sowie Präsident der Italienischen Elektrotechnischen Gesellschaft.

Seine wissenschaftliche Arbeit beschäftigte sich vor allem mit elektrischer Kraftübertragung, Transformatoren sowie Wechselstrom- und Drehstromtechnik.

Nach Galileo Ferraris wurde der Ferraris-Zähler, ein auf dem Wirbelstromprinzip basierender Stromzähler benannt. Diese weitverbreiteten Energiemessgeräte sind an der horizontal rotierenden Scheibe, dem Ferrarisläufer, leicht zu erkennen. 

Im Jahr 1886 wurde er zum Mitglied der Leopoldina gewählt.

Im Jahre 1888 erfand er praktisch zeitgleich und unabhängig von Nikola Tesla das Drehstromsystem und war zehn Jahre im Streit mit Westinghouse Electric, an die Tesla die Patente seiner Erfindung verkauft hatte.



</doc>
<doc id="291576" url="https://de.wikipedia.org/wiki?curid=291576" title="Frederick Reines">
Frederick Reines

Frederick Reines (* 16. März 1918 in Paterson, New Jersey; † 26. August 1998 in Orange, Kalifornien) war ein US-amerikanischer Physiker. Für den experimentellen Nachweis des Neutrinos erhielt er 1995 den Nobelpreis für Physik (zusammen mit Martin L. Perl, dem Entdecker des Tauons).

Reines wuchs als Sohn jüdischer Einwanderer aus Russland auf, sein Vater hatte einen kleinen Laden auf dem Land im Staat New York. Er besuchte bis 1935 die High School in Union City, New Jersey. Er studierte Ingenieurwissenschaften (Bachelor 1939) und dann Physik am Stevens Institute of Technology (Master-Abschluss 1941) und promovierte 1944 an der New York University bei R. D. Present (The Liquid Drop Model for Nuclear Fission). Im Zweiten Weltkrieg arbeitete er (noch bevor er seine Promotion vollendete) im Manhattan Project unter Richard Feynman in Los Alamos. Auch nach dem Krieg arbeitete er in der Kernwaffenforschung, unter anderem untersuchte er die Effekte der Explosions-Stoßwelle mit John von Neumann.

Anfang der 1950er Jahre entwickelte er mit Clyde Cowan verschiedene Methoden, Neutrinos experimentell nachzuweisen. Die erste direkte Beobachtung von Neutrinos, das Cowan-Reines-Neutrinoexperiment, gelang ihrer Arbeitsgruppe 1956 am neu gebauten Savannah River-Kernreaktor.

1959 wurde er Professor an der Case Western Reserve University und Leiter der dortigen Physik-Fakultät. Dort leitete er eine Gruppe, die erstmals von der kosmischen Strahlung erzeugte Neutrinos nachwies. 1966 wechselte er als Dekan der Naturwissenschaftlichen Fakultät (Dean of Natural Sciences) an die neu gegründete University of California, Irvine (UCI). Dort beschäftigte er sich auch mit Strahlungsdetektoren für die Medizin. Seine Neutrino-Arbeitsgruppe in Irvine nahm am IMB (Irvine-Michigan-Brookhaven)-Protonenzerfalls-Experiment teil, wies zuerst den doppelten Betazerfall direkt im Labor nach und richtete ihre Neutrino-Detektoren auch frühzeitig auf die Suche von Neutrinos aus Supernova-Explosionen aus (auch auf Initiative von Reines). Bei der Supernova 1987A konnten tatsächlich solche Neutrinos mit dem IMB, bei dem er Ko-Sprecher war, und dem japanischen Kamiokande-Detektor nachgewiesen werden. Dafür erhielt Reines 1989 den Bruno-Rossi-Preis. 1988 wurde er Professor Emeritus an der UCI.

1958/59 war er Guggenheim Fellow und 1959 bis 1963 Sloan Research Fellow. 1966 wurde er in die American Academy of Arts and Sciences gewählt. 1981 erhielt er den Oppenheimer-Preis und 1992 den Panofsky-Preis. Außerdem erhielt er die National Medal of Science (1985). Er war Mitglied der National Academy of Sciences (1980) und auswärtiges Mitglied der Russischen Akademie der Wissenschaften (1994). 1990 erhielt er den Michelson-Morley Award und 1992 die Franklin Medal.

Er war seit 1940 mit Sylvia Samuels verheiratet. Sie hatten zwei Kinder, einen Sohn und eine Tochter.



</doc>
<doc id="302499" url="https://de.wikipedia.org/wiki?curid=302499" title="Strömungswiderstand">
Strömungswiderstand

Der Strömungswiderstand ist die physikalische Größe, die in der Fluiddynamik die Kraft bezeichnet, die das Fluid als Medium einer Bewegung entgegensetzt.
Ein Körper, der sich relativ zu einem gasförmigen oder flüssigen Medium bewegt, erfährt einen Strömungswiderstand, eine der Relativgeschwindigkeit entgegengesetzt wirkende Kraft.
Bewegt sich ein Objekt wie ein Flugzeug durch die Luft, so spricht man auch vom Luftwiderstand oder von der Luftreibung, siehe auch Aerodynamik. Bei Bewegungen im Wasser von spricht man von Wasserwiderstand, siehe auch Hydrodynamik. 

Auf die Oberfläche eines umströmten Körpers übt die Strömung örtlich verschiedene Schubspannung und Druck (Normalspannung) aus. Werden Druck und Schubspannung über die gesamte Oberfläche integriert, erhält man die resultierende Kraft, die die Strömung auf den Körper ausübt.
Diese Kraft hat eine bestimmte Richtung im Raum. Die Kraftkomponente, die in Richtung der Anströmrichtung liegt, ist die Widerstandskraft. Neben der Widerstandskraft sind andere Kraftkomponenten die Auftriebskraft und die Seitenkraft. Oft werden diese Kräfte im Windkanal gemessen.

Bei Kraftfahrzeugen ist es üblich, die Kraftkomponenten bezüglich eines fahrzeugfesten Koordinatensystems anzugeben.

Es sind die physikalischen Größen Druck und Schubspannung, die an der Oberfläche eines Körpers wirken und damit zum Strömungswiderstand beitragen können. Dementsprechend kann der Strömungswiderstand in einen Druckwiderstand und einen Schubspannungswiderstand aufgeteilt werden. In Abhängigkeit von der Form des umströmten Körpers und der Anströmrichtung kann der Druckwiderstand oder der Schubspannungswiderstand überwiegen.

Je nach vorliegendem Fall erweist es sich für die Betrachtung und Berechnung als günstig, bestimmte Effekte, die bei der Umströmung des Körpers auftreten, separat zu behandeln. Dies ist der Hintergrund für den Interferenzwiderstand, induzierte Widerstände und den Wellenwiderstand.

Der Druckwiderstand folgt aus der Druckverteilung (Normalspannung) um einen Körper. Der Druck im Ablösegebiet am Heck von Körpern ist geringer als der im Staupunkt. Die wirksame Fläche dieses Widerstandes ist die projizierte Fläche in Richtung der Anströmung.

Der Schubspannungswiderstand ist Ergebnis der Reibung, also des viskosen Impulsaustausches. Er beruht auf den Schubspannungen, die auf der Oberfläche des Körpers auftreten, indem die Strömung über die Oberfläche streicht.

Der Interferenzwiderstand beschreibt die strömungstechnische Widerstandsgröße, die auftritt, wenn vormals völlig unabhängige Strömungskörper zu beieinander liegenden Strömungskörpern werden. Er ist definiert als die Differenz zwischen dem Gesamtwiderstand des Bauteils und der Summe des Widerstands der Einzelbauteile oder Bauteilgruppen nach dem Zusammenbau. Konstruktiv wird man immer einen negativen Interferenzwiderstand anstreben.
Ein Beispiel ist ein Flugzeugrumpf und die Flugzeugtragflächen vor dem Zusammenbau und nach erfolgter Montage. Die Summe der Einzelwiderstände der Bauteile Flügel und Rumpf ist höher als der Gesamtwiderstand nach dem Zusammenbau. Qualitativ betrachtet ist der Interferenzwiderstand die gegen die Anströmrichtung wirkende Komponente der Luftkraft an einem Strömungskörper, die durch die gegenseitige Beeinflussung der von verschiedenen Teilen des Flugzeuges ausgelösten Wirbel oder durch Überlagerung der Grenzschichten in den Ecken entsteht.

Der induzierte Widerstand entsteht immer, wenn ein Objekt in einem Fluid Strömungen im Fluid erzeugt. Das ist zum Beispiel bei der Auftriebserzeugung durch Tragflächen eines Flugzeugs der Fall, bei der zum einen Luft nach unten beschleunigt wird (downwash) und zum anderen durch Wirbelbildung (Randwirbel) dabei entstehende Druckunterschiede ausgeglichen werden. Die Bewegungsenergie, die dabei der Luft zugeführt wird, geht dem Flugzeug verloren.

Der Wellenwiderstand tritt bei umströmten Körpern auf, die sich mit Überschall- oder transsonischer Geschwindigkeit bewegen. An Körperkanten, die der Anströmung entgegen geneigt sind, tritt eine Druckerhöhung auf, während an den Kanten, die der Anströmung abgeneigt sind, eine Druckverminderung auftritt. Dieser Druck führt zu einer entgegen der Bewegung gerichteten Kraft.

Die Strömungswiderstandskraft formula_1 eines Körpers in einer bestimmten Lage ist abhängig von der Anströmgeschwindigkeit formula_2, der Dichte formula_3 und der Viskosität (Zähigkeit) formula_4 des Fluids sowie der geometrischen Abmessung (einer charakteristischen Länge) formula_5 des Körpers.
Dieser Zusammenhang, der fünf Variablen umfasst, kann mit Hilfe einer Dimensionsanalyse nach dem Buckinghamschen Π-Theorem auch mittels zwei dimensionsloser Ähnlichkeitskennzahlen formuliert werden. Diese Ähnlichkeitskennzahlen sind der Strömungswiderstandskoeffizient formula_7 und die Reynolds-Zahl formula_8, die definiert sind als

Dabei ist die Größe formula_11 eine Bezugsfläche, welche definiert sein muss. Üblicherweise wird die Stirnfläche des Körpers als Bezugsfläche verwendet, bei Tragflügeln aber die Flügelfläche.

Der physikalische Zusammenhang kann damit beschrieben werden in der Form
Die Widerstandskraft formula_1 ist proportional zum Produkt aus formula_7-Wert und Bezugsfläche, welches als "Widerstandsfläche" bezeichnet wird. Man erhält die Strömungswiderstandkraft aus

Der Faktor formula_16 wird als Staudruck bezeichnet.

Für praktische Anwendungen, z. B. dem Luftwiderstand von Kraftfahrzeugen, kann die Abhängigkeit von der Reynolds-Zahl häufig vernachlässigt werden. Dann wird der formula_7-Wert als konstanter Wert angesetzt, so dass der Widerstand quadratisch mit der Geschwindigkeit zunimmt. Für einen Vergleich des Strömungswiderstands verschiedener Fahrzeuge ist die Widerstandsfläche das maßgebliche Kriterium.

Bei laminarer Strömung wird der Strömungswiderstand nur durch die innere Reibung des Mediums verursacht. Ist formula_4 die dynamische Viskosität des Mediums, so gilt für kugelförmige Körper vom Radius formula_19 das Stokessche Gesetz

In einer turbulenten Strömung lässt sich der Strömungswiderstand nur durch Experimente bestimmen, bzw. durch aufwendige numerische Rechnung, z. B. mittels Finite-Volumen-Verfahren, annähern.

Bei Kraftfahrzeugen, aber auch z. B. Fahrradfahrern und Läufern, kann im relevanten Geschwindigkeitsbereich von turbulenter Strömung ausgegangen werden.

Im modernen Automobilbau ist der formula_7-Wert, der Luftwiderstandsbeiwert, von großer Bedeutung. Er kann im optimalen Falle 0,07 betragen (TERA Fennek 2013), beim Ford Model T war er 0,9.




</doc>
<doc id="485988" url="https://de.wikipedia.org/wiki?curid=485988" title="Alhazen">
Alhazen

Alhazen (, verkürzt auch Ibn al-Haiṯam und "Ibn al-Heithem", (), latinisiert Alhacen, Avennathan oder Avenetan, geboren um 965 in Basra; gestorben nach 1040 in Kairo), war ein Mathematiker, Optiker und Astronom in der Blütezeit des Islam. Er verfasste grundlegende Beiträge zur Optik, Astronomie, Mathematik und Meteorologie.

Über das Leben von Alhazen ist wenig bekannt, es waren aber zahlreiche Legenden über ihn im Umlauf, die auch ihren Eingang in spätere westliche Biographien fanden. Er wirkte in Kairo am Hof al-Hakims, wo er ein Projekt zur Regulierung der Nilüberschwemmungen vorschlug, das aber vom Kalifen abgelehnt wurde. Der Legende nach täuschte er, entweder um sein Versagen zu vertuschen oder um sich weniger Verwaltungsaufgaben als der Wissenschaft zu widmen, da er den Zorn al-Hakims fürchtete, eine Geisteskrankheit vor, wonach er sich an dem von al-Hakim gegründeten "Haus der Weisheit" ganz der Wissenschaft zuwandte. Nach dem Tode al-Hakims im Jahr 1021 soll er nach der Legende auf scheinbar „wunderbare Weise“ genesen sein.

In seinen zahlreichen mathematischen Werken beschäftigte er sich mit Problemen der Zahlentheorie und der Geometrie.

Von größter Bedeutung sind jedoch seine optischen Experimente: Die meisten Wissenschaftler der Antike, darunter Euklid und Ptolemäus, nahmen an, der visuelle Eindruck im Gehirn werde von „Sehstrahlen“ erzeugt, die vom menschlichen Auge ausgingen und die Umgebung abtasteten, ähnlich wie bei einem Blinden, der seine Umgebung mit einem Stab abtastet. Aristoteles hingegen war der Ansicht, Licht existiere unabhängig vom menschlichen Auge und bahne sich seinen Weg von den Gegenständen in das Auge über ein Medium. Alhazen jedoch ging auf neue Weise an die Frage heran, indem er den Aufbau des Auges analysierte. Er erkannte die Bedeutung der Linse im Auge und widerlegte in wissenschaftlichen Experimenten die Sehstrahlen-Theorie.

Aufbauend auf Ibn Sahl verfeinerte und erweiterte er auch die Theorien Ptolemäus' zur Lichtbrechung und Lichtreflexion; insbesondere hat er die Eignung gewölbter Glasoberflächen zur optischen Vergrößerung erkannt und beschrieben. Mit diesen Erkenntnissen stellte er Lesesteine aus Glas her. Damit gilt er als Erfinder der Lupe und inspirierte wahrscheinlich mit seinen Schriften Roger Bacon zur Erfindung der Brille. Er führte auch Versuche zur Farbmischung und Camera Obscura aus.
Noch heute ist sein Name mit einem Problem der Optik verbunden, das "Alhazensche Problem": Er löste geometrisch mit Kegelschnitten die Aufgabe, in einem sphärischen Spiegel den Punkt zu berechnen, von dem ein Gegenstand von gegebener Entfernung zu einem gegebenen Bild projiziert wird, was auf eine Gleichung vierten Grades führt bzw. auf die Bestimmung der Wurzel einer Gleichung dritten Grades (damit war sie nicht mit Zirkel und Lineal lösbar). Die vollständige algebraische Lösung fand Peter Neumann 1997. Das Problem geht bis auf Ptolemäus zurück und beschäftigte zum Beispiel Christiaan Huygens. Alhazen selbst gab in diesem Zusammenhang mit einer frühen Anwendung der vollständigen Induktion die erste Formel für die Summe von vierten Potenzen (die auch auf Summen ganzzahliger Potenzen verallgemeinert werden kann), und fand damit das Volumen des Paraboloids. Damit spielt er auch eine Rolle in der Frühgeschichte der Analysis.

Ausgehend von seinen Erkenntnissen auf dem Gebiet der Optik entdeckte Alhazen, dass Brechung des Lichts auch in der Lufthülle der Erde stattfindet. Er stellte fest, dass der Mond sowohl am Horizont als auch im Zenit die gleiche Größe hat. Er erkannte also den scheinbar größeren Durchmesser des Mondes in Horizontnähe als eine Wahrnehmungstäuschung (Mondtäuschung). Auch berechnete er die Höhe der Atmosphäre aus der Beobachtung von Sonnenuntergängen.

Er befasste sich nach der Optik auch mit Astronomie und entwickelte dazu neue Methoden der sphärischen Geometrie. Sein "Liber de mundo et coelo" und sein Konzept einer geometrisch-perspektivischen Optik waren ab etwa 1200 im christlichen Abendland (grundlegend etwa auch für Dietrich von Freiberg) weit verbreitet.

Er machte sich auch um die Wissenschaftstheorie verdient: Als erster wandte er systematisch die induktiv-experimentelle wissenschaftliche Arbeitsweise an, bei der zuerst Experimente durchgeführt und erst danach anhand der Versuchsergebnisse Theorien aufgestellt werden; bis dahin war es üblich, Erkenntnisse nur durch logische Schlussfolgerungen zu gewinnen und Experimente allenfalls zur Veranschaulichung der so gefundenen Theorien durchzuführen.

Alhazens eventuell von Gerhard von Cremona selbst oder in seinem Umkreis ins Lateinische übersetzter Kitāb al-Manāzir, der unter dem Titel "Perspectiva" oder "De aspectibus" verbreitet war, beeinflusste optische und darüber hinaus philosophische Theorien seit dem ausgehenden 13. Jahrhundert, insbesondere sind die Werke von Roger Bacon, Witelo und Johannes Peckham von Alhazens Auffassungen geprägt. Die Übersetzung wurde 1572 durch Friedrich Risner in Basel zusammen mit Witelos Optik publiziert, auf sie beziehen sich Keplers Paralipomena ad Vitellionem.

Der Mondkrater Alhazen und der Asteroid (59239) Alhazen sind nach al-Haitham benannt.

Um ihn zu ehren, nannte die Aga-Khan-Universität (Pakistan) ihren Lehrstuhl für Augenheilkunde (med.: Ophthalmologie) "The Ibn-e-Haitham Associate Professor and Chief of Ophthalmology".

Ein fiktives Bildnis Alhazens befindet sich auf der seit 2003 im Umlauf befindlichen 10.000-Dinar-Banknote des irakischen Dinars.





</doc>
<doc id="555153" url="https://de.wikipedia.org/wiki?curid=555153" title="Rudolf-Kaiser-Preis">
Rudolf-Kaiser-Preis

Mit dem Rudolf-Kaiser-Preis wird seit 1989 jährlich ein Nachwuchswissenschaftler für besondere wissenschaftliche Leistungen im Bereich der Experimentalphysik ausgezeichnet. Er ist benannt nach dem deutschen Physiker Rudolf Kaiser.

Ausgeschlossen sind Arbeiten an Großgeräten wie beispielsweise Teilchenbeschleunigern, da diese in der Regel schwierig einer bestimmten Person zugeordnet werden können. Voraussetzung ist, dass der Preisträger noch nicht auf eine Professur berufen worden ist. Vergeben wird der Preis von der Rudolf-Kaiser-Stiftung im Stifterverband für die Deutsche Wissenschaft. Das Preisgeld beträgt 35.000 € (Stand 2019).



</doc>
<doc id="651066" url="https://de.wikipedia.org/wiki?curid=651066" title="Albrecht Unsöld">
Albrecht Unsöld

Albrecht Otto Johannes Unsöld (* 20. April 1905 in Bolheim (Württemberg); † 23. September 1995) war ein deutscher Astrophysiker mit prägendem Einfluss auf die Physik der Sternatmosphären.

Unsöld war Sohn eines Pfarrers und studierte Physik in Tübingen und München, wo Arnold Sommerfeld Theoretische Physik und insbesondere Quantenmechanik lehrte. Dort schloss er 1927 seine Dissertation "Beiträge zur Quantenmechanik der Atome" ab. Während seines Studiums wurde er Mitglied der AMV Stochdorphia Tübingen. Nach Aufenthalten in Potsdam, München, Pasadena und Hamburg wurde er 1932 Professor und Direktor des Instituts für Theoretische Physik an der Universität Kiel. 1958/59 war er ihr Rektor. 1973 wurde er emeritiert. 

Schon früh begann Unsöld, quantenphysikalische Methoden auf die Untersuchung von Sternatmosphären anzuwenden. Er gewann die ersten detaillierten Analysen von Sternatmosphären und ihrer Elementhäufigkeiten und beschrieb Entstehung und Verbreiterung von Linien in den Spektren der Sonne und anderer Sterne. Unsölds Analyse des Spektrums des B0-Sterns Tau Scorpii, aufgenommen 1939 bei einem Besuch der Yerkes- und McDonald-Observatorien, lieferte die erste detaillierte Analyse eines Sterns außer der Sonne. Unsöld und seine Kieler Schule erarbeiteten wesentliche Grundlagen der Bestimmung der physikalischen Bedingungen in Sternatmosphären. 

Neben seinem einflussreichen Werk über die Physik der Sternatmosphären und dem Standardlehrbuch der Astronomie "Der neue Kosmos", gab er die "Zeitschrift für Astrophysik" heraus, bis sie mit anderen europäischen Zeitschriften in die Astronomy and Astrophysics verschmolz.

1951 wurde er zum korrespondierenden Mitglied der Bayerischen und 1955 der Göttinger Akademie der Wissenschaften gewählt. 1962 wurde er zum Mitglied der Deutschen Akademie der Naturforscher Leopoldina gewählt. Seit 1946 war er Mitglied der Braunschweigischen Wissenschaftlichen Gesellschaft.

Anfang der 1980er Jahre kam es zu einer Kontroverse um einen Artikel von Unsöld in den Physikalischen Blättern (November 1980), der sich kritisch zu Albert Einstein äußerte. Unsöld sah unter anderem die Rolle Einsteins bei der Entwicklung der Atombombe kritisch und sie wäre im Einsteinjahr 1979 seiner Meinung nach übergangen worden. Das führte zu personellen Konsequenzen und Umbrüchen in dieser Zeitschrift der Deutschen Physikalischen Gesellschaft.





</doc>
<doc id="654605" url="https://de.wikipedia.org/wiki?curid=654605" title="Dilepton">
Dilepton

Mit Dilepton bezeichnet man ein korreliertes Paar von einem Lepton und einem Antilepton, seinem Antiteilchen, das in Atomkernkollisionen in Teilchenbeschleuniger-Experimenten erzeugt wird. Dileptonen entstehen aus elektromagnetischen Zerfalls- oder Annihilationsprozessen. Mit Dileptonen sind entweder Dielektronen (Elektron-Antielektron-Paare) oder Dimyonen (Myon-Antimyon-Paare) gemeint. Tau-Leptonen-Paare werden in der Regel nicht als Dilepton bezeichnet, weil sie wegen ihrer hohen Masse und kurzen Lebenszeit nicht gemessen werden.

Da (elektrisch geladene) Leptonen nur der elektromagnetischen und schwachen Wechselwirkung unterliegen, werden sie durch das stark wechselwirkende Medium, das durch die Kollisionen entsteht, nicht abgelenkt. Dileptonen werden daher als Sonden für die Untersuchung von Eigenschaften des Mediums und der, in Dileptonen zerfallenden, Teilchen im Medium verwendet. Besonderes Interesse besteht an der Untersuchung des Quark-Gluon-Plasma und der Wiederherstellung der chiralen Symmetrie.

Es gibt verschiedene Quellen für Dileptonenemission in einer Atomkernkollision.

In initiale Stößen zweier Nukleonen der kollidierenden Kerne anihilieren ein Quark des einen Nukleons mit einem Quark eines anderen Nukleons. Dieser Prozess resultiert in einem virtuellen Photon, das dann in ein Dilepton zerfällt (Drell-Yan-Prozess).

Während der Kollision entstehende Charm- oder Bottom-Quark-Antiquark-Paare, können in der Folge mit leichteren Quarks ein D- oder B-Mesonen-Paar (formula_1, formula_2) bilden. Die Mesonen zerfallen anschließend in einem semileptontischen Zerfall. Die Zerfälle der beiden Mesonpartner produzieren zusammen ein Dilepton.

Eine weitere Quelle ist die Annihilation von Quark-Antiquark-Paaren (formula_3) aus dem Quark-Gluon-Plasma, das während der Kollision entsteht und in dem Quarks nicht in Hadronen gebunden sind. Dieser Beitrag zur Messung ist häufig dominiert von anderen Beiträgen und deswegen experimentell schwer zugänglich. Die Messung dieses Beitrags ist eine der wenigen Messungen, die direkte Rückschlüsse auf die Temperatur des Quark-Gluon-Mediums ermöglicht.

Dilepton werden auch durch hadronische Prozesse produziert. Ein Beispiel stellt die Kollision eines positiven Pi-Mesons mit seinem negativen Antiteilchen dar, bei der ein Leptonenpaar entsteht. Zudem werden Dileptonen in Resonanzzerfällen produziert. Einige Resonanzen (z. B. die Vektormesonen wie ρ-Meson, ω-Meson, φ-Meson oder J/ψ-Meson) zerfallen direkt in ein Leptonpaar. Dieses trägt den gleichen Viererimpuls und erlaubt dadurch direkten Zugriff auf Eigenschaften, wie die Masse, der zerfallenden Resonanz. Die Messung von Resonanzeigenschaften ist von besonderem Interesse, weil diese sich theoretisch mit der Wiederherstellung der chiralen Symmetrie verknüpfen lassen. Diese Änderung der Symmetrie könnte, ähnlich wie der formula_3-Beitrag, das Entstehen des Quark-Gluon-Plasmas anzeigen.


</doc>
<doc id="665982" url="https://de.wikipedia.org/wiki?curid=665982" title="Thermophorese">
Thermophorese

Als Thermophorese, Thermodiffusion oder Ludwig-Soret-Effekt wird in den Naturwissenschaften die Bewegung von Teilchen aufgrund eines Temperaturgradienten innerhalb eines Fluids bezeichnet. Benannt ist der Effekt nach dem deutschen Physiologen Carl Ludwig (1856) und dem Schweizer Physiker und Chemiker Charles Soret (1879), die das Phänomen beschrieben. In den meisten Fällen erfolgt die Bewegung von heiß nach kalt, jedoch ist, abhängig von der Art der Teilchen und des Fluids, auch eine Bewegung zur heißeren Region möglich.

Thermodiffusion tritt in allen Stoffen auf, deutlich beobachten lässt sich dieser Effekt bei Aerosolen und Staubteilchen in Luft (siehe dazu Schwarzstaub). Sie lässt sich auch bei einfachen Gas- oder Flüssigkeitsmischungen, bei Polymeren in Lösung oder kolloidalen Suspensionen und auch bei magnetischen Fluiden gut beobachten. Thermodiffusion ist noch immer Gegenstand aktueller Forschung.

Erklärt wird der Effekt in Gasen folgendermaßen: Auf ein Staubteilchen prasseln von allen Seiten im Mittel gleichmäßig Luftmoleküle ein – statistische Fluktuationen führen zur Brown'schen Bewegung, jedoch ist die Bewegung statistisch und ungerichtet. Falls sich das Teilchen jedoch in einem Temperaturgradienten befindet, treffen auf der heißen Seite schnellere Moleküle auf als auf der kalten – das Teilchen erfährt also einen Nettoimpuls in Richtung der kalten Seite. Die Bewegung ist immer noch statistisch, jedoch bewegt sich das Teilchen über lange Zeiten in Richtung kalt.

In Flüssigkeiten ist das Ganze schwieriger, denn die Theorie für Gase kann die Wanderung mancher großen Moleküle auf die Wärmequelle zu nicht erklären. Es existieren bereits Erklärungsversuche durch Strömungen an der Oberfläche der Moleküle oder durch Änderung der Oberflächenenergie in verschiedenen Temperaturzuständen, jedoch ist die Sache Gegenstand aktueller Forschung (2005).
Theoretische Ansätze basieren auf den Arbeiten von Lars Onsager und Eli Ruckenstein und natürlich auch auf aktuellen experimentellen Forschungsergebnissen.

Thermodiffusion in Festkörpern ist wiederum weniger verstanden als die in Flüssigkeiten.

In einer binären Mischung (Fluid bestehend aus zwei Komponenten) kann die zeitliche Entwicklung des Molenbruchs (=Stoffmengenanteil) formula_1 einer Komponente mit einer erweiterten Diffusionsgleichung beschrieben werden (für den Molenbruch gilt formula_2 und der Molenbruch der zweiten Komponente ist formula_3). Der erste Term auf der rechten Seite beschreibt die Fick'sche Diffusion, der zweite die Thermodiffusion, die vom räumlichen Verlauf der Temperatur formula_4 abhängt:

Dabei ist formula_6 der Diffusionskoeffizient und formula_7 der Thermodiffusionskoeffizient. Der Quotient beider Koeffizienten

heißt Soret-Koeffizient. Dieser ist ein Maß für die Stofftrennung in Gegenwart eines Temperaturgradienten im stationären Zustand. Im Allgemeinen ist der Soret-Koeffizient von der Temperatur und vom Stoffmengenanteil abhängig.

Für eine Mischung aus zwei Gasen kann die kinetische Gastheorie die Koeffizienten formula_6 und formula_7 gut abschätzen. Dagegen existiert für Flüssigkeiten noch keine adäquate Theorie, selbst das Vorzeichen des Soret-Koeffizienten lässt sich hier nicht vorhersagen. Dies ist ein Problem der statistischen Thermodynamik die intermolekulare Wechselwirkung in einem mehrkomponentigen Nichtgleichgewichtssystem zu beschreiben.

Da der Thermodiffusionskoeffizient in den meisten Systemen um einen Faktor 10 bis 10 kleiner ist als der Diffusionskoeffizient für Gase, Elektrolyte und gelöste Nichtelektrolyte, hat Thermophorese für Lebewesen wahrscheinlich keine besondere Bedeutung. 

Anwendung findet die Thermophorese bei der Trennung von Isotopen in Gasen. So können Kr und Kr oder HCl und HCl in einer vertikalen Röhre, die mittels eines elektrischen Drahts längs ihrer Achse beheizt wird, getrennt werden. Unterstützt wird der Vorgang dabei durch Konvektion, da die zum Heizdraht strömenden Komponente aufsteigt, während die andere, die sich zur kälteren Wand bewegt gleichzeitig nach unten sinkt. Durch das Zusammenspiel dieser beiden Vorgänge ergibt sich eine viel effektivere Trennung der Komponenten, als allein aufgrund des thermischen Diffusionskoeffizienten zu erwarten wäre.

Verschiedene Staubprobensammler verwenden die Thermophorese. Ein Aerosolstrom streicht über einen Objektträger für ein Mikroskop, über dem ein geheizter Draht angebracht ist. Die Thermophorese scheidet die Staubteilchen aus dem Luftstrom quantitativ auf dem Objektträger ab. Ein derartiges Gerät heißt Thermalpräzipitator.

Ansammlung (Bioakkumulation) von DNA-Molekülen in Lösungen: Durch geschicktes Design einer geheizten Flüssigkeitskammer ist es möglich, durch Zusammenspiel von Konvektion und Thermophorese DNA auf einem Fleck bis zu 1000fach anzureichern.

Bei einem neueren Verfahren, der optisch erzeugten Thermophorese (), wird mit Hilfe eines Infrarot-Lasers in einer flüssigkeitsgefüllten Glaskapillare ein definierter mikroskopischer Temperaturgradient erzeugt. Die darin befindlichen Moleküle sind zunächst gleichmäßig verteilt, bewegen sich aber innerhalb von Sekunden typischerweise von höheren zu niedrigen Temperaturen.

Bei der Analyse liegen die Analyten frei in Lösung vor. Die Messungen können zudem in beliebigen Puffern und komplexen biologischen Flüssigkeiten durchgeführt werden und erlauben die Messung unter in vivo ähnlichen Bedingungen.

Diese Methode findet Anwendung bei der Affinitätsbestimmung zwischen allen Arten von Biomolekülen einschließlich Proteinen, DNA, und RNA und chemische Verbindungen sowie bei der Bestimmung von Enzymaktivitäten. Auch die Bestimmung der Stabilität, sowie des Adsorptions- und Aggregations-Verhaltens von Biomolekülen in Blutserum und die biochemische Untersuchung gereinigter Proteine ist möglich.

Zur Frage nach dem Ursprung des Lebens: Möglicherweise entstanden die ersten Biomoleküle in der Nähe hydrothermaler Quellen in der Tiefsee. Durch konvektive Durchmischung in Hohlräumen porösen Gesteins und Anreicherung durch Thermophorese lässt sich die erdgeschichtlich kurze Zeitspanne, die das Leben zum Entstehen brauchte, möglicherweise erklären.





</doc>
<doc id="728615" url="https://de.wikipedia.org/wiki?curid=728615" title="Ettingshausen-Nernst-Effekt">
Ettingshausen-Nernst-Effekt

Unter dem Namen Ettingshausen-Nernst-Effekt oder auch Nernst-Ettingshausen-Effekt sind zwei physikalische Effekte bekannt, die in der Elektrodynamik, Festkörperphysik und Thermodynamik betrachtet werden. Sie gehören zu den thermomagnetischen Erscheinungen. Der Erste Ettingshausen-Nernst-Effekt ist auch als Nernst-Effekt bekannt.

Als "1. Ettingshausen-Nernst-Effekt" oder "Nernst-Effekt". bezeichnet man das Phänomen, dass bei der Einwirkung eines Magnetfeldes auf einen Wärmestrom in einem elektrischen Leiter senkrecht zum Wärmestrom und senkrecht zum Magnetfeld eine elektrische Spannung entsteht. Dies stellt das thermische Analogon zum Hall-Effekt dar. 

Der "2. Ettingshausen-Nernst-Effekt" betrifft die Erscheinung, dass bei der Einwirkung eines Magnetfeldes auf einen Wärmestrom in einem elektrischen Leiter eine longitudinale Potentialdifferenz (Spannung) entsteht. Die longitudinalen Effekte sind aus Symmetriegründen nicht von der Richtung des Magnetfelds abhängig und damit quadratisch in der Magnetfeldstärke.

Die Effekte sind benannt nach den Physikern Albert von Ettingshausen und Walther Nernst, der 1886 als Student bei v. Ettingshausen in Graz auf diesem Gebiet forschte und die Thematik 1887 in seiner Doktorarbeit bei Friedrich Kohlrausch in Würzburg vertiefte. 

Die beiden Ettingshausen-Nernst-Effekte sind vom „Ettingshausen-Effekt“ sowie von einem weiteren „Nernst-Effekt“ zu unterscheiden, die galvanomagnetische Effekte sind.




</doc>
<doc id="864600" url="https://de.wikipedia.org/wiki?curid=864600" title="Vektorpotential">
Vektorpotential

Das Vektorpotential formula_1 ist, historisch gesehen, ein mathematisches Hilfsmittel, das in der klassischen Elektrodynamik dazu eingeführt wurde, den Umgang mit der magnetischen Induktion bzw. Flussdichte formula_2 (anschaulich gesprochen mit dem „Magnetfeld“) zu vereinfachen.

Mathematisch ist das Vektorpotential (im Unterschied zum Skalarpotential) ein Vektorfeld formula_1, dessen Rotation gemäß folgender Formel 

ein zweites Vektorfeld formula_2 liefert.

Vektorpotentiale lassen sich u. a. dazu verwenden, die zur Beschreibung des elektromagnetischen Felds verwendeten Maxwell-Gleichungen zu entkoppeln und dadurch leichter lösbar zu machen. So zeigt sich, dass das Vektorpotential über eine Faltung aus einer gegebenen ortsabhängigen Stromdichte formula_6 (also eine Anordnung von stromdurchflossenen Leitern im Raum, wie zum Beispiel eine Spule) hervorgeht, man also das Vektorpotential zu einer gegebenen Stromdichte berechnen kann, und daraus dann die messbare magnetische Induktion bzw. Flussdichte formula_2, die durch diese Anordnung erzeugt wird (Biot-Savart-Gesetz). Dieses Vektorpotential hat die Einheit formula_8.

Obwohl es zunächst nur als mathematisches Hilfsmittel eingeführt wurde, kommt ihm in der Quantenmechanik physikalische Realität zu, wie das Aharonov-Bohm-Experiment zeigte.

Das Vektorpotential formula_1 wird so definiert, dass

gilt. Hierbei ist formula_11 die Rotation des Vektorpotentials. Durch diesen Ansatz ist die Divergenz von formula_12 Null, da formula_13 für alle zweifach stetig differenzierbaren Vektorfelder. Dies wird durch die Maxwellgleichungen gefordert.

In der Elektrodynamik gilt die obige Formel unverändert, wohingegen für das elektrische Feld formula_14

gilt. Hierbei ist formula_16 das skalare Potential.

Diese beiden Ansätze, zusammen mit der Lorenz-Eichung, werden benutzt, um die Maxwellgleichungen zu entkoppeln. In der Magnetostatik wird für gewöhnlich die Coulomb-Eichung benutzt, die den statischen Grenzfall der Lorenz-Eichung darstellt.

Skalares Potential und Vektorpotential werden in der Relativitätstheorie und der Quantenelektrodynamik zum Viererpotential
zusammengefasst.







Bei der Berechnung von Feldern in ladungs- und leitungsstromfreien Gebieten, z. B. in Hohlleitern begegnet man dem elektrischen Vektorpotential formula_40.

Aufgrund der Quellenfreiheit der betrachteten Felder gilt 

Um einen funktionalen Zusammenhang zwischen formula_44 und formula_45 zu erhalten, subtrahiert man die Gleichungen formula_41 und formula_43 voneinander und erhält:

Das Wirbelfeld formula_40 nennt man "elektrisches" Vektorpotential. Es beschreibt nur zeitlich "veränderliche elektrische" Felder.

Gemäß dem helmholtzschen Theorem kann (fast) "jedes" Vektorfeld formula_50 als Superposition zweier Komponenten formula_51 und 
formula_52 aufgefasst werden, deren erste der Gradient eines Skalarpotentials formula_53 ist, die zweite dagegen die Rotation eines Vektorpotentials formula_54:

Ist formula_56 ein konservatives Kraftfeld, in dem die Kraft formula_57 dem Prinzip des kleinsten Zwanges folgend stets der Richtung des maximalen Anstiegs des Potentials formula_58 entgegengerichtet ist, gilt alternativ die Schreibweise 



</doc>
<doc id="928691" url="https://de.wikipedia.org/wiki?curid=928691" title="Magnesiumdiborid">
Magnesiumdiborid

Magnesiumdiborid ist eine intermetallische Verbindung, welche die aktuell höchste Sprungtemperatur (39 K) unter den metallischen Supraleitern aufweist. Dies ist fast eine Verdoppelung zu dem bis dahin bekannten Spitzenreiter (Niobgermanium, NbGe, bei 23 K). Diese Eigenschaft wurde erst 2001 von dem japanischen Wissenschaftler Jun Akimitsu entdeckt, obwohl Magnesiumdiborid schon seit über 50 Jahren bekannt und einfach herzustellen ist (jedoch nicht in reiner Form).

Bereits 1914 wurde über die Darstellung eines Magnesiumborid berichtet, das durch das Erhitzen von amorphem, fein verteiltem Bor mit Magnesium-Pulver bis zur Rotglut in einem Wasserstoff-Strom entstand. Die gleiche Verbindung wurde bei der Reaktion von Magnesium mit Bortrioxid neben Magnesiumoxid beobachtet.

Die Herstellung von reinem Magnesiumdiborid, wie es für Supraleiter benötigt wird, ist aufwendig. Da Magnesium bei 650 °C, Bor jedoch erst bei über 2000 °C schmilzt (dort ist Magnesium schon gasförmig), ist eine Herstellung von Magnesiumdiborid durch Einschmelzen nicht möglich. Stattdessen werden die beiden Ausgangsstoffe bei 900 °C zusammengebracht, also bei einer Temperatur, bei der Magnesium noch nicht siedet. Der dennoch auftretende Magnesiumdampf diffundiert in das Bor, wo sich leicht auslösbare Magnesiumdiborid-Kügelchen bilden. In einem ähnlichen Verfahren können dünne Drähte hergestellt werden.

Ebenfalls möglich ist die Abscheidung von Dünnschichten von Magnesiumdiborid durch Reaktion von Magnesiumdampf in einer Wasserstoffatmosphäre mit Diboran.

Magnesiumdiborid ist ein geruchloses dunkelgraues bis schwarzes Pulver. Untersuchungen zu Magnesiumdiborid wurden erstmals 1954 von Jones und March veröffentlicht. Dieses bislang metallreichste Magnesium-Borid (daneben sind bis zum jetzigen Zeitpunkt in der Literatur mit MgB, MgB, MgB und MgB vier weitere binäre Magnesiumboride bekannt) kristallisiert im Aluminiumdiborid-Typ in der und den Gitterkonstanten a = 3,0834(3) Å und c = 3,522(2) Å. Dabei bilden die Boratome in der ab-Ebene ein graphitartiges Netz aus planaren kantenverknüpften Sechsringen, wobei die Magnesium-Atome sich jeweils ober- und unterhalb der Ringzentren befinden und die Bor-Atome trigonal prismatisch von diesen umgeben werden.



</doc>
<doc id="1133650" url="https://de.wikipedia.org/wiki?curid=1133650" title="Albert von Ettingshausen">
Albert von Ettingshausen

Albert von Ettingshausen (* 30. März 1850 in Wien; † 9. Juni 1932 in Graz) war ein österreichischer Physiker. 

Alberts Großvater Andreas von Ettingshausen und sein Vater Constantin von Ettingshausen waren ebenfalls Naturwissenschaftler.

Nach Physikstudium und Promotion war er Assistent von Ludwig Boltzmann an der Karl-Franzens-Universität Graz. Während seiner außerordentlichen Professur entdeckte er 1886 am Physikalischen Institut zusammen mit Walther Nernst einige galvanometrische und thermomagnetische Effekte (Ettingshausen-Effekt, Ettingshausen-Nernst-Effekt). Zur Promotion über diese Thematik wechselte Nernst zu Friedrich Kohlrausch an die Universität Würzburg.

1888 erhielt er als Nachfolger von Jakob Pöschl einen Ruf auf die Lehrkanzel für Physik im Neubau des Physikalischen Instituts der Technischen Hochschule Graz. Bis Emeritierung 1920 erweiterte er die Lehrkanzel zur „Lehrkanzel für Physik und Elektrotechnik“. Sein Nachfolger wurde 1920 Fritz Kohlrausch.

Seit 1884 war er Mitglied der Leopoldina. Von 1888 bis 1890 war er Dekan der Technischen Hochschule.



</doc>
<doc id="1184745" url="https://de.wikipedia.org/wiki?curid=1184745" title="Tagesgang">
Tagesgang

Der Tagesgang stellt die Entwicklung eines Parameters im Verlauf eines Tages dar. 

Bei den Parametern kann es sich z. B. um Temperaturen, Niederschläge, Windstärken, Verkehrsstärken, aber auch Produktionsmengen etc. handeln. Es kann nach verschiedenen den Tag beschreibenden Zeiträumen ausgewertet werden, z. B. stundenweise oder beschränkt auf die Morgen- oder Abendstunden. Der Tagesgang wird in vielen Bereichen der Wirtschaft und Wissenschaft verwendet, um Entwicklungen über den Tag, etwa als Ganglinie, darzustellen. Einen Überblick über längere Zeiträume bietet der Jahresgang.


</doc>
<doc id="1358413" url="https://de.wikipedia.org/wiki?curid=1358413" title="Karlovitz-Zahl">
Karlovitz-Zahl

Die Karlovitz-Zahl wird zur Beschreibung von turbulenten Verbrennungsprozessen verwendet und setzt sich aus dem Verhältnis der Zeitskala für die Ausbreitung der laminaren Flamme formula_1 zur kleinsten turbulenten Zeitskala formula_2 (Kolmogorov-Zeit) zusammen:

Die laminare Flammen-Zeitskala formula_1 wird dabei üblicherweise als diejenige Zeit definiert, die die laminare Flammenfront benötigt, um sich durch Flammenpropagation mit der laminaren Flammengeschwindigkeit formula_7 um eine Strecke fortzubewegen, die gleich groß wie die laminare Flammenfrontdicke (inklusive ihrer Vorwärmschicht) formula_8 ist. Sie lässt sich auch über das Verhältnis der Diffusionskonstante formula_9 zum Quadrat der laminaren Flammengeschwindigkeit beschreiben:

Falls formula_11, läuft die Wärme- und Stoffdiffusion innerhalb der Flammenfront viel schneller als alle Turbulenzzeitskalen ab. Somit werden die lokale Flammenstruktur und der Bereich der chemischen Reaktion nicht von Turbulenzen verändert, bzw. beeinflusst und es herrschen innerhalb der Flamme laminare Bedingungen. Die Flamme lässt sich in diesem Fall meist gut mit einem Flamelet-Ansatz beschreiben, bei dem angenommen wird, dass sich die Flammenfront in lokaler Näherung vollständig laminar verhält.

Falls formula_12, sind die kleinsten turbulenten Wirbel gleich groß oder kleiner als die Dicke der Vorwärmschicht in der Flammenfront. Dadurch kann es zu einem turbulenten Wärme- und Stofftransport innerhalb der Flammenfront kommen. Dieser führt sowohl zu einer Verbreiterung der Flammenfront als auch zu einer Erhöhung der turbulenten Flammengeschwindigkeit.

Die Namensgebung der dimensionslosen Kennzahl bezieht sich auf den ungarischen Physiker Béla Karlovitz.

Nach obiger Definition der Karlovitz-Zahl lässt sich das Verhältnis auch über Längenskalen oder Geschwindigkeiten ausdrücken:

Hierbei stehen formula_14 für die Kolmogorov-Länge (also der kleinsten Längenskala, die von der Turbulenz beeinflusst wird) und formula_15 für die Kolmogorov-Geschwindigkeit (also der Umlaufgeschwindigkeit von Wirbeln mit dem Durchmesser der Kolmogorov-Länge).

Unter der Annahme, dass für die Schmidt-Zahl formula_16 gilt, dass also die kinematische Viskosität etwa gleich groß wie die stoffliche Diffusionskonstante ist, kann man die Karlovitz-Zahl, die Damköhler-Zahl formula_17 und die Reynolds-Zahl formula_18 näherungsweise in folgende Beziehung zueinander setzen:

Ersetzt man in obiger Gleichung die laminare Flammen-Zeitskala durch die Reaktions-Zeitskala formula_20, lässt sich eine Karlovitz-Zahl für den Einfluss der Turbulenz auf die Reaktionsschicht definieren:

Analog zur Definition der laminaren Flammen-Zeitskala beschreibt die Reaktions-Zeitskala diejenige Zeit, die die Flammenfront benötigt, um durch Flammenpropagation eine Strecke zurückzulegen, die gleich groß wie die Dicke der Reaktionsschicht formula_22 ist. Die Reaktionsschicht ist derjenige Abschnitt innerhalb der Flammenfront, in dem die chemischen Reaktionen ablaufen. In einer laminaren Flammenfront ist die Reaktionsschicht erheblich dünner als die durch Stoff- und Wärmediffusion geprägte Vorwärmschicht. Das Größenverhältnis wird oft mit einem Faktor formula_23 beschrieben. Es gilt also

Typischerweise ist formula_25 eine Zahl der Größenordnung formula_26.

Analog zur obigen Beschreibung lässt sich hier feststellen:

Falls formula_27, laufen die chemischen Reaktionen viel schneller als alle Turbulenzzeitskalen ab. Somit wird die interne Struktur der Reaktionsschicht nicht von Turbulenzen verändert, und es herrschen innerhalb der Reaktionsschicht laminare Bedingungen.

Falls formula_28, sind die kleinsten turbulenten Wirbel gleich groß oder kleiner als die Dicke der Reaktionsschicht in der Flammenfront. Dadurch kann es rein theoretisch zu einer turbulenten Verbreiterung der Reaktionsschicht kommen. In extremen Fällen würde dies zu einer homogenen Verteilung der chemischen Reaktionen über ein makroskopisches Volumen führen (perfekter Rührreaktor). Viel wahrscheinlicher ist es allerdings, dass bei einer derart intensiven turbulenten Störung der Reaktionsschicht lokale Verlöschungen auftreten und die Flammenfront aufbricht oder sogar gänzlich erlischt.


</doc>
<doc id="1431009" url="https://de.wikipedia.org/wiki?curid=1431009" title="Pierre-Auger-Observatorium">
Pierre-Auger-Observatorium

Das Pierre-Auger-Observatorium ist ein internationales physikalisches Großexperiment zur Untersuchung der kosmischen Strahlung bei höchsten Energien.

Das Observatorium wurde 1992 durch den Physiknobelpreisträger Jim Cronin und Alan Andrew Watson entworfen und nach dem französischen Physiker Pierre Auger, welcher 1938 die ausgedehnten Luftschauer entdeckte, benannt.

Das zu beobachtende Strahlungsfenster liegt im Energiebereich von 10 eV bis 10 eV (Elektronenvolt). Die Strahlung besteht hauptsächlich aus Protonen, selten auch schwereren Atomkernen, die beim Auftreffen auf die Erdatmosphäre eine Vielzahl (mehr als 10) anderer Teilchen erzeugen. Diese Kaskade von Teilchen wird als Luftschauer bezeichnet. Da bei Energien über ca. 10 eV kosmische Strahlung nicht mehr direkt mit Satelliten- oder Ballonexperimenten beobachtbar ist, beobachtet das Pierre-Auger-Observatorium diese Schauer und somit die kosmische Strahlung nur indirekt.

Das Pierre-Auger-Observatorium wurde in der Pampa Amarilla in der Nähe der argentinischen Kleinstadt Malargüe gebaut und im November 2008 in Anwesenheit von Jim Cronin offiziell eingeweiht. Die Versuchsanlage besteht hauptsächlich aus zwei unabhängigen Detektorsystemen, dem Oberflächendetektor (SD, nach engl. Surface Detector) und dem Fluoreszenzdetektor (FD). Später wurden in einem Teil des Detektorfelds zusätzlich Radioantennen (RD) und Myon-Detektoren (MD) aufgebaut, um für niedrigere Energien die Messgenauigkeit zu erhöhen. Derzeit findet unter dem Namen AugerPrime ein Upgrade des Observatoriums statt, das aus mehreren Verbesserungen besteht, vor allem einer Erhöhung der Messgenauigkeit der Oberflächendetektoren.

Der Oberflächendetektor besteht aus 1660 Stationen, die in einem Dreiecksmuster mit je 1500 Meter Abstand auf einer Fläche von etwa 3000 km² auf einer Hochebene ca. 1400 m über Meereshöhe aufgestellt sind. Jede einzelne Station besteht aus einem mit 12 m³ hochreinem Wasser gefüllten Tank, in welchem einfallende Teilchen Tscherenkow-Strahlung erzeugen. Diese wird von drei Photomultipliern im Tankdeckel registriert. Ein Luftschauer erzeugt ein Signal in mehreren Tanks. Aus Stärke und Zeitpunkt der Einzelsignale kann dann auf Energie und Richtung des Primärteilchens geschlossen werden.

Der Fluoreszenzdetektor besteht aus 27 Teleskopen, die von vier Standorten aus das Feld des Oberflächendetektors überblicken. Mit dem Fluoreszenzdetektor wird durch den Schauer in der Atmosphäre erzeugtes Fluoreszenzlicht registriert. So kann die Entwicklung des Schauers ergründet werden und unabhängig vom Oberflächendetektor auf Eigenschaften des Primärteilchens geschlossen werden.

Das erzeugte Fluoreszenzlicht ist sehr schwach, weshalb der Fluoreszenzdetektor nur während mondloser Nächte betrieben werden kann, welche ca. 13 % der Betriebszeit ausmachen. Diese geringe Betriebsdauer wird jedoch durch eine gegenüber dem Oberflächendetektor deutlich höhere Genauigkeit ausgeglichen.

Der Radiodetektor, das Auger Engineering Radio Array (AERA), besteht aus über 150 Antennenstationen auf einer Fläche von 17 km². Für die Stationen kommen hauptsächlich zwei Antennentypen zum Einsatz: Log Periodic Dipole Antenna (LPDA) und Active Bow tie Antenna (Butterfly). Jede Station besitzt zwei Antennen um das elektronische Feld anteilig in der Ost-West und Nord-Süd-Polarisation zu messen. Beide Antennentypen messen zwischen 30 und 80 MHz. Während zunächst die technische Machbarkeit der Radiotechnik im Vordergrund stand, liegt der Fokus inzwischen auf einer Erhöhung der Messgenauigkeit für Luftschauer durch gemeinsame Auswertung mit den anderen Detektoren.

Der Myondetektor besteht aus vergrabenen Szinitllations-Teilchendetektoren. Bisher wurden bei sieben SD-Detektoren zusätzliche Myondetektoren installiert, die die Genauigkeit für die Zusammensetzung der kosmischen Strahlung erhöhen sollen. In den nächsten Jahren, sollen über 20 km² des Oberflächendetektors mit Myondetektoren ausgerüstet werden, und zwar genau dort, wo sich auch die Radioantennen befinden. Denn an dieser Stelle ist der Oberflächendetektor auf 750 m Abstand verdichtet, was eine geringere Energieschwelle von unter 1 EeV (Exaelektronenvolt) ermöglicht.

Die Pierre-Auger-Kollaboration hat beschlossen, 1 % der Daten öffentlich verfügbar zu machen. Auf einer Webseite, die täglich aktualisiert wird, können die seit 2004 gesammelten Ereignisse angezeigt werden.

Die ersten Beobachtungen der hochenergetischen kosmischen Strahlung oberhalb formula_1 zeigen ein gehäuftes Auftreten aus der Richtung der Zentren von aktiven galaktischen Kernen. Es liegt daher nahe, dass sie mit der Energie der Schwarzen Löcher im Zentrum der Galaxien ins All geschleudert werden. Es ergeben sich auch neue Fragen, so wird ein erhöhtes Vorkommen von Myonen gemessen, das nicht in die bisherigen Luftschauermodelle passt.




</doc>
<doc id="1532087" url="https://de.wikipedia.org/wiki?curid=1532087" title="Akustooptischer Modulator">
Akustooptischer Modulator

Ein akustooptischer Modulator, abgekürzt AOM, ist ein optisches Bauelement, das einfallendes Licht in Frequenz und Ausbreitungsrichtung oder Intensität beeinflusst. Hierzu wird in einem transparenten Festkörper mit Schallwellen ein optisches Gitter erzeugt. An diesem Gitter wird der Lichtstrahl gebeugt und gleichzeitig in seiner Frequenz verschoben. Akustooptische Modulatoren, die zur Ablenkung des Lichts eingesetzt werden, werden auch "Braggzellen" genannt.

Ein akustooptischer Modulator besteht traditionell aus einem durchsichtigen Quader (z. B. Quarzglas oder ein Kristall), in dem mittels eines Piezoschwingers Körperschall (Ultraschall) erzeugt wird. Gegenüber dem Piezoerreger befindet sich ein Schallabsorber, um Reflexionen und stehende Wellen zu vermeiden.

Weiterhin gibt es auch faseroptische akustooptische Modulatoren. Sie bieten geringere Einfügedämpfung, bessere Strahlqualität und eine leichtere Integration in faseroptische Systeme. Solche "All-Fiber AOM's" nutzen entlang der Faser laufende transversale oder longitudinale akustische Wellen. Letztere können z. B. ihrerseits im Faserkern befindliche Faser-Bragg-Gitter modulieren.

Die Ablenkung des Lichts in einem traditionellen akustooptischen Modulator funktioniert nach dem Prinzip der Beugung von Licht an einem optischen Gitter. Das optische Gitter besteht aus den Dichteschwankungen der den Kristall durchlaufenden Schallwelle. 

Die Schallwelle mit Frequenzen formula_1 von typischerweise 10 bis 2000 MHz bewirkt im Kristall eine periodische Änderung der Dichte und damit eine periodische Modulation des Brechungsindex. Der Abstand formula_2 dieser „Gitterlinien“ ist gleich der Wellenlänge formula_3 der Ultraschallwelle und lässt sich aus der Schallgeschwindigkeit formula_4 und der Schallfrequenz formula_1 berechnen zu

Für den Kristall wird meist LiNbO oder PbMoO für sichtbares Licht und nahes Infrarot sowie Ge für mittleres Infrarot verwendet. Typische Schallgeschwindigkeiten in solchen Kristallen liegen zwischen 3700 und 4300 m/s. Eine Frequenz von 195 MHz ergibt eine Gitterkonstante von 19 bis 22 µm. Dies sind typische Werte. Der genaue Wert hängt von der verwendeten Ultraschallfrequenz und der Schallgeschwindigkeit des verwendeten Mediums ab.

Typischerweise ist der Querschnitt des einfallenden Lichtbündels deutlich größer als die räumliche Periode der Brechungsindexmodulation, und da die Lichtgeschwindigkeit sehr viel größer als die Schallgeschwindigkeit ist, kann man näherungsweise annehmen, dass das Licht eine statische Brechungsindexmodulation sieht und eine konstruktive Interferenz des Lichtes für die Braggwinkel formula_7 mit 

erfährt, wobei formula_9 die Wellenlänge des Lichtes im Kristall und formula_2 die Periode der Brechungsindexmodulation sind.

Das gestreute Licht erfährt eine Doppler-Frequenzverschiebung mit der Frequenz formula_11 des Ultraschalls. Der Vorgang ähnelt der Reflexion an einem bewegten Spiegel.

Eine andere, dazu äquivalente Betrachtungsweise betrachtet die Schallwelle im Festkörper als Phononen, die mit den Photonen des Lichts wechselwirken. Die Ablenkung des Lichts kommt dadurch zustande, dass der Impuls der Phononen zum Impuls der Photonen vektoriell addiert wird:
Hierbei ist formula_13 das durch formula_14 dividierte Plancksche Wirkungsquantum und formula_15 der Wellenvektor der Photonen bzw. Phononen.
In dieser Betrachtungsweise folgt aus der Energieerhaltung, dass sich durch die Wechselwirkung die Frequenz des Lichts um die Frequenz der Schallwelle ändert:
Hier ist formula_17 das Plancksche Wirkungsquantum und formula_18 die Frequenz des Lichts, also der Photonen. Der Ausdruck formula_19 bezeichnet die Frequenz der Schallwelle. Die Frequenz des Lichts wird also genau um die Frequenz der Schallwelle verschoben.

Die relative Frequenzverschiebung des Lichtes ist sehr klein, da die Ultraschall-Frequenz (≈ 10 … 10 Hz) wesentlich kleiner als die Frequenz des Lichts (≈ 10 … 10 Hz) ist. Sie ist jedoch für einige Anwendungen wesentlich.

AOM werden zur Manipulation von Laserstrahlung verwendet. Die Anwendungen lassen sich folgendermaßen gliedern:








</doc>
<doc id="1794801" url="https://de.wikipedia.org/wiki?curid=1794801" title="Viktor von Lang">
Viktor von Lang

Viktor von Lang (* 2. März 1838 in Wiener Neustadt; † 3. Juli 1921 in Wien) war ein österreichischer Physiker und Mitbegründer und Pionier der Erforschung der Kristallographie.

Während des achtsemestrigen Studiums an der Philosophischen Fakultät der Universität Wien arbeitete Lang drei Jahre hindurch am dortigen Physikalischen Institut. Er promovierte 1859 über „Physikalische Verhältnisse kristallisierter Körper“ an der Universität Gießen und verfügte über frühe wissenschaftliche Verbindungen zu Paris und London.

Lang arbeitete nach seiner Promotion ein Jahr bei Gustav Robert Kirchhoff und Robert Wilhelm Bunsen in Heidelberg und übersiedelte dann nach Paris zu dem Experimentalphysiker Henri Victor Regnault.

1861 kehrte er nach Wien zurück und habilitierte sich über „Die Physik der Kristalle“. Er wurde zunächst an das Kensington Museum in London berufen; nach zweijähriger Tätigkeit erhielt er 1864 einen Ruf als außerordentlicher Professor für Physik in Graz, 1866 folgte schließlich die Berufung auf den Lehrstuhl für Physik in Wien als Nachfolger von Kunzek.

Lang arbeitete wissenschaftlich auf einem breiten Gebiet der Physik. Sein lebenslanges Hauptthema waren die physikalischen Eigenschaften der Kristalle. 

Er starb 1921 im 84. Lebensjahr in Wien. 1929 wurde ihm im Arkadenhof der Universität ein Denkmal errichtet. Er war Ehrendoktor der Universität Oxford, Mitglied der Deutschen Akademie der Naturforscher Leopoldina und Mitglied der Französischen Ehrenlegion. 1897 war er Vorsitzender der Gesellschaft Deutscher Naturforscher und Ärzte.

Er wird zu den Pionieren und Begründern der Kristallphysik gezählt. Zu seinen Schülern zählten Franz-Serafin Exner, Johann Puluj, Ernst Lecher, Anton Lampa, Felix Ehrenhaft. Die beiden letzteren schlossen sich später dem Kreis von Franz-Serafin Exner an.




</doc>
<doc id="1990797" url="https://de.wikipedia.org/wiki?curid=1990797" title="Otto Haxel">
Otto Haxel

Otto Haxel (* 2. April 1909 in Neu-Ulm; † 26. Februar 1998 in Heidelberg) war ein deutscher Physiker, der sich insbesondere mit Kernphysik beschäftigte.

Haxel studierte Ingenieurwissenschaften und Technische Physik an der Technischen Universität München, wo er sich dem Corps Cisaria anschloss, und an der Universität Tübingen. 1933 wurde er in Tübingen bei Hans Geiger über "Protonenemission von Aluminium angeregt durch α-Strahlen von Radium C und Thor C" promoviert. 1936 wurde er ebenfalls bei Geiger Oberassistent an der Technischen Hochschule in Berlin-Charlottenburg, wo er ebenfalls 1936 mit einer Arbeit über "Die Kernspektren der leichten Elemente" habilitiert wurde und bis 1945 war. Während der Arbeit an radioaktiven Nukliden die bei Bestrahlung von Uran entstehen kam er dort auch mit Otto Hahn in Kontakt. Während des Zweiten Weltkriegs arbeitete er am Uranprojekt mit.

Haxel war mit Hans E. Suess und J. Hans D. Jensen (der dafür mit Maria Goeppert-Mayer den Nobelpreis erhielt) 1949 an der Formulierung des Schalenmodells der Atomkerne beteiligt.

Nach dem Zweiten Weltkrieg wurde er Mitarbeiter des Max-Planck-Instituts für Physik in Göttingen (Leitung Werner Heisenberg) und wurde 1947 außerordentlicher Professor an der Universität Göttingen. An der Universität Heidelberg baute er ab 1950 das II. Physikalische Institut auf. Dort befasste er sich mit dem Zusammenhang von kosmischer Strahlung und Radioaktivität, aber auch in anderer Weise mit atmosphärischer Radioaktivität und baute ein Labor für die Altersbestimmung mit der C14-Methode auf. Das wurde später wichtig bei der Gründung des Instituts für Umweltphysik in Heidelberg.

Haxel war langjähriger Mitherausgeber der "Zeitschrift für Physik", Mitglied der "Deutschen Atomkommission" seit ihrer Gründung 1956 und wirkte 1956 maßgeblich an der Gründung des Kernforschungszentrums Karlsruhe mit. Von 1970 bis 1975 war er wissenschaftlich-technischer Direktor des Kernforschungszentrums Karlsruhe, an dessen Gründung er beteiligt war und in dessen Aufsichtsrat er von seiner Gründung bis 1970 war. Nach 1975 war er wieder an der Universität Heidelberg und befasste sich unter anderem mit anthropogenen Einflüssen auf das Klima.

1957 gehörte er zu den Göttinger Achtzehn, einer Gruppe führender Wissenschaftler, die sich in einer gemeinsamen Erklärung gegen die geplante atomare Bewaffnung der Bundeswehr wandten.

Er war Mitglied der Leopoldina und seit 1951 der Heidelberger Akademie der Wissenschaften, deren Präsident er von 1978 bis 1982 war. 1980 wurde Haxel mit dem Otto-Hahn-Preis der Stadt Frankfurt am Main ausgezeichnet. 1971 erhielt er das Große Verdienstkreuz des Bundesverdienstkreuzes. 1973 wurde er Ehrendoktor in Karlsruhe.





</doc>
<doc id="2628200" url="https://de.wikipedia.org/wiki?curid=2628200" title="Diffusionsspannung">
Diffusionsspannung

Die Diffusionsspannung formula_1, selten auch Antidiffusionsspannung genannt, ist die Potentialdifferenz (elektrische Spannung) über eine Raumladungszone, die der Diffusion von Ladungsträgern (Elektronen und Defektelektronen) "entgegen"wirkt. Sie ist materialabhängig und beträgt für Silizium ≈ 0,7 V und für Germanium ≈ 0,3 V.

Betrachtet wird eine Halbleiterdiode mit einem p-n-Übergang: An der Grenze zwischen p- und n-dotiertem Halbleiter kommt es aufgrund des Konzentrationsgradienten zur Diffusion von Ladungsträgern, d. h. freie Elektronen aus dem n-Gebiet wandern in das p-Gebiet (Diffusionsstrom), analog dazu wandern die Löcher (Defektelektronen) vom p- in das n-Gebiet.

Durch diese Ladungsträgerbewegung bildet sich zwischen den Raumladungen im Inneren des Kristalls ein elektrisches Gegenfeld. Dieses wirkt der weiteren Diffusion beweglicher Ladungsträger entgegen, da es einen entgegengesetzten "Driftstrom" erzeugt.

Die durch das elektrische "Gegen"feld erzeugte Spannung wird als Diffusionsspannung bezeichnet (daher auch der Name "Anti"diffusionsspannung):

mit


</doc>
<doc id="2878778" url="https://de.wikipedia.org/wiki?curid=2878778" title="Eichtransformation">
Eichtransformation

Eine Eichtransformation verändert die Eichfelder einer physikalischen Theorie (z. B. die elektromagnetischen Potentiale oder die potentielle Energie) dergestalt, dass die physikalisch wirksamen Felder (z. B. das elektromagnetische Feld oder ein Kraftfeld) und damit alle beobachtbaren Abläufe dabei die gleichen bleiben. Dies wird als Eichfreiheit bezeichnet.

Man unterscheidet:

Eine physikalische Wirkung, die invariant unter lokalen Eichtransformationen ist, wird als "eichinvariante" Wirkung bezeichnet. Eine Theorie, die nach dem Prinzip der kleinsten Wirkung aus einer eichinvarianten Wirkung die physikalischen Bewegungsgleichungen gewinnt, wird als Eichtheorie bezeichnet. Alle fundamentalen Wechselwirkungen – Gravitation, Elektromagnetismus, schwache Wechselwirkung (Beta-Zerfall des Neutrons) und die
starke Wechselwirkung (Kernkräfte) – werden durch solche Eichtheorien beschrieben.

Nach dem Noether-Theorem weist die einer Eichtransformation zugrundeliegende Symmetrie auf die Existenz einer Erhaltungsgröße hin.

Die Elektrodynamik ist invariant unter der Eichtransformation
welche das elektrische Potential formula_3 und das magnetische Potential formula_4 
um die partiellen Ableitungen einer beliebig wählbaren Funktion formula_5 ändert.

Diese Transformation ändert weder das Magnetfeld

noch das elektrische Feld

Zur Definition von formula_8 und formula_9 siehe Gradient und Rotation.

Eichtransformationen können genutzt werden, um Berechnungen zu vereinfachen.

Die Beispiele verwenden das Maßsystem mit formula_10.

Durch die nach Ludvig Lorenz benannte Eichtransformation mit einer Eichfunktion formula_11, die

erfüllt, werden die inhomogenen Maxwellgleichungen zu zwei unabhängigen Wellengleichungen von formula_13 und formula_4.

Erfüllt die Eichfunktion formula_15 hingegen

so hilft die Transformation, das Skalarfeld formula_3 gerade zum Coulomb-Potential der Ladungen zu transformieren; formula_3 erfüllt dann die elektrostatische Poissongleichung.

Ebenso ist die Allgemeine Relativitätstheorie eine Eichtheorie, deren Eichtransformation neue Koordinaten als frei wählbare Funktionen der bisherigen Koordinaten festlegt:
Die Wirkung der Allgemeinen Relativitätstheorie ändert sich unter dieser Eichtransformation nicht.


</doc>
<doc id="3001019" url="https://de.wikipedia.org/wiki?curid=3001019" title="Zugkraftdiagramm">
Zugkraftdiagramm

Das nach Alfred Jante benannte Zugkraftdiagramm (nach Jante) bzw. Jante-Normalfahrdiagramm oder Normal-Fahrzustands-Diagramm stellt den Verlauf von Fahrwiderständen und ihrer Summe und den Verlauf der Antriebskraft in Abhängigkeit von der Fahrgeschwindigkeit dar. Es hat sich in der Fachliteratur durchgesetzt und wird auch oft in Testberichten verwendet.

Damit ein Kraftfahrzeug auf einer Straße fahren kann, muss es verschiedene Widerstände überwinden. Die von den Antriebsrädern auf die Fahrbahnoberfläche übertragene Antriebskraft muss in einem bestimmten Verhältnis zur Summe der Fahrwiderstände stehen. Ist sie größer, wird beschleunigt, ist sie kleiner, so verliert das Kraftfahrzeug an Geschwindigkeit.

Oft wird der Verlauf der Antriebskraft nur für Volllast in den einzelnen Vorwärtsgängen dargestellt. Ein Kraftüberschuss im Vergleich zur Fahrwiderstands-Summe für die Ebene zeigt das Vermögen des Kraftfahrzeugs an, Steigungen ohne Geschwindigkeitseinbuße zu überwinden.

Aussagekräftig ist dieses Diagramm auch für die Auswahl von Übersetzungen („Schongang“ oder „sportlicher Gang“).

Als Formel bedeutet das:

formula_1

wobei:

formula_2 Antriebskraft

formula_3 Rollreibungskraft

formula_4 Steigungswiderstandskraft

formula_5 Luftwiderstandskraft

formula_6 Beschleunigungswiderstandskraft




</doc>
<doc id="3421333" url="https://de.wikipedia.org/wiki?curid=3421333" title="Karl Glazebrook">
Karl Glazebrook

Karl Glazebrook (* 11. Mai 1965 in Chatham, Kent) ist ein anglo-australischer Astronom und Kosmologe, der sich mit der Entwicklung von Galaxien und der Natur der Dunklen Energie beschäftigt. In der Öffentlichkeit bekannt wurde Glazebrook mit einer Arbeit zur „Farbe“ des Universums.

Karl Glazebrook wurde 1965 in Chatham geboren. Er studierte Physik und Theoretische Physik an der University of Cambridge und promovierte 1992 an der Universität Edinburgh mit einer Studie zur Infrarotastronomie. Nach einem Postdoc-Aufenthalt an der University of Durham kehrte er 1993 nach Cambridge zurück, wo er in einer Forschungsgruppe an der Beobachtung von bis zu 8 Milliarden Jahre alten Galaxien arbeitete. Bei diesem Projekt gelang es Glazebrook, unter Zuhilfenahme des Hubble-Weltraumteleskops das Problem der "faint blue galaxies" zu lösen und so entscheidende Beiträge zur Erforschung des frühen Universums zu liefern.

1995 ging er als Astronom zum Anglo-Australian Observatory (AAO) nach Sydney. Dort war er unter anderem für die optischen Instrumente zur Erstellung der "2dF-Galaxien-Rotverschiebungskarte" verantwortlich. Glazebrook entwickelte zusammen mit Joss Hawthorn die "nod-and-shuffle"-Methode, mit der das Auflösungsvermögen der erdgebundenen Teleskope deutlich erhöht werden konnte. Während seiner Zeit am AAO war Glazebrook maßgeblich an der Entwicklung der Perl Data Language (PDL), einer Open-Source-Alternative zur Programmiersprache IDL beteiligt.

Im Jahr 2000 wechselte er zur Johns Hopkins University nach Baltimore, wo er 2004 zum Professor für Astronomie ernannt wurde. Glazebrook wurde einer der verantwortlichen Wissenschaftler für das Gemini-Observatorium, eine Aufgabe, die er fortführte, als er 2006 nach Australien zum " Centre for Astrophysics & Supercomputing" der Swinburne University of Technology in Melbourne wechselte.

Karl Glazebrook ist einer der Leiter der "Gemini Deep Deep Survey" und des "GLARE"-Projekts, mit dem Galaxien aus der Frühzeit des Universums untersucht werden. Durch diese Projekte wurde nachgewiesen, dass massenreiche Galaxien schon viel früher als bisher angenommen gebildet wurden. Außerdem gelang es Glazebrook zusammen mit Chris Blake, eine verbesserte Methode zur Messung der Oszillation von Baryonen ("Baryon Acoustic Oscillation", BAO) zu entwickeln, mit der indirekt Dunkle Energie untersucht werden kann.

Mit (März 2008) über 230 Veröffentlichungen und über 14.000 Zitierungen zählt Glazebrook zu den meistzitierten Astronomen der Gegenwart.

Nach Karl Glazebrook wurde im Jahr 2000 der Asteroid (10099) Glazebrook benannt.

Die bekannteste Veröffentlichung Glazebrooks entstand als „Nebenprodukt“ der 2dF-Rotverschiebungskarte, in der rund 200.000 Galaxien aufgenommen wurden. Im Rahmen der Messungen wurde das Alter der Galaxien über ihre Farbe bestimmt. Während junge Sterne eine bläuliche Ausprägung in ihrem Emissionsspektrum haben, nehmen die Rotanteile während der Entwicklung der Sterne zu Roten Riesen immer mehr zu.

Karl Glazebrook und Ivan Baldry stellten sich daraufhin die Frage, welche „durchschnittliche“ Farbe die beobachteten Sterne haben und berechneten dafür aus allen gemessenen Emissionsspektren einen Mittelwert. Glazebrook betrachtete die so ermittelte Farbe als eine „skurrile intellektuelle Übung“, da man sie nur von einem hypothetischen Punkt aus beobachten könnte, an dem alle Sterne gleich weit entfernt wären und sich die Sterne nicht bewegen würden (zur Unterdrückung der Rotverschiebung).

In einer ersten Veröffentlichung im Januar 2002 bezeichneten Glazebrook und Baldry die Farbe des Weltall als blass türkis. Zwei Monate später mussten sie allerdings ihre Aussage korrigieren, da durch einen falschen Weißabgleich der Rotanteil der Farbe zu gering berechnet wurde. Die tatsächliche Farbe des Universums wurde nun als ein heller Beigeton angegeben, der im RGB-Farbraum mit den Werten RGB = {255, 248, 231} dezimal, RGB = FFF8E7 hexadezimal angegeben wurde.

Da die ermittelten Werte keinem benannten Farbton entsprachen, rief Karl Glazebrook zu Namensvorschlägen auf. Nach einer Umfrage unter den Astronomen der Johns Hopkins University setzte sich die Bezeichnung "cosmic latte" (Kosmisch-Latte) durch, da der Beigeton dem Milchschaum eines Latte macchiato ähnelt. Der beige Farbton wurde später durch Messungen der Europäischen Südsternwarte bestätigt.




</doc>
<doc id="4450979" url="https://de.wikipedia.org/wiki?curid=4450979" title="Claudio Bunster">
Claudio Bunster

Claudio Bunster, früher Claudio Teitelboim (* 15. April 1947 in Santiago de Chile), ist ein chilenischer theoretischer Physiker.

Bunster studierte ab 1965 in Santiago Physik (Abschluss 1969). Dabei beschäftigte er sich mit dem Selbstenergieproblem der Elektrodynamik und stieß auf die Arbeiten von John Archibald Wheeler und Richard Feynman von 1945, die unter Umgehung des Feldkonzepts eine Fernwirkungstheorie mit avancierten und retardierten Potentialen entwickelt hatten. Er kam in Kontakt mit Wheeler, wurde an der Princeton University zugelassen und promovierte dort 1973 bei Wheeler mit einer Arbeit über dessen Geometrodynamik. Teitelboim wurde Assistant Professor in Princeton, wo er viel mit Tullio Regge zusammenarbeitete. Ab 1977 (als Wheeler nach Austin ging) war er am Institute for Advanced Study (IAS) in Princeton. 1980 folgte er Wheeler an die University of Texas at Austin.

In Einverständnis mit seiner Universität war er halbjährig an dem 1984 von ihm neu gegründeten Forschungsinstitut Centro de Estudios Cientificos de Santiago (CECS) in Santiago. Als es deswegen Probleme in Austin gab, wechselte er wieder ans IAS und ging schließlich ganz nach Chile. In seinem Institut, mit dem er 2000 nach Valdivia in Südchile ging, arbeiten heute (2008) 80 Angestellte (gruppiert um 15 Spitzenforscher) auf den Gebieten Klimaforschung und Gletscherkunde, theoretische Physik und Biochemie. Während der Regierung von Eduardo Frei leitete er 1994 bis 2000 das Wissenschafts-Beratungsgremium des Präsidenten, wobei er auch erheblichen Einfluss auf die Wissenschaftsorganisation im Land nahm (er schuf z. B. hochdotierte Forschungsstipendien und Fördermittelstellen, die von einem Gutachtergremium ausländischer Wissenschaftler vergeben werden) und die Millennium-Science-Initiative der Weltbank mit ins Leben rief, die Wissenschaftszentren in Entwicklungsländern unterstützt. Bunster war auch im Diskussionsforum für Menschenrechte aktiv, das Brücken zu den Militärs schlagen sollte, um die Demokratie zu sichern (Bunster profitierte dann auch in seiner Gletscherforschung von logistischer Unterstützung des chilenischen Militärs).

Neben seinen ersten Arbeiten zum Selbstwechselwirkungsproblem der klassischen Elektrodynamik befasste sich Teitelboim mit der Quantisierung von Systemen mit Nebenbedingungen (wie Eichtheorien und Gravitation), Allgemeiner Relativitätstheorie (z. B. Hamiltonsche Struktur), Supergravitation und der Theorie Schwarzer Löcher.
Bunster gilt als einflussreichster Wissenschaftler Chiles im Bereich der exakten Naturwissenschaften.


Bunster wuchs unter dem Namen Claudio Teitelboim als Sohn des kommunistischen Politikers und Schriftstellers Volodia Teitelboim (1916–2008) und dessen Frau Raquel Weitzman auf. Erst 2005 erfuhr er, dass Teitelboim nur sein Adoptivvater und die Vaterschaft des Juristen Alvaro Bunster auf Empfehlung der Kommunistischen Partei geheimgehalten worden war. Bunster nahm daraufhin den Namen seines leiblichen Vaters an.





</doc>
<doc id="4773562" url="https://de.wikipedia.org/wiki?curid=4773562" title="BORAX-Experimente">
BORAX-Experimente

BORAX-Experimente (Boiling Reactor Experiments) wurde eine Serie von Reaktorexperimenten im Idaho National Laboratory (USA) genannt, mit welchen in den 1950er-Jahren das Stabilitätsverhalten von Siedewasserreaktoren untersucht wurde.

Die Technologie der Leichtwasserreaktoren stand in den 1950er-Jahren noch in den Anfängen. Was den Siedewasser-Typ anbelangt, war man sich im Ungewissen, wie sich Blasen im Reaktorkern auf die Stabilität des Betriebsverhaltens auswirken. Es war bekannt, dass ein Zusammendrücken der Blasen durch Druckerhöhung im Reaktorsystem kernphysikalisch die Kritikalität erhöht, der Reaktor wird überkritisch und damit instabil. Die Experimente haben aber den Beweis erbracht, dass ein längerfristig stabiles Betriebsverhalten möglich ist und der Reaktor nur punktuell zu Überkritikalität neigt, die in der Regel durch den negativen Dampfblasenkoeffizienten aufgefangen werden kann, d. h. durch den Umstand, dass nach der Leistungsexkursion in Form eines unerwünschten Anstiegs der Kernspaltungen durch stärkere Blasenbildung im Wasser-Moderator postwendend die Leistung wieder abgesenkt wird.

Höhepunkt der Experimente mit dem Reaktor "BORAX-I" war ein zerstörender Test, bei dem ein Totalschaden der Anlage in Kauf genommen wurde. Es wurden an den Abschaltstäben Sprengladungen angebracht. Das dadurch verursachte schlagartige Ausschleudern der Stäbe aus dem Reaktor führte zu einer schlagartigen Leistungsexkursion, die allerdings dank des oben genannten Doppler-Effekts nicht zu einer Totalzerstörung der Anlage führte. Allerdings waren die Konsequenzen dennoch unterschätzt worden: Man hatte erwartet, dass nur einige wenige Brennelemente schmelzen würden, musste jedoch zur Kenntnis nehmen, dass ein großer Teil des ganzen Reaktorkerns abgeschmolzen war.


</doc>
<doc id="5503836" url="https://de.wikipedia.org/wiki?curid=5503836" title="Otto von Baeyer">
Otto von Baeyer

Otto von Baeyer [] (* 12. September 1877 in Reichenhall; † 15. August 1946 in Tutzing) war ein deutscher Physiker.

Otto von Baeyer war ein Sohn des Chemikers Adolf von Baeyer. 1895 legte er die Abiturprüfung am Münchner Maximiliansgymnasium ab, unter anderem mit Fritz Gablonsky und Otto Groth, und studierte anschließend Physik in München und Leipzig. 1905 wurde er mit der Arbeit „Absorption elektrischer Schwingungen von 70 cm Wellenlänge“ in Leipzig bei Otto Wiener promoviert. Er wurde Assistent am Physikalischen Institut der Universität Berlin, wo er 1908 habilitierte. Damit wurde er Privatdozent und zwei Jahre später erhielt er Rang und Titel eines (außerordentlichen) Professors.
Nach dem Militärdienst im Ersten Weltkrieg wurde er 1921 ordentlicher Professor für Physik an der Landwirtschaftlichen Hochschule in Berlin. Diese Stelle behielt er bis 1939. Die letzte Zeit seines Lebens verbrachte von Baeyer in der Nähe von München.
Er starb 1946 in Tutzing und wurde auf dem Münchner Waldfriedhof beerdigt.

1905 bis 1945 war Otto von Baeyer Mitglied der Deutschen Physikalischen Gesellschaft.

Über langsame Kathodenstrahlen veröffentlichte er ein Papier. Als Messgerät benutzte er eine Hochvakuum-Triode als Glühkathode-Ionisationsvakuummeter. Weil das Gitter positiv und die Anode negativ geladen waren, konnte von Baeyer nicht feststellen, dass sein Gerät ein Verstärker war. 
Otto Hahn und Lise Meitner widmeten sich gemeinsam mit Otto von Baeyer der magnetischen Ablenkung der β-Strahlen. Es gelang ihnen zum ersten Mal, Betastrahl-Spektren von einer Reihe radioaktiver Strahler zu erhalten.




</doc>
<doc id="5849508" url="https://de.wikipedia.org/wiki?curid=5849508" title="Verschiebungssatz von Kossel-Sommerfeld">
Verschiebungssatz von Kossel-Sommerfeld

Der Verschiebungssatz von Sommerfeld und Kossel, benannt nach Arnold Sommerfeld und Walther Kossel, beschreibt Spektren wasserstoffähnlicher Atome. Dabei kommt zu ähnlichen Energieniveaus zwischen aufeinanderfolgenden einfach positiv geladenen Atomen. Sommerfeld und Kossel formulierten dies wie folgt:
"Beispiel:"

So kommt es zu etwa gleichen Energieniveaus von ca. 13,6 eV bei Wasserstoff mit einer Hauptquantenzahl formula_1, bei He mit formula_2 und Li mit formula_3.


</doc>
<doc id="6047586" url="https://de.wikipedia.org/wiki?curid=6047586" title="Sputter-Ionenquelle">
Sputter-Ionenquelle

Bei einer Sputter-Ionenquelle werden Teilchen auf ein Target hin beschleunigt, wo sie Atome des Materials herausschlagen (vgl. Sputtern) und ionisieren. Diese Ionen werden dann durch elektrische Felder abgesaugt und bilden einen Ionenstrahl, zum Beispiel für einen Teilchenbeschleuniger.

In der am häufigsten verwendeten Form wird Cäsium als Hilfsmaterial verwendet. Ein Ofen erhitzt Cäsium auf ≈ 120 °C. Der entstehende Cäsiumdampf gelangt in den Quellenraum und füllt diesen aus. Ein Teil des Dampfes kondensiert auf dem gekühlten Sputtertarget (der Kathode) und bildet dort eine Schicht auf der Oberfläche. Das Sputtertarget besteht aus einer chemischen Verbindung oder dem puren Element, das für den Ionenstrahl gewünscht ist. Es ist üblicherweise in eine Kupferform eingepresst. Ein anderer Teil des Dampfes kommt mit dem Ionisator in Berührung. Der halbkugelförmige Ionisator (meist aus Wolfram und Tantal) wird auf ≈ 1000 °C aufgeheizt. Die auftreffenden Cäsiumatome werden ionisiert und durch eine zwischen Ionisator und Kathode anliegende Potentialdifferenz (elektrische Spannung) von ungefähr 6 kV auf die Sputter-Kathode hin beschleunigt. Dort werden verschiedene Atome und Ionen des Targetmaterials aus der Oberfläche herausgeschlagen. Die für den Strahl relevanten Ionen nehmen beim Durchgang durch die Cäsiumschicht auf der Oberfläche mit einer gewissen Wahrscheinlichkeit Elektronen auf und liegen danach als negativ geladene Ionen vor. Durch die bestehende Potentialdifferenz werden die Ionen dann von der Quelle wegbeschleunigt.




</doc>
<doc id="6551580" url="https://de.wikipedia.org/wiki?curid=6551580" title="Ringresonator">
Ringresonator

Ein Ringresonator ist ein optischer Resonator, der Anwendung in der Lasertechnik findet und meist direkt als Laserresonator verwendet wird.

Im Gegensatz zu einem linearen Resonator mit nur zwei Spiegeln besteht ein Ringresonator typischerweise aus vier Spiegeln, die den Laserstrahl auf einen geschlossenen Weg lenken. Dabei können die Laserstrahlen auf sich kreuzende Wegen („Sanduhr“) geleitet werden oder auf einen Weg in Form eines „Vierecks“.

Durch Verwendung eines Ringresonators beim Aufbau eines Laser lässt sich das sogenannte räumliche Lochbrennen verringern, da sich beim Ringresonator keine stehende Welle ausbildet, sondern eine fortlaufende Welle. Weiterhin ist der Einmodenbetrieb (nur eine einzige Frequenz im Laserlicht) mit weniger frequenzselektiven Elementen zu erreichen.

Anwendung findet er zum Beispiel im Laserkreisel.


</doc>
<doc id="7449286" url="https://de.wikipedia.org/wiki?curid=7449286" title="Jan van Paradijs">
Jan van Paradijs

Jan van Paradijs (* 9. Juni 1946 in Haarlem; † 2. November 1999 in Amsterdam) war ein niederländischer Astrophysiker, bekannt für die erste Entdeckung des optischen Nachglühens eines Gammablitzes (28. Februar 1997). Er war einer der führenden Experten für Hochenergie-Astronomie.

Seine Entdeckung des optischen Nachglühens von GRB 970228 (GRB für Gamma Ray Burst) ermöglichte die Klärung der lange offenen Frage der Natur der GRB, insbesondere zeigten sie dass der beobachtete GRB aus einer weit entfernten Galaxie stammte.

Van Paradijs war das älteste von sieben Kindern eines Maurers. Er besuchte dank der Fürsprache seines Schulleiters die Höhere Bürgerschule in Haarlem und studierte ab 1963 Mathematik, Physik und Astronomie an der Universität Amsterdam. 1966 erhielt er seinen Kandidaten-Abschluss und 1970 sein Diplom. Daneben spielte er semiprofessionell Basketball. 1975 wurde er bei David Koelbloed über die chemische Zusammensetzung kühler Riesensterne promoviert, wobei er 1972 zeigte, dass die im Spektrum beobachtete "Mikroturbulenz" echter hydrodynamischer Turbulenz entsprach. Er wies dies anhand der Änderung der Turbulenz während der Pulsationsphase in einem Cepheiden nach. Nach seiner Promotion befasste er sich mit Röntgen-Doppelsternen und Neutronensternen. 1977 bis 1979 war er als Post-Doktorand am Massachusetts Institute of Technology, wo er sich mit Röntgenausbrüchen (X-Ray Bursts) befasste und eine langjährige Zusammenarbeit mit Walter Lewin begann. X-Ray-Bursts und sie begleitende optische Transienten blieben sein Forschungsgebiet. 1988 wurde er Professor an der Universität Amsterdam. 

1997 gelang ihm kurz nach der Beobachtung von Nachglühen im Röntgenbereich durch den BeppoSAX Satelliten die erstmalige Beobachtung des Nachglühens im optischen Bereich (mit seinen Studenten Paul Groot, Titus Galama).

Seine letzte Arbeit 1999 betraf den Zusammenhang von GRB und Supernovae, wo mehrere seiner Schüler Pionierarbeit leisteten (Galama, Vreeswijk bei einer gleichzeitigen Beobachtung einer SN und eines GRB 1998).

Er war seit 1992 mit der griechischen Astrophysikerin Chryssa Kouveliotou am NASA Marshall Space Flight Center in Huntsville verheiratet und war seit 1993 in Teilzeit Professor an der University of Alabama. Er war Ko-Autor einer Arbeit von Kouveliotou von 1998, in der sie Magnetare entdeckte (Neutronensterne mit sehr hohen magnetischen Feldern). Aus erster Ehe hatte er einen Sohn und eine Tochter.




</doc>
<doc id="7922764" url="https://de.wikipedia.org/wiki?curid=7922764" title="Watt W. Webb">
Watt W. Webb

Watt Wetham Webb (* 27. August 1927 in Kansas City, Missouri) ist ein US-amerikanischer Biophysiker.

Webb erwarb 1955 bei John Torrey Norton und Ernest Carl Wagner mit der Arbeit "Oxidation studies in metal-carbon systems" am Massachusetts Institute of Technology (MIT) einen Ph.D. in Physik. Anschließend arbeitete er bei Union Carbide, zunächst als Forschungsmitarbeiter, dann (1959–1960) als Koordinator der Grundlagenforschung und schließlich (1960–1961) als stellvertretender Forschungsdirektor. 1961 erhielt Webb eine Professur "(Assistant Professor)" an der Cornell University in Ithaca, New York. 1965 erhielt er dort eine ordentliche Professur für Angewandte Physik. Seit 1998 ist er "S.B. Eckert Professor" für Ingenieurwesen an der Cornell University.

Webb war seit 1950 mit Page Chapman Webb (1928–2010) verheiratet. Das Paar hatte drei Söhne.

Anfang der 1970er Jahre war Webb an der Entwicklung der Fluoreszenzkorrelationsspektroskopie beteiligt. Webb leistete grundlegende Beiträge zur Biophysik von Zellmembranen und zur Zellmigration. Gemeinsam mit Winfried Denk entwickelte Webb Anfang der 1990er Jahre die Grundlagen der Zwei-Photonen-Fluoreszenz-Mikroskopie, mit der sich Zellstrukturen dreidimensional auch tief in Geweben darstellen ließen. Webbs strenge Anwendung physikalischer Prinzipien bei der Entwicklung von optischen Geräten hatte somit einen erheblichen Einfluss auf die Beobachtbarkeit biologischer Systeme.

Webb betreute an der Cornell University mehr als 75 Doktoranden.





</doc>
<doc id="8000485" url="https://de.wikipedia.org/wiki?curid=8000485" title="Johannes Harting">
Johannes Harting

Carl August Johannes „Hans“ Harting (* 15. Februar 1868 in Rummelsburg; † 21. September 1951 in Jena) war ein deutscher Optiker und Physiker, der nach 1945 maßgeblich am Wiederaufbau des VEB Carl Zeiss in Jena beteiligt war.

Seine Eltern waren der Rendant "Carl Frdrich Fides Ferdinand Harting" (1826–1910) und dessen Ehefrau "Anna Dittmann" (1842–1868).

Nach dem Abitur studierte Harting von 1885 bis 1891 Physik, Astronomie und Mathematik an den Universitäten in Berlin und München sowie an der Technischen Hochschule in München. 1889 wurde er an der Universität München mit "Untersuchungen ueber den Lichtwechsel des Sternes [Beta] Persei" promoviert. Von 1893 bis 1897 war er Mitarbeiter der Physikalisch-Technischen Reichsanstalt in Berlin. Anschließend holte ihn Ernst Abbe als Assistent zum Zeisswerk nach Jena. 
Von 1899 bis 1907 war er Direktor der Optischen Werke Voigtländer & Sohn in Braunschweig, wo er an der Entwicklung verschiedener photographischer Objektive beteiligt war.

1908 wurde er als Regierungsrat an das Reichspatentamt in Berlin berufen. Hier hatte er verschiedene Funktionen; zuletzt war er bis 1933 Präsident dieses Amtes. Von 1919 bis 1934 war er Schriftleiter der "Central-Zeitung für Optik und Mechanik, Elektrotechnik und verwandte Berufszweige". Außerdem wurde auf seine Initiative 1923 die "Deutsche Gesellschaft für angewandte Optik" gegründet. Er war der erste Vorsitzende dieser Gesellschaft.
Harting wurde 1934 Mitglied der Geschäftsleitung der Zeisswerke in Jena. 1940 ging er in den Ruhestand.

1945 kehrte er als 77-Jähriger zu den Carl-Zeiss-Werken in Jena zurück und hatte als wissenschaftlicher Hauptleiter einen wichtigen Anteil bei der Neugestaltung der Forschungs- und Entwicklungstätigkeit in diesem Werk.
Er wurde 1949 für sein Wirken beim Wiederaufbau der Zeisswerke nach dem Zweiten Weltkrieg und als „Schöpfer einer Reihe optischer Systeme, die Spitzenleistungen der optischen Industrie sind“ mit dem Nationalpreis der DDR I. Klasse für Wissenschaft und Technik ausgezeichnet.
1950 wurde er Ehrenmitglied der Deutschen Akademie der Wissenschaften zu Berlin.

Er heiratete 1895 in Berlin "Margarete Hergesell", eine Tochter des Pfarrers "Friedrich Hergesell" und der "Elise Worthmann". Das Paar hatte einen Sohn der im Krieg fiel. Nach dem Tod seiner ersten Frau heiratete er 1916 die Lehrerin "Agathe Sohr", eine Tochter des Studienrats Dr. phil. "Maximilian Sohr" und der "Adele von Negelein" verwitwete von Hobe. Das Paar hatte zwei Töchter.





</doc>
<doc id="8497956" url="https://de.wikipedia.org/wiki?curid=8497956" title="Eugene Guth">
Eugene Guth

Eugene Guth, in Europa veröffentlichte er auch früher als Eugen Guth, (* 21. August 1905 in Budapest; † 5. Juli 1990) war ein ungarisch-US-amerikanischer Physiker.
Guth wurde 1928 an der Universität Wien bei Hans Thirring in theoretischer Physik promoviert. Als Post-Doktorand war er an der ETH Zürich bei Wolfgang Pauli und 1930/31 an der Universität Leipzig bei Werner Heisenberg. Ab 1932 war er Professor an der Universität Wien. Damals befasste er sich mit Kernphysik, wobei er mit Theodor Sexl zusammenarbeitete. 1937 ging er in die USA als an die University of Notre Dame, wo er 1941 Professor wurde und 1941 bis 1955 das von ihm gegründete Labor für Polymerphysik leitete (mit einer Forschungsprofessur). Im Zweiten Weltkrieg war er 1943 bis 1945 Direktor des Office of Rubber Research Project und 1946 bis 1955 war er Direktor der Projekte Polymerphysik und Theoretische Physik des Office of Naval Research. 1956 bis 1971 war er technischer Berater des Oak Ridge National Laboratory. 

1971/72 war er Gastprofessor an der Rice University und 1968 bis zu seinem Tod 1990 in Teilzeit Professor an der University of Tennessee.

1942 wurde er US-Staatsbürger. Er war seit 1947 mit Roma Claire Lynch verheiratet und hatte vier Kinder.

1938 wurde er Fellow der American Physical Society. 1965 erhielt er die Bingham Medal. 1979 erhielt er das Ehrenkreuz für Wissenschaft und Kunst der Republik Österreich.

Zu seinen Schülern zählt Paul Urban.

Er gilt als einer der Pioniere in Physik und Physikalischer Chemie der Polymere und deren Rheologie, wobei er sowohl theoretisch als auch experimentell arbeitete. Mit Herman Mark entwickelte er eine kinetische Theorie der Gummielastizität mit dem Modell eines Polymers als langkettiges, flexibles Molekül, das in einer Flüssigkeit Zufallsstößen einer Brownschen Bewegung unterworfen ist, und konnte eine Formel für den Entropie-Gewinn beim "Verknäueln" der Polymer-Moleküle angeben. Mit H. M. James entwickelte er 1939 eine Netzwerk-Theorie der Gummi-Elastizität. Außerdem befasste sich Guth mit der Viskosität von Suspensionen (wobei er auf Arbeiten von Albert Einstein und George Barker Jeffery aufbaute).

In der Kernphysik betrachtete er schon 1934 die Streuung von schnellen Elektronen an Kernen als Mittel zur Erforschung der Kernstruktur, ein Forschungsgebiet, für das Robert Hofstadter später den Nobelpreis erhielt, und Hofstadter würdigte auch Guth als Vorläufer seiner Forschung. Er behandelte die Anregung schwerer Kerne mit Röntgenstrahlen, veröffentlichte 1949 eine schalentheoretische Behandlung der Photo- und Elektrodisintegration von Berylliumkernen mit einer der frühesten Anwendungen der "Distorted Wave Born Approximation" (DWBA) in der Kernphysik (frühe Behandlung des Photoeffekts in Kernen, auch bei Deuterium), gab eine frühe Theorie die Coulombanregung von Kernen und schlug 1966 mit L. Wilets Coulomb-Spaltung von Kernen vor.

1962 verallgemeinerte er die Theorie der "Zitterbewegung" von Erwin Schrödinger in der relativistischen Quantenmechanik auf Teilchen beliebigen Spins.

Weitere Arbeiten befassten sich unter anderem mit elektrischen Leitungsphänomenen.

Schon 1929 veröffentlichte er ein Buch über die Geschichte der Quantentheorie.

Neben den in den Fußnoten angeführten Schriften:



</doc>
<doc id="8900863" url="https://de.wikipedia.org/wiki?curid=8900863" title="Junge Deutsche Physikalische Gesellschaft">
Junge Deutsche Physikalische Gesellschaft

Die junge Deutsche Physikalische Gesellschaft ("jDPG" oder "junge DPG") ist ein bundesweites Netzwerk für Schüler, Studierende und Promovierende. Juristisch ist sie ein Arbeitskreis der DPG. Sie ergänzt das fachliche Programm der DPG um Angebote für junge Physiker und die breite Öffentlichkeit.

Die jDPG wurde 2006 in Dresden gegründet. Im Jahr 2011 feierten die Mitglieder das 5-jährige Jubiläum mit einem Kongress zum Thema "Physik ist Mobilität" in Wolfsburg. Im November 2011 würdigte die DPG das Engagement des jDPG-Bundessprechers Alexander Heinrich mit der Verleihung der DPG-Ehrennadel. Er war der jüngste Träger der Ehrennadel, bis 2014 diese Ehre auch der ehemaligen Bundessprecherin Anna Bakenecker und Matthias Zimmermann zuteil wurde.

Im März 2014 wurde zwischen der DPG und der European Physical Society ein Vertrag ausgehandelt, der die junge DPG in das internationale "Young Minds" eingliedert. So ist der Bundesvorstand als Nationales Koordinationskomitee tätig und ermöglicht den Regionalgruppen durch ein Bewerbungsverfahren gleichzeitig zur "Young Minds Section" zu werden. Deren Projekte werden dann von der EPS unterstützt und gefördert. Im Gegenzug dafür sind die Regionalgruppen der EPS Rechenschaft schuldig. Außerdem ist es in Deutschland nur jDPG-Regionalgruppen möglich, zur "Young Minds Section" zu werden. Die junge DPG nimmt, stellvertretend für die DPG, die Mitgliedschaft in der International Association of Physics Students war.

Seit Januar 2015 hat die jDPG mehr als 3400 Mitglieder und war damit die DPG-Arbeitsgruppe mit dem stärksten Wachstum. Etwa die Hälfte aller DPG-Mitglieder gehört in die Zielgruppe der jDPG. Derzeit gibt es deutschlandweit 33 Regionalgruppen und acht lokale Ansprechpartner (Stand 2018). Im November wurde die Arbeitsgruppe junge DPG in einen Arbeitskreis umgewandelt. Dies ist verbunden mit einer Vertretung im DPG-Vorstandsrat und höherer Sichtbarkeit in der DPG. Die Wichtigkeit der Nachwuchsförderung in der DPG wurde mit der zusätzlichen Schaffung eines Vorstandsamts für junge Mitglieder und Berufsfragen unterstrichen.

Neben wissenschaftlichen Programmen bietet die jDPG auch Berufsvorbereitungen und Veranstaltungen für Schüler an. In den Seminaren und Workshops der jDPG können Studierende ihre Kenntnisse verfeinern und Wissen austauschen. Zudem beteiligen sich die Aktiven der jungen DPG mit Vorträgen und Symposien an den Frühjahrstagungen der DPG. Auf den bundesweiten Sommerexkursionen werden Forschungsstandorte in ganz Deutschland vorgestellt. Die 10. Sommerexkursion fand anlässlich des Internationalen Jahrs des Lichts 2015 in Jena statt. Vor Ort bieten die Regionalgruppen Vorträge, Exkursionen sowie regelmäßige Treffen an.

Wichtiger Bestandteil der Berufsvorbereitung ist das Mentoring-Programm. Es vermittelt Studierenden erfahrene Physiker aus Forschung oder Wirtschaft. Außerdem bieten Berufsvorbereitungsseminare einen umfassenden Eindruck von der Arbeitswelt von Physikern, beispielsweise am Patentgericht, in der Strategieberatung oder im Wissenschaftsjournalismus.

Auch am Physik Journal, der offiziellen Mitgliederzeitschrift der DPG, beteiligt sich die junge DPG mit Artikeln, Berichten sowie Interviews. In der Hochschulpolitik arbeitet die junge DPG eng mit der Zusammenkunft aller Physikfachschaften ZaPF zusammen. Durch Teilnahme an internationalen Veranstaltungen wie der International Conference of Physics Students (ICPS) fördert die jDPG die europaweite Vernetzung von Studierenden der Physik. Im Jahr 2014 fand die 29. ICPS in Heidelberg statt, organisiert von Aktiven der jungen DPG.

Mit dem online Schülermagazin „Detektor“ erhielten von 2012 bis 2016 interessierte Kinder und Jugendliche ab der achten Jahrgangsstufe die Möglichkeit, einen eigenen wissenschaftlichen Artikel zu verfassen und zu veröffentlichen. Dabei standen ihnen Studierende mit fachlichem Rat zur Seite. Der Schwerpunkt lag auf Physik, aber auch mathematisch-naturwissenschaftliche, technische Themen fanden ihren Platz. Zudem wurde die interdisziplinäre Arbeit gefördert. Das Projekt wurde von der Wilhelm und Else Heraeus-Stiftung unterstützt. Ein Nachfolgeprojekt ist in Arbeit.

Sogenannte "Physikfrühstücke" sind Teil des schulischen Bereichs. Dort gewinnen Schüler einen Einblick in das Physikstudium und können mit Physikern beim gemeinsamen Frühstücken in Kontakt treten. Der Schwerpunkt liegt auf der Möglichkeit zum persönlichen Gespräch. Bei anschließenden Vorträgen und Vorlesungen sowie Laborführungen werfen die Teilnehmer einen Blick hinter die Kulissen und erfahren mehr über den Alltag an der Universität. Die Projektleiterin Annika Tebben wurde 2014, so wie im Jahr zuvor das Team von Detektor, mit dem MINT-Botschafterpreis ausgezeichnet.

Seit 2015 organisiert das Arbeitsteam Schule und Nachwuchs der jDPG jährlich die Schülertagung. Dort kommen aus ganz Deutschland Schüler der gymnasialen Oberstufe zusammen, um eigene Forschungsarbeiten (bspw. im Rahmen von Jugend forscht) zu präsentieren und die Vorträge Gleichaltriger zu besuchen. In Symposien und Gesprächspausen können sie mit internationalen Experten über aktuelle Forschung und die gesellschaftliche Bedeutung der Physik diskutieren. Plenarvorträge und Laborführungen ergänzen das Programm. 2017 fand zum ersten Mal eine mehrtägige Schülertagung statt, an der insgesamt 100 Jugendliche teilnahmen.

Die "Deutsche Olympiade im Physik-Probleme-Lösen Eifrig Rätselnder Studierender" (DOPPLERS) wurde 2015 von Markus Schmitt und Thomas Kotzott ins Leben gerufen und findet seitdem jährlich statt. Kern der Veranstaltung ist ein vierstündiger Team-Wettbewerb im Lösen von Aufgaben der Theoretischen Physik.
Die erfolgreichsten Teams des Wettbewerbs nehmen in der Folge für Deutschland am internationalen Pendant "Physics League Across Numerous Countries for Kick-ass Students" (PLANCKS) teil.
Um die eigentliche Wettbewerbsklausur herum gibt es ein Rahmenprogramm zum gegenseitigen Kennenlernen und zur Erkundung des Wettbewerbsortes.

Die Bundesvorsitzenden der jungen DPG leiten den Bundesvorstand, vertreten die jDPG gegenüber inter/nationalen Partnern und nehmen an den Sitzungen des DPG-Vorstands teil. Sie werden auf den Mitgliederversammlungen mit dem restlichen Bundesvorstand gewählt.



</doc>
<doc id="9055052" url="https://de.wikipedia.org/wiki?curid=9055052" title="Bindungskoeffizient">
Bindungskoeffizient

Der Bindungskoeffizient (synonym "Koeffizient bevorzugter Wechselwirkung", ) bezeichnet in der physikalischen Chemie und Biochemie das Ausmaß der Bindung zwischen einem Makromolekül wie einem Protein und den kleinen Molekülen einer Mischung von Lösungsmitteln (Solvens und Cosolvens) nach der Kirkwood-Buff-Theorie.

Der Bindungskoeffizient kann aus der Kirkwood-Buff-Theorie abgeleitet werden. Die bevorzugte Wechselwirkung ist als thermodynamischer Ausdruck definiert, der die Bindung des zweier Lösungsmittel (Solvens und Cosolvens) beschreibt. Der Bindungskoeffizient ist ein Maß für die Wechselwirkungen zwischen gelösten Bestandteilen.



</doc>
<doc id="10080936" url="https://de.wikipedia.org/wiki?curid=10080936" title="Christian David Ott">
Christian David Ott

Christian David Ott (* 1977 in Offenbach am Main) ist ein deutscher theoretischer Astrophysiker, der insbesondere für Forschungen zu Supernovae bekannt ist.

Ott studierte an der Universität Heidelberg mit dem Diplom 2003 bei Wolfgang J. Duschl in theoretischer Astrophysik und wurde 2007 "summa cum laude" an der Universität Potsdam (wobei er am Max-Planck-Institut für Gravitationsphysik in Golm bei Potsdam arbeitete) bei Bernard Schutz promoviert. Als Post-Doktorand war er 2006 bis 2008 am Joint Institute of Nuclear Physics (JINA) der University of Arizona bei Adam Burrows und 2008 bis 2009 Sherman Fairchild Prize Fellow am Caltech. 2009 war er Assistant Professor am Niels-Bohr-Institut und ab 2009 Assistant Professor für theoretische Astrophysik am Caltech, wo er "tenure" erhielt. Außerdem war er 2009 bis 2014 Adjunct Assistant Professor an der Louisiana State University. Er ist Mitglied der TAPIR (Theoretical Astrophysics including Relativity) Gruppe am Caltech. 2017 trat er von seiner Professur zurück. 2017/18 war er am Yukawa Institute of Theoretical Physics in Kyoto.

Er befasst sich mit den Mechanismen von Kollaps-Supernovae (allgemein-relativistische Simulation in drei Raumdimensionen mit realistischer Mikrophysik, einschließlich Strahlungstransport über Neutrinos, Zustandsgleichungen für Kernmaterie und thermonukleare Prozesse und Elementbildung, Rolle von Turbulenz), Erzeugung von Gravitationswellen und sich daraus ergebende Signaturen von verschiedenen Quellen, Theorie langer Gammablitze (Kollapsar-Modell) und allgemein numerischer Relativität einschließlich wissenschaftlichem Rechnen für massiv-parallele Hochleistungsrechner. Er ist Teil der Simulating eXtreme Spacetimes (SXS) Kollaboration, trug zum Open-Source-Einstein-Toolkit für numerische Relativität bei und ist Mitglied der LIGO-Kollaboration.

2006 schlug er in seiner Dissertation einen neuen Mechanismus für Kollaps-Supernovae vor (Pulsationsmoden des Proto-Neutronensterns und dessen Dämpfung durch akustische Moden), was sich auch in der Signatur von Gravitationswellen ausdrückt. 2015 demonstrierte er in magnetohydrodynamischen Simulationen mit Philipp Mösta und anderen, dass sich bei schnell rotierenden massereichen Sternen hohe Magnetfelder wie bei Magnetaren aufbauen können (Szenario der Ic-Supernovae und der Entstehung langer Gammablitze).

2007 erhielt er den Potsdamer Nachwuchswissenschaftler-Preis und 2008 die Otto-Hahn-Medaille der Max-Planck-Gesellschaft. 2012 erhielt er einen NSF Career Grant und 2012 bis 2014 war er Sloan Fellow.



</doc>
<doc id="10336412" url="https://de.wikipedia.org/wiki?curid=10336412" title="Jean-Loup Puget">
Jean-Loup Puget

Jean-Loup Puget (* 7. März 1947 in Chalon-sur-Saône) ist ein französischer Astrophysiker.

Puget studierte 1966 bis 1970 an der École normale supérieure de Cachan. 1973 wurde er bei Evry Schatzman promoviert, wobei er seine Dissertation 1970/71 an der University of Maryland und am Goddard Space Flight Center ausführte. Seit 1973 forscht er für das CNRS. 1978 bis 1982 war er Directeur adjoint des Pariser Observatoriums (und ist seit 2002 in deren wissenschaftlichem Rat). Ab 1990 war er stellvertretender Direktor und 1998 bis 2005 Direktor des Institut d´astrophysique spatiale (IAS) in Orsay. 

Er befasst sich sowohl mit theoretischer als auch mit beobachtender Astrophysik, speziell galaktische und extragalaktische Gammastrahlenquellen, interstellarem Medium und Entstehung von Sternen und Kosmologie. Insbesondere ist er bekannt für Astronomie im Infraroten und bei Submillimeterwellenlängen. Zum Beispiel identifizierte er als Erster den diffusen kosmischen Infrarot-Hintergrund mit COBE aus der Strahlung von Galaxien. Ihm gelang die Entdeckung der Infrarotlinien polyzyklischer aromatischer Kohlenwasserstoffe im interstellaren Medium und der Nachweis, dass diese dort in großen Mengen vorhanden sind.

Er ist leitender Wissenschaftler des HFI Moduls im Planck-Weltraumteleskops und war leitender Wissenschaftler am Infrared Space Observatory.

Er ist Mitglied der Académie des Sciences (korrespondierendes Mitglied seit 1994, volles Mitglied seit 2002). 1989 erhielt er den Prix Jean Ricard und 2014 den COSPAR Space Science Award sowie 2015 den Edison-Volta-Preis. 2018 erhielt er den Gruber-Preis für Kosmologie mit dem Team des Planck-Weltraumteleskops und Nazzareno Mandolesi, ebenso 2018 den Shaw Prize in Astronomie.



</doc>
<doc id="10495047" url="https://de.wikipedia.org/wiki?curid=10495047" title="Friedrich Narr">
Friedrich Narr

Friedrich Gustav Narr (* 16. August 1844 in Würzburg; † 6. Oktober 1893) war ein deutscher Physiker.

Friedrich Narr besuchte das Gymnasium in Würzburg und studierte nach dem Abitur 1863 Naturwissenschaften an den Universitäten Würzburg, Heidelberg, Göttingen und München, wo er am 31. Juli 1869 zum Dr. phil. promoviert wurde. 1870 wurde er für Physik habilitiert, am 25. Dezember 1870 Privatdozent und am 2. August 1886 außerordentlicher Professor in der Philosophischen Fakultät der Universität München. Friedrich Narr hatte eine Krankheit, die ihn über Jahre stark beeinträchtigte und im Alter von 49 Jahren am 1. Mai 1893 wegen Dienstunfähigkeit zu seiner Versetzung in den Ruhestand führte. 

Narr behandelte in seinen Vorlesungen Theoretische und Experimentalphysik. Zudem beschäftigte er sich wissenschaftlich mit der Physik der Gase und der Elektrizität.

Am 23. Dezember 1891 wurde Friedrich Narr als Mitglied (Matrikel-Nr. 2924) in die Leopoldina aufgenommen.





</doc>
<doc id="10537588" url="https://de.wikipedia.org/wiki?curid=10537588" title="Gustav Eduard Lösche">
Gustav Eduard Lösche

Gustav Eduard Lösche auch "Gustav Eduard Loesche" (* 3. Januar 1821 in Dresden; † 25. Januar 1879 ebenda) war ein sächsischer Physiker und Naturforscher.

Gustav Eduard Lösche besuchte die Kreuzschule Dresden und absolvierte anschließend ein Studium der Medizin und der Naturwissenschaften an der Universität Leipzig. Nach seiner Promotion zum Dr. med. und Dr. phil. wurde er 1848 Lehrer für Physik an der Technischen Bildungsanstalt zu Dresden und bereits ein Jahr später 1849 zum ordentlichen Professor für Physik und Theoretische Chemie berufen.

Von 1862 bis 1876 wirkte er als Professor für Experimental und höhere Physik an der Königlich Sächsischen Polytechnischen Schule (ab 1871 Königlich Sächsisches Polytechnikum) und setzte danach noch seine Vorlesungstätigkeit im Fach "Mathematische Physik" bis 1879 fort. 

Lösche unternahm mehrere Forschungsreisen in die östlichen Alpen und veröffentlichte seine Erkenntnisse darüber in der Allgemeinen deutschen Naturhistorischen Zeitung, dem damaligen Organ der Naturwissenschaftlichen Gesellschaft ISIS zu Dresden, bei der er sich bis zuletzt engagierte. Seine diversen bei den Reisen zusammengetragenen Sammlungen erregten dabei das ausgesprochene Interesse von Leopold von Buch.

Am 15. Juni 1865 wurde Gustav Eduard Loesche mit dem akademischen Beinamen "Dalton" als Mitglied (Matrikel-Nr. 2047a) in die Leopoldina aufgenommen.





</doc>
<doc id="24537" url="https://de.wikipedia.org/wiki?curid=24537" title="Photoelektrischer Effekt">
Photoelektrischer Effekt

Unter dem Begriff photoelektrischer Effekt (auch lichtelektrischer Effekt oder kurz Photoeffekt) werden drei nah verwandte, aber unterschiedliche Prozesse der Wechselwirkung von Photonen mit Materie zusammengefasst. In allen drei Fällen wird ein Elektron aus einer Bindung – z. B. in einem Atom oder im Valenzband oder im Leitungsband eines Festkörpers – gelöst, indem es ein Photon absorbiert. Die Energie des Photons muss dazu mindestens so groß wie die Bindungsenergie des Elektrons sein.

Man unterscheidet drei Arten des photoelektrischen Effekts:


Die Freisetzung von Ladungsträgern aus einer blanken Metalloberfläche in Elektrolyten durch Licht wurde erstmals 1839 von Alexandre Edmond Becquerel beim sogenannten Becquerel-Effekt beobachtet.

Im Jahr 1886 konnte Heinrich Hertz den Einfluss von Ultraviolettstrahlung (UV) auf die Metalloberflächen in einer Funkenstrecke demonstrieren. Dabei beobachtete er, dass das ultraviolette Licht, das von einem „Primärfunken“ A ausgesandt wird, die Länge eines zweiten Funkens B vergrößert. Die Länge von B hing reziprok vom Abstand der Funken ab, verschiedene Absorber für Ultraviolett (auch solche, die im sichtbaren Spektralbereich durchsichtig sind) verkleinerten den Funken. Einen Einfluss des sichtbaren Lichts auf die Funkenlänge konnte Hertz nicht nachweisen. Die Erklärung dieser Beobachtungen ist, dass das ultraviolette Licht Elektronen aus den Elektroden der Funkenstrecke herausschlägt, die dann schon bei geringerer elektrischer Feldstärke zu einem Überschlag führen, da nicht erst die Austrittsarbeit aufgewendet werden muss.
Wilhelm Hallwachs, damals Assistent von Gustav Wiedemann in Leipzig, führte weitere systematische Untersuchungen durch (daher auch die Bezeichnung "Hallwachs-Effekt"). Dabei zeigte er z. B. mit einem „Goldblattelectroskop“ (siehe Abbildung rechts), dass sich eine Metallplatte durch Bestrahlung mit einer Lichtbogenlampe elektrisch aufladen ließ.

Philipp Lenard untersuchte als erster den Photoeffekt im Hochvakuum. Er konnte 1899 durch Ablenkung der Ladungsträger im Magnetfeld ihre spezifische Ladung bestimmen und sie so als Elektronen identifizieren. Er entdeckte die oben beschriebenen Abhängigkeiten von der Frequenz und der Bestrahlungsstärke. Albert Einstein lieferte 1905 in § 8 seiner Arbeit "Ueber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt", für die er den Nobelpreis für Physik des Jahres 1921 erhielt, die Erklärung des Effekts. Robert Andrews Millikan konnte ab 1912 bis 1915 mit Hilfe der Gegenfeldmethode (siehe unten) bestätigen, dass der Proportionalitätsfaktor der einsteinschen Gleichung mit dem bereits bekannten planckschen Wirkungsquantum übereinstimmt.

Die Gegenfeldmethode, eine von verschiedenen Methoden zur Vermessung des Photoeffekts, ist u. A. hilfreich für Demonstrationsversuche zum äußeren Photoeffekt in Schule und Universität.
Aus dem Licht einer Quecksilberdampflampe wird durch einen Interferenzfilter oder einen Monochromator ein schmaler Wellenlängenbereich gefiltert und (gegebenenfalls durch eine Linse) auf die Kathode (im Bild rot) einer Vakuum-Photozelle gebündelt. Vakuum ist erforderlich, um die Oberfläche der Photokathode vor Oxidation zu schützen, vor allem jedoch, damit die mittlere freie Weglänge der ausgetretenen Elektronen ausreicht, um die gegenüberliegende, oft ringförmige Anode zu erreichen. Eine Spannung formula_1 kann zwischen den beiden Elektroden angelegt werden, und es kann der Strom formula_2 mittels eines empfindlichen Amperemeters gemessen werden. Eine genauere Versuchsbeschreibung findet sich z. B. in der genannten Arbeit von Millikan oder in einem Praktikums-Skript.

Wird die Kathode mit Licht ausreichend kurzer Wellenlänge bestrahlt, so werden dort Elektronen „herausgeschlagen“. Diese bewegen sich aufgrund ihrer kinetischen Energie formula_3 zur Anode. Die Photozelle wird also zur Stromquelle und der fließende "Photostrom" formula_2 kann mit einem empfindlichen Amperemeter gemessen werden. Wird nun eine Gegenspannung formula_1 angelegt, so müssen die Elektronen, die die Anode erreichen und zu einem Photostrom führen, neben der Austrittsarbeit formula_6 aus der Kathode auch das erzeugte elektrische Feld überwunden haben.

Die Gegenspannung formula_7, ab der jeweils kein Photostrom mehr fließt, kann für verschiedene Frequenzen formula_8 des Lichts ermittelt werden; bei dieser Spannung ist die Potentialdifferenz formula_9, die die Elektronen (elektrische Ladung formula_10) überwinden müssen, gleich der maximalen kinetische Energie der Elektronen formula_3 nach ihrem Austritt aus der Kathode. Nimmt man an, dass die Energie des Lichts nur durch Energiequanten mit der Energie formula_12 (mit dem Planckschen Wirkungsquantum formula_13) an die Elektronen übertragen wird, kann man aus der Steigung der gemessenen Geraden formula_14 das Wirkungsquantum formula_13 bestimmen (siehe auch Millikan). Auch die Austrittsarbeit formula_6 kann bestimmt werden.

Am Beispiel von Zink (Abbildung rechts) ergibt sich die Steigung im Diagramm mit Hilfe des Steigungsdreiecks zu
ungefähr dem Planckschen Wirkungsquantum. Der y-Achsenabschnitt der gestrichelten Geraden entspricht der Austrittsarbeit; bei Zink liest man diesen Wert als ca. (−) 4,3 eV ab. Der wahre Wert beträgt 4,34 eV.

In den eben beschriebenen Versuchen können folgende Beobachtungen gemacht werden:


Bis auf die letzte Beobachtung stehen alle gefundenen Zusammenhänge im Widerspruch zur klassischen Vorstellung von Licht als Wellenerscheinung. Nach dieser hängt die Energie einer Welle allein von ihrer Amplitude, nicht jedoch von ihrer Frequenz ab. Somit müsste mit sinkender Bestrahlungsstärke auch die kinetische Energie der Elektronen abnehmen. Der Effekt sollte dann verzögert auftreten, da die Übertragung der zur Freisetzung der Elektronen nötigen Energie dann länger dauert. Statt einer Minimalfrequenz wäre nach klassischer Vorstellung zu erwarten, dass mit sinkender Frequenz lediglich die Zeit, bis ein Elektron genügend Lichtenergie aufgesammelt hat, zunimmt.

Physiker wie Isaac Newton hatten zwar schon angenommen, dass Licht aus Teilchen, sogenannten Korpuskeln besteht. Spätestens Ende des 19. Jahrhunderts galt die Vorstellung von Lichtteilchen allerdings als überholt, da zum einen Maxwells Elektrodynamik Licht als elektromagnetische Welle auffasste und in Übereinstimmung damit Interferenzexperimente den Wellencharakter des Lichts unzweifelhaft belegten.

Einsteins Erklärung des photoelektrischen Effekts durch Lichtteilchen 1905 war vor diesem Hintergrund eine mutige Hypothese. Grundlage war die plancksche Strahlungshypothese aus dem Jahre 1900, nach der das Licht aus einem Strom von Teilchen besteht, den "Photonen", deren Energie formula_20 das Produkt aus der Frequenz formula_8 des Lichts und dem planckschen Wirkungsquantum formula_13 ist (formula_12). Mit Hilfe dieser Annahme lässt sich zunächst der Zusammenhang zwischen der Frequenz und der kinetischen Energie des (zunächst als Teilchen mit Masse gedachten) Photons erklären, darauf aufbauend auch alle weiteren experimentellen Beobachtungen.

Der damit gefundene scheinbare Widerspruch, dass Licht in bestimmten Experimenten Wellen-, in anderen aber Teilchenverhalten zeigt (Welle-Teilchen-Dualismus), wurde erst durch die Quantenmechanik aufgelöst. Der photoelektrische Effekt war eines der Schlüsselexperimente zur Begründung der Quantenphysik. Einstein wurde 1921 für die Erklärung des Effekts mit dem Nobelpreis für Physik ausgezeichnet.

Mit der Entwicklung der Quantentheorie des Lichts in den 1960er Jahren war es möglich, den Photoeffekt semi-klassisch zu erklären: Eine klassische elektromagnetische Welle wechselwirkt dabei mit dem quantisierten Detektor. Der Photoeffekt ist somit kein eindeutiger Nachweis für die Quantennatur von Licht.

Verschiedene physikalische Geräte, wie Photozellen und Photokathoden von Photomultipliern und Bildwandlerröhren, sowie eine wichtige oberflächenphysikalische Messmethode, die Photoelektronenspektroskopie, nutzen den photoelektrischen Effekt aus. Dabei werden photoelektrische Messverfahren angewendet.

Unter Photoleitung versteht man die Erhöhung der elektrischen Leitfähigkeit von Halbleitermaterialien aufgrund der Bildung von ungebundenen Elektron-Loch-Paaren bei Bestrahlung. Die Elektronen werden dabei mittels der Energie der Photonen vom Valenzband in das energetisch höher gelegene Leitungsband gehoben, wofür die Energie des einzelnen Photons mindestens der Bandlücke des bestrahlten Halbleiters entsprechen muss. Da die Größe der Bandlücke materialabhängig ist, unterscheidet sich die maximale Wellenlänge des Lichtes, bis zu der Photoleitung auftritt, je nach Halbleiter (Galliumarsenid: 0,85 μm, Germanium: 1,8 μm, Silizium: 1,1 μm).

Spektren der Photoleitung zeigen die Abhängigkeit der elektrischen Leitfähigkeit von der Energie (beziehungsweise der Wellenlänge) des eingestrahlten Lichts. Die Leitfähigkeit steigt ab der Bandlückenenergie deutlich an, so dass man auf diese Weise die (direkte) Bandlücke bestimmen kann. Die detaillierte Analyse solcher Photoleitungsspektren ist in Kombination mit den Erkenntnissen aus anderen Untersuchungen eine wichtige Grundlage für das Verständnis der Bandstruktur des verwendeten Materials (siehe auch Bändermodell).

Wenn die Untersuchungen im Magnetfeld vorgenommen werden, können noch weitere Details bestimmt werden, die sich sonst in ihren Auswirkungen untrennbar überlagern, durch das Magnetfeld aber getrennt werden. Beispiele sind der magnetooptische Kerr-Effekt und der Halleffekt, mit welchem die Elektronenbeweglichkeit bestimmt werden kann.

Für Messungen der Wellenlängenabhängigkeit der Photoleitung verwendet man Monochromatoren. Messungen erfolgen meistens im Vakuum, um z. B. Wasserbanden (siehe Infrarotspektroskopie) im nahen Infrarot zu vermeiden, oder bei tiefen Temperaturen, um z. B. Magnetfeldeffekte vom Rauschen zu trennen.

Die Photoleitung wird in Photowiderständen, Phototransistoren, Photodioden und CCD-Sensoren ("siehe auch pin-Diode und Avalanche-Photodiode") ausgenutzt, welche bei der Herstellung einer Vielzahl von Lichtsensoren Verwendung finden.

In Photowiderständen und auch anderen Halbleitern können durch Licht erzeugte Ladungsträger unter Umständen auch nach Abdunkelung sehr lange (Stunden bis Tage) bestehen bleiben, man spricht dann vom langanhaltenden Photoeffekt (kurz PPE, von engl. "persistent photoeffect").

Phototransistoren enthalten photoempfindliche PN-Übergänge. Sie verstärken den in ihrer Basis auftretenden Strom.

Für Messungen im sichtbaren und im infraroten Spektralbereich werden als Photoleiter meist Photodioden im Quasikurzschluss oder im Sperrbereich betrieben – sie liefern dann einen zum einfallenden Strahlungsfluss über viele Größenordnungen proportionalen Strom.

Persistente Photoleitung wird in Strontiumtitanat-Einkristallen bei Raumtemperatur beobachtet. Nach Belichtung erhöht sich die freie Elektronen-Konzentration um zwei Größenordnungen und bleibt über Tage erhöht.<ref name="DOI10.1103/PhysRevLett.111.187403">Marianne C. Tarun, Farida A. Selim, Matthew D. McCluskey: "Persistent Photoconductivity in Strontium Titanate." In: "Physical Review Letters." 111, 2013, S. , .</ref>

Der photovoltaische Effekt basiert ebenfalls auf dem inneren photoelektrischen Effekt. Ladungsträgerpaare, die in der Raumladungszone, also am p-n-Übergang einer Photodiode, entstehen, werden in p- und n-Schicht getrennt. Dabei gehen die Elektronen in die n-Schicht und die Löcher in die p-Schicht über und es entsteht ein Strom gegen die Durchlassrichtung des Übergangs. Dieser Strom wird Photostrom genannt.

Großflächige Photodioden (Solarzellen) dienen der Wandlung der Strahlungsenergie der Sonne in elektrische Energie.

Werden die Atome oder Moleküle eines Gases durch kurzwellige Strahlung eines oder mehrerer ihrer Elektronen beraubt, spricht man von "Photoionisation" oder auch "atomarem" oder "molekularem Photoeffekt." Dazu sind Photonen mit wesentlich höheren Energien nötig als für das Lösen der Bindung in einem Festkörper. Diese sind in Ultraviolett-, Röntgen- oder Gammastrahlung enthalten.

Wird das Photon absorbiert und gibt seine "gesamte" Energie an ein Elektron ab, wird dies in der Kernphysik gemeinhin als "Photoeffekt" bezeichnet. Dieser wird z. B. in Strahlungsdetektoren ausgenutzt. Daneben trägt zur Photoionisation auch der Compton-Effekt bei, bei dem das Elektron nur einen Teil der Energie übernimmt, während der Rest der Energie als Photon größerer Wellenlänge wieder emittiert wird.
Der Wirkungsquerschnitt formula_24, also die Wahrscheinlichkeit für das Eintreten der Photoionisation, hängt von der Photonenenergie formula_25 und der Ordnungszahl formula_26 des Materials ab:

Er ist also näherungsweise proportional der fünften Potenz der Ordnungszahl. Das bedeutet, dass Materialien mit hoher Ordnungszahl besonders gut Röntgen- und Gammastrahlung absorbieren. Blei (formula_28) ist daher besser zur Abschirmung von Röntgenstrahlung geeignet als beispielsweise Aluminium (formula_29).

Mit steigender Photonenenergie nimmt der Wirkungsquerschnitt ab, wie die negative Potenz in der Formel zeigt; dies gilt allerdings nur, solange eine gleichbleibende Zahl der Elektronen des Atoms zur Ionisation verfügbar ist. Sobald die Photonenenergie die Bindungsenergie der jeweils nächst fester gebundenen Elektronenschale erreicht, springt der Wirkungsquerschnitt auf einen entsprechend höheren Wert, von dem er dann bei weiterem Energieanstieg wieder allmählich abfällt. Dies führt im Absorptionsspektrum zu charakteristischen Strukturen, den "Absorptionskanten." Elektronen-Bindungsenergien reichen von wenigen eV bis zu rund 100 keV in Elementen hoher Ordnungszahl.

Die Photoionisation von Luft mittels Ultraviolettstrahlung durch Ionisatoren wird zur Erhöhung ihrer Leitfähigkeit und dadurch zur Ableitung elektrostatischer Aufladungen genutzt.

Die Messung der Leitfähigkeit der Luft wurde zum erstmaligen Nachweis der kosmischen Herkunft eines Teiles der natürlichen Radioaktivität herangezogen, indem sie bei Ballonaufstiegen gemessen wurde: die kosmische Strahlung erzeugt Schauer ionisierender Teilchen und teilweise radioaktive Spallationsprodukte.

Es gibt auch einen Kernphotoeffekt, bei dem ein sehr energiereiches Gamma-Quant im Atomkern absorbiert wird und mit einer Kernreaktion ein Neutron, Proton oder Alphateilchen freisetzt. Dies wird auch als (γ,n)-, (γ,p)- beziehungsweise (γ,α)-Reaktion bezeichnet.





</doc>
<doc id="60241" url="https://de.wikipedia.org/wiki?curid=60241" title="Hexagonales Kristallsystem">
Hexagonales Kristallsystem

Das hexagonale Kristallsystem gehört zu den sieben Kristallsystemen der Kristallographie. Es umfasst alle Punktgruppen mit einer sechszähligen Dreh- oder Drehinversionsachse. Das hexagonale Kristallsystem ist mit dem trigonalen Kristallsystem eng verwandt und bildet zusammen mit ihm die hexagonale Kristallfamilie.

Das hexagonale Kristallsystem umfasst die Punktgruppen formula_1 und formula_2. Dies sind alle die Punktgruppen der hexagonalen Kristallfamilie, in denen es keine Raumgruppe mit rhomboedrischer Zentrierung gibt. Die Raumgruppen des hexagonalen Kristallsystems können alle mit dem hexagonal primitiven Achsensystem beschrieben werden.
Die hexagonalen Punktgruppen haben keine kubische Obergruppe. Somit ist die hexagonale Holoedrie zusammen mit der kubischen die höchstsymmetrische kristallographische Punktgruppe.

In der hexagonalen Kristallfamilie gibt es das hexagonale und das trigonale Kristallsystem, sowie das hexagonale und das rhomboedrische Gitter-System. Die Einteilung in Kristallsysteme beruht auf der Symmetrie der Kristalle, die Einteilung in Gittersysteme bezieht sich auf die Metrik des Gitters. Während in den fünf anderen Kristallfamilien bzw. Kristallsystemen diese unterschiedlichen Sichtweisen zur selben Einteilung führen, ist dies in der hexagonalen Kristallfamilie nicht so. Darüber hinaus erfolgt hier die Einteilung in Gittersysteme nicht auf Basis der Punktgruppen, sondern der Raumgruppen. Da die Verhältnisse relativ kompliziert sind, werden sie an dieser Stelle ausführlicher beschrieben.

Wie in allen wirteligen Kristallsystemen wird die Drehachse mit der höchsten Zähligkeit in die Richtung der c-Gitterachse gelegt. Die Ebene senkrecht dazu wird durch zwei gleich lange Achsen a und a beschrieben, die im Winkel von 120° zueinander stehen. Daraus ergibt sich folgende Metrik: formula_3 und formula_4 Die durch diese Basisvektoren gebildete Elementarzelle ist in Bild 1 dargestellt. Sie hat ein Volumen von formula_5

In der Mineralogie und besonders in der Metallkunde ist es üblich, noch eine zusätzliche Achse a in der (a, a) Ebene zu verwenden (vgl. Bild 3). Diese hat dieselbe Länge wie a und steht im Winkel zu 120° sowohl zu a als auch zu a. Die Millerschen Indizes werden um den Index i zu so genannten "Miller-Bravais-Indizes" erweitert und haben dann vier Komponenten: (h, k, i, l). Dabei ist der Index i redundant, da gilt: i =  -(h+k). Ähnlich werden in der Metallkunde auch Richtungen durch viergliedrige Symbole [uvtw], die "Weber-Indizes", dargestellt.

Oft wird in der Literatur die hexagonale Zelle als sechseckiges Prisma dargestellt (vgl. Bild 2). Da dieses Prisma kein Parallelepiped ist, handelt es sich aber nicht um eine Elementarzelle. Dieses Prisma besteht aus drei hexagonalen Elementarzellen.

Bild 3 stellt die a-a-Ebene des hexagonalen Achsensystems dar. Im Einzelnen:


Bei der Betrachtung möglicher Zentrierungen kommt es in diesem Achsensystem zu einer Besonderheit. Fügt man zusätzliche Gitterpunkte so ein, dass die volle Symmetrie des hexagonalen Gitters erhalten bleibt, so ergeben sich nur Punktgitter, die auch durch ein primitives hexagonales Gitter (mit anderen Gitterkonstanten) beschrieben werden können.

Fügt man aber zusätzliche Gitterpunkte an den Stellen formula_6 und formula_7 beziehungsweise formula_8 und formula_9 ein, so ergibt sich ein neues Gitter, das aber nicht mehr die volle Symmetrie des hexagonalen Punktgitters, sondern die niedrigere Symmetrie formula_10 hat.

Dieses Gittersystem kann auch mit einer primitiven Elementarzelle beschrieben werden. Für die Metrik dieser Zelle gilt: formula_11 und formula_12.
Diese Elementarzelle hat die Form eines Rhomboeders, eines entlang seiner Raumdiagonalen verzerrten Würfels. Diese Elementarzelle ist zwar primitiv, aber nicht konventionell, da die dreizählige Achse nicht in Richtung eines Gittervektors, sondern in Richtung der Raumdiagonalen liegt.
Dieses Gittersystem wird rhomboedrisch genannt, hat die Holoedrie formula_10 und wird unabhängig von der Aufstellung (hexagonale oder rhomboedrische Achsen) als R-Gitter bezeichnet.

Die Lage der rhomboedrischen zu den hexagonalen Achsen hängt davon ab, welche der beiden Möglichkeiten zur Zentrierung der hexagonalen Zelle verwendet wurde. Im ersten Fall heißt die Aufstellung der Achsen obvers, im zweiten Fall revers. In der ersten Ausgabe der International Tables von 1935 wurde die reverse Aufstellung verwendet, in den darauffolgenden die obverse. Der Unterschied zwischen beiden Aufstellungen besteht in einer Drehung der hexagonalen zu den rhomboedrischen Achsen um 60°, 180° oder 300°.

Da dieses Gittersystem nicht die volle Symmetrie des hexagonalen hat, kommt es nicht in allen Punktgruppen der hexagonalen Kristallfamilie vor.

Das hexagonale Achsensystem wird zur Beschreibung aller Punktgruppen der hexagonalen Kristallfamilie eingesetzt. Punktgruppen deren Raumgruppen ausschließlich mit dem primitiv hexagonalen Gitter beschrieben werden können, bilden das hexagonale Kristallsystem. Alle Punktgruppen, in denen auch das rhomboedrisch zentrierte Gitter vorkommt, bilden das trigonale Kristallsystem. Auch in diesem System werden alle nicht zentrischen Raumgruppen mit dem hexagonalen Achsensystem beschrieben. Eine Beschreibung dieser Raumgruppen mit dem rhomboedrischen Gittersystem ist nicht möglich, auch wenn sie zur Holoedrie des rhomboedrischen Gittersystems gezählt werden. Nur bei den zentrischen Raumgruppen (Symbol R) hat man die Auswahl zwischen dem hexagonalen und dem rhomboedrischen Achsensystem.

Im Gegensatz zur rhomboedrischen Zelle ist die hexagonale Zelle eine konventionelle Zelle, daher wird in der Regel das hexagonale Achsensystem verwendet. Bei den Strukturdaten der Minerale spielt das rhomboedrische System nur eine untergeordnete Rolle.

Das Rhomboeder ist ein in Richtung der Raumdiagonalen verzerrter Würfel. Daher ist der Einsatz dieser Aufstellung in den Fällen angebracht, in denen eine kubische und eine rhomboedrische Struktur miteinander verglichen werden, da man hierbei das Achsensystem nicht ändern muss.

Da das hexagonale Achsensystem kein orthogonales System ist, ist seine Metrik komplizierter. Einer der Ansätze damit umzugehen ist die Beschreibung durch ein orthorhombisches Gittersystem, das sogenannte orthohexagonale System. Es handelt sich dabei um eine orthorhombisch C-zentrierte Zelle. Die Grundfläche dieses Systems ist ein Rechteck mit dem Seitenlängenverhältnis b:a von formula_14. Sie ist in Bild 3 grün eingezeichnet. Die dritte Achse entspricht der hexagonalen c-Achse.

Der Vorteil dieser Aufstellung ist die einfachere Metrik, der Nachteil ist der Verlust einer expliziten drei- bzw. sechszähligen Achse.

Bei der Beschreibung von Ober- beziehungsweise Untergruppen wird in den International Tables eine dreifach vergrößerte hexagonale Zelle, die sogenannte H-Zelle verwendet.

Es ist auch möglich das hexagonale Gitter mit sechs zentrierten rhomboedrischen Zellen zu beschreiben. Diese Zellen werden D-Zellen genannt. Zur Beschreibung von Strukturen werden sie nicht verwendet.

Die Einteilung der Kristalle in Kristallsysteme beruhte ursprünglich auf der Morphologie. Im trigonalen bzw. hexagonalen System wurden alle die Kristalle zusammengefasst, deren Kristallform auf das Vorhandensein einer drei- bzw. sechszähligen Drehachse schließen lässt. Da aber die sechszählige Drehinversionsachse eine dreizählige Kristallform bewirkt, wurden die Punktgruppen formula_15 (trigonal-dipyramidal) und formula_16 (ditrigonal-dipyramidal) anfangs zum trigonalen Kristallsystem gezählt, wie man an den Bezeichnungen für die Kristallformen heute noch sieht.

Zur Beschreibung der hexagonalen Kristallklassen in Hermann-Mauguin-Symbolik werden die Symmetrieoperationen bezüglich vorgegebener Richtungen im Gitter-System angegeben.

Im hexagonalen Achsensystem: 1. Symbol in Richtung der c-Achse (<001>). 2. Symbol in Richtung einer a-Achse (<100>). 3.Symbol in einer Richtung senkrecht zu einer a und der c-Achse (<120>). Für die 3. Richtung wird auch oftmals die im Allgemeinen nicht äquivalente Richtung <210> angegeben. Auch wenn dies speziell für die Angabe der Lage der Symmetrieelemente keine Rolle spielt, so entspricht diese Angabe nicht den Konventionen.

Charakteristisch für alle Raumgruppen des hexagonalen Kristallsystems ist die 6 (oder ) an 1. Stelle des Raumgruppensymbols.

Weitere hexagonal kristallisierende chemische Stoffe siehe 

Die hexagonal dichteste Kugelpackung kann wie folgt beschrieben werden: Die Verbindungslinien benachbarter Atome bilden einen Körper mit sechseckiger Grundfläche und Deckfläche. In der Mitte dieser beiden Flächen befindet sich je ein weiteres Atom. Zwischen Grund- und Deckfläche haben zusätzlich drei Atome Platz. Mit der Annahme von gleich großen Kugeln entspricht dies einer dichtesten Kugelpackung, deren Raumerfüllung ca. 74,05 % beträgt. Die Stapelabfolge lässt sich mit ABA beschreiben. So findet man hier auch die Bezeichnung "hexagonal-dichteste Kugelpackung" (hdp, engl. ). Hierbei beträgt das Seitenverhältnis im idealen Fall: formula_17.

Eine Elementarzelle mit hexagonal dichtester Packung (hdp) besteht aus zwei rautenförmigen Grundflächen. Die Atome befinden sich innerhalb der Elementarzelle auf den kristallographischen Lagen 1/3, 2/3, 1/4 und 2/3, 1/3, 3/4 (ein Symmetriezentrum der Struktur liegt dann konventionsgemäß in 0, 0, 0).

Viele Metalle kristallisieren in einer hexagonal dichtesten Kugelpackung: Be, Mg, Sc, Ti, Co, Zn, Y, Zr, Tc, Ru, Cd, Lu, Hf, Re, Os, Tl und einige Lanthanoide.
Als prominentester Vertreter gilt Magnesium, weshalb dieser Strukturtyp auch Magnesiumtyp genannt wird.



</doc>
<doc id="74651" url="https://de.wikipedia.org/wiki?curid=74651" title="Signal-Rausch-Verhältnis">
Signal-Rausch-Verhältnis

Das Signal-Rausch-Verhältnis, auch "Störabstand" formula_1 oder "(Signal-)Rauschabstand" formula_2, abgekürzt SRV oder S/R beziehungsweise SNR oder S/N von , ist ein Maß für die technische Qualität eines Nutzsignals (z. B. Sprache oder Video), das von einem Rauschsignal überlagert ist. Es ist definiert als das Verhältnis der mittleren Leistung des Nutzsignals zur mittleren Rauschleistung des Störsignals.

Das SNR ist ein Begriff aus der Hochfrequenz-, Mess- und Nachrichtentechnik sowie der Akustik, der auch in vielen weiteren Bereichen wie etwa der Automatisierungstechnik oder der Signal- und Bildverarbeitung verwendet wird. Verwandte Größen sind das "Spitzen-Signal-Rausch-Verhältnis" (PSNR), das "Träger-Rausch-Verhältnis" (C/N) und das "Träger-Interferenz-Verhältnis" (C/(I+N) oder C/I).


Das Signal-Rausch-Verhältnis ist definiert als das Verhältnis der vorhandenen mittleren Signalleistung "P" zur vorhandenen mittleren Rauschleistung "P" (dem Integral der spektralen Rauschleistungsdichte über die Bandbreite), wobei der Ursprung der Rauschleistung nicht berücksichtigt wird.

Als Verhältnis von Größen gleicher Maßeinheit ist das Signal-Rausch-Verhältnis dimensionslos. Es ist also:

Da die Signalleistung bei vielen technischen Anwendungen um mehrere Größenordnungen größer ist als die Rauschleistung, wird das Signal-Rausch-Verhältnis oft im logarithmischen Maßstab dargestellt. Man benutzt dazu die Pseudoeinheit Dezibel (dB):

Bei niedrigen Frequenzen und schmalbandiger elektromagnetischer Nutzsignal- und Rauschleistung können Signal-Rausch-Verhältnisse über effektive Spannungs- oder Stromamplituden ausgedrückt werden (→ Rauschspannung). Das ist z. B. in der Audiotechnik üblich. Da die verfügbaren Leistungen in diesem Fall dem Quadrat des Effektivwerts der Spannungen ("u", "u") proportional ist, gilt:
woraus folgt:

Eine alternative Definition des Signal-Rausch-Verhältnisses wird überwiegend beispielsweise in der Spektroskopie oder der Bildverarbeitung (insbesondere in der medizinischen Bildgebung) verwendet. Hier ist das SNR definiert als Verhältnis der mittleren Signal"amplitude" "A" (anstelle der Leistung) und der Standardabweichung "σ" des Rauschens:
Dies ist von der vorhergehenden Definition auf Basis der Spannungsamplituden zu unterscheiden, da dort zunächst die Leistung mittels der quadrierten Amplituden berechnet wird, während hier das nicht-quadrierte Amplitudenverhältnis zugrunde liegt. Bei Verwendung dieser Definition ist auch die Umrechnung in Dezibel weniger häufig zu finden; das SNR wird meist als einheitenlose Größe der Dimension 1 angegeben.

Bei Modulationsverfahren wie der Phasenmodulation oder Frequenzmodulation lassen sich Signal- und Trägerleistung nicht voneinander trennen. Deshalb bezieht man dort das Rauschen nicht auf das Signal "S", sondern den Träger "C" (engl. ). Das Verhältnis heißt Träger-Rausch-Verhältnis (engl. , kurz C/N).

Neben dem Rauschen können auch Interferenzen "I" das Signal überlagern. Dabei kann das Signal sowohl mit sich selbst durch Mehrwegeempfang, verursacht durch Reflexionen, interferieren, als auch mit ähnlichen Signalen, beispielsweise von Nachbarfunkzellen beim Mobilfunk. Je nachdem, ob die Rauschleistung mit berücksichtigt wird, kürzt man das Träger-Interferenz-Verhältnis ab als C/I oder C/(I+N).

Das Träger-Rausch-Verhältnis C/N einer Funkstrecke verbessert sich mit der Sendeleistung "P" und den Antennengewinnen "G" und "G" von Sender und Empfänger. Sie verringert sich mit der Rauschleistung, dem Produkt aus Boltzmann-Konstante "k", Rauschtemperatur "T" und Bandbreite "B". Zusätzlich nimmt sie mit der Freiraumdämpfung "F" = (4π·"R"/λ) ab ("R" ist der Abstand, λ die Wellenlänge):
Ein Umstellen der Größen liefert den Zusammenhang zwischen Träger-Rausch-Verhältnis und Empfangsgüte (G/T).

Wird ein Bild oder Video komprimiert übertragen, muss es an der Empfängerseite dekomprimiert und dargestellt werden. Als Kenngröße für die Qualität dieser Übertragung wird das Spitzen-Signal-Rausch-Verhältnis (PSNR von engl. ) verwendet. Typische Werte sind, bei einer Bittiefe von 8 Bit, 30 dB bis 40 dB. Bei einer Bittiefe von 16 Bit sind Werte zwischen 60 dB und 80 dB üblich.

Als Störwert wird üblicherweise die mittlere quadratische Abweichung (englisch , MSE) verwendet, die für zwei "m"×"n"-Schwarz-Weiß-Bilder "I" und "K", eines davon das Original, das andere die gestörte Annäherung (z. B. durch (verlustbehaftetes) Komprimieren und Dekomprimieren), folgendermaßen angegeben wird:

Das PSNR ist damit definiert als:

"I" ist die maximal mögliche Signalintensität (bei einem Bild der maximal mögliche Pixelwert). Werden 8 Bit zur Darstellung eines abgetasteten Wertes verwendet, ist das 255. Falls mit linearer Puls-Code-Modulation (PCM) gearbeitet wird, sind das im Allgemeinen "B" Bits für einen abgetasteten Wert; der maximale Wert von "I" ist dann 2−1.

Für Farbbilder mit drei RGB-Werten pro Pixel ist die Definition des PSNR dieselbe; die MSE ist dann die Summe über alle Differenzwerte dividiert durch die Bildgröße und dividiert durch 3.

Diese Metrik ignoriert jedoch viele Effekte im visuellen System des Menschen, andere Metriken sind (SSIM, englisch für „strukturelle Ähnlichkeit“) und DVQ.

Die um Kontrastwahrnehmungs- und Maskierungskriterien erweiterte Metrik PSNR-HVS-M bietet nach einer Untersuchung der Entwickler von 2007 die bis dahin beste Annäherung an die subjektiven Bewertungen menschlicher Beobachter, mit großem Vorsprung vor PSNR, UQI und MSSIM aber auch deutlichem Abstand zu DCTune und PSNR-HVS.

Je mehr über das Nutzsignal bekannt ist, desto stärker lässt sich das SNR anheben. Einige Verfahren zur SNR-Verbesserung sind in den folgenden Abschnitten aufgezählt.

Bei konstantem Rauschanteil steigt die SNR, wenn man das Nutzsignal vergrößert. In einer lärmenden Menschenmenge ist Flüstern kaum zu verstehen, während lautes Rufen deutlich wahrzunehmen ist.

Bei konstantem Rauschen (z. B. eines Magnetbands) ist das SNR für kleine Signale sehr klein. Kompressor/Expander-Systeme, die sogenannten Kompander, reduzieren deshalb den Dynamikbereich. Beispielsweise werden beim System "Dolby" leise Abschnitte mit überhöhter Lautstärke aufgenommen. Das Verfahren stellt sicher, dass sich das System bei der Wiedergabe an die richtige Lautstärke erinnert.

Rauschen tritt im gesamten Frequenzspektrum auf. Um es zu begrenzen, filtert man es außerhalb der Bandbreite des Systems aus. Beispielsweise sorgt beim Telefon ein Tiefpassfilter dafür, dass die Frequenzen oberhalb von ca. 3 kHz unterdrückt werden.

Bei digitalen Übertragungsverfahren (z. B. Telefonmodem, jegliche Art von digitaler drahtloser Datenübertragung) wird im Empfänger zur Optimierung des SNRs ein signalangepasstes Filter (engl. ) verwendet. Vereinfacht gesprochen wird im Empfänger die gleiche Filtercharakteristik angewendet wie im Sender. Häufig findet hier ein Root-Raised-Cosine-Filter Verwendung.

Ist man nicht am gesamten Signal interessiert, sondern beispielsweise nur an dessen Frequenz, kann man das Signal durch Autokorrelation verstärken.

Obwohl das Rauschen deutlich gemindert wird, wird auch das Nutzsignal abgeschwächt. Mit dieser Methode kann man die Cramer-Rao-Grenze nicht unterschreiten. Die Cramer-Rao-Grenze gibt die Mindestgröße für die Frequenzunsicherheit in Abhängigkeit von der Abtastfrequenz, der Anzahl der vorhandenen Signalperioden und dem SNR an.

Durch mehrfaches Senden einer Information lässt sich das Rauschen reduzieren. Da Rauschen stochastisch auftritt, wächst die Standardabweichung des Rauschsignals bei Summation von formula_11 Übertragungen nur um den Faktor formula_12, während das Signal um den Faktor formula_11 zunimmt. Das SNR bezogen auf die Signalamplituden (eine übliche Konvention in der Bildverarbeitung) steigert sich um formula_14. Dies ergibt sich aus dem zentralen Grenzwertsatz.
Das Teilbild links ist eines von 8 Bildern, die mit einer gaußschen Unschärfe von ca. 80 Grauwertunterschieden verrauscht wurden. Das Ergebnis der Mittelung zweier Bilder zeigt das mittlere Teilbild. Die SNR hat von ca. 6 dB um formula_15 auf 7 dB zugenommen. Nach der Summation von 8 Bildern, rechtes Teilbild, steigt es um formula_16 auf ca. 10 dB. Das SNR der Bilder wurde aus dem Verhältnis von Kontrastumfang des Bildes und Streuung eines kontrastarmen Teilbereichs bestimmt.

Die Mittelung von Bilddaten wird zum Beispiel gerne in der Astronomie eingesetzt, etwa bei der Lucky-Imaging-Technik. Durch die Erdatmosphäre hindurch sind prinzipiell sehr scharfe Aufnahmen möglich, aber Langzeitbelichtungen leiden unter der Unruhe der Luft – die Sterne wirken verschwommen. Fertigt man nun mehrere tausend Kurzzeit-Aufnahmen an, sind aus reinem Zufall (deshalb der Name der Methode) einige hundert davon ziemlich scharf. Diese Bilder werden dann gemittelt, um das Signal-Rausch-Verhältnis zu verbessern und eine Langzeitaufnahme zu rekonstruieren.





</doc>
<doc id="96354" url="https://de.wikipedia.org/wiki?curid=96354" title="Mittlere freie Weglänge">
Mittlere freie Weglänge

Die mittlere freie Weglänge formula_1 ist die Weglänge, die ein Teilchen (z. B. Atom, Molekül, Ion oder Elektron) in einem gegebenen Material im Durchschnitt zurücklegt, bevor es zum Stoß (irgendeiner Art) mit einem anderen Teilchen kommt. Hat ein Teilchenstrom in einem Material die mittlere freie Weglänge durchlaufen, so haben knapp 2/3 der Teilchen bereits einen Stoß ausgeführt, das übrige Drittel (genau ein Bruchteil 1/"e") noch keinen.

Die mittlere freie Weglänge hängt mit der Teilchendichte formula_2  (Anzahl der Teilchen pro Volumen) und dem totalen Wirkungsquerschnitt formula_3 zusammen:

Anschaulich ist formula_3 die Größe der Zielscheibe, die ein Teilchen den anderen Teilchen für einen Stoß bietet. Nachdem es eine freie Weglänge formula_1 geflogen ist, hat es mit dieser Fläche das Volumen formula_7 überstrichen, das ist das Volumen, in dem sich durchschnittlich ein Teilchen befindet. Das Teilchen ist also im Durchschnitt einmal mit einem anderen Teilchen zusammengestoßen.

Der geometrische Wirkungsquerschnitt beim Stoß zweier kugelförmiger Teilchen mit gleichem Durchmesser formula_8 ergibt sich zu

Daraus folgt die geometrische mittlere freie Weglänge zu

Die oben gegebene Deutung gilt, wenn die Stoßpartner des fliegenden Teilchens in Ruhe sind.

Wenn sich aber "alle" Teilchen ungeordnet bewegen (d. h. auch die Stoßpartner des fliegenden Teilchens), führen Gleichgewichtsbetrachtungen unter Annahme einer Maxwellschen Geschwindigkeitsverteilung zu einer freien Weglänge, die um den Faktor formula_11 kürzer ist:

In einem Raumbereich, der zwei Arten von Teilchen enthält, sind drei Arten von Stößen möglich:
Die Teilchendichten der Teilchenarten seien formula_13 bzw. formula_14 und die Wirkungsquerschnitte formula_15, formula_16 und formula_17. 

Die mittleren freien Weglängen für Stöße von Teilchen jeweils an ihresgleichen sind schon mit obiger Formel definiert:

bzw. 

Entsprechend definiert man die mittlere freie Weglänge
eines Teilchens vom Typ 2 im Medium vom Typ 1:

bzw. analog formula_21 beim Stoß eines Teilchens vom Typ 1 im Medium vom Typ 2:

wobei in beiden Fällen der Wirkungsquerschnitt formula_17 gleich ist. 

Der Wirkungsquerschnitt und die mittlere freie Weglänge zweier unterschiedlicher Teilchen werden meist ohne Index oder andere Auszeichnungszeichen geschrieben, also formula_24 und formula_25:

Der Index der Anzahldichte formula_13 wird in der Regel ebenfalls weggelassen, wenn es nur um die mittlere freie Weglänge von Teilchen in irgendeinem Medium mit Teilchen vom Typ 1 geht. Dann scheinen die eingangs dieses Artikels gegebene Definition der mittleren freien Weglänge und die zuletzt gegebene Gleichung formelmäßig gleich zu sein (und werden deshalb auch manchmal verwechselt). Die Anzahldichten der Teilchen formula_2 und formula_13 sind in Wirklichkeit aber unterschiedliche Größen: bei formula_2 handelt es sich um die Anzahldichte der Teilchen von nur einem Typ, bei formula_13 aber um die Anzahldichte der Teilchen des Mediums.

Ähnlich wie beim Billardspiel bewirkt der elastische Stoß zweier Teilchen Richtungsänderungen beider Teilchen, allerdings, im Unterschied zum Billardspiel, im dreidimensionalen physikalischen Raum und mit Teilchen unterschiedlicher Größe. Häufig bedeutet ein Stoß zweier unterschiedlicher Teilchenarten auch, dass sie eine Reaktion miteinander eingehen. Sind die Stoßpartner zum Beispiel zwei Atome, kann ein Molekül gebildet werden, sind die Stoßpartner ein Neutron und ein Atomkern, kann ein anderes Nuklid entstehen oder ein Atomkern gespalten werden.

Implizit gehen wir davon aus, dass beide Stoßpartner Teilchen eines Gases sind, denn nur dort können sich "beide" Stoßpartner frei bewegen, was überhaupt erst nahelegt, von einer freien Weglänge zu sprechen. Es gibt aber auch Stöße und mittlere freie Weglängen, wenn ein Stoßpartner ein Teilchen eines Festkörpers oder einer Flüssigkeit ist (Teilchen vom Typ 1), und sich nur der zweite Stoßpartner wie ein Teilchen eines Gases verhält (Teilchen vom Typ 2). Dabei sind in der Regel Stöße von Teilchen vom Typ 1 mit Teilchen vom Typ 2 von Interesse (und nicht Stöße der beiden jeweiligen Teilchen-Typen mit ihresgleichen).

Der geometrische Wirkungsquerschnitt beim elastischen Stoß zweier starrer Kugeln mit den Radien formula_32 bzw. formula_33 ist

Damit wird die geometrische mittlere freie Weglänge:

Der geometrische Wirkungsquerschnitt und damit die geometrische mittlere freie Weglänge der elastischen Streuung hängen also "nicht" von den kinetischen Energien der Kugeln ab. "Reale" Wirkungsquerschnitte und damit die mittleren freien Weglängen können dagegen stark von der kinetischen Energie der Stoßpartner abhängen und sind folglich nicht unbedingt durch das o. g. einfache geometrische Modell zu berechnen. Auch im Fall "realer" Wirkungsquerschnitte kann es jedoch hilfreich sein, den geometrischen Wirkungsquerschnitt als Bezugsgröße zu verwenden, in der Art: der reale Wirkungsquerschnitt ist 10-mal größer als der geometrische, dann ist die reale mittlere freie Weglänge nur ein Zehntel der geometrischen.

Stöße von Neutronen und Atomkernen sind (gegenwärtig) der wichtigste Fall in der Physik für Stöße von zwei Arten von Teilchen, sie prägen die Reaktorphysik.

Wenn in der Reaktorphysik von "mittlerer freier Weglänge" die Rede ist, ist stets die zweite Definition dieser Größe gemeint, also die mittlere freie Weglänge von Neutronen in Materie. Dabei bewegen sich freie Neutronen (Teilchen vom Typ 2 mit einer Teilchendichte formula_14) in einem Festkörper oder einer Flüssigkeit („Wirtsmedium“) i. allg. so chaotisch wie Moleküle in einem Gas. Wir nehmen an, das Wirtsmedium bestehe aus nur einer Teilchen- bzw. einer Atomart (Teilchen von Typ 1), man denke etwa an Graphitatome. Da jedes Atom nur einen Atomkern besitzt, ist die Teilchendichte formula_13 der Atome gleich der ihrer Atomkerne.

Der Kehrwert der mittleren freien Weglänge diese Typs ist unter dem Namen "Makroskopischer Wirkungsquerschnitt" eine der wichtigsten Größen der Reaktorphysik:

Wirkungsquerschnitte von Kernreaktionen hängen extrem stark von der Energie ab und sind somit auch nicht mehr geometrisch zu erklären. Nur im Fall der elastischen Streuung von Neutronen an den Atomkernen gebräuchlicher Moderatoren führt das oben angegebene geometrische Modell auf mittlere freie Weglängen, die in der Größenordnung der gemessenen Werte liegen; dies zumindest für Neutronen mit kinetischen Energien in einem gewissen mittleren Intervall. 

Neutronen untereinander stoßen sich auch. Dies ist ein Fall für die erste Definition der mittleren freien Weglänge. Die Anzahldichte der Neutronen ist selbst im Hochflussreaktor vergleichsweise gering. Klein ist auch der Wirkungsquerschnitt für den Stoß zweier Neutronen. Deshalb wird vermutlich in keinem Lehrbuch der Reaktor- oder Neutronenphysik die mittlere freie Weglänge für diesen Typ von Stoß auch nur erwähnt.

Atomkerne untereinander können sich nicht stoßen. Selbst im Fall, dass die Neutronen sich in Helium-Gas bewegen, wie im Hochtemperaturreaktor, stoßen allenfalls Helium-Atome aufeinander. Allerdings lässt sich, falls erforderlich, die mittlere freie Weglänge für solche Atomstöße mit der o. g. ersten Definitionsformel und dem geometrischen Wirkungsquerschnitt berechnen, der denjenigen der Atomkerne um Größenordnungen übertrifft.

Die mittlere freie Weglänge eines Gasmoleküls beträgt in Luft unter Standardbedingungen etwa 68 Nanometer.

Nachfolgende Tabelle listet ungefähre Zahlen für freie Weglängen für Gasmoleküle bei verschiedenen Drücken auf:

Die mittlere freie Weglänge freier Elektronen ist wichtig bei Anwendungen von Elektronenstrahlen im Vakuum (z. B. bei bestimmten oberflächensensitiven analytischen Methoden oder in Braunschen Röhren). Sie hängt ab von der kinetischen Energie des Elektrons.

Die inelastische freie Weglänge im Festkörper kann für die meisten Metalle mit einer „Universellen Kurve“ abgeschätzt werden (s. Abb.): bei Energien um 100 eV ist sie für die meisten Metalle am geringsten, da hier Prozesse im Festkörper angeregt werden können, z. B. Plasmonen; bei höheren und niedrigeren Energien sind die mittleren freien Weglängen im Festkörper größer. Der Wirkungsquerschnitt für elastische Stöße ist meist kleiner als der für inelastische Stöße, damit ist bei gleicher Teilchendichte die elastische freie Weglänge "größer" als die inelastische.

In gasförmigen Isolierstoffen (z. B. Schwefelhexafluorid) beeinflusst die mittlere freie Weglänge die elektrische Durchschlagfestigkeit.

Für Elektronen im Impulsraum (siehe Fermi-Kugel) betrachtet man statt der Weglänge die mittlere freie Flugzeit.


</doc>
<doc id="105513" url="https://de.wikipedia.org/wiki?curid=105513" title="Max-Planck-Medaille">
Max-Planck-Medaille

Die Max-Planck-Medaille ist eine Auszeichnung, die seit 1929 jährlich von der Deutschen Physikalischen Gesellschaft (DPG) für besondere Leistungen auf dem Gebiet der Theoretischen Physik verliehen wird. Diese Auszeichnung gilt als die bedeutendste in diesem Fach in Deutschland. Sie besteht aus einer Urkunde und einer goldenen Medaille mit dem Porträt Max Plancks.

Die entsprechende höchste Auszeichnung der DPG für Leistungen auf dem Gebiet der experimentellen Physik ist die Stern-Gerlach-Medaille.
Die Beschaffung der Planck-Medaille war 1943 ein Problem, da die Berliner Gießerei durch eine Bombe getroffen wurde. Der Vorstand der Deutschen Physikalischen Gesellschaft entschied sich, die Medaillen zunächst in einem Ersatzmetall herzustellen und später in Gold nachzuliefern.




</doc>
<doc id="148129" url="https://de.wikipedia.org/wiki?curid=148129" title="Nebelkammer">
Nebelkammer

Als Nebelkammer wird in der Physik ein Teilchendetektor bezeichnet, der dem Nachweis von ionisierender Strahlung dient und für manche Teilchen dabei auch deren Weg sichtbar macht. Nebelkammern werden heute fast nur noch zu Demonstrationszwecken verwendet. Früher waren Nebelkammern bedeutende wissenschaftliche Instrumente zur Erforschung der von radioaktiven Stoffen ausgehenden Strahlen. So wurde Charles Thomson Rees Wilson für die Entwicklung der Expansionsnebelkammer (auch Wilsonschen Nebelkammer) 1927 mit dem Nobelpreis für Physik ausgezeichnet.

Eine Nebelkammer ist meist mit einem übersättigten Luft-Alkohol-Gemisch (Ethanol oder Isopropanol) gefüllt. Wenn ein energiereiches, geladenes Teilchen das Gas durchquert, erzeugt es durch Stoßionisation zahlreiche Ionen, die einzeln als Kondensationskerne für die Bildung feinster Tröpfchen wirken. In ihrer Gesamtheit bilden sie eine sichtbare Spur, einen Kondensstreifen.
Durch Ablenkung des Teilchens mittels eines geeigneten elektrischen oder magnetischen Feldes können anhand der entstehenden Bahnkurven (siehe Abb.) Aussagen über die Masse, Ladung und Energie, und damit letztlich über die Art des betreffenden Teilchens und dessen Entstehungsprozess gemacht werden. In einfachen Nebelkammern befindet sich dazu meist ein starker Permanentmagnet am Boden der Kammer, der die geladenen Teilchen mittels der Lorentzkraft auf eine Spiralbahn zwingt (die Krümmung nimmt zu, weil das Teilchen durch die Stöße abgebremst wird).

Auch ohne Präparat befindet sich in unserer Umwelt ein gewisses Maß an Alpha- und Betastrahlung, die man mit der Nebelkammer sichtbar machen kann:

Je nach Art der Erzeugung des übersättigten Luft-Alkohol-Gemischs wird zwischen nicht-kontinuierlichen sowie kontinuierlichen Nebelkammern unterschieden:

Die Wilsonsche Nebelkammer (benannt nach ihrem Erfinder Charles Thomson Rees Wilson) erzeugt die Übersättigung durch eine schnelle Expansion. Durch Herausziehen eines Kolbens (siehe nebenstehende Abb.) vergrößert sich das Volumen der Luft in der Nebelkammer, der Druck und damit auch die Temperatur sinkt. Dadurch ist der Dampf übersättigt und man braucht nur kleine Kondensationskeime, um eine Nebelspur zu erzeugen. Da die Luft nur kurze Zeit abkühlt, ist die Expansionsnebelkammer nur ungefähr eine Sekunde lang fähig, Nebelspuren zu erzeugen. Man kann somit nur einen kurzen "Schnappschuss" erzeugen und muss nach einer Pause dann erneut den Kolben herausziehen.

Die Diffusionsnebelkammer (von Alexander Langsdorf 1936 erfunden) erzeugt die Übersättigung durch eine Kühlung der Bodenplatte auf ca. −30 °C. Ungefähr 10 cm über dem Boden befinden sich Heizdrähte, die das Luft-Alkohol-Gemisch im oberen Bereich auf einer Temperatur von ca. +15 °C halten. Zwischen Boden und Decke gibt es somit ein Temperaturgefälle und es entsteht knapp über dem Boden eine übersättigte Schicht, in der die Erzeugung von Nebelspuren möglich ist. Die Diffusionsnebelkammer kann viele Stunden in Betrieb bleiben. Nebelspuren, die sich an den Ionen bilden, verschwinden eher wieder und lassen neue Spuren eher sichtbar werden, wenn durch eine „Saugspannung“ zwischen Boden und Decke die freien Ionen der alten Nebelspuren immer wieder „abgesaugt“ werden. Ein solcher „Ionensauger“ ist nützlich, aber nicht zwingend notwendig.



</doc>
<doc id="150927" url="https://de.wikipedia.org/wiki?curid=150927" title="Größenordnung (Masse)">
Größenordnung (Masse)

Dies ist eine Zusammenstellung von Massen verschiedener Größenordnungen zu Vergleichszwecken. Die Angaben sind oft als „typische Werte“ zu verstehen; die umgerechneten Werte sind gerundet.

Grundeinheit der Masse im internationalen Einheitensystem ist 1 Kilogramm (Einheitenzeichen "kg"), das Formelzeichen "m".












</doc>
<doc id="278639" url="https://de.wikipedia.org/wiki?curid=278639" title="Pierre Auger">
Pierre Auger

Pierre Victor Auger () (* 14. Mai 1899 in Paris; † 24. Dezember 1993 ebenda) war ein französischer Physiker. Er arbeitete im Bereich der Atom- und Kernphysik.

Besonders beschäftigte sich Auger mit der kosmischen Strahlung, die er 1938 am Schweizer Jungfraujoch in 3.500 m Höhe detailliert untersuchte. Mit mehreren Nachweisgeräten in 300 m Abständen gelang es ihm, gleichzeitige Ereignisse in benachbarten Detektoren zu messen und so den kosmischen Ursprung der Teilchenschauer nachzuweisen.

Im Jahr 1926 entdeckte er bei Untersuchung der Einwirkung von Röntgenstrahlen auf Materie den nach ihm benannten Auger-Effekt. Allerdings wurde dieser strahlungslose Übergang schon 1922 von der österreichisch-schwedischen Physikerin Lise Meitner entdeckt.

Am Ende des Zweiten Weltkriegs stellte sich Pierre Auger mit einer Handvoll anderer visionärer Wissenschaftler die Einrichtung eines europäischen Atomphysiklabors vor. Dies führte zur Gründung des Europäisches Organisation für Kernforschung (CERN).

Auger war ab 1937 Professor an der Sorbonne in Paris und von 1962 bis 1967 Generaldirektor der European Space Research Organisation (ESRO). 1961 wurde er mit dem internationalen Antonio-Feltrinelli-Preis ausgezeichnet, 1971 mit dem Kalinga-Preis für die Popularisierung der Wissenschaft.

Nach Auger ist das multinationale Pierre-Auger-Observatorium benannt, das seit November 2005 in der Pampa Amarilla in Argentinien mit 1600 Teilchendetektoren auf 3000 km² sowie 24 Fluoreszenzteleskopen die höchstenergetische kosmische Strahlung erforscht.

Seit 1977 war er Mitglied der Académie des sciences.




</doc>
<doc id="332848" url="https://de.wikipedia.org/wiki?curid=332848" title="Bewegungszustand">
Bewegungszustand

Als Bewegungszustand bezeichnet man in der Mechanik die momentane Bewegung eines Körpers. Diese kann in einer Translations- und/oder Rotationsbewegung bestehen.



Die gleichförmige Bewegung ist ein Beispiel für eine Bewegung, bei der der Bewegungszustand unverändert bleibt. Dagegen wird bei einer gleichförmigen Kreisbewegung eines Körpers der Bewegungszustand nicht beibehalten, denn hier ändert sich fortwährend die Richtung der Geschwindigkeit.

Der Trägheitssatz oder das erste newtonsche Gesetz der Mechanik besagt, dass jeder Körper, der nicht von äußeren Kräften beeinflusst wird, in seinem Bewegungszustand verharrt. Mit anderen Worten ist das Bestreben eines Körpers, seinen Bewegungszustand beizubehalten, Ausdruck seiner Trägheit. Insbesondere bewegt sich bei einem Körper ohne äußere Kräfte der Massenmittelpunkt mit gleichbleibender Geschwindigkeit geradlinig weiter. Im Fall der Rotation um den Massenmittelpunkt bleibt dann der Drehimpuls nach Betrag und Richtung konstant, jedoch nicht unbedingt die Drehachse und Rotationsgeschwindigkeit.

Im scheinbaren Gegensatz zum Trägheitssatz ist es Alltagserfahrung, dass ein sich bewegender Körper gerade dann langsamer wird, wenn keine Kraft feststellbar ist, die ihn antreibt. Das erklärt sich dadurch, dass bei jeder Bewegung Bremskräfte wie der Luftwiderstand und sonstige Reibungskräfte vorhanden sind. Diese sind für die Abbremsung des Körpers, also die Änderung seines Bewegungszustandes, ursächlich. Allgemein besagt das zweite newtonsche Gesetz der Mechanik, wie sich der Bewegungszustand ändert, wenn eine resultierende äußere Kraft auf den Körper wirkt.


</doc>
<doc id="389882" url="https://de.wikipedia.org/wiki?curid=389882" title="Teilchen">
Teilchen

In der Physik bezeichnet man als Teilchen einen Körper, der klein gegenüber dem Maßstab des betrachteten Systems ist. Die innere Struktur eines einzelnen Teilchens spielt dabei keine Rolle, sondern lediglich sein Verhalten als Ganzes gegenüber anderen Teilchen oder äußeren Einflüssen. Oft werden die Teilchen dann als ausdehnungslose Punktteilchen (im Sinne von Punktmassen) aufgefasst. Teilchen sind ideale Objekte. In der Regel beschränkt man sich nur auf bestimmte Eigenschaften des realen physikalischen Objekts, wie die Masse oder die elektrische Ladung, um die Wechselwirkung zu studieren, die mit dieser Eigenschaft zusammenhängt. Je nach Betrachtungsweise kann also ein und dasselbe physikalische Objekt als Teilchen oder als System von Teilchen angesehen werden. Das gilt insbesondere für Atome, aber auch für Atomkerne und auch für die Protonen und Neutronen. Die nach derzeitigem Verständnis nicht mehr aus kleineren Bestandteilen zusammengesetzten Teilchen werden als Elementarteilchen bezeichnet und im Standardmodell der Elementarteilchenphysik beschrieben (siehe "Standardmodell").

Das Wort Teilchen wird auch als Kurzwort für Elementarteilchen benutzt. Diese bedeuten einerseits „die kleinsten Bausteine der Materie“, die nicht wiederum aus kleineren Teilchen zusammengesetzt sind, andererseits bezogen auf „Austauschteilchen“ wie das Photon, welche die elementaren Kräfte vermitteln.

In der Quantenmechanik wird ein Teilchen durch eine Wellenfunktion dargestellt, deren Amplitude die Aufenthaltswahrscheinlichkeit des Teilchens angibt (siehe Quantenmechanische Sichtweise).

In der Festkörperphysik redet man sowohl bei den Gitteratomen von Teilchen als auch bei den Wellen, mit denen sich deren Anregungen über einem Grundzustand ausbreiten. Dies führt dazu, dass dabei eine Vielzahl von Erscheinungen als Teilchen idealisiert werden, deren Verhalten so anschaulicher beschrieben werden kann: So werden in der quantenphysikalischen Beschreibung die Anregungen eines Kristallgitters als Teilchen aufgefasst, beispielsweise als Polaronen, Excitonen oder Phononen. Löcher in den ansonsten voll besetzten Energiebändern der Elektronen in einem Halbleiter weisen die Charakteristika von Teilchen auf und werden wie positiv geladene Teilchen behandelt.

Der Begriff "Partikel" ist im Allgemeinen nicht für Teilchen zu verwenden. In bestimmten Bereichen werden diese beiden Begriffe andererseits vollkommen synonym gebraucht: 

In der Hydrodynamik ist mit Teilchen manchmal ein Volumenelement des Fluids gemeint. Dieses Teilchen ist zwar „klein“, aber makroskopisch, d. h., es enthält so viele Moleküle, dass ihm außer den mechanischen Eigenschaften Ort und Impuls auch Eigenschaften der Thermodynamik wie Druck, Temperatur und Entropie zugeschrieben werden können.

Die Bezeichnung ‚Korpuskel‘ für Teilchen ist veraltet. Sie tritt beispielsweise in der historischen Auseinandersetzung zwischen Korpuskeltheorie und Wellentheorie bei der Beschreibung des Lichts auf.

Im µm-Bereich bewegt sich die Ausdehnung von Staubpartikeln.

Im 5. Jahrhundert v. Chr. postulierte Demokrit, dass die Materie aus kleinsten, unteilbaren Einheiten zusammengesetzt ist. Diesem Gedanken folgend verwendete John Dalton 1803 für die kleinsten, seiner Meinung nach untrennbaren Teilchen die Bezeichnung Atom (von altgriechisch "" „nicht zerschneidbar, unteilbar“).

Atome als untrennbare Teilchen zu betrachten, ergibt in der Chemie durchaus Sinn. Sie werden als Objekte verwendet, von denen man als Eigenschaft zunächst nur die Massezahl betrachtet. Ordnet man sie nach der Massezahl (ohne dabei zu wissen, dass diese Ordnungszahl dabei gleichzeitig die Kernladungszahl ist!) und betrachtet die chemischen Eigenschaften der so sortierten Elemente, dann erhält man das Periodensystem. Diese Einschränkung auf einzelne Eigenschaften ist durchaus wesentlich für alle Verwendungen des Begriffs Teilchen in der Physik.

Es dauerte von Daltons Zeit ein weiteres Jahrhundert (siehe den geschichtlichen Abriss unter Atom), bis Zweifel an dieser Unteilbarkeit der Atome aufkamen: Marie Curie erkannte, dass ein radioaktives Element in ein anderes übergehen kann; Ernest Rutherford konnte in seinem Streuexperiment zeigen, dass die mit Alphastrahlung beschossene Goldfolie weitgehend durchlässig ist. In der Betrachtung des Rutherford-Experiments werden sowohl die einfallenden Alpha-Teilchen als auch die im Gitter festsitzenden, positiv geladenen Atomkerne als Teilchen idealisiert (es könnten genauso geladene Billardkugeln sein), von denen man nur wenige Eigenschaften betrachtet: die Masse, die Ladung, den Durchmesser und die Geschwindigkeit. Es spielt bei diesem Experiment keine Rolle, ob die Atomkerne irgendeine weitere Struktur besitzen, oder ob sie aus weiteren, kleineren Teilchen zusammengesetzt sind. Diese wenigen Eigenschaften der betrachteten Teilchen reichen für die Beschreibung des Experiments und die theoretische Herleitung des Streumusters aus.

Bei der Betrachtung des Bohrschen Atommodells sind die betrachteten Teilchen ein Elektron und ein Atomrumpf (bestehend aus dem Atomkern und möglicherweise weiteren Elektronen). Wiederum werden die Teilchen auf ihre wesentlichen Eigenschaften, Ladung und Masse, reduziert. 

Otto Hahn, Lise Meitner und Fritz Straßmann gelang es nachzuweisen, dass bei Beschuss von Uran-Atomen mit Neutronen nicht nur durch Erhöhung der Massezahl Transurane (mit höherer Kernladungszahl) entstehen, wie man bis dahin annahm (siehe Enrico Fermi, 1934), sondern manchmal eine Kernspaltung in mittelgroße Atomkerne stattfindet. Hier lässt sich der Kern nicht mehr als ein einzelnes Teilchen verstehen, sondern nur als aus Nukleonen, also Protonen und Neutronen zusammengesetzt. Weitere wichtige Teilchen in der Kernphysik sind Alpha-Teilchen, Elektronen und Neutrinos. Es stellt sich schnell die Frage, was denn die Protonen und Neutronen im Kern zusammenhält, da ja die Protonen alle positiv geladen sind und sich abstoßen müssten. Diese starke Wechselwirkung wird dadurch erklärt, dass man in der Quantenchromodynamik die Nukleonen jeweils als aus drei Quarks zusammengesetzt sieht, die von Gluonen (von englisch to glue „zusammenkleben“) zusammengehalten werden. Die Restwechselwirkung dieser Kraft außerhalb der Nukleonen hält diese ähnlich zusammen, wie die Van-der-Waals-Kräfte z. B. Wassermoleküle zusammenhalten.

Die Teilchenphysik unterscheidet zwischen den Materieteilchen und den Wechselwirkungsteilchen (Austauschteilchen), sowie bei den Materieteilchen zwischen den Elementarteilchen und den zusammengesetzten Teilchen.

Die Elementarteilchen werden durch das Standardmodell der Elementarteilchenphysik beschrieben. Da es sich bei diesem Modell um eine Quantenfeldtheorie handelt, werden hier die Teilchen als Feldquanten, d. h. als gequantelte Energiemengen von Feldern aufgefasst. Die Frage, ob die Teilchen oder die Felder letztlich das „Fundamentalere“ in der Natur sind, wird bis heute (2018) kontrovers diskutiert. Die meisten Physiker sind allerdings der quantenfeldtheoretischen Ansicht, dass es keine lokalisierten Teilchen gibt, sondern nur Felder (und deren Quanten, die räumlich so ausgedehnt sind wie das Feld selbst).
Die elementaren Felder bzw. ihre Quanten gliedern sich im Standardmodell in drei Familien von Leptonen und drei Familien von Quarks. Die Leptonen (von griechisch λεπτος (leptos) „leicht, fein“) sind das Elektron und sein Neutrino, das Myon und sein Neutrino, sowie das Tau und sein Neutrino. Die Familien der Quarks werden mit "up" und "down", "charm" und "strange", sowie "top" und "bottom" bezeichnet.

Quarks können in der Natur nicht einzeln auftreten, was als Farb-Confinement bezeichnet wird (siehe hier). Vielmehr bilden sie immer zusammengesetzte Teilchen, die in Abgrenzung von den Leptonen als Hadronen (von griechisch ἁδρός, "hadrós", „dick“) bezeichnet werden. Hadronen werden dabei in Mesonen (von griechisch μεσος "mesos" „Mittel-“) und in Baryonen (von griechisch βαρύς "barys" „schwer“) unterteilt. Mesonen bestehen aus einem Quark und einem Antiquark, Baryonen aus drei Quarks. Die bekanntesten Baryonen sind das Proton und das Neutron.

Bei den Austauschteilchen betrachtet das Standardmodell das Photon als das Austauschteilchen der elektromagnetischen Wechselwirkung. Es ist sehr eng mit den W-Bosonen und dem Z-Boson verwandt, die gemeinsam mit dem Photon die Austauschteilchen für die elektroschwache Wechselwirkung sind. Die Austauschteilchen für die starke Wechselwirkung sind die Gluonen.

Von den vier Grundkräften der Physik fehlt dabei im Standardmodell die Gravitation und ihr Austauschteilchen, das Graviton. Die Ergebnisse des Standardmodells stimmen sehr gut mit Ergebnissen von Beschleunigerexperimenten überein. Jedoch ist es bisher nicht gelungen, denselben mathematischen Formalismus auch auf die Gravitation auszudehnen. Dies ist eine der großen offenen Fragen der Theoretischen Physik.

Im Standardmodell erhalten die Teilchen durch Wechselwirkung mit dem Higgs-Feld ihre Masse.

Beim Übergang zur Quantenmechanik werden aus Teilchen Wellen, die ihre Aufenthaltswahrscheinlichkeiten beschreiben. Trifft z. B. Licht (oder ein Elektronenstrahl) auf einen Doppelspalt, so bildet diese Welle hinter dem Spalt ein Beugungsmuster. Auf einem Fotopapier (oder Schirm) wird das auftreffende Licht (der Elektronenstrahl) immer nur einzelne Punkte treffen. Erst im stochastischen Mittel vieler auftreffender Photonen (Elektronen) wird wieder das Beugungsmuster sichtbar. Diese gleichzeitige Interpretation als Welle und Teilchen wird als Welle-Teilchen-Dualismus bezeichnet.

Im Gegensatz zur Klassischen Mechanik, in der der Zustand des Teilchens durch Ort und Impuls festgelegt ist, können Ort und Impuls in der Quantenmechanik nie gleichzeitig genau gemessen werden (siehe Heisenbergsche Unschärferelation).

In Mehrteilchensystemen werden die Teilchen durch die Anwendung eines Erzeugungsoperators aus einem Vakuumzustand erzeugt. Solche Operatoren spielen insbesondere in der Quantenfeldtheorie eine Rolle. Zwischen den Anfangs- und Endzuständen physikalischer, wechselwirkender Teilchen können dabei virtuelle Teilchen entstehen und wieder verschwinden, die keiner Energie-Impuls-Beziehung genügen und deren Energie keine untere Schranke hat.

Der Teilchenbegriff in der Mathematischen Physik erstreckt sich von Zuständen in Hilbert-Räumen, auf denen man Algebren von Operatoren betrachtet, bis hin zu Wellen, bei denen beispielsweise ein bestimmtes Streuverhalten berechnet werden kann: hierzu zählen unter anderem Solitonen, bei denen es sich um nicht auseinanderlaufende Wellen handelt.


</doc>
<doc id="534690" url="https://de.wikipedia.org/wiki?curid=534690" title="Wiederkehrsatz">
Wiederkehrsatz

Der poincarésche Wiederkehrsatz ist ein mathematischer Satz über dynamische Systeme. Er besagt, dass es bei autonomen hamiltonschen Systemen, deren Phasenraum ein endliches Volumen hat, in jeder offenen Menge formula_1 im Phasenraum Zustände gibt, deren Trajektorien beliebig oft wieder nach formula_1 zurückkehren. Insbesondere ist der poincarésche Wiederkehrsatz ein Satz der Ergodentheorie und kann auch als das erste Resultat der Chaostheorie angesehen werden.

Der poincarésche Wiederkehrsatz wurde 1890 in der schwedischen Zeitschrift "Acta Mathematica" in einer Arbeit von Henri Poincaré über das Dreikörperproblem zum ersten Mal veröffentlicht. Die erste Formulierung des Wiederkehrsatzes findet sich darin auf Seite 69:

Poincaré beweist diesen Satz auf den beiden folgenden Seiten seiner Arbeit; aus seinem Beweis wird klar, dass die Dimension des Volumens keine Rolle spielt. In der Tat formuliert Poincaré auf Seite 72f. diesen Satz auch für beliebige Dimension formula_8.
Der Kontext bei Poincaré ist der Hamilton-Formalismus der klassischen Mechanik, wobei der Punkt formula_3 den zeitlich veränderlichen Zustand des mechanischen Systems beschreibt und die Hamilton-Funktion autonom, also nicht explizit von der Zeit abhängig, ist. Z.B. beim Dreikörperproblem hat formula_3 insgesamt 18 Komponenten, nämlich für jeden Körper drei (generalisierte) Orts- und drei (generalisierte) Impulskoordinaten; in diesem Fall ist der Phasenraum also 18-dimensional.
Bei autonomen Hamiltonschen Systemen ergibt sich aus dem Satz von Liouville, dass das Volumen im Phasenraum unter der Bewegung erhalten bleibt.

Unter Hinzunahme des ursprünglichen Kontextes ergibt sich folgende Formulierung des poincaréschen Wiederkehrsatzes:

Die wichtigsten Schritte des poincaréschen Beweis sind (in heutiger Notation):

Die Schritte 1 und 2 dieser Argumentation waren bereits vor Poincaré wohl bekannt. Die restlichen Beweisideen finden sich wohl erstmals in Poincarés Arbeit.

Bei Poincarés Beweis spielt der Begriff "Volumen" eine wichtige Rolle. Mit Hilfe der Maßtheorie und der damit verbundenen Begriffe lässt sich der Beweis klarer strukturieren. Man beginnt mit einem Maßraum formula_36 und nennt eine messbare Abbildung
"maßerhaltend", wenn für jede messbare Menge formula_13 die Gleichung formula_39 gilt, also wenn das Maß formula_40 und sein Bildmaß unter formula_41 übereinstimmen. Des Weiteren muss man die "Endlichkeit" des Maßraums voraussetzen, also formula_42. So gelangt man zur maßtheoretischen Variante, wobei formula_43 die formula_44-fache Iteration von formula_41 bezeichnet:

Eine genaue Analyse des poincaréschen Beweises mit Hilfe der Maßtheorie führt zu folgender maßtheoretischen Verschärfung:

Die maßtheoretischen Varianten lassen sich leicht auf diskrete dynamische Systeme anwenden, bringen dort aber nichts Neues: Als Maß nimmt man hier einfach das Zählmaß. Die Forderung formula_42 bedeutet dann, dass die zugrundeliegende Menge endlich ist. Damit wird "maßerhaltend" gleichbedeutend mit bijektiv, und die Aussage des poincaréschen Wiederkehrsatzes wird zu der einfachen Tatsache, dass jede Permutation einer endlichen Menge in Zykel zerfällt.

Physikalisch bedeutet der poincarésche Wiederkehrsatz, dass ein mechanisches System, dessen Bahnen beschränkt bleiben (also z. B. das Sonnensystem), die Eigenschaft hat, dass es in jeder Umgebung des Anfangszustands Systemzustände gibt, deren Bahnen beliebig oft in besagte Umgebung des Anfangszustands zurückkehren. Daraus folgt etwa das folgende Resultat: Verbindet man zwei Behälter, die unterschiedliche Gase beinhalten, so vermischen sich diese zunächst. Nach dem Wiederkehrsatz gibt es jedoch eine beliebig kleine Änderung des Anfangszustands mit der Konsequenz, dass sich die Gase zu einem späteren Zeitpunkt von selbst trennen und entmischt sind. Die Entmischung widerspricht einer deterministischen Formulierung des zweiten Hauptsatzes der Thermodynamik, die eine Abnahme der Entropie ausschließt. Darüber entspann sich eine Auseinandersetzung zwischen Ernst Zermelo und Ludwig Boltzmann, in deren Verlauf Boltzmann einige Artikel über die Zusammenhänge zwischen dem poincaréschen Wiederkehrsatz und dem zweiten Hauptsatz der Thermodynamik verfasste. Danach verschwindet der Widerspruch, wenn man den zweiten Hauptsatz statistisch interpretiert:

Demgemäß ist eine Abnahme der Entropie nicht prinzipiell unmöglich, aber innerhalb einer „kurzen“ Zeitspanne sehr unwahrscheinlich. Betrachtet man jedoch das Verhalten eines hamiltonschen Systems mit beschränktem Phasenraum für beliebig große Zeiten, so ist die Wiederkehr fast sicher – wie aus der maßtheoretischen Verschärfung des poincaréschen Wiederkehrsatzes folgt. Im Anhang der zitierten Abhandlung gibt Boltzmann eine Schätzung der Wiederkehrzeit für die Moleküle von Luft gewöhnlicher Dichte in einem Gefäß von einem cm³ Volumen. Nach etwa einer Seite kombinatorischer Überlegungen kommt er zu einer Zahl formula_62 (wobei formula_63 eine Abschätzung für die Zahl der Kombinationen diskretisierter Teilchenimpulse ist und formula_64 die Zahl der Gasteilchenkollisionen pro Sekunde beschreibt), die noch „mit einer zweiten von ähnlicher Größenordnung multipliziert werden“ müsse, und von der er schreibt:


</doc>
<doc id="759332" url="https://de.wikipedia.org/wiki?curid=759332" title="Lorentzkurve">
Lorentzkurve

Die Lorentzkurve, nach Hendrik Antoon Lorentz, oder Breit-Wigner-Funktion, nach Gregory Breit und Eugene Wigner, ist eine Kurve, die in der Physik bei der Beschreibung von Resonanzen auftritt.

In die Breit-Wigner-Funktion gehen zwei Parameter ein. Der Parameter formula_1 bestimmt die Position des Maximums, der Parameter formula_2 wird Breite der Kurve genannt. Aus physikalischer Sicht ist eine Interpretierbarkeit der Kurve nur für formula_3 gegeben, da mit formula_4 in der Regel eine Kreisfrequenz assoziiert ist und negative Frequenzen physikalisch unsinnig sind. Die Funktionsvorschrift lautet:
Eine andere Form der Kurve erhält man durch Reparametrisierung, indem man statt der Parameter formula_1 und formula_2 folgenden Satz Parameter verwendet:
Dann ist
insbesondere gilt für formula_10, dass die gestrichenen und ungestrichenen Parameter nahezu identisch werden. Die erste Form wird für gewöhnlich in der Teilchenphysik bevorzugt, die zweite Form in der klassischen Physik, da sie sich in ihren jeweiligen Gebieten aus der Physik in den entsprechenden Formen ergeben. Zur Rückkonversion dienen die Beziehungen


</doc>
<doc id="785225" url="https://de.wikipedia.org/wiki?curid=785225" title="Einstein-Modell">
Einstein-Modell

In der Festkörperphysik beschreibt das Einstein-Modell (nach Albert Einstein) eine Methode, um den Beitrag der Gitterschwingungen (Phononen) zur Wärmekapazität eines kristallinen Festkörpers zu berechnen. Da sich das Einstein-Modell ausschließlich auf optische Phononen anwenden lässt, ist es nicht so erfolgreich wie das Debye-Modell, das akustische Phononen beschreibt.

Die Gitterschwingungen des Kristalls werden gequantelt, d. h. der Festkörper kann Schwingungsenergie nur in diskreten Quanten formula_1 aufnehmen. Diese Quanten nennt man auch Phononen. Man beschreibt den Festkörper dann als aus "N" quantenharmonischen Oszillatoren bestehend, die jeweils in drei Richtungen unabhängig schwingen können. Die Besetzungswahrscheinlichkeit formula_2 einer solchen Schwingungsmode (eines Phonons) hängt von der Temperatur "T" ab und folgt (da Phononen Bosonen sind) der Bose-Einstein-Verteilung:

mit

Damit ergibt sich die innere Energie "U" im Festkörper zu (Es wurde die Quantisierungsbedingung des harmonischen Oszillators verwendet):

mit

Der Beitrag formula_9 gibt die Nullpunktenergie an.

Der Beitrag der Phononen zur Wärmekapazität ist dann:

mit

Mit der Einstein-Temperatur formula_12 ergibt sich eine einfachere Schreibweise:

Wie das Debye-Modell liefert das Einstein-Modell das korrekte Hochtemperaturlimit nach dem Dulong-Petit-Gesetz:

Im Limes kleiner Temperaturen ergibt sich:

Dieser Verlauf von "C(T)" für kleine Temperaturen weicht allerdings erheblich von Messungen ab. Dies hängt mit der Annahme zusammen, alle harmonischen Oszillatoren im Festkörper würden mit einer einheitlichen Frequenz schwingen. Die Verhältnisse im realen Festkörper sind jedoch deutlich komplizierter.



</doc>
<doc id="815867" url="https://de.wikipedia.org/wiki?curid=815867" title="Eugen Goldstein">
Eugen Goldstein

Gotthilf-Eugen Goldstein (* 5. September 1850 in Gleiwitz; † 25. Dezember 1930 in Berlin) war ein deutscher Physiker, beschäftigte sich mit der Gasentladungsforschung und ist der Entdecker der Kanalstrahlen.

Goldstein entstammte einer wohlhabenden jüdischen Weinhändler-Familie, wuchs nach dem frühen Tod seiner Eltern bei Verwandten im schlesischen Ratibor auf und begann 1869 ein Medizinstudium in Breslau. In der Kriegseuphorie zog er nach Berlin und arbeitete ab 1871 als Praktikant und Schüler bei Hermann von Helmholtz im Berliner Physikalischen Institut, wo er 1879 auf dem Gebiet der Gasentladungen promovierte. Er machte dort durch die Beschreibung isolierter Gasentladungs-Erscheinungen auf seine Arbeit aufmerksam, insbesondere führte er den Begriff Kathodenstrahlen ein und entdeckte 1886 die Kanalstrahlen. In Helmholtz hatte er einen Förderer, der ihm zwar Stipendien verschaffte und seine Veröffentlichungen unterstützte, jedoch zu keiner festen Anstellung verhelfen konnte.
Er heiratete 1925 die verwitwete Laura Kempke. Eugen Goldstein starb am 25. Dezember 1930 und wurde auf dem Jüdischen Friedhof in Berlin-Weißensee beigesetzt. Seine Frau übergab 1932 seine wissenschaftlich-literarischen Arbeiten der Königlich-Preußischen Akademie der Wissenschaften. Sie ist 1943 im Konzentrationslager Theresienstadt umgekommen.

Sein Kollege Wilhelm Westphal erinnerte sich an ihn als einen liebenswürdigen und "höchst witzigen", altväterlichen Sonderling, schmächtig und mit Vollbart. Sein Leben hatte nach Westphal einen Hauch von Tragik, da ihm die eigentliche Anerkennung in Deutschland erst in seinen späteren Jahren zu Teil wurde und ihm eine Karriere versagt blieb. Im Ausland und besonders in England war er dagegen bekannter und als er 1909 zur Tagung der British Association in Kanada eingeladen war, liess Ernest Rutherford ihn bei der Eröffnung neben sich Platz nehmen.

Möglicherweise spielte Goldsteins jüdische Herkunft eine Rolle, die seine Anstellung mitten im „Berliner Antisemitismusstreit“ ab 1879 zumindest erschwerte. In der Bekanntschaft mit Wilhelm Foerster, dem Direktor der Berliner Sternwarte, fand er einen Förderer auf dem Gebiet der kosmischen Physik und glänzenden Wissenschaftsorganisator am Rande der etablierten Physik und Astronomie. Auf Foersters Initiative begann Goldstein ab 1885 mit Arbeiten über die Elektrizität im Weltraum, die vor allem um Kometenphänomene kreisten und diese Schicksalsboten in der evakuierten Glasflasche nachstellen und begreifbar machen sollten. Weitergehende Experimente betrafen die Polarlichter und die Häufigkeit ihres Auftretens, Sonnenfleckenerscheinungen und Erdmagnetfeldschwankungen – diese Phänomene zeigten einen seinerzeit rätselhaften Zusammenhang, dessen Geheimnis Foerster mit Goldstein und einem damals avantgardistisch anmutenden Forschungsprogramm einer kosmischen Physik lüften wollte. 1887 bekam Goldstein – noch ohne Anstellung – für seine wissenschaftlichen Leistungen ohne Habilitation den Professorentitel verliehen, im Folgejahr erhielt er auf Betreiben Foersters erstmals eine Assistentenstelle an der Berliner Sternwarte, die er zeit seines Lebens innehaben sollte.

In dieser Funktion leitete er die Physikabteilung der von Foerster mitgegründeten Berliner Urania, wodurch die Sternwarte von Besuchern entlastet werden sollte. Er entwickelte einzigartige Schauexperimente, die sich von den Besuchern mit einem elektrischen Schalter bedienen ließen, und konnte gleichzeitig seinen Forschungen mit Entladungsröhren nachgehen. 1892 bis 1896 konnte er seiner Forschungstätigkeit als Gast in Räumlichkeiten der neu gegründeten Physikalisch-Technischen Reichsanstalt ausüben, an deren Gründung Helmholtz wie Foerster beteiligt waren – er arbeitete dort jedoch weiterhin als Assistent der Sternwarte.

Die Assistenzstelle sollte sich jedoch als hinderlich für seine wissenschaftliche Karriere erweisen: Er war nicht für die Grundlagenforschung angestellt, sondern als Physiker im Dienste der Sternwarte. Nachbetrachtend bedeutsame Entdeckungen etwa über die Verfärbung von Stoffen durch die Kathodenstrahlung (von ihm als "Nachfarben" bezeichnet), das entstehende Phosphoreszenzlicht und die kurzwellige UV-Strahlung beim Auftreffen auf feste Körper wurden kaum wahrgenommen. Er war ein genauer Beobachter, der erstmals genau die verschiedenen "Schichten" der Gasentladung beschrieb. Goldstein entdeckte auch die "Funkenspektren" ionisierter Atome, davor waren nur die "Bogenspektren" neutraler Atome bekannt. 1898 erhielt Goldstein einen in der Grunewaldstraße Schöneberg bei Berlin angemieteten Raum als „Physikalisches Laboratorium der Berliner Sternwarte“ mit einem angestellten Glasbläser. Es bestand bis 1927 und Goldstein wohnte dort auch. 1913 zog das Laboratorium in die neuerrichtete Sternwarte Babelsberg um, das nach seinem Tod in „Goldstein-Laboratorium“ umbenannt wurde. Dieser Name musste unter der NS-Herrschaft 1935 jedoch wieder verschwinden.

Aufbauend auf den Arbeiten von Julius Plücker und dessen Schüler Johann Wilhelm Hittorf stellte Goldstein umfangreiche Gasentladungs-Experimente an, für die er bis 1885 nahezu 2.000 teilweise aus der eigenen Tasche bezahlte Röhren herstellen ließ. Hittorf gelang es 1869, das Phänomen der dunklen „Glimmstrahlen“ in der Erscheinungsvielfalt der Gasentladungen zu isolieren, die Goldstein 1876 erstmals als „Kathodenstrahlen“ bezeichnete und als grundlegendes Phänomen der Elektrizität begriff, es aber – in Helmholtzscher Tradition – als Äthererscheinung interpretierte. Experimente von Philipp Lenard, der vorangegangene Entladungsversuche von Heinrich Hertz aufgriff, führten zur Konstruktion einer Röhre zur Erzeugung reiner Kathodenstrahlung (Lenard-Fenster), die Conrad Röntgen Ende 1895 zur epochalen Entdeckung der Röntgenstrahlung verwendete. Mit der Diskussion um die Röntgen- und damit auch der Kathodenstrahlung wurde Goldstein vor allem im Ausland als glänzender Experimentalphysiker gewürdigt.

Die heute in der Rezeption mit Eugen Goldstein verbundene Entdeckung der Kanalstrahlen blieb aus heutiger Sicht für die Entwicklung der modernen Physik bedeutungslos: Der neunseitige Akademiebeitrag von 1886 blieb seinerzeit unbeachtet, zumal die Kanalstrahlhypothese aus der Vielfalt der Gasentladungserscheinungen nicht besonders hervorstach. Dies änderte sich ebenfalls mit der Entdeckung der Röntgenschen X-Strahlen, als auch die Kanalstrahlen für eine kurze Zeit Bedeutung erlangten. Mit dem Siegeszug der entstehenden Atomphysik konnten sie jedoch als Ionenstrahlung identifiziert werden und verloren ihren besonderen Status.

Goldstein hat die Gasentladungsforschung wie kein anderer geprägt und mit Kometenschweif-Experimenten in der Entladungsröhre publikumswirksam auf dem dunklen wie Randgebiet der kosmischen Physik geforscht. Er erkannte früh die eigenständige Bedeutung der Kathodenstrahlung in Gasentladungsvorgängen wie auch in Kometenschweif-, Sonnen- oder Polarlicht-Erscheinungen. Prioritätsstreits mit etablierten Wissenschaftlern wie William Crookes 1879 über die korpuskulare Deutung von Gasentladungen oder dem Pariser Astrophysiker Henri Deslandres über die Erklärung von Sonnenaktivitäten mit Kathodenstrahlen 1897 brachten ihm vor allem im Ausland Anerkennung. Unter anderem erhielt er 1903 den Prix Hébert der Pariser Académie des sciences oder 1909 die Hughes-Medaille der Royal Society und wurde mehrfach zum Nobelpreis vorgeschlagen. 1919 wurde er Ehrenmitglied der Deutschen Physikalischen Gesellschaft. Trotzdem blieb ihm in Deutschland die Anerkennung bis nach dem Krieg weitgehend versagt, er war bis zuletzt Assistent der Sternwarte. Besonders Sommerfeld maß ihm durch die frühe Beschreibung eines sehr viel später als Photoelektrischer Effekt identifizierten Phänomens hohe Bedeutung bei und würdigte ihn in einem Beitrag zu seinem 70. Geburtstag:

Diese Einschätzung des in der Äthertheorie und phänomenologischen Experimentierstils verfangenen Physikers war sicherlich übertrieben, zeugt aber von der späten Anerkennung seiner Arbeit in der inzwischen etablierten Atomphysik. Nicht zuletzt durch die NS-Herrschaft musste der jüdische Wissenschaftler in Vergessenheit geraten.




</doc>
<doc id="879463" url="https://de.wikipedia.org/wiki?curid=879463" title="Christopher Polhem">
Christopher Polhem

Christopher Polhem (auch "Polhammar"; * 18. Dezember 1661 in Tingstäde (nach anderen Quellen Visby, Gotland); † 30. August 1751 in Stockholm) war ein schwedischer Wissenschaftler und Erfinder sowie Mitglied der schwedischen Akademie der Wissenschaften.
Polhem studierte ab 1687 an der Universität Uppsala Mathematik und Physik. 1697 gründete er die erste schwedische Ingenieursschule, das "Laborium mechanicum" in Stockholm (später nach Falun bzw. Stjärnsund verlegt). Für die Unterweisung seiner Schüler schuf Polhem eine Sammlung von Holzmodellen, anhand derer verschiedene mechanische Bewegungsabläufe erklärt werden konnten, das so genannte "mechanische Alphabet". Er wurde 1716 geadelt und änderte seinen Namen von Polhammar in Polhem. In Stjärnsund baute er mehrere Manufakturen auf, deren Anlagen mit Wasserkraft betrieben wurden.

Polhem war auch mit dem Entwurf der Schleusen zur Umgehung der Trollhättanfälle im Göta älv beauftragt, war damit jedoch nicht erfolgreich.

Von 1876 bis 1920 war ein nach ihm benanntes wassergetriebenes Antriebssystem zur Entwässerung von Erzgruben, das sogenannte Polhemsrad, in der schwedischen Region Bergslagen in Betrieb.

Sein Neffe Anders Polhammer (1705–1767) wurde als Uhrmacher bekannt.


In dem schwedischen Kinderbuch "Morgen, Findus, wird’s was geben" von Sven Nordqvist träumt der Protagonist Pettersson davon, dass Christopher Polhem ihm im Traum Ratschläge zum Bau einer Maschine erteilt. 



</doc>
<doc id="902993" url="https://de.wikipedia.org/wiki?curid=902993" title="Reluktanzkraft">
Reluktanzkraft

Die Reluktanzkraft formula_1 oder auch Maxwellsche Kraft entsteht aufgrund der Änderung des magnetischen Widerstands, der auch als Reluktanz bezeichnet wird. Die Reluktanzkraft wirkt immer so, dass sich der magnetische Widerstand verringert und die Induktivität steigt und ist der Magnetostatik zuzurechnen. 

Diese Eigenschaft wird bei einigen Typen von elektrischen Maschinen benutzt, zum Beispiel bei geschalteten Reluktanzmaschinen, Transversalflussmaschinen, dem Synchron-Reluktanzmotor oder elektromagnetischen Lagern. 

Eine verwandte Kraft ist die Lorentzkraft, welche die Kraftwirkung auf eine bewegte elektrische Ladung in einem äußeren elektromagnetischen Feld beschreibt.

Die Reluktanzkraft kann hergeleitet werden aus der Änderung der Energie formula_2, die sich bei einer infinitesimalen Verschiebung formula_3 des beweglichen Stücks zur Seite ergibt:

Darin ist

Die Induktivität formula_8 eines magnetischen Kreises mit Luftspalt ist gegeben durch

mit

Die (idealisierte) Fläche, die für den magnetischen Kreis zur Verfügung steht, ergibt sich zu

Dabei ist die Richtung der Auslenkung formula_18 unerheblich, daher die Betragsstriche. Die Größe formula_19 bezeichnet die Tiefe.

Einsetzen liefert

so dass auf den beweglichen Teil des ausgelenkten Kerns eine Kraft 

wirkt, die ihn zur Mitte hin zieht. Diese ist "unabhängig" von der Größe der Auslenkung, außer wenn die obige Ableitung formula_22 ihre Gültigkeit verliert. Dies ist der Fall, wenn formula_23 zu groß wird.

Analog zu oben gilt
Für die Induktivität gilt auch hier näherungsweise
Mit der Potenzregel erhält man

Einsetzen in die Formel für formula_1 liefert das Ergebnis:

Da bei einer Verkleinerung des Luftspalts die Induktivität steigt, wirkt die Reluktanzkraft in diese Richtung. Die Kraft nimmt mit der Breite des Luftspalts ab. Das Maximum der Reluktanzkraft ist erreicht, wenn der Luftspalt gegen null geht. Allerdings gilt bei sehr kleinem Luftspalt die Näherungsformel für die Induktivität nicht mehr, da dann der magnetische Widerstand des Kerns nicht mehr vernachlässigt werden kann.



</doc>
<doc id="1128679" url="https://de.wikipedia.org/wiki?curid=1128679" title="Molekulare Modellierung">
Molekulare Modellierung

Unter molekularer Modellierung ( (AE) bzw. (BE)) werden Techniken für computerunterstütztes Modellieren chemischer Moleküle zusammengefasst. Das Design von neuen Molekülen und deren Modellierung ist ein Teilgebiet der molekularen Modellierung (englisch , CAMD).

Diese Techniken ermöglichen neben der räumlichen Darstellung von einfachsten bis zu hochkomplexen Molekülen auch die Berechnung ihrer physikochemischen Eigenschaften. Besonders in der medizinischen Chemie liegt die Anwendung darin, Strukturen für neue Wirkstoffe zu optimieren (Homologiemodelling). In diesem Bereich erfährt die molekulare Modellierung zunehmend eine Ergänzung durch die kombinatorische Chemie (Virtual Screening, QSAR, CoMFA, CoMSIA).

Mechanistische (semiempirische) und statistische Ansätze (Moleküldynamik, Monte-Carlo-Simulation) eignen sich eher für sehr große Moleküle oder Wechselwirkungen zwischen einer großen Anzahl an Molekülen, quantenchemische Berechnungsverfahren sind hingegen weitaus präziser, aber wegen des höheren Rechenaufwandes nur bei Molekülen kleiner und mittlerer Größe erste Wahl. Von der stetig wachsenden Rechenleistung moderner Computer profitiert die molekulare Modellierung.




Die Potentialhyperfläche (PES) ist eine Darstellung, in der die potentielle Energie und die Struktur eines Moleküls einen mehrdimensionalen Raum aufspannen. Die Energie eines Moleküls wird in Abhängigkeit seiner Kernkoordinaten dargestellt.

In der molekularer Modellierung ist die Wahl eines geeigneten Koordinatensystems wichtig. Es wird generell zwischen globalen und lokalen Koordinatensystemen unterschieden. Unter die globalen Koordinatensysteme fallen z. B. die orthonormierten kartesischen Koordinaten und die Kristallkoordinaten. Im Unterschied zu kartesischen Koordinaten sind die Winkel in Kristallkoordinaten nicht gleich 90°. Die lokalen Koordinatensysteme sind immer bezogen auf bestimmte Atome oder die Beziehungen der Atome zueinander (z. B. Symmetrieeigenschaften), hier wäre u. a. die Z-Matrix oder die Normalkoordinaten der molekularen Schwingungen zu nennen.

Die "Optimierung" ist nach "Roland W. Kunz" das Auffinden eines vorteilhaften Zustandes eines Systems, also die Suche nach einem lokalen Minimums in der Nähe eines gegebenen Startpunktes. Generell wird der Begriff "Optimierung" für die Suche nach "kritischen Punkten" nahe der Ausgangsstruktur verwendet. Als kritische Punkte bezeichnet man Minima, Maxima und Sattelpunkte der Potentialhyperfläche.

Die meisten Optimierungsmethoden bestimmen den nächstgelegenen kritischen Punkt, eine multidimensionale Funktion kann allerdings sehr viele verschiedene kritische Punkte derselben Art enthalten.

Das Minimum mit dem niedrigsten Wert wird als globales Minimum bezeichnet, während alle anderen lokale Minima sind.

Das "Verfahren des steilsten Abstiegs", auch "Gradientenmethode" genannt nutzt die differenziellen Eigenschaften der Zielfunktion aus. Die Suchrichtung ist durch einen festgelegten negativen Gradienten (differentielle Eigenschaft der Hyperfläche) definiert. Die Optimierung erfolgt hier in die Richtung des negativen Gradienten, der die Richtung des steilsten Abstiegs von einem Ausgangswert angibt, bis keine Verbesserung mehr erzielt werden kann.

Da die potentielle Energie als Funktion der Kernkoordinaten die PES bildet sind für die kritischen Punkte die Ableitungen der Kernkoordinaten-Funktionen besonders wichtig. Innerhalb der Gradientenmethode wird zwischen dem numerischen und dem analytischen Gradienten unterschieden. In der Praxis sollte nur mit Methoden gearbeitet werden, für die ein analytischer Gradient vorhanden ist.

Beim Durchlauf der Gradientenmethode werden keine Informationen aus vorhergegangenen Suchschritten verwendet.

Ein Hauptproblem der molekularen Modellierung ist nicht das Fehlen einer geeigneten Methode zur Berechnung der Moleküleigenschaften, sondern ein Zuviel an Methoden. Die Wahl der Methode für ein bestimmtes Problem sollte daher sorgfältig getroffen werden.

Das Ziel der Dichtefunktionaltheorie ist ein geeignetes (Energie-)Funktional der Dichte zu finden. Die möglichen Funktionale sind auf drei Dimensionen, unabhängig von der Molekülgröße, beschränkt.

Die Leistung verschiedener Dichtefunktionaltheorie-Methoden (DFT) ähnelt den Hartree-Fock-Ergebnissen (HF) (siehe unten).

Eine der am häufigsten verwendeten Verfahren zur Geometrieoptimierung ist "B3LYP/6-31G*". Bei "B3LYP" (Kurzform für: Becke, 3-Parameter, Lee-Yang-Parr) handelt es sich um eine Approximationen zum Austausch-Korrelations-Energiefunktional in der Dichtefunktionaltheorie (DFT), "B3LYP" steht somit für die gewählte Methode. Die weitere Bezeichnung "6-31G" bezieht sich auf den verwendeten Basissatz von John Pople. Die allgemeine Bezeichnung der Basissätze ist "X-YZG." Hierbei steht das "X" für die Anzahl primitiver Gaussianer (primitiver Gauß-Funktionen), die jede Kernatomorbitalbasisfunktion umfassen. "Y" und "Z" zeigen an, dass die Valenzorbitale jeweils aus zwei Basisfunktionen zusammengesetzt sind. Die erste Basisfunktion besteht aus einer Linearkombination von Y primitiven Gaußfunktionen. Die zweite Basisfunktion besteht entsprechend aus einer Linearkombination von Z primitiven Gaußfunktionen. Die Verwendung von Linearkombinationen ist notwendig, da die primitiven Gaussianer in Kernnähe ein anderes Verhalten zeigen als Atomorbitale, durch die Linearkombination wird dieser Fehler minimiert. In diesem Fall impliziert das Vorhandensein von zwei Zahlen nach dem Bindestrich, dass dieser Basissatz ein Split-Valenz-Basissatz ist.

Ein "6-31G"-Basissatz beschreibt somit die inneren Orbitale als eine Linearkombination von 6 primitiven Gauß-Funktionen, die zu einer kontrahiert sind. Valenzorbitale werden entsprechend durch zwei kontrahierte Gaußfunktionen beschrieben. Eine der kontrahierten Gaußfunktionen ist eine Linearkombinationen von drei primitiven Gauß-Funktionen und die andere eine Linearkombination mit einer primitiven Gauß-Funktion.

Das Sternchen in "6-31G*" weißt auf eine Korrektur für die räumliche Abhängigkeit der Ladungsverteilung im Molekül hin. Dies geschieht durch sogenannte Polarisationsfunktionen.

Im Unterschied zur Dichtefunktionaltheorie wird bei der Hartree-Fock-Approximation die Vielteilchenwellenfunktion durch eine Slater-Determinante der Einteilchenzustände (Produktzustände) ausgedrückt, dies führt dazu, dass das Pauli-Prinzip automatisch berücksichtigt wird. Ziel ist somit ein geeignetes (Energie-)Funktional der Wellenfunktion zu finden, diese Wellenfunktionen sind meist hochdimensional, was einen, im Vergleich zur DFT erhöhten Rechenaufwand zur Folge hat. Ein zu beachtender Fehler des Hartree-Fock-Produktansatzes ist die Annahme, dass die Gesamt-Wahrscheinlichkeitsdichte ein einfaches Produkt der Einzel-Wahrscheinlichkeitsdichten ist:

formula_1

Dies ist eine physikalisch zweifelhafte Annahme, da Elektronen unkorreliert (also unabhängig voneinander) behandelt werden. Für eine genaue Betrachtung dürfen die Wechselwirkungen der Elektronen untereinander nicht vernachlässigt werden. Auch in der Form der Slater-Determinante werden die Elektronen unkorreliert behandelt.

Für die Geometrieoptimierung auf Hartree-Fock-Niveau werden u. a. häufig folgende Basissätze verwendet: "STO-3G", "3-21G", "6-31G*" bzw. "6-31G**", "cc-pVDZ" und "cc-pVQZ".

Die Wahl des verwendeten Basissatzes spielt ebenso wie die Wahl der Methode (HF, DFT, ...) eine wichtige Rolle in der molekularen Modellierung. Ein Basissatz beschreibt eine Reihe von Basisfunktionen, die die Gesamtelektronenwellenfunktion durch Linearkombination der Basisfunktionen annähert. Heute werden als Basisfunktionen fast ausschließlich primitive Gaußfunktionen verwendet, dies liegt vor allem an dem geringeren Rechenaufwand bei der Verwendung von Gauß-Funktionen im Vergleich zu anderen Basisfunktionen. Wenn andere Basisfunktionen verwendet werden sollen ist darauf zu achten, dass sie ein Verhalten aufweisen, das mit der Physik des Problems übereinstimmt; auch sollte die gewählte Basisfunktion schnell zu berechnen sein.

Ein Basissatz heißt "minimal", wenn er so viele Basisfunktionen enthält, dass alle Elektronen des Moleküls beschrieben werden können und nur ganze Sätze von Basisfunktionen vorkommen.

Die hier gezeigten Basissätze sind klein, eine typische DFT-Rechnungen weist ca. 100.000 Basisfunktionen auf.




</doc>
<doc id="1201540" url="https://de.wikipedia.org/wiki?curid=1201540" title="Masse-Leuchtkraft-Beziehung">
Masse-Leuchtkraft-Beziehung

Eine Masse-Leuchtkraft-Beziehung stellt einen Zusammenhang dar zwischen der Masse eines Objektes, beispielsweise eines Sterns, und seiner Leuchtkraft. Dieser Zusammenhang kann verwendet werden zur Abschätzung der Masse eines Sterns allein aus der Beobachtung seiner Leuchtkraft.

Alternativ zur Leuchtkraft kann auch die Effektivtemperatur oder der Spektraltyp eines Sterns verwendet werden, die meist einfacher zu bestimmen sind.

Für Hauptreihensterne ist die empirisch bestimmte Masse-Leuchtkraft-Beziehung gut bekannt. Zum ersten Mal beschrieben wurde sie 1926 in dem Buch „"The Internal Constitution of Stars"“ (der innere Aufbau der Sterne) von Sir Arthur Stanley Eddington:

Die Beziehung gilt nur für Hauptreihensterne und lässt sich "nicht" einfach auf Riesensterne oder Braune Zwerge übertragen.

Die Masse-Leuchtkraft-Beziehung bedeutet konkret, dass ein Stern mit doppelter Sonnenmasse die 11,3-fache Leuchtkraft der Sonne hat; ein Stern mit 4 Sonnenmassen ist schon 128 mal so hell wie die Sonne. Diese starke Abhängigkeit bewirkt, dass massereiche Sterne eine sehr viel kürzere Verweildauer („Lebenszeit“) auf der Hauptreihe haben als massearme, da ihr Kernbrennstoff viel schneller verbraucht wird.

Der Exponent der Masse-Leuchtkraft-Beziehung (hier also der Wert 3,5) ergibt sich aus der Anpassung der Beziehung an Messdaten. Mit Hilfe der Grundgleichungen des Sternaufbaus lässt sich ebenfalls eine Masse-Leuchtkraft-Beziehung ableiten; dabei erhält man einen Wert von 3 für den Exponenten. 

Die so erhaltene Relation stellt allerdings keine exakte Lösung der Gleichungen dar, sondern lediglich eine Abschätzung und stimmt in Anbetracht dessen recht gut mit der beobachteten Masse-Leuchtkraft-Beziehung überein. Die Ableitung der Beziehung aus den Grundgleichungen des Sternaufbaus (im Grunde werden dabei nur Mittelwerte betrachtet) setzt übrigens eine gewisse Ähnlichkeit (Homologie) des Aufbaus von Sternen mit verschiedenen Massen voraus. Die Bestätigung der Beziehung durch Beobachtungen lässt also schon gewisse Rückschlüsse auf das Innere der Sterne zu.




</doc>
<doc id="1595188" url="https://de.wikipedia.org/wiki?curid=1595188" title="Karl Exner">
Karl Exner

Karl Exner (* 26. März 1842 in Prag; † 11. Dezember 1914 in Wien) war ein österreichischer Mathematiker und Physiker.

Karl Exner war der zweite Sohn des Franz Serafin Exner und seiner Frau Charlotte Dusensy. Er hatte vier Geschwister: Adolf, Sigmund, Marie und Franz-Serafin. Er studierte nach dem Besuch des Schottengymnasiums in Wien, Zürich und Freiburg Mathematik und Physik, legte 1865 die Lehramtsprüfung für diese Fächer ab, erhielt 1870 das Diplom eines Dr. phil. der Universität Freiburg i.Br. und war zunächst von 1871 bis 1874 als Supplent (Hilfslehrer) in Mödling, Troppau und Wien tätig. Er war mit Henriette Wagner (* 1863) verheiratet. Vorübergehend hörte er Vorlesungen am Eidgenössischen Polytechnikum in Zürich. 1892 habilitierte er sich als Privatdozent für theoretische Physik an der Universität Wien. 1894 folgte er einem Ruf auf das Ordinariat für mathematische Physik in Innsbruck, das er bis 1912 innehatte.

Er beschäftigte sich unter anderem mit der Wellentheorie des Lichts und geometrischen Problemen und zeichnete sich durch innovative Ideen und experimentelles Geschick aus. So gelang es ihm, die Newtonschen Farbenringe theoretisch zu deuten, er beschäftigte sich mit meteorologischer Optik und dem Funkeln der Sterne. Er erhielt 1890 den sehr angesehenen Baumgartner-Preis der Wiener Akademie, den später vor allem bedeutende Wissenschaftler aus dem Ausland wie Lenard, Röntgen und Einstein erhielten, und einen Preis der chemisch-physikalischen Gesellschaft in Wien. 

Eine schwere Krankheit zwang Exner 1904, sich in den Ruhestand zurückzuziehen.




</doc>
<doc id="1989279" url="https://de.wikipedia.org/wiki?curid=1989279" title="Schnittweite">
Schnittweite

Unter der Schnittweite s bzw. s' versteht man in der Optik:



</doc>
<doc id="2225882" url="https://de.wikipedia.org/wiki?curid=2225882" title="Gustav Lachmann">
Gustav Lachmann

Gustav Victor Lachmann (* 3. Februar 1896 in Dresden; † 30. Mai 1966 in Chorley Wood, Hertfordshire, Großbritannien) war ein deutscher Aerodynamiker österreichischer Herkunft. Er entwickelte während des Ersten Weltkrieges unabhängig von dem Briten Fredrick Handley das Prinzip der Vorflügel und gilt mit dieser Erfindung als Pionier der europäischen Luftfahrt.

Lachmann diente zuerst bei den "Hessischen Leibdragonern" und dann im Ersten Weltkrieg im Rang eines Leutnants als Pilot in der Armee. 1917 erlitt er bei einem Absturz schwere Verletzungen. Nach Kriegsende studierte er ab 1918 an der TH Darmstadt Maschinen- und Flugzeugbau. Er wurde 1919 Mitglied des Corps Hassia Darmstadt. 1921 ging er an die Universität Göttingen, um dort sein Studium in den Fächern Mathematik, Mechanik und Aerodynamik bei Ludwig Prandtl fortzusetzen, bei dem er schließlich im Fachgebiet Strömungsmechanik und Grenzschichttheorie 1923 promovierte.

Seine schon 1918 angemeldete, bahnbrechende Idee, mit Vorflügeln (Lachmann-Klappen) die Grenzschicht am Tragflügelprofil zu stabilisieren wurde vom Deutschen Patentamt in ihrer Bedeutung nicht erkannt und abgelehnt. 1921 nutzte der englische Flugzeugfabrikant Fredrick Handley diese Idee und ließ Lachmann im Göttinger Windkanal für ihn forschen. Von 1924 war er als Konstrukteur für die Flugzeugwerke Franz Schneider in Berlin tätig, zwei Jahre später wurde er technischer Berater für die Flugzeugfabrik Ishikawajima in Tokio. Schließlich übersiedelte er 1929 nach Großbritannien, wo er in der Fabrik von Handley Direktor der Forschungsabteilung wurde und für die Entwicklung der Bomber Harrow und Hampden verantwortlich zeichnete.

Mit Ausbruch des Zweiten Weltkriegs wurde er interniert, Handley konnte aber durchsetzen, dass Lachmann als "consultant engineer" weiterhin für seine Firma arbeiten konnte. 1949 wurde er britischer Staatsangehöriger. Für Handley Page blieb er bis fünf Monate vor seinem Tod tätig.



</doc>
<doc id="2324360" url="https://de.wikipedia.org/wiki?curid=2324360" title="Inversion (Halbleiter)">
Inversion (Halbleiter)

Als Inversion wird in der Halbleitertechnologie sowohl ein Betriebszustand eines MIS-Feldeffekttransistors als auch der allgemeine Fall, dass in einem Halbleiter die Dichte der Minoritätsladungsträger die Dichte der Majoritätsladungsträger erreicht oder übersteigt, bezeichnet.

Bei der Dotierung eines Halbleiters entstehen je nach Art der Dotanden entweder eine n- oder p-Dotierung. Die Art der Dotierung gibt die dominante Ladungsträgerart (Mehrheits- oder auch Majoritätsladungsträger genannt) an, bei einer n-Dotierung sind dies Elektronen und bei der p-Dotierung Defektelektronen (Löcher). In einem Halbleiter treten immer sowohl Elektronen als auch Defektelektronen gemeinsam auf. Bei der Dotierung steigt die Konzentration der Majoritätsladungsträger, gleichzeitig sinkt die Konzentration der Minoritätsladungsträger, da diese mit den Majoritätsladungsträgern rekombinieren.

Mithilfe einer MIS-Struktur, wie dem MOSFET, können diese natürlichen Verhältnisse (Gleichgewichtszustand) verändert oder gar umgekehrt werden. Dazu wird beispielsweise bei einem n-Kanal-MOSFET (p-dotiertes Substrat) eine positive Spannung am Gate angelegt. Durch elektrostatische Anziehung sammeln sich vermehrt Elektronen (Minoritätsladungsträger) an der Grenzschicht Halbleiter–Isolator (z. B. Silizium−Siliziumdioxid), und rekombinieren verstärkt mit den Defektelektronen (Majoritätsladungsträgern). Mit zunehmender Spannung nimmt so die Konzentration der Minoritätsladungsträger zu und die der Majoritätsladungsträger ab. Die Inversion tritt ein, wenn an der Grenzschicht Halbleiter–Isolator die Konzentration der eigentlichen Minoritätsladungsträger (im Beispiel Elektronen) gleich oder größer der Konzentration der Majoritätsladungsträgern ist. Beim n-Kanal-MOSFET bildet sich in diesem Fall ein quasi n-leitendes Gebiet an der Grenzfläche, der n-leitende Kanal, der selbstsperrende Transistor ist nun leitend.


</doc>
<doc id="2711937" url="https://de.wikipedia.org/wiki?curid=2711937" title="Quantenchaos">
Quantenchaos

Der Begriff Quantenchaos bezeichnet ein interdisziplinäres Fachgebiet der Physik. Analog zum Gebiet der klassischen Mechanik, wo es für bestimmte Systeme zu deterministischem Chaos kommen kann, z. B. für die Navier-Stokes-Gleichungen, die für die Wettervorhersage und Klimaprognosen von Bedeutung sind (siehe Schmetterlingseffekt), gibt es auch in der Quantenmechanik Systeme mit chaotischem Verhalten, und zwar auf folgenden Gebieten:

Das Korrespondenzprinzip beschreibt den Übergang von der quantenmechanischen Betrachtung zum klassischen Grenzfall, der Bereich zwischen klassischen und quantenmechanischen Systemen wird als "semiklassisch" bezeichnet.

Das chaotische Verhalten von Quantensystemen wird dabei z. B. durch die Analyse der Spektralverteilungsfunktion festgestellt, die von deterministischen Quantensystemen abweicht. Man stellt in chaotischen Quantensystemen z. B. Niveau-Abstoßung fest oder verstärkte Aufenthaltswahrscheinlichkeiten, wo das klassische System nur instabile Trajektorien besitzt. Eine andere Möglichkeit ist die zeitliche Entwicklung des Quantensystems und seine Reaktion auf äußere Einflüsse (Kräfte) mit irregulären Amplitudenverteilungen.

Zum Beispiel hat der Hamiltonoperator mit stochastischem (random-) Potential "kritische Wellenfunktionen" als Lösung und eine Cantor-Verteilung als Spektrum mit dem Lebesgue-Maß Null. In der Praxis zeigen diese Quantensysteme starke Fluktuationen auf mesoskopischer Ebene.

Als alternativer Name für Quantenchaos wurde von Sir Michael Berry "Quanten-Chaologie" vorgeschlagen. Bedeutende Methoden, die zur Untersuchung des Quantenchaos verwendet werden, sind die Random-Matrix-Theorie von Oriol Bohigas und die Periodic-Orbit-Theorie von Martin Gutzwiller.





</doc>
<doc id="2719414" url="https://de.wikipedia.org/wiki?curid=2719414" title="Transkritische Bifurkation">
Transkritische Bifurkation

Die transkritische Bifurkation beschreibt einen Vorgang, bei dem die Stabilität („anziehend“ oder „abstoßend“) zweier Ruhelagen eines Systems vertauscht wird. Sie ist damit ein bestimmter Typ einer Bifurkation eines nichtlinearen Systems.

Die Normalform der transkritischen Bifurkation ist:
wobei formula_2 der Bifurkationsparameter ist.

Die transkritische Bifurkation hat folgende Gleichgewichtspunkte:
Setzt man formula_5 mit formula_6 in die Normalform ein (d. h. man stört den Fixpunkt) und vernachlässigt alle Terme der Ordnung formula_7, erhält man
für die zeitliche Entwicklung der Störung formula_9.

Für formula_10 ist also formula_11 ein stabiler Fixpunkt (d. h. die Störung nimmt mit der Zeit ab) und formula_12 ein instabiler (die Störung wächst). Für formula_13 ist es umgekehrt. Bei dem kritischen Wert des Bifurkationsparameters formula_14 ist der (in diesem Fall einzige) Fixpunkt formula_15 indifferent stabil.

Für ein diskretes System geht die Differentialgleichung über in eine Differenzengleichung: 
Die Lage der Fixpunkte bleibt gegenüber dem kontinuierlichen System unverändert.

Bei einem logistischen Wachstum ist die zeitliche Änderung einer Ressource formula_17 proportional zu ihrem derzeitigen Wert und von der Differenz dieses Werts zu einer Schranke formula_18, zum Beispiel bei der Anzahl an Tieren in einem bestimmten Gebiet. Die Proportionalitätskonstante sei formula_19. Tritt zusätzlich ein Konsum dieser Ressource proportional zu ihrer momentanen Verfügbarkeit mit Proportionalitätskonstante formula_20 auf, beispielsweise durch Bejagung, dann lautet die Differentialgleichung
Dies lässt sich durch die Variablentransformation formula_22 in die Normalform überführen und man identifiziert formula_23. Für formula_24 ist also formula_25 ein stabiler Fixpunkt: Würde ein Tier in das Gebiet ausgesetzt, würden die Jäger dieses sofort schießen und ein Anwachsen unterbinden. Der Fixpunkt formula_26 ist hingegen instabil: Schießen die Jäger auch nur kurzzeitig zu viel Wild, kann es sich nicht erholen und stirbt bei gleichbleibender Bejagung aus (strebt gegen formula_27). Für formula_28 ändert sich das Verhalten der Fixpunkte: formula_27 wird instabil, bei kurzzeitiger Erhöhung der Population wird nicht genügend Wild geschossen, um ein Anwachsen auf den Fixpunkt formula_30 zu verhindern. Dieser ist stabil, das heißt, sowohl bei kurzzeitig zu viel als auch zu wenig geschossenem Wild schwankt die Population nur um formula_30. 


</doc>
<doc id="2829595" url="https://de.wikipedia.org/wiki?curid=2829595" title="Alexander M. Bradshaw">
Alexander M. Bradshaw

Alexander Marian Bradshaw CBE (* 12. Juli 1944 in Bushey, Großbritannien) ist ein britischer Physiker. 

Bradshaw studierte Chemie am Queen Mary College der Universität London und promovierte 1969 im Fach Physikalische Chemie. 1974 folgte die Habilitation am Institut für Physikalische Chemie der Technischen Universität München. Von 1976 bis 1998 arbeitete Bradshaw am Fritz-Haber-Institut der Max-Planck-Gesellschaft in Berlin. Dort wurde er 1980 als Leiter der neu gegründeten Abteilung Oberflächenphysik zum wissenschaftlichen Mitglied und Direktor am Institut berufen. Daneben war er auch von 1981 bis 1985 sowie von 1988 bis 1989 wissenschaftlicher Geschäftsführer der Berliner Elektronenspeicherring-Gesellschaft für Synchrotronstrahlung.

Bradshaw war von 1998 bis 2000 Präsident der Deutschen Physikalischen Gesellschaft, anschließend bis 2002 deren Vizepräsident. Von 1999 bis 2008 war er wissenschaftlicher Direktor und Vorsitzender der wissenschaftlichen Leitung des Max-Planck-Instituts für Plasmaphysik. Seit 1997 ist Bradshaw Honorarprofessor für Experimentalphysik an der Technischen Universität Berlin (zurzeit entpflichtet), und seit 2000 auch an der Technischen Universität München. Seit Dezember 2008 ist Bradshaw als Emeritus erneut am Fritz-Haber-Institut tätig.

Bradshaw ist Mitbegründer und ehemaliger Editor-in-Chief des New Journal of Physics sowie Mitglied der Berlin-Brandenburgischen Akademie der Wissenschaften, der Academia Europaea, der Deutschen Akademie der Technikwissenschaften (Acatech), der Deutschen Akademie der Naturforscher Leopoldina (seit 2002) und der Royal Society London.

Bradshaw erhielt unter anderem 1994 den Max-Planck-Forschungspreis (gemeinsam mit Philip Woodruff, University of Warwick) und 2001 die Medaille der European Physical Society for Public Understanding of Physics. Er ist Ehrendoktor der Universität London und Ehrenmitglied des American Institute of Physics. Er wurde mit dem Rudolf-Jaeckel-Preis 2007 der Deutschen Vakuumgesellschaft (DVG) geehrt. Ebenfalls 2007 wurde er für seine europaweite Koordination der Fusionsforschung von der britischen Königin Elisabeth II. zum Commander des Order of the British Empire ernannt. Im März 2012 wurde Alexander Bradshaw von der Deutschen Physikalischen Gesellschaft zum Ehrenmitglied ernannt.



</doc>
<doc id="2969195" url="https://de.wikipedia.org/wiki?curid=2969195" title="Anatole Abragam">
Anatole Abragam

Anatole Abragam (* 5. Dezember 1914 in Grīva, Gouvernement Witebsk; † 8. Juni 2011 in Paris) war ein französischer Physiker lettischer Herkunft, der große Beiträge im Bereich der Kernspinresonanzspektroskopie geliefert hat.

Abragram emigrierte 1925 mit seiner Familie nach Frankreich. Nachdem er von 1933 bis 1936 an der Universität von Paris studierte, diente er im Zweiten Weltkrieg. Nach dem Krieg ging er seinen Studien an der École supérieure d’électricité weiter nach und wurde 1950 bei Maurice Pryce an der Universität Oxford promoviert. Von 1960 bis 1985 war er Professor für nuklearen Magnetismus am Collège de France.

Sein 1961 erschienenes Buch "Principles of Nuclear Magnetism" über Kernmagnetismus gilt heute als Standardwerk.

Er war seit 1947 beim Kommissariat für Atomenergie (CEA) und leitete von 1962 bis 1970 dessen Physikabteilung. Ab 1967 war er zudem Präsident der französischen physikalischen Gesellschaft.


Er war Ehrendoktor der Universitäten von Kent, Oxford (1976) und des Technion in Haifa (1985). 1976 wurde er Honorary Fellow sowohl des Merton College und des Jesus College der Oxford University. Er war Kommandeur der Ehrenlegion und der Palmes Academiques und erhielt das Großkreuz des Ordre national du Mérite.

1980 hatte er die "Lorentz Professur" an der Universität Leiden.

Er war Mitglied der Académie des sciences (1973), der American Academy of Arts and Sciences (1974), der National Academy of Sciences (1977), der Academia Europaea (1990), der Russischen Akademie der Wissenschaften (1999), der Royal Society (1983) und der Päpstlichen Akademie der Wissenschaften.

von A. Abragam
von und über A. Abragam



</doc>
<doc id="3228245" url="https://de.wikipedia.org/wiki?curid=3228245" title="Detlev Buchholz (Physiker)">
Detlev Buchholz (Physiker)

Detlev Buchholz (* 31. Mai 1944 in Danzig) ist ein deutscher Physiker.

Buchholz studierte Physik in Hannover und Hamburg, wo er 1968 sein Diplom machte. 1970/71 war er an der University of Pennsylvania. Nach der Promotion 1972 in Hamburg bei Rudolf Haag arbeitete er an der Universität Hamburg und war 1974/75 am CERN. 1975 bis 1978 arbeitete er als Forschungsassistent in Hamburg, wo er sich 1977 habilitierte. 1978/79 war er als Max-Kade-Stipendiat an der University of California, Berkeley. 1979 wurde er Professor in Hamburg und wechselte 1997 an die Universität Göttingen. Seit 2010 ist er im Ruhestand.

Buchholz lieferte wichtige Beiträge zur relativistischen Quantenphysik und Quantenfeldtheorie, insbesondere im Bereich der axiomatischen Quantenfeldtheorie.

Detlev Buchholz erhielt 2008 die Max-Planck-Medaille der Deutschen Physikalischen Gesellschaft „für seine herausragenden Beiträge zur Quantenfeldtheorie“. 1998 war er Invited Speaker auf dem Internationalen Mathematikerkongress in Berlin ("Scaling algebras in local relativistic quantum physics").





</doc>
<doc id="3479030" url="https://de.wikipedia.org/wiki?curid=3479030" title="Alfred Gierer">
Alfred Gierer

Alfred Gierer (* 15. April 1929 in Berlin) ist ein deutscher Physiker, Professor und Direktor (Emeritus) am Max-Planck-Institut für Entwicklungsbiologie in Tübingen, der sich auf Biologie (unter anderem Erforschung des Tabakmosaikvirus TMV), Biophysik, Geschichte und Philosophie der Naturwissenschaften spezialisiert hat.

Als Kind lebte er in Shanghai, wo sein Vater arbeitete. Er studierte Physik in Göttingen. 1953 promovierte er in München am Max-Planck-Institut für Physik über ein Thema aus der theoretischen Chemie.

1953 war es nach dem Krieg erstmals wieder für deutsche Postdoktoranden möglich, in den USA zu forschen. Gierer erhielt ein Fulbright-Stipendium und arbeitete am MIT über Enzymkinetik. Mit Hans Meinhardt forschte er an der Musterbildung in der Biologie und entwickelte mit ihm das Modell für den Aktivator-Inhibitor. 1958 folgte die Habilitation in Tübingen. Später widmete er sich vor allem philosophischen Fragestellungen, so zog er z. B. Vergleiche zwischen technologischem Fortschritt und biologischer Evolution.

Im Jahr 1964 wurde Gierer zum Mitglied der Leopoldina gewählt, im Jahr 2005 erhielt er die Cothenius-Medaille der Leopoldina.




</doc>
<doc id="3580364" url="https://de.wikipedia.org/wiki?curid=3580364" title="Richardson-Zahl">
Richardson-Zahl

Die Richardson-Zahl formula_1 (nach dem britischen Mathematiker und Meteorologen Lewis Fry Richardson) ist eine dimensionslose Kennzahl. Sie beschreibt in der Strömungslehre den Zusammenhang zwischen potentieller und kinetischer Energie.

Für Flüssigkeiten oder Gase in einem Schwerefeld kann man z. B. definieren

mit

In dieser Definition wird der Kehrwert der Quadratwurzel aus der Richardson-Zahl auch als Froude-Zahl bezeichnet:

Im Flugbetrieb gibt die Richardsonzahl z. B. Anhaltspunkte, ob Turbulenzen auftreten: je kleiner formula_1, desto wahrscheinlicher sind Turbulenzen – bei typischen Ri-Werten von 0,1 bis 10.

In Problemen mit thermischer Konvektion wird folgende Definition benutzt:

dabei ist

Diese Definition entspricht einer alternativen Definition der Archimedes-Zahl und kann mit der Grashof-Zahl formula_10 und der Reynoldszahl formula_11 auch geschrieben werden als:

Natürliche Konvektion ist für formula_13 vernachlässigbar, erzwungene Konvektion ist für formula_14 vernachlässigbar. In den Zwischenbereichen formula_15 müssen beide berücksichtigt werden.

Insbesondere bei der Auslegung von Wärmespeichern dient die Richardson-Zahl dazu, eine ordentliche Temperaturschichtung im Speicher zu dimensionieren. Die Einströmung in den Speicher muss also so gestaltet werden, dass der einströmende Impuls nicht die Speicherschichtung zerstört.

Ende 2007 wurde Europas größter Fernwärmespeicher mit über 2 GWh Speichervermögen im Kraftwerk Theiß der EVN AG in Betrieb gesetzt. Er weist einen Durchmesser von 50 Metern und eine Höhe von gut 20 Metern auf. Trotz dieser geometrisch eigentlich ungünstigen Verhältnisse hat er eine perfekte Temperaturschichtung, da die Ein- und Ausströmungsvorgänge mittels der Richardson-Zahl berechnet und die Ein- und Ausströmimpulse entsprechend angepasst wurden.


</doc>
<doc id="4082443" url="https://de.wikipedia.org/wiki?curid=4082443" title="Ladungsausgleich (Elektrostatik)">
Ladungsausgleich (Elektrostatik)

Von Ladungsausgleich spricht man in der Elektrostatik immer dann, wenn zwei oder mehrere, vorerst voneinander getrennte (isolierte) und unterschiedlich geladene Objekte mit einem elektrischen Leiter verbunden werden. Es kommt zum Ladungstransport durch den Leiter, sodass der ursprünglich vorhandene Potentialunterschied zwischen den Körpern verschwindet, die elektrische Spannung zwischen den Körpern wird zu null. Während der Dauer des Ladungsausgleichs fließt somit Strom durch den Leiter. Die Gesamtladung aller betrachteten Objekte ändert sich dabei aber nicht.



</doc>
<doc id="4544954" url="https://de.wikipedia.org/wiki?curid=4544954" title="Washburn-Gleichung">
Washburn-Gleichung

Die Washburn-Gleichung (nach Edward W. Washburn, der sie 1921 herleitete) beschreibt in der Physik die kapillare Strömung in porösen Materialien vereinfacht als:

mit
in ein vollständig benetzbares Material

Popularität erlangte diese Gleichung in England durch den Physiker Len Fisher der Universität Bristol. Er demonstrierte die Anwendung der Gleichung anhand eines Kekstauchexperiments, um die Wissenschaft der Physik durch die Beschreibung alltäglicher Probleme zugänglicher zu machen.

Das Gesetz von Hagen-Poiseuille
wird angewendet auf die Kapillarströmung einer Flüssigkeit in einem zylindrischen Rohr ohne Einwirkung eines äußeren Gravitationsfeldes.

Nach Einsetzen des Ausdrucks
für ein differentielles Volumen, welches über die differentielle Länge formula_10 einer Flüssigkeit in einem Rohr definiert wird, erhält man folgende Gleichung:

Darin ist

Die einzelnen Druckkomponenten können folgendermaßen ausgedrückt werden:

mit

Das Einsetzen dieser Gleichungen für die einzelnen Drücke führt zu einer Differentialgleichung erster Ordnung, die die Eindringtiefe formula_22 der Flüssigkeit in das Rohr beschreibt:



</doc>
<doc id="4624648" url="https://de.wikipedia.org/wiki?curid=4624648" title="Nuklidkarte/Ordnungszahl 81 bis 100">
Nuklidkarte/Ordnungszahl 81 bis 100


</doc>
<doc id="5035266" url="https://de.wikipedia.org/wiki?curid=5035266" title="Heinz Bittel">
Heinz Bittel

Heinz Bittel (* 8. März 1910 in Heidenheim an der Brenz; † 10. Februar 1980 in Münster) war ein deutscher Physiker.

Die ferro- und ferrimagnetische sowie ferroelektrische Materialforschung kannte Bittel von Grund auf und förderte sie nachhaltig, Messtechnik beherrschte er weit über diese Gebiete hinaus und war international anerkannter Experte des Phänomens Rauschen.
Industrietätigkeit während des Krieges und fünfjährige Auslandserfahrung nach dem Kriege brachte er erfolgreich ein in die Wissenschaftsverwaltung in Münster sowie in Nordrhein-Westfalen und darüber hinaus. Seine Mitarbeit in hohen nationalen und internationalen Gremien wurde häufig erbeten. Der Schwabe pflegte den Austausch mit Frankreich zum Wohle der Wissenschaft sowie von Universität und Stadt Münster. Bittel war Rektor der Universität Münster, mehrfacher Ehrendoktor und erhielt weitere bedeutende Auszeichnungen.

Leben und Arbeit mit dem "Menschen Heinz Bittel" in seinem Institut in Münster vermitteln einfühlsam seine Schüler Horst E. Müser und Karl-August Hempel.

Die unterschiedlichen Anlagen und Interessen von Heinz und seines älteren Bruders, des späteren Prähistorikers Kurt Bittel, wurden durch Eltern und Großeltern früh gefördert; beide machten ihre jeweiligen Erkenntnisse schon als Schüler in Ausstellungen und Vorträgen öffentlich.

Heinz Bittel begann nach der Reifeprüfung am Heidenheimer Hellenstein-Gymnasium im Jahre 1929 das Studium der Physik und Mathematik an der Universität Tübingen noch unter Walther Gerlach, der den Ruf an das Physikalische Institut der Universität München zum Sommersemester 1930 annahm. Bittel folgte ihm; als seine weiteren Lehrer seien Arnold Sommerfeld und Constantin Carathéodory genannt. Gerlach hielt ihn vom Wechsel des Studienortes nach Göttingen ab, indem er ihm ein Promotionsthema anbot; er vermutete Abweichungen vom Additivitätsgesetz des Brechungsindex eines Gasgemisches durch Wechselwirkung zwischen den Molekülen. Bittel wurde 1935 promoviert. Die quantitativ genaue Mischung der Gase stellte eine besondere Herausforderung dar. Mit dem Michelson-Interferometer war unter Beobachtung mit Photozellen kein signifikanter Einfluss beobachtet worden.

Das Forschungsgebiet erschien bei der schon erreichten Genauigkeit nicht hinreichend vielversprechend; Bittel wechselte zur Festkörperphysik und erforschte den Ferromagnetismus des Nickels. Spontane Polarisation und elektrischer Widerstand am Curie-Punkt, reversible und irreversible Vorgänge bei thermischer Zustandsänderung sowie Verhalten nach Kaltbearbeitung und Wärmebehandlung von möglichst reinem Nickel (1938) sind die Fragestellungen.
1938 erfolgte die Habilitation in München, 1939 die Ernennung zum Dozenten. Kurz zuvor (1937) hatte Bittel die zeitbedingten Anforderungen für die Universitätslaufbahn erfüllt; er genoss keinen weiteren Vorzug, und ein persönliches Bekenntnis zum Nationalsozialismus ist daraus nicht zu folgern. Bittel wurde Ende August 1939 vor Beginn des Feldzuges gegen Polen zur bespannten Artillerie eingezogen.

Seit Anfang 1940 beteiligte sich Bittel auf Drängen Gerlachs zunächst an Forschungen für die Marine, zu denen die führenden Magnetiker verpflichtet worden waren. Für diese Aufgabe wurde er auf Anforderung durch den Oberbefehlshaber der Kriegsmarine vom Heer entlassen. Im September 1941 wurde Bittel Abteilungsleiter bei der Firma Askania-Werke AG, Berlin-Friedenau, unter Beurlaubung von der Universität München; gegen Kriegsende wurde Bittel außerplanmäßiger Professor. Nach Vorlesungen im Sommer 1945 in Schleswig, vgl. Universität Kiel, dem stagnierenden Aufbau dort und dem vom Alliierten Kontrollrat in Berlin ab Mai 1946 verhinderten Aufbau eines Ingenieurbüros der Askania in Immenstaad am Bodensee, siehe dazu Bodenseewerk, arbeitete Bittel seit 1946 in Saint Raphaël (Var) im Dienst der Marine nationale als Leiter eines Laboratoriums mit zwei Dutzend deutschen Mitarbeitern an elektro-akustischen Entwicklungen und Signalverarbeitung für Ortung im Seewasser.

Zuletzt bestand Kontakt zu Forschern und Instituten in Frankreich insbesondere auf magnetischem Gebiet und dem der Signalverarbeitung.

Heinz Bittel wurde 1951 Professor an der Universität Münster als Gründungsdirektor des Instituts für Angewandte Physik. In der damaligen schwach besetzten Hochschullandschaft des neu gebildeten Bundeslandes Nordrhein-Westfalen sollte der zunächst Technische Physik genannte Zweig in Westfalen nahe dem östlichen Ruhrgebiet verfügbar sein. Das Land Nordrhein-Westfalen hatte zunächst einen schnellen Neubau der Physikalischen Institute geplant, der sich jedoch um fünfzehn beziehungsweise fast dreißig Jahre verzögerte. Das Institut konnte in dem 1903 errichteten ehemaligen Preußischen Oberpräsidium am Schlossplatz Räume nach und nach hinzu gewinnen, ehe im Herbst 1966 mit dem Bezug des Institutsneubaus Angewandte Physik das Naturwissenschaftliche Zentrum am Coesfelder Kreuz erste Gestalt annahm.


Bittel pflegte an seinem Lehrstuhl die Forschungsgebiete "Rauschen", einschließlich Stromrauschen, vgl. Wärmerauschen, elektrischer "Leitungsmechanismus" in dünnen Metalldrähten und magnetische Widerstandsänderung extrem dünner Nickeldrähte, "Piezo-" und "Ferroelektrika" in Bauelementen und letztere als Beispiele von Festkörpern mit "Phasenumwandlung" und Polarisationsschwankungen am Umwandlungspunkt, "Ferrimagnetische Resonanz", vgl. Ferrimagnetismus, nichtlineare "Magnetisierungsprozesse" und Barkhausen-Effekt, sowie Flusstransportrauschen beim "Supraleiter vom Typ II". Schließlich sei "Spannungsoptik" als ein Beispiel von der Industrie gewünschter Untersuchungen erwähnt. Zusätzlich zur Anregung oder Aufnahme dieser Themen ließ Bittel selbst manche seiner Vorkriegsfragen aufleben, grundlegende Begriffe der Physik modellhaft bearbeiten sowie neueste Konzepte wie den Laserstrahl und Holographie bearbeiten.

Mit "Ferroelektrika", "Ferrimagnetischer Resonanz" und "Supraleitung" verließen drei Forschungsgebiete das Institut für Angewandte Physik als sie außerhalb Münsters Keimzellen für die Lehrstühle von vier seiner Schüler wurden.

Die besondere Klarheit in Anlage und Vortrag seiner Vorlesungen mit einprägsamen Formulierungen ließ die Studierenden den Gehalt leichter verarbeiten und gewann sie als Mitarbeiter. Weit über die Forschungsgebiete hinaus ließ Bittel physikalische Themen im Hauskolloquium vortragen, wobei seine Erklärungen häufig erhellender waren als die von der vorgetragenen Publikation dargebotenen. Diese Veranstaltung gewährte besonders die günstige breite Ausbildung, die seine Schüler erfolgreich in Industrie, Forschungsinstituten, Verwaltungen oder als Lehrende an Fachhochschulen umsetzten und – sie war für die vielen Studierenden mit Ziel Staatsexamen besonders förderlich. Streben nach Verkürzung der Ausbildungsdauer war Bittels früh erkanntes Ziel.

Als die Universität 1958 die erste elektronische "Rechenanlage" Zuse Z22 erhielt, war Bittel gern bereit, sie in sein Institut aufzunehmen. Die röhrenbestückte Anlage wurde 1962 gegen die transistorisierte Z23 ausgetauscht; sie ging 1966 an das 1964 gegründete Institut für Numerische und Instrumentelle Mathematik.

Für das Institut für Angewandte Physik erreichte Bittel ein zweites Ordinariat, das 1967 mit Wilfried Hampe besetzt wurde unter Erweiterung um die Thematik "Spektrum der magnetischen Permeabilität" und Forschungsgebiete der "Halbleiterphysik".

Heinz Bittel wurde im Jahre 1976 emeritiert.

Bittel wurden höchste Leitungsfunktionen in Industrie und staatlicher Wissenschaftsverwaltung auf nationalen sowie internationalen Positionen angeboten; getreu seinen "ganz besonderen Fähigkeiten als Lehrer" (Gerlach 1951) nahm er jedoch nur der akademischen Forschung und Lehre nahestehende Ämter in Münster und außerhalb an, von denen die Mitgliedschaft im Verwaltungsrat der Kernforschungsanlage Jülich (1954–1959) und im Gründungsausschuss der Ruhr-Universität Bochum (1961–1966) genannt seien.

Mit Eugen Kappler und Wilhelm Klemm gehörte er zu den führenden Persönlichkeiten, die für die Verwirklichung des Naturwissenschaftlichen Zentrums der Universität Münster am Coesfelder Kreuz handelten. Heinz Bittel war Rektor der Universität Münster im Akademischen Jahr 1963/64. Die Pflege der Verbindungen zu Forschungsinstituten und Hochschulen Frankreichs war ihm ein besonderes Anliegen. Heinz Bittel war Ehrendoktor der Universität Lille und der Universität Orléans-Tour. Die Stadt Lille ehrte ihn mit ihrer "Silbermedaille". Sein "anhaltendes Interesse an der französischen Kultur" wurde 1972 durch die Ernennung zum Offizier des Ordre des Palmes Académiques gewürdigt.

Heinz Bittel wurde 1973 zum Ordentlichen Mitglied der Rheinisch-Westfälischen Akademie der Wissenschaften gewählt.




</doc>
<doc id="6009137" url="https://de.wikipedia.org/wiki?curid=6009137" title="Karl Hecht (Physiker)">
Karl Hecht (Physiker)

Karl Hecht (* 17. November 1903 in Hannover; † 24. November 1994) war ein deutscher Physik-Didaktiker.

Hecht wurde 1930 an der Universität Göttingen mit einer Arbeit "Über lichtelektrische Untersuchungen an Alkalihalogenidkristallen" promoviert. Er forschte an den Universitäten Göttingen und Bonn und ging 1934 in die Industrie, wo er Prokurist und Abteilungsleiter bei der Firma Leybold in Köln wurde, die auch physikalische Unterrichtsgeräte herstellte. Er war Gründer des Instituts für Pädagogik der Naturwissenschaften (IPN) an der Universität Kiel, das er 1966 gründete und dessen erster Direktor er bis 1971 war. Außerdem war er Professor in Kiel.

1981 erhielt er den Robert-Wichard-Pohl-Preis.




</doc>
<doc id="6078047" url="https://de.wikipedia.org/wiki?curid=6078047" title="Enrico Fermi Institute">
Enrico Fermi Institute

Das (EFI) ist ein Institut der University of Chicago. Es wurde 1945 unter dem Namen gegründet. Der Physiker Samuel Allison wurde 1946 erster Direktor des Instituts, das am 20. November 1955 umbenannt wurde in . Im Januar 1968 bekam es schließlich seinen heutigen Namen. Forschungsschwerpunkte sind folgende Gebiete:

Bedeutende Forscher am Institut sind oder waren u. a. Herbert L. Anderson, James Cronin, James Hartle, Enrico Fermi, Yōichirō Nambu, Harold C. Urey, Faheem Hussain, Gregor Wentzel.



</doc>
<doc id="6092562" url="https://de.wikipedia.org/wiki?curid=6092562" title="Mario Livio">
Mario Livio

Livio lebte bis zu seinem fünften Lebensjahr bei seinen Großeltern, da seine Eltern aus politischen Gründen Rumänien verlassen mussten. Die vereinte Familie ließ sich in Israel nieder, wo Livio an den drei Kriegen der 1960er bis 1980er Jahre als Feldscher der israelischen Streitkräfte (IDF) teilnahm.

Livios akademische Ausbildung erfolgte in Israel und schloss mit seiner Promotion an der Universität Tel Aviv zur theoretischen Astrophysik ab. Von 1981 bis 1991 lehrte er Physik am Technion in Haifa. Danach ging er in die Vereinigten Staaten und arbeitet seither am Space Telescope Science Institute in Baltimore, Maryland, welches auch das Hubble-Weltraumteleskop betreibt. Livio ist auch Professor in der Abteilung Physik und Astronomie an der Johns Hopkins University in Baltimore.

Livios Forschungstätigkeit konzentrierte sich im 21. Jahrhundert auf Supernova-Explosionen und deren Auswirkungen auf die Ausdehnung des Weltraums. Ferner forschte er über Schwarze Löcher und die Entwicklung von Planetensystemen um neuentstandene Sterne.

Livio ist mit der Mikrobiologin Sofie verheiratet. Das Paar hat drei Kinder.




</doc>
<doc id="6992642" url="https://de.wikipedia.org/wiki?curid=6992642" title="Carlos Frenk">
Carlos Frenk

Carlos Silvestre Frenk (* 27. Oktober 1951) ist ein britisch-mexikanischer Astrophysiker. Er befasst sich mit Computermodellen zur Strukturentstehung im Universum.

Frenk studierte ab 1972 theoretische Physik an der Universität von Mexiko-Stadt und ab 1976 an der Universität Cambridge, wo er 1981 in Astrophysik promoviert wurde. 1981 bis 1983 war er als Post-Doktorand an der University of California, Berkeley, und danach an der University of California, Santa Barbara und der University of Sussex. Ab 1985 war er als Lecturer an der University of Durham, wo er 1991 Reader und 1993 Professor wurde. Seit 2001 ist er dort Ogden Professor of Fundamental Physics und Direktor des Institute for Computational Cosmology (ICC).

Frenk war in den 1980er Jahren an frühen Untersuchungen mit George Efstathiou, Marc Davis und Simon White zu Cold Dark Matter Modellen des frühen Universums beteiligt. 2011 erhielt er mit Marc Davis, George Efstathiou und Simon White dafür den Gruber-Preis für Kosmologie. Er ist an führender Stelle im Virgo Consortium beteiligt, einer europäischen Initiative zu kosmologischen Studien mit Supercomputern, darunter der "Cosmology Machine" in Durham und einem weiteren Schwerpunkt am Max Planck Institut für Astrophysik in Garching.

2004 wurde Frenk Fellow der Royal Society, deren Wolfson Research Merit Award er 2006 erhielt. 2010 erhielt er den George Darwin Preis der Royal Astronomical Society und den Hoyle Preis des Institute of Physics. 2014 wurde er mit der Goldmedaille der Royal Astronomical Society ausgezeichnet, für 2017 wurde ihm der Max-Born-Preis der Deutschen Physikalischen Gesellschaft zugesprochen.




</doc>
<doc id="7321556" url="https://de.wikipedia.org/wiki?curid=7321556" title="Jesse W. Beams">
Jesse W. Beams

Jesse Wakefield Beams (* 25. Dezember 1898 in Belle Plaine, Kansas; † 23. Juli 1977) war ein US-amerikanischer Experimentalphysiker.

Beams war der Sohn von Farmern und wuchs auf einer Farm auf. Er studierte Physik und Mathematik am Fairmount College in Wichita (Bachelor-Abschluss 1921) und an der University of Wisconsin–Madison mit dem Master-Abschluss in Physik 1922. Er lehrte ein Jahr Physik (und Mathematik) am Alabama Polytechnic Institute. Danach ging er an die University of Virginia, wo er 1925 in Physik bei Carroll M. Sparrow promoviert wurde. Gegenstand der Dissertation war der photoelektrische Effekt (Messung der Zeitdauer zwischen Absorption des Photons und Emission des Elektrons), wofür er mit hoher Geschwindigkeit rotierende Spiegel baute zur Erzeugung kurzer Lichtpulse und Schaltkreise für die Messung sehr kurzer Zeitintervalle. 1925/26 war er National Research Fellow. Als Post-Doktorand war er drei Jahre bei Ernest O. Lawrence (damals an der Yale University), mit dem er die Themen seiner Dissertation weiterverfolgte. Auch jetzt konnte er den Zeitabstand beim photoelektrischen Effekt nicht genau bestimmen, er fand aber eine obere Schranke von drei Nanosekunden. Während dieser Zeit war er auch Instructor in Yale. 1928 wurde er Associate Professor und 1930 Professor an der University of Virginia, wo er den Rest seiner Karriere blieb, 1953 zum "Francis H. Smith Professor" für Physik ernannt wurde und 1969 emeritierte, forschte dort aber weiter bis zu seinem Tod 1977. 1948 bis 1962 stand er der Physik-Fakultät vor.

Er war ab 1931 mit Maxine Sutherland Beams verheiratet.

Im Zweiten Weltkrieg arbeitete er im Manhattan Project, wo er seine Ultrazentrifuge zur Urananreicherung einsetzte und deren prinzipielle Funktionsfähigkeit demonstrierte (1941), die aber in den USA im Januar 1944 zugunsten der Diffusionsmethode aufgegeben wurde. Die Gaszentrifugentechnik wurde dann durch Gernot Zippe, Max Steenbeck und sowjetische Wissenschaftler unabhängig zur Reife entwickelt und später in Westeuropa zur Urananreicherung angewandt. Zippe besuchte Beams auch 1958 bis 1960 an der University of Virginia. Beams arbeitete schon seit Anfang der 1930er Jahre an der Zentrifugentechnologie, die er bis zu rund 1,5 Millionen Umdrehungen pro Sekunde entwickelte (Ultrazentrifuge). Er entwickelte eine magnetische Lagerung für die Rotoren (ab etwa 1934) der Ultrazentrifuge, mit frühen Patenten ab 1941. Außerdem waren die Rotoren der Ultrazentrifugen im Hochvakuum.

Die Ultrazentrifugentechnik von Beams wurde seit den 1930er Jahren in der Biologie angewandt, auch Beams selbst wandte sich insbesondere ab den 1960er Jahren Anwendungen der von ihm entwickelten Apparate in der Biologie und Biophysik zu, teilweise in Zusammenarbeit mit dem Biochemie-Professor der Universität von Virginia Donald Kupke. Anwendungen in den Werkstoffwissenschaften untersuchte er schon in den 1930er Jahren und fand z. B., dass dünne metallische Filme viel stabiler waren als z. B. Metallkugeln. Er schrieb auch 1974 den Artikel Zentrifuge in der 15. Auflage der Encyclopedia Britannica.

Er entwickelte auch 1933/34 mit Kollegen an der University of Virginia einen der ersten Linearbeschleuniger für Elektronen, indem sie das Konstruktionsprinzip von Rolf Wideröe weiter verfolgten (unabhängig tat dies ab etwa 1930 William Webster Hansen in Stanford). und einen Apparat zur genaueren Messung der Gravitationskonstante, eine Weiterentwicklung des klassischen Cavendish-Experiments. Er verbesserte damit vor seinem Tod die Genauigkeit der bisherigen Messungen um eine Größenordnung (mit weiterem Potential). Vor seinem Tod entwarf er auch ein Experiment, um Paul Dirac´s Hypothese der kontinuierlichen Erzeugung von Materie zu testen und zum Test einer möglichen Variabilität der Gravitationskonstante (Theorie von Dirac).

Ebenfalls in den 1970er Jahren entwickelte er ein hochgenaues Gerät zur Messung der Dichte und Viskosität von Flüssigkeiten, das auf der Verwendung einer magnetischen Aufhängung eines kleinen Zylinders in einer Flüssigkeit beruht.

1967 erhielt er die National Medal of Science, 1958 den Lewis-Preis der American Philosophical Society, 1956 die John Scott Medal der American Physical Society und 1942 die Howard N. Potts Medal. Außerdem erhielt er den ersten Thomas Jefferson Award der University of Virginia. Er war Mitglied der National Academy of Sciences (1943), der Virginia Academy of Sciences (Präsident 1947), der American Academy of Arts and Sciences (1949), der American Philosophical Society (Vizepräsident 1960 bis 1963) und der American Association for the Advancement of Science (Vizepräsident 1943). Er war mehrfacher Ehrendoktor (College of William and Mary, University of North Carolina, Washington and Lee University, Florida Institute of Technology, Yale). Er war lange im wissenschaftlichen Rat des Oak Ridge Institute of Nuclear Studies (1948 bis 1954 und 1960 bis 1970), 1951 bis 1955 im Leitungsrat des National Research Council, 1954 bis 1960 im Rat der Atomic Energy Commission und 1942 bis 1960 im wissenschaftlichen Beratungsgremium des Aberdeen Proving Ground (Ballistiklabor der US Army). 1971 wurde er Fellow auf Lebenszeit des Franklin Institute.

1958 bis 1959 war er Präsident der American Physical Society. Der 1973 gestiftete Jesse W. Beams Award der American Physical Society ist ihm zu Ehren benannt. Sie wird für physikalische Forschung im Südosten der USA verliehen.




</doc>
<doc id="7341319" url="https://de.wikipedia.org/wiki?curid=7341319" title="Michael R. Douglas">
Michael R. Douglas

Michael R. Douglas (* 19. November 1961 in Baton Rouge) ist ein US-amerikanischer theoretischer Physiker, der sich mit Stringtheorie befasst.

Douglas ist der Sohn des Mathematikprofessors Ronald G. Douglas. Douglas studierte Physik an der Harvard University mit dem Bachelor Abschluss 1983 und wurde 1988 bei John Schwarz am Caltech promoviert. Als Post-Doktorand war er 1988/89 Enrico Fermi Fellow an der University of Chicago und danach bei Daniel Friedan und Stephen Shenker an der Rutgers University am neu gegründeten New High Energy Theory Center (NHETC). 1990 wurde er dort Assistant Professor und 1995 Associate Professor. 1990 war er Gastwissenschaftler an der École normale supérieure und dem Artificial Intelligence Laboratory des MIT. Am MIT war er im Team von Gerald J. Sussman beim Bau eines Spezialcomputers für Rechnungen in der Himmelsmechanik (Digital Orrery). 1997/98 wechselte er für ein Jahr als Professor an das IHES in Frankreich, an dem er seit 1999 Visiting Fellow ist. 1999 kehrte er an die Rutgers University zurück, wo er 2000 Direktor des NHETC wurde. 2008 ging er ans neu gegründete Simons Center for Geometry and Physics der SUNY.

Douglas ist für seine Beteiligung an der Entwicklung der Matrix-Modelle der Superstringtheorie (M-Theorie), für Forschungen zu D-Branen und nichtkommutativer Geometrie in der Stringtheorie bekannt. Außerdem befasst er sich mit der String-Landscape, der Konstruktion von Vakua der Stringtheorie und Ableitung der statistischen Verteilung ihrer beobachtbaren Konsequenzen. Seit seiner Zeit am MIT interessiert er sich auch für Computeranwendungen in der Physik.

1991 war er Sloan Research Fellow und erhielt einen Presidential Young Investigator Award. 2000 erhielt er den Sackler-Preis in theoretischer Physik. 2002 war er Invited Speaker auf dem Internationalen Mathematikerkongress in Peking (Dirichlet branes, homological mirror symmetry and stability).

Er ist seit 1997 verheiratet und hat zwei Kinder. Seine Frau Nina schreibt und illustriert Kinderbücher.



</doc>
